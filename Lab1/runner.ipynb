{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver import Solver\n",
    "from visualize import plot_loss_and_acc, plot_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.4464\t Accuracy 0.1000\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 2.0088\t Accuracy 0.3200\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 1.7461\t Accuracy 0.4400\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 1.3743\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 1.1783\t Accuracy 0.7800\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 1.1380\t Accuracy 0.7800\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 1.1019\t Accuracy 0.7300\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.9523\t Accuracy 0.7500\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.9638\t Accuracy 0.7800\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.9066\t Accuracy 0.7900\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.8500\t Accuracy 0.8100\n",
      "\n",
      "Epoch [0]\t Average training loss 1.2655\t Average training accuracy 0.6725\n",
      "Epoch [0]\t Average validation loss 0.7274\t Average validation accuracy 0.8582\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.7658\t Accuracy 0.8200\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.7599\t Accuracy 0.8100\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.7265\t Accuracy 0.8200\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.7373\t Accuracy 0.8100\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.7084\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.6411\t Accuracy 0.8800\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.7356\t Accuracy 0.7700\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.6436\t Accuracy 0.8700\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.5901\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.6345\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.6117\t Accuracy 0.8500\n",
      "\n",
      "Epoch [1]\t Average training loss 0.6928\t Average training accuracy 0.8360\n",
      "Epoch [1]\t Average validation loss 0.5244\t Average validation accuracy 0.8842\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.5621\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.6101\t Accuracy 0.8300\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.5328\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.7157\t Accuracy 0.8400\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.5683\t Accuracy 0.8300\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.4225\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.6494\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.5741\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.5991\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.5784\t Accuracy 0.8200\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.5466\t Accuracy 0.8800\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5709\t Average training accuracy 0.8577\n",
      "Epoch [2]\t Average validation loss 0.4464\t Average validation accuracy 0.8980\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.5379\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.4533\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.5210\t Accuracy 0.8600\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.5442\t Accuracy 0.8500\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.4301\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.5270\t Accuracy 0.8700\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.4570\t Accuracy 0.8500\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.4481\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.4703\t Accuracy 0.8800\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3957\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.4605\t Accuracy 0.8700\n",
      "\n",
      "Epoch [3]\t Average training loss 0.5127\t Average training accuracy 0.8683\n",
      "Epoch [3]\t Average validation loss 0.4039\t Average validation accuracy 0.9032\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.3849\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.4900\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.2730\t Accuracy 0.9800\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.5385\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.4218\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.4480\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.5681\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.6028\t Accuracy 0.8200\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.4642\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.4494\t Accuracy 0.8600\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.5888\t Accuracy 0.7900\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4775\t Average training accuracy 0.8749\n",
      "Epoch [4]\t Average validation loss 0.3766\t Average validation accuracy 0.9080\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.3581\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.4813\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.4504\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.4534\t Accuracy 0.8600\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.4112\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.4334\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.5115\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.4482\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.5690\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.3590\t Accuracy 0.9500\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.4468\t Accuracy 0.8600\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4532\t Average training accuracy 0.8798\n",
      "Epoch [5]\t Average validation loss 0.3578\t Average validation accuracy 0.9090\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.3617\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.4237\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.3597\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.5373\t Accuracy 0.8400\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.3747\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.2466\t Accuracy 0.9600\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.3761\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.5253\t Accuracy 0.8400\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.3168\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.4646\t Accuracy 0.8700\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.2870\t Accuracy 0.9100\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4353\t Average training accuracy 0.8828\n",
      "Epoch [6]\t Average validation loss 0.3436\t Average validation accuracy 0.9120\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.3158\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.5568\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.2781\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3901\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.4204\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.4928\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.2794\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.3727\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.3732\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.3287\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.5254\t Accuracy 0.8900\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4213\t Average training accuracy 0.8862\n",
      "Epoch [7]\t Average validation loss 0.3326\t Average validation accuracy 0.9142\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.4083\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.4577\t Accuracy 0.8700\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.4527\t Accuracy 0.8600\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.3316\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.3993\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.3610\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.3961\t Accuracy 0.8700\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.4575\t Accuracy 0.8600\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.3847\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.4309\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.4004\t Accuracy 0.8800\n",
      "\n",
      "Epoch [8]\t Average training loss 0.4101\t Average training accuracy 0.8885\n",
      "Epoch [8]\t Average validation loss 0.3236\t Average validation accuracy 0.9170\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.3674\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.4736\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.5874\t Accuracy 0.8200\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.4187\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.3892\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.5328\t Accuracy 0.8400\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.3284\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.3359\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.5082\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.3669\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.4092\t Accuracy 0.8600\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4007\t Average training accuracy 0.8907\n",
      "Epoch [9]\t Average validation loss 0.3163\t Average validation accuracy 0.9176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train without momentum=0\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "loss1, acc1 = runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.8994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.4687\t Accuracy 0.1300\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 1.7571\t Accuracy 0.5300\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 1.3128\t Accuracy 0.7100\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 1.0014\t Accuracy 0.8100\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.9512\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.8584\t Accuracy 0.7900\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.8402\t Accuracy 0.8500\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.6812\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.5892\t Accuracy 0.8500\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.6304\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.5471\t Accuracy 0.9000\n",
      "\n",
      "Epoch [0]\t Average training loss 0.9767\t Average training accuracy 0.7554\n",
      "Epoch [0]\t Average validation loss 0.5189\t Average validation accuracy 0.8922\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.5893\t Accuracy 0.8400\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.5775\t Accuracy 0.8800\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.6426\t Accuracy 0.8400\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.5574\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.5947\t Accuracy 0.8800\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.5945\t Accuracy 0.8100\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.6191\t Accuracy 0.8200\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.4050\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.4535\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.4940\t Accuracy 0.8500\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.4574\t Accuracy 0.9100\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5420\t Average training accuracy 0.8639\n",
      "Epoch [1]\t Average validation loss 0.4021\t Average validation accuracy 0.9050\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.5031\t Accuracy 0.8500\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.5670\t Accuracy 0.8400\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.4693\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.5049\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.5266\t Accuracy 0.8500\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.5452\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.4558\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.4579\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.3372\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.5683\t Accuracy 0.8400\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.5259\t Accuracy 0.9000\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4665\t Average training accuracy 0.8775\n",
      "Epoch [2]\t Average validation loss 0.3575\t Average validation accuracy 0.9108\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.4828\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.3949\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.4885\t Accuracy 0.8700\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.3290\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.4733\t Accuracy 0.8400\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.4452\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.4837\t Accuracy 0.8700\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.3875\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.4432\t Accuracy 0.8500\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3046\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.4586\t Accuracy 0.8600\n",
      "\n",
      "Epoch [3]\t Average training loss 0.4298\t Average training accuracy 0.8844\n",
      "Epoch [3]\t Average validation loss 0.3326\t Average validation accuracy 0.9154\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.4043\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.3548\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.4457\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.2407\t Accuracy 0.9800\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.3925\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.4213\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.3576\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.4836\t Accuracy 0.8700\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.3372\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.3829\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.4131\t Accuracy 0.8900\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4070\t Average training accuracy 0.8893\n",
      "Epoch [4]\t Average validation loss 0.3168\t Average validation accuracy 0.9166\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.3390\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.3382\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.3547\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.5244\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.3618\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.3308\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.4031\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.3845\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.4624\t Accuracy 0.8600\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.3869\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.4421\t Accuracy 0.8900\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3910\t Average training accuracy 0.8924\n",
      "Epoch [5]\t Average validation loss 0.3054\t Average validation accuracy 0.9202\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.4435\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.4410\t Accuracy 0.8700\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.4316\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.5968\t Accuracy 0.8300\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.3676\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.4656\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.4071\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.4093\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.3531\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.3113\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.3882\t Accuracy 0.9000\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3790\t Average training accuracy 0.8954\n",
      "Epoch [6]\t Average validation loss 0.2964\t Average validation accuracy 0.9226\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.3819\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.3666\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.4014\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3776\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.2222\t Accuracy 0.9700\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.3019\t Accuracy 0.9800\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.2796\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.3254\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.4368\t Accuracy 0.8500\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.2437\t Accuracy 0.9400\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.2936\t Accuracy 0.9100\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3695\t Average training accuracy 0.8974\n",
      "Epoch [7]\t Average validation loss 0.2896\t Average validation accuracy 0.9230\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.3836\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.4117\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.2822\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.2862\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.2294\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.2458\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.4047\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.4678\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.3348\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.2730\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.3647\t Accuracy 0.9100\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3618\t Average training accuracy 0.8997\n",
      "Epoch [8]\t Average validation loss 0.2841\t Average validation accuracy 0.9248\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.3945\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.4293\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.3270\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.3445\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.3908\t Accuracy 0.8600\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.5254\t Accuracy 0.8600\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.4146\t Accuracy 0.8600\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.6047\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.3737\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.3651\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.3243\t Accuracy 0.8900\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3553\t Average training accuracy 0.9010\n",
      "Epoch [9]\t Average validation loss 0.2798\t Average validation accuracy 0.9248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train with momentum=0.5\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.5,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "loss2, acc2 = runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.9113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.5328\t Accuracy 0.1000\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 0.8030\t Accuracy 0.7600\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.5996\t Accuracy 0.8600\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.6594\t Accuracy 0.8200\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.5828\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.3958\t Accuracy 0.8900\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.5128\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.5197\t Accuracy 0.8700\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.4862\t Accuracy 0.9100\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.4871\t Accuracy 0.8200\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.4392\t Accuracy 0.8900\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5828\t Average training accuracy 0.8404\n",
      "Epoch [0]\t Average validation loss 0.3166\t Average validation accuracy 0.9170\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.3695\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.3854\t Accuracy 0.8800\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.3094\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.4050\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.3183\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.3209\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.4807\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.5149\t Accuracy 0.8700\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.2681\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.4055\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.3757\t Accuracy 0.8800\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3749\t Average training accuracy 0.8960\n",
      "Epoch [1]\t Average validation loss 0.2789\t Average validation accuracy 0.9248\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.2927\t Accuracy 0.9300\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.3392\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.3479\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.4047\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.4138\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.2913\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.3484\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.2790\t Accuracy 0.9400\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.3418\t Accuracy 0.9400\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.2932\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.4549\t Accuracy 0.8700\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3440\t Average training accuracy 0.9043\n",
      "Epoch [2]\t Average validation loss 0.2642\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.2345\t Accuracy 0.9400\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.2854\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.3417\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.3124\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.3734\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.2750\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.2655\t Accuracy 0.9400\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.2695\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.4060\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3767\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.2799\t Accuracy 0.9000\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3280\t Average training accuracy 0.9083\n",
      "Epoch [3]\t Average validation loss 0.2558\t Average validation accuracy 0.9296\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.2647\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.3842\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.3334\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.3879\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.2261\t Accuracy 0.9500\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.3853\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.5278\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.3480\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.4804\t Accuracy 0.8500\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.2584\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.3906\t Accuracy 0.9000\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3178\t Average training accuracy 0.9110\n",
      "Epoch [4]\t Average validation loss 0.2483\t Average validation accuracy 0.9314\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.4209\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.3749\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.2239\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.2993\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.2498\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.4499\t Accuracy 0.8600\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.3153\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.2627\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.2971\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.5508\t Accuracy 0.8600\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.4872\t Accuracy 0.8600\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3106\t Average training accuracy 0.9133\n",
      "Epoch [5]\t Average validation loss 0.2447\t Average validation accuracy 0.9302\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.2575\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.2175\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.3470\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.3513\t Accuracy 0.8800\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.3285\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.4322\t Accuracy 0.8400\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.1279\t Accuracy 0.9700\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.1608\t Accuracy 0.9700\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.1913\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.2619\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.5073\t Accuracy 0.9000\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3048\t Average training accuracy 0.9148\n",
      "Epoch [6]\t Average validation loss 0.2407\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.3423\t Accuracy 0.8800\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.3831\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.2084\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3571\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.2476\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.2046\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.3751\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.3655\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.2492\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.3987\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.3481\t Accuracy 0.8600\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3002\t Average training accuracy 0.9160\n",
      "Epoch [7]\t Average validation loss 0.2396\t Average validation accuracy 0.9334\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.2150\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.2394\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.3419\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.2171\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.2007\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.2642\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.3165\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.4005\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.2722\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.2831\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.3537\t Accuracy 0.8800\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2964\t Average training accuracy 0.9175\n",
      "Epoch [8]\t Average validation loss 0.2376\t Average validation accuracy 0.9334\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.2278\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.2729\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.2847\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.2309\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.4335\t Accuracy 0.8700\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.4058\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.3497\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.2304\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.4018\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.2938\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.1639\t Accuracy 0.9600\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2934\t Average training accuracy 0.9182\n",
      "Epoch [9]\t Average validation loss 0.2351\t Average validation accuracy 0.9352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train with momentum=0.9\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "loss3, acc3 = runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.9204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.4259\t Accuracy 0.0600\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 0.5580\t Accuracy 0.7900\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.6097\t Accuracy 0.8200\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.6335\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.6830\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.5216\t Accuracy 0.8600\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.2592\t Accuracy 0.9100\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.5967\t Accuracy 0.9100\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.3513\t Accuracy 0.8600\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.3132\t Accuracy 0.8700\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.3780\t Accuracy 0.8900\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5273\t Average training accuracy 0.8624\n",
      "Epoch [0]\t Average validation loss 0.2730\t Average validation accuracy 0.9300\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.2107\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.3419\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.5681\t Accuracy 0.8800\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.3027\t Accuracy 0.9400\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.2167\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.1791\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.1284\t Accuracy 0.9400\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.2328\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.2944\t Accuracy 0.9200\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.2002\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.4657\t Accuracy 0.9200\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3228\t Average training accuracy 0.9116\n",
      "Epoch [1]\t Average validation loss 0.2334\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.2766\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.4385\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.2341\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.2117\t Accuracy 0.9300\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.5108\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.2740\t Accuracy 0.9400\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.3494\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.2816\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.3618\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.1241\t Accuracy 0.9700\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.1951\t Accuracy 0.9600\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2929\t Average training accuracy 0.9184\n",
      "Epoch [2]\t Average validation loss 0.2310\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.5399\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.2571\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.2545\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.2725\t Accuracy 0.9500\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.3115\t Accuracy 0.8700\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.2582\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.2563\t Accuracy 0.9600\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.2889\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.4026\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.2462\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.3403\t Accuracy 0.8900\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2864\t Average training accuracy 0.9200\n",
      "Epoch [3]\t Average validation loss 0.2327\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.3369\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.3555\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.2619\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.2309\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.4022\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.2623\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.2105\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.2000\t Accuracy 0.9500\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.4467\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.3945\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.2781\t Accuracy 0.9300\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2805\t Average training accuracy 0.9211\n",
      "Epoch [4]\t Average validation loss 0.2337\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.4644\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.1887\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.2436\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.2243\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.1610\t Accuracy 0.9500\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.4012\t Accuracy 0.8900\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.3186\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.1950\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.4261\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.2784\t Accuracy 0.8900\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.2550\t Accuracy 0.9400\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2755\t Average training accuracy 0.9235\n",
      "Epoch [5]\t Average validation loss 0.2260\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.1798\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.3381\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.3812\t Accuracy 0.8800\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.2486\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.2861\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.3218\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.2011\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.4604\t Accuracy 0.8600\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.3560\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.2430\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.2121\t Accuracy 0.9300\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2720\t Average training accuracy 0.9235\n",
      "Epoch [6]\t Average validation loss 0.2325\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.4374\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.2969\t Accuracy 0.8800\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.2337\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3217\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.1191\t Accuracy 0.9800\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.1086\t Accuracy 0.9800\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.2734\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.3426\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.3082\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.1882\t Accuracy 0.9600\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.2931\t Accuracy 0.9300\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2739\t Average training accuracy 0.9232\n",
      "Epoch [7]\t Average validation loss 0.2269\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.4350\t Accuracy 0.8700\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.1833\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.1754\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.2016\t Accuracy 0.9600\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.2360\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.2112\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.2032\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.4360\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.1235\t Accuracy 0.9800\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.2456\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.1336\t Accuracy 0.9700\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2703\t Average training accuracy 0.9235\n",
      "Epoch [8]\t Average validation loss 0.2230\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.2407\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.4394\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.2541\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.3448\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.1543\t Accuracy 0.9600\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.3042\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.1521\t Accuracy 0.9800\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.1704\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.2273\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.2156\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.1815\t Accuracy 0.9600\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2672\t Average training accuracy 0.9256\n",
      "Epoch [9]\t Average validation loss 0.2268\t Average validation accuracy 0.9370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train with momentum=0.99\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': 10,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.99,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "loss4, acc4 = runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.9213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = runner.test()\n",
    "print('Final test accuracy {:.4f}\\n'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({\n",
    "    \"momentum=0\": [loss1, acc1],\n",
    "    \"momentum=0.5\": [loss2, acc2],\n",
    "    \"momentum=0.9\": [loss3, acc3],\n",
    "    \"momentum=0.99\": [loss4, acc4]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.5579\t Accuracy 0.1000\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 1.1726\t Accuracy 0.6900\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.7895\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.6709\t Accuracy 0.8500\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.7139\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.5776\t Accuracy 0.8600\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.5424\t Accuracy 0.8700\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.4807\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.4682\t Accuracy 0.8900\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.4542\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.3774\t Accuracy 0.9100\n",
      "\n",
      "Epoch [0]\t Average training loss 0.7131\t Average training accuracy 0.8133\n",
      "Epoch [0]\t Average validation loss 0.3749\t Average validation accuracy 0.9088\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.4212\t Accuracy 0.9200\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.4791\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.4663\t Accuracy 0.8700\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.4684\t Accuracy 0.8300\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.3407\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.4149\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.3961\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.3702\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.5416\t Accuracy 0.8400\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.3943\t Accuracy 0.8800\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.3511\t Accuracy 0.8900\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4267\t Average training accuracy 0.8854\n",
      "Epoch [1]\t Average validation loss 0.3182\t Average validation accuracy 0.9160\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.5008\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.3884\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.3449\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.3187\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.4904\t Accuracy 0.8500\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.3676\t Accuracy 0.9300\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.5137\t Accuracy 0.8400\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.2818\t Accuracy 0.9400\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.3232\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.4414\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.4032\t Accuracy 0.8800\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3833\t Average training accuracy 0.8953\n",
      "Epoch [2]\t Average validation loss 0.2932\t Average validation accuracy 0.9232\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.4203\t Accuracy 0.8400\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.4477\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.2873\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.3261\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.3944\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.4843\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.3661\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.4325\t Accuracy 0.8600\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.2484\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3810\t Accuracy 0.8600\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.3431\t Accuracy 0.9100\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3613\t Average training accuracy 0.9000\n",
      "Epoch [3]\t Average validation loss 0.2799\t Average validation accuracy 0.9248\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.2099\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.5113\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.3994\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.2830\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.4410\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.2611\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.3036\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.2317\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.2769\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.2661\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.3944\t Accuracy 0.8700\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3472\t Average training accuracy 0.9040\n",
      "Epoch [4]\t Average validation loss 0.2705\t Average validation accuracy 0.9282\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.2933\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.3344\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.1999\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.2594\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.3935\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.4182\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.3087\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.1899\t Accuracy 0.9600\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.2570\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.2165\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.3525\t Accuracy 0.9100\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3371\t Average training accuracy 0.9057\n",
      "Epoch [5]\t Average validation loss 0.2644\t Average validation accuracy 0.9284\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.3646\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.2300\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.3070\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.2779\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.3337\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.2936\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.2655\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.4234\t Accuracy 0.8400\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.3214\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.2552\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.4411\t Accuracy 0.8900\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3294\t Average training accuracy 0.9080\n",
      "Epoch [6]\t Average validation loss 0.2592\t Average validation accuracy 0.9288\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.2640\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.2210\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.4788\t Accuracy 0.8800\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3622\t Accuracy 0.8800\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.4363\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.3626\t Accuracy 0.9400\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.1899\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.2583\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.3586\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.2479\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.3780\t Accuracy 0.9100\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3232\t Average training accuracy 0.9099\n",
      "Epoch [7]\t Average validation loss 0.2558\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.4196\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.3847\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.3327\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.2644\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.3049\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.2032\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.2036\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.2247\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.3480\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.2679\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.1982\t Accuracy 0.9400\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3183\t Average training accuracy 0.9111\n",
      "Epoch [8]\t Average validation loss 0.2524\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.2200\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.2754\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.2350\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.3515\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.3839\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.3180\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.2612\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.5106\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.4144\t Accuracy 0.8300\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.1979\t Accuracy 0.9700\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.2335\t Accuracy 0.9300\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3140\t Average training accuracy 0.9125\n",
      "Epoch [9]\t Average validation loss 0.2492\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.4618\t Accuracy 0.1400\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 0.7801\t Accuracy 0.8100\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.6113\t Accuracy 0.8300\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.5904\t Accuracy 0.8600\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.5766\t Accuracy 0.8200\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.3889\t Accuracy 0.9200\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.3931\t Accuracy 0.8700\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.4609\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.2681\t Accuracy 0.9500\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.4718\t Accuracy 0.8700\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.4012\t Accuracy 0.8900\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5825\t Average training accuracy 0.8415\n",
      "Epoch [0]\t Average validation loss 0.3132\t Average validation accuracy 0.9190\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.2963\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.3663\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.5282\t Accuracy 0.8700\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.3525\t Accuracy 0.9200\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.3974\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.3491\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.3106\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.3173\t Accuracy 0.8800\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.5430\t Accuracy 0.8800\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.3243\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.4580\t Accuracy 0.8300\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3729\t Average training accuracy 0.8966\n",
      "Epoch [1]\t Average validation loss 0.2805\t Average validation accuracy 0.9254\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.3672\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.2718\t Accuracy 0.9400\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.3198\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.2249\t Accuracy 0.9600\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.3089\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.4232\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.2213\t Accuracy 0.9300\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.4735\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.3703\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.4386\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.2390\t Accuracy 0.9300\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3429\t Average training accuracy 0.9041\n",
      "Epoch [2]\t Average validation loss 0.2634\t Average validation accuracy 0.9288\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.3113\t Accuracy 0.8800\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.3586\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.3657\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.3390\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.3673\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.2135\t Accuracy 0.9500\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.2336\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.3943\t Accuracy 0.8700\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.2873\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3889\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.2623\t Accuracy 0.9300\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3271\t Average training accuracy 0.9085\n",
      "Epoch [3]\t Average validation loss 0.2537\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.3179\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.2024\t Accuracy 0.9500\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.5350\t Accuracy 0.8600\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.3713\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.3545\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.4584\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.3232\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.2630\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.3436\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.2138\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.3388\t Accuracy 0.9300\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3167\t Average training accuracy 0.9122\n",
      "Epoch [4]\t Average validation loss 0.2478\t Average validation accuracy 0.9310\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.2301\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.3879\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.3401\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.4833\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.3012\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.3308\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.3211\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.2592\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.4436\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.2804\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.3341\t Accuracy 0.8800\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3095\t Average training accuracy 0.9135\n",
      "Epoch [5]\t Average validation loss 0.2435\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.2589\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.2553\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.2626\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.2997\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.2909\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.2781\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.2416\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.1942\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.2314\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.3513\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.3660\t Accuracy 0.9100\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3039\t Average training accuracy 0.9149\n",
      "Epoch [6]\t Average validation loss 0.2407\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.4656\t Accuracy 0.8600\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.4069\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.2896\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3079\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.3192\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.4036\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.2258\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.2982\t Accuracy 0.9400\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.3221\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.4695\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.3237\t Accuracy 0.9000\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2992\t Average training accuracy 0.9169\n",
      "Epoch [7]\t Average validation loss 0.2377\t Average validation accuracy 0.9336\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.4051\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.2457\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.4830\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.3199\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.2551\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.2804\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.2533\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.4024\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.2910\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.1989\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.4521\t Accuracy 0.8400\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2955\t Average training accuracy 0.9173\n",
      "Epoch [8]\t Average validation loss 0.2353\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.2066\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.4659\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.2802\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.3038\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.3113\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.2056\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.2424\t Accuracy 0.9600\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.2200\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.3249\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.2531\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.2878\t Accuracy 0.9200\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2925\t Average training accuracy 0.9185\n",
      "Epoch [9]\t Average validation loss 0.2339\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.6678\t Accuracy 0.0200\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 0.5797\t Accuracy 0.8500\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.3188\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.3122\t Accuracy 0.9300\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.4164\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.3647\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.2861\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.3153\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.2888\t Accuracy 0.9200\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.3144\t Accuracy 0.9200\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.3706\t Accuracy 0.9200\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4086\t Average training accuracy 0.8815\n",
      "Epoch [0]\t Average validation loss 0.2496\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.2898\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.2998\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.4261\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.2427\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.4467\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.3348\t Accuracy 0.9200\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.1694\t Accuracy 0.9500\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.4462\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.1433\t Accuracy 0.9500\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.2377\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.1843\t Accuracy 0.9500\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3117\t Average training accuracy 0.9129\n",
      "Epoch [1]\t Average validation loss 0.2364\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.3698\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.2585\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.2292\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.2656\t Accuracy 0.9500\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.2467\t Accuracy 0.9400\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.3275\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.2440\t Accuracy 0.9400\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.1883\t Accuracy 0.9600\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.4785\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.4602\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.3277\t Accuracy 0.9000\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2966\t Average training accuracy 0.9166\n",
      "Epoch [2]\t Average validation loss 0.2323\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.1808\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.4106\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.2888\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.4018\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.1318\t Accuracy 0.9800\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.3002\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.2607\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.2953\t Accuracy 0.9400\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.2233\t Accuracy 0.9400\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3691\t Accuracy 0.8800\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.2692\t Accuracy 0.9200\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2883\t Average training accuracy 0.9191\n",
      "Epoch [3]\t Average validation loss 0.2295\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.3218\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.3654\t Accuracy 0.8600\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.3465\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.1676\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.3793\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.2573\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.3733\t Accuracy 0.8700\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.2923\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.3742\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.2754\t Accuracy 0.9500\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.3757\t Accuracy 0.8800\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2830\t Average training accuracy 0.9210\n",
      "Epoch [4]\t Average validation loss 0.2282\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.3316\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.1927\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.2029\t Accuracy 0.9700\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.2508\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.1849\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.2673\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.0899\t Accuracy 0.9800\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.1540\t Accuracy 0.9500\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.4523\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.3880\t Accuracy 0.8900\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.2789\t Accuracy 0.9100\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2788\t Average training accuracy 0.9221\n",
      "Epoch [5]\t Average validation loss 0.2326\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.1712\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.2863\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.1779\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.2687\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.1883\t Accuracy 0.9600\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.2261\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.3789\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.2932\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.3592\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.2313\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.1429\t Accuracy 0.9700\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2760\t Average training accuracy 0.9234\n",
      "Epoch [6]\t Average validation loss 0.2224\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.2378\t Accuracy 0.9600\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.3738\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.3263\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3027\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.4250\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.1350\t Accuracy 0.9700\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.2292\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.2485\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.3537\t Accuracy 0.8500\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.3018\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.3245\t Accuracy 0.9000\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2737\t Average training accuracy 0.9231\n",
      "Epoch [7]\t Average validation loss 0.2245\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.1476\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.2169\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.2433\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.2515\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.2667\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.2344\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.3044\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.3505\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.1756\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.2333\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.1696\t Accuracy 0.9600\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2709\t Average training accuracy 0.9237\n",
      "Epoch [8]\t Average validation loss 0.2213\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.3180\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.1593\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.2441\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.2527\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.2972\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.3047\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.3255\t Accuracy 0.9500\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.2743\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.3476\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.3362\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.3793\t Accuracy 0.8900\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2682\t Average training accuracy 0.9255\n",
      "Epoch [9]\t Average validation loss 0.2261\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.4373\t Accuracy 0.1300\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 0.4542\t Accuracy 0.8600\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.5202\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.4030\t Accuracy 0.8900\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.5283\t Accuracy 0.8300\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.3688\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.4605\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.3339\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.4042\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.2303\t Accuracy 0.9400\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.3536\t Accuracy 0.8900\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3806\t Average training accuracy 0.8897\n",
      "Epoch [0]\t Average validation loss 0.2494\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.2827\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.2096\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.3174\t Accuracy 0.9200\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.2033\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.3308\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.3541\t Accuracy 0.8700\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.4203\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.2029\t Accuracy 0.9500\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.2116\t Accuracy 0.9500\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.2503\t Accuracy 0.9500\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.3082\t Accuracy 0.9000\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3061\t Average training accuracy 0.9136\n",
      "Epoch [1]\t Average validation loss 0.2366\t Average validation accuracy 0.9332\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.1586\t Accuracy 0.9300\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.1932\t Accuracy 0.9300\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.2736\t Accuracy 0.9500\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.3296\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.3069\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.2107\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.5112\t Accuracy 0.9300\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.3781\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.2627\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.3381\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.1816\t Accuracy 0.9400\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2953\t Average training accuracy 0.9166\n",
      "Epoch [2]\t Average validation loss 0.2336\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.2400\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.3797\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.2777\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.1907\t Accuracy 0.9400\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.3899\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.1834\t Accuracy 0.9600\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.1373\t Accuracy 0.9600\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.1973\t Accuracy 0.9600\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.2072\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.1948\t Accuracy 0.9500\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.4109\t Accuracy 0.8900\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2895\t Average training accuracy 0.9182\n",
      "Epoch [3]\t Average validation loss 0.2390\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.3087\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.4902\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.2001\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.2110\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.2985\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.2432\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.2442\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.2672\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.3234\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.2113\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.3815\t Accuracy 0.8400\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2836\t Average training accuracy 0.9206\n",
      "Epoch [4]\t Average validation loss 0.2270\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.2977\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.2368\t Accuracy 0.9600\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.4603\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.3350\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.2976\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.1703\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.2830\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.4416\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.2504\t Accuracy 0.9500\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.3400\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.1699\t Accuracy 0.9400\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2782\t Average training accuracy 0.9216\n",
      "Epoch [5]\t Average validation loss 0.2283\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.2073\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.1475\t Accuracy 0.9700\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.3737\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.1789\t Accuracy 0.9700\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.3260\t Accuracy 0.9400\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.1459\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.3107\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.3611\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.3728\t Accuracy 0.8800\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.2344\t Accuracy 0.9800\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.1520\t Accuracy 0.9700\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2762\t Average training accuracy 0.9218\n",
      "Epoch [6]\t Average validation loss 0.2300\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.3756\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.3666\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.1660\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3795\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.4968\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.2393\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.1163\t Accuracy 0.9600\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.2852\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.2852\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.3063\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.3264\t Accuracy 0.9100\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2733\t Average training accuracy 0.9238\n",
      "Epoch [7]\t Average validation loss 0.2280\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.2009\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.3410\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.4107\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.1897\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.1583\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.2299\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.2258\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.2497\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.1983\t Accuracy 0.9500\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.3356\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.5523\t Accuracy 0.8700\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2720\t Average training accuracy 0.9235\n",
      "Epoch [8]\t Average validation loss 0.2387\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.2594\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.2873\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.3062\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.1574\t Accuracy 0.9500\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.4764\t Accuracy 0.8200\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.2401\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.2474\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.1960\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.3537\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.1605\t Accuracy 0.9500\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.4202\t Accuracy 0.9000\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2710\t Average training accuracy 0.9234\n",
      "Epoch [9]\t Average validation loss 0.2273\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.4048\t Accuracy 0.1000\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 0.2281\t Accuracy 0.9100\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.6045\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.5934\t Accuracy 0.8500\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.2372\t Accuracy 0.9100\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.6960\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.9688\t Accuracy 0.8300\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.4339\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.4981\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.4104\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.4359\t Accuracy 0.9000\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4852\t Average training accuracy 0.8758\n",
      "Epoch [0]\t Average validation loss 0.3633\t Average validation accuracy 0.9028\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.3435\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.5572\t Accuracy 0.8200\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.2195\t Accuracy 0.9500\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.3133\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.2783\t Accuracy 0.9500\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.3358\t Accuracy 0.8700\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.3830\t Accuracy 0.9400\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.3062\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.5162\t Accuracy 0.8500\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.5638\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.3405\t Accuracy 0.9300\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4361\t Average training accuracy 0.8924\n",
      "Epoch [1]\t Average validation loss 0.3665\t Average validation accuracy 0.9054\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.4014\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.3422\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.1620\t Accuracy 0.9700\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.4945\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.4846\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.2118\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.3601\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.3500\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.6527\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.4877\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.1940\t Accuracy 0.9100\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4151\t Average training accuracy 0.8977\n",
      "Epoch [2]\t Average validation loss 0.3086\t Average validation accuracy 0.9252\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.2068\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.4875\t Accuracy 0.8200\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.2311\t Accuracy 0.9400\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.8795\t Accuracy 0.8700\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.5307\t Accuracy 0.8800\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.1144\t Accuracy 0.9600\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.4111\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.3400\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.2186\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.2244\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.4634\t Accuracy 0.8900\n",
      "\n",
      "Epoch [3]\t Average training loss 0.4301\t Average training accuracy 0.8957\n",
      "Epoch [3]\t Average validation loss 0.4000\t Average validation accuracy 0.8992\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.4332\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.4593\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.5307\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.6896\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.2582\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.4641\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.2016\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.7010\t Accuracy 0.8600\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.5612\t Accuracy 0.8500\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.3731\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.4462\t Accuracy 0.9400\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4274\t Average training accuracy 0.8969\n",
      "Epoch [4]\t Average validation loss 0.4704\t Average validation accuracy 0.8902\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.4783\t Accuracy 0.8900\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.4903\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.2290\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.4561\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.3736\t Accuracy 0.9600\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.6614\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.3114\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.3604\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.5312\t Accuracy 0.8600\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.3018\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.4341\t Accuracy 0.8700\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4194\t Average training accuracy 0.8989\n",
      "Epoch [5]\t Average validation loss 0.3339\t Average validation accuracy 0.9228\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.2907\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.3151\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.3990\t Accuracy 0.8600\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.4615\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.2954\t Accuracy 0.8800\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.3227\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.2443\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.2896\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.4820\t Accuracy 0.8500\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.4214\t Accuracy 0.8800\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.2952\t Accuracy 0.9200\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4183\t Average training accuracy 0.8990\n",
      "Epoch [6]\t Average validation loss 0.3586\t Average validation accuracy 0.9128\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.4946\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.3329\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.3616\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3266\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.4195\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.3145\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.4534\t Accuracy 0.8500\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.3566\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.3786\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.4638\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.3867\t Accuracy 0.9200\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4039\t Average training accuracy 0.9024\n",
      "Epoch [7]\t Average validation loss 0.3648\t Average validation accuracy 0.9152\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.3724\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.1803\t Accuracy 0.9600\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.5627\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.5067\t Accuracy 0.8700\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.3885\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.4149\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.1964\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.2076\t Accuracy 0.9600\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.2077\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.3935\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.3165\t Accuracy 0.9000\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3981\t Average training accuracy 0.9028\n",
      "Epoch [8]\t Average validation loss 0.3954\t Average validation accuracy 0.9080\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.6857\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.4328\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.3063\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.5142\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.3019\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.3565\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.4088\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.4450\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.3693\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.1774\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.1659\t Accuracy 0.9400\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4005\t Average training accuracy 0.9017\n",
      "Epoch [9]\t Average validation loss 0.3501\t Average validation accuracy 0.9168\n",
      "\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.5208\t Accuracy 0.1600\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 1.5397\t Accuracy 0.8100\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.3914\t Accuracy 0.9400\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.6558\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.6987\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 1.0264\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.6291\t Accuracy 0.8900\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.8768\t Accuracy 0.8400\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.5933\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.5706\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.5988\t Accuracy 0.8700\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8478\t Average training accuracy 0.8588\n",
      "Epoch [0]\t Average validation loss 0.5139\t Average validation accuracy 0.9148\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.9285\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.7886\t Accuracy 0.8500\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.6340\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.8526\t Accuracy 0.8600\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.7726\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.7173\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 1.2960\t Accuracy 0.8500\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.8806\t Accuracy 0.8500\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.6207\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 1.5422\t Accuracy 0.8200\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.5357\t Accuracy 0.8700\n",
      "\n",
      "Epoch [1]\t Average training loss 0.7813\t Average training accuracy 0.8778\n",
      "Epoch [1]\t Average validation loss 0.6845\t Average validation accuracy 0.8974\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 1.0326\t Accuracy 0.8800\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.8906\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.7818\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.6507\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 1.2380\t Accuracy 0.8500\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.2458\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 1.0797\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.5457\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.8422\t Accuracy 0.8200\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.4650\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 1.1399\t Accuracy 0.8500\n",
      "\n",
      "Epoch [2]\t Average training loss 0.7766\t Average training accuracy 0.8807\n",
      "Epoch [2]\t Average validation loss 0.5778\t Average validation accuracy 0.9100\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 1.2244\t Accuracy 0.8400\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.2307\t Accuracy 0.9500\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 1.0866\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.4170\t Accuracy 0.8900\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.5369\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.4107\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 2.5522\t Accuracy 0.7400\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.4298\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.6761\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.7898\t Accuracy 0.8700\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 1.2144\t Accuracy 0.8600\n",
      "\n",
      "Epoch [3]\t Average training loss 0.7606\t Average training accuracy 0.8829\n",
      "Epoch [3]\t Average validation loss 0.7007\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.9216\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 1.1742\t Accuracy 0.8200\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.4755\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 1.1313\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.6237\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.5421\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.2984\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.3928\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.5828\t Accuracy 0.8600\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.3314\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.7933\t Accuracy 0.8800\n",
      "\n",
      "Epoch [4]\t Average training loss 0.7352\t Average training accuracy 0.8875\n",
      "Epoch [4]\t Average validation loss 0.5634\t Average validation accuracy 0.9100\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.4195\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.9297\t Accuracy 0.8500\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.7447\t Accuracy 0.8700\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.7042\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.2847\t Accuracy 0.9700\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 1.1414\t Accuracy 0.8800\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.3507\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.4724\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.7171\t Accuracy 0.8900\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 1.1517\t Accuracy 0.8600\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.6327\t Accuracy 0.8900\n",
      "\n",
      "Epoch [5]\t Average training loss 0.7191\t Average training accuracy 0.8869\n",
      "Epoch [5]\t Average validation loss 0.7337\t Average validation accuracy 0.8778\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.7919\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.6973\t Accuracy 0.8600\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.4234\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.2426\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.3646\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 1.0543\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.4390\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.3695\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.6360\t Accuracy 0.8900\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.8635\t Accuracy 0.8500\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.7640\t Accuracy 0.8800\n",
      "\n",
      "Epoch [6]\t Average training loss 0.7374\t Average training accuracy 0.8880\n",
      "Epoch [6]\t Average validation loss 0.6182\t Average validation accuracy 0.9138\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.3804\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.5018\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.4383\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 1.1437\t Accuracy 0.8700\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 1.0762\t Accuracy 0.8400\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 1.0193\t Accuracy 0.8200\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.9098\t Accuracy 0.8700\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 1.4039\t Accuracy 0.8400\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.8480\t Accuracy 0.9700\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.6659\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.3636\t Accuracy 0.9000\n",
      "\n",
      "Epoch [7]\t Average training loss 0.6897\t Average training accuracy 0.8902\n",
      "Epoch [7]\t Average validation loss 0.5294\t Average validation accuracy 0.9212\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 1.0360\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 1.1423\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.4603\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.5859\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.8803\t Accuracy 0.9300\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 1.2848\t Accuracy 0.8200\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.6990\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.3141\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.6901\t Accuracy 0.8500\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.9212\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.6697\t Accuracy 0.8900\n",
      "\n",
      "Epoch [8]\t Average training loss 0.7393\t Average training accuracy 0.8872\n",
      "Epoch [8]\t Average validation loss 0.5932\t Average validation accuracy 0.9130\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.3058\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.4175\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.9108\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.3819\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.9093\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.3203\t Accuracy 0.9400\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 1.0085\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.4845\t Accuracy 0.9200\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.8216\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 1.0415\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.3954\t Accuracy 0.9000\n",
      "\n",
      "Epoch [9]\t Average training loss 0.7304\t Average training accuracy 0.8891\n",
      "Epoch [9]\t Average validation loss 0.6337\t Average validation accuracy 0.9142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (other) hyperparameter tuning\n",
    "\n",
    "# train with different learning_rate\n",
    "learning_rates = [0.005, 0.01, 0.05, 0.1, 0.5, 1.]\n",
    "loss5, acc5 = [], []\n",
    "for rate in learning_rates:\n",
    "    cfg = {\n",
    "        'data_root': 'data',\n",
    "        'max_epoch': 10,\n",
    "        'batch_size': 100,\n",
    "        'learning_rate': rate,\n",
    "        'momentum': 0.9,\n",
    "        'display_freq': 50,\n",
    "    }\n",
    "\n",
    "    runner = Solver(cfg)\n",
    "    loss, acc = runner.train()\n",
    "    loss5.append(loss)\n",
    "    acc5.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({\n",
    "    \"learning_rate=0.005\": [loss5[0], acc5[0]],\n",
    "    \"learning_rate=0.01\": [loss5[1], acc5[1]],\n",
    "    \"learning_rate=0.05\": [loss5[2], acc5[2]],\n",
    "    \"learning_rate=0.1\": [loss5[3], acc5[3]],\n",
    "    \"learning_rate=0.5\": [loss5[4], acc5[4]],\n",
    "    \"learning_rate=1.0\": [loss5[5], acc5[5]],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][5500]\t Training Loss 2.5482\t Accuracy 0.1000\n",
      "Epoch [0][10]\t Batch [50][5500]\t Training Loss 1.2813\t Accuracy 0.5000\n",
      "Epoch [0][10]\t Batch [100][5500]\t Training Loss 0.7397\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [150][5500]\t Training Loss 0.5451\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [200][5500]\t Training Loss 0.3656\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [250][5500]\t Training Loss 0.7104\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [300][5500]\t Training Loss 0.6626\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [350][5500]\t Training Loss 0.3449\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [400][5500]\t Training Loss 0.3389\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [450][5500]\t Training Loss 0.4632\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [500][5500]\t Training Loss 0.2980\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [550][5500]\t Training Loss 0.2259\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [600][5500]\t Training Loss 0.6909\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [650][5500]\t Training Loss 0.6022\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [700][5500]\t Training Loss 0.3992\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [750][5500]\t Training Loss 0.2952\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [800][5500]\t Training Loss 0.3739\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [850][5500]\t Training Loss 0.1702\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [900][5500]\t Training Loss 0.1796\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [950][5500]\t Training Loss 0.0989\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [1000][5500]\t Training Loss 0.3619\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [1050][5500]\t Training Loss 0.1964\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [1100][5500]\t Training Loss 0.8533\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [1150][5500]\t Training Loss 0.6032\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [1200][5500]\t Training Loss 1.1966\t Accuracy 0.6000\n",
      "Epoch [0][10]\t Batch [1250][5500]\t Training Loss 0.3774\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [1300][5500]\t Training Loss 0.2033\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [1350][5500]\t Training Loss 0.1648\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [1400][5500]\t Training Loss 0.5028\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [1450][5500]\t Training Loss 1.4712\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [1500][5500]\t Training Loss 1.0163\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [1550][5500]\t Training Loss 0.6559\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [1600][5500]\t Training Loss 0.6179\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [1650][5500]\t Training Loss 0.1856\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [1700][5500]\t Training Loss 0.3767\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [1750][5500]\t Training Loss 0.5371\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [1800][5500]\t Training Loss 0.3128\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [1850][5500]\t Training Loss 1.1647\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [1900][5500]\t Training Loss 0.1465\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [1950][5500]\t Training Loss 0.0687\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [2000][5500]\t Training Loss 0.2632\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [2050][5500]\t Training Loss 0.1636\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [2100][5500]\t Training Loss 0.1090\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [2150][5500]\t Training Loss 0.1403\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [2200][5500]\t Training Loss 0.1158\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [2250][5500]\t Training Loss 0.6336\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [2300][5500]\t Training Loss 0.2556\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [2350][5500]\t Training Loss 0.5605\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [2400][5500]\t Training Loss 0.2155\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [2450][5500]\t Training Loss 0.0691\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [2500][5500]\t Training Loss 0.0246\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [2550][5500]\t Training Loss 0.1290\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [2600][5500]\t Training Loss 1.4064\t Accuracy 0.6000\n",
      "Epoch [0][10]\t Batch [2650][5500]\t Training Loss 0.4148\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [2700][5500]\t Training Loss 0.8433\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [2750][5500]\t Training Loss 0.0718\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [2800][5500]\t Training Loss 0.4711\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [2850][5500]\t Training Loss 0.0433\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [2900][5500]\t Training Loss 0.5119\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [2950][5500]\t Training Loss 0.4538\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3000][5500]\t Training Loss 0.2965\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3050][5500]\t Training Loss 0.8396\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3100][5500]\t Training Loss 0.3113\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3150][5500]\t Training Loss 1.1678\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [3200][5500]\t Training Loss 0.9865\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3250][5500]\t Training Loss 1.1182\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [3300][5500]\t Training Loss 1.6345\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [3350][5500]\t Training Loss 0.1527\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [3400][5500]\t Training Loss 0.1985\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3450][5500]\t Training Loss 1.4124\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [3500][5500]\t Training Loss 0.1835\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3550][5500]\t Training Loss 0.0797\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [3600][5500]\t Training Loss 0.5085\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3650][5500]\t Training Loss 0.2671\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3700][5500]\t Training Loss 0.0430\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [3750][5500]\t Training Loss 0.8316\t Accuracy 0.7000\n",
      "Epoch [0][10]\t Batch [3800][5500]\t Training Loss 0.4592\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3850][5500]\t Training Loss 0.2221\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3900][5500]\t Training Loss 0.1369\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [3950][5500]\t Training Loss 0.2838\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [4000][5500]\t Training Loss 0.2980\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4050][5500]\t Training Loss 0.1103\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [4100][5500]\t Training Loss 0.1056\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [4150][5500]\t Training Loss 0.0506\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [4200][5500]\t Training Loss 0.1658\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [4250][5500]\t Training Loss 0.3003\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4300][5500]\t Training Loss 0.2573\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [4350][5500]\t Training Loss 0.3065\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4400][5500]\t Training Loss 0.0859\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4450][5500]\t Training Loss 0.4939\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [4500][5500]\t Training Loss 0.1360\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [4550][5500]\t Training Loss 0.0193\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [4600][5500]\t Training Loss 0.4763\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4650][5500]\t Training Loss 0.2745\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4700][5500]\t Training Loss 0.3395\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4750][5500]\t Training Loss 0.1671\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [4800][5500]\t Training Loss 0.4145\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4850][5500]\t Training Loss 0.3610\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4900][5500]\t Training Loss 0.1540\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [4950][5500]\t Training Loss 0.2318\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [5000][5500]\t Training Loss 0.7613\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [5050][5500]\t Training Loss 0.5926\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [5100][5500]\t Training Loss 0.5114\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [5150][5500]\t Training Loss 0.0972\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [5200][5500]\t Training Loss 0.4915\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [5250][5500]\t Training Loss 0.4205\t Accuracy 0.8000\n",
      "Epoch [0][10]\t Batch [5300][5500]\t Training Loss 0.4767\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [5350][5500]\t Training Loss 0.2426\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [5400][5500]\t Training Loss 0.0606\t Accuracy 1.0000\n",
      "Epoch [0][10]\t Batch [5450][5500]\t Training Loss 0.0806\t Accuracy 1.0000\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3710\t Average training accuracy 0.8946\n",
      "Epoch [0]\t Average validation loss 0.2478\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [1][10]\t Batch [0][5500]\t Training Loss 0.3138\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [50][5500]\t Training Loss 0.2646\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [100][5500]\t Training Loss 0.4603\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [150][5500]\t Training Loss 0.3750\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [200][5500]\t Training Loss 0.2538\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [250][5500]\t Training Loss 0.0151\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [300][5500]\t Training Loss 0.4016\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [350][5500]\t Training Loss 0.1041\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [400][5500]\t Training Loss 0.1681\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [450][5500]\t Training Loss 0.0185\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [500][5500]\t Training Loss 0.0321\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [550][5500]\t Training Loss 0.5173\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [600][5500]\t Training Loss 0.1358\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [650][5500]\t Training Loss 0.0669\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [700][5500]\t Training Loss 0.1620\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [750][5500]\t Training Loss 0.0709\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [800][5500]\t Training Loss 1.3757\t Accuracy 0.7000\n",
      "Epoch [1][10]\t Batch [850][5500]\t Training Loss 0.2792\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [900][5500]\t Training Loss 0.1803\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [950][5500]\t Training Loss 0.4957\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [1000][5500]\t Training Loss 0.3252\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [1050][5500]\t Training Loss 0.0451\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1100][5500]\t Training Loss 0.1418\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1150][5500]\t Training Loss 0.3431\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [1200][5500]\t Training Loss 0.0310\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1250][5500]\t Training Loss 0.1071\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1300][5500]\t Training Loss 0.2018\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [1350][5500]\t Training Loss 0.7229\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [1400][5500]\t Training Loss 0.1791\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [1450][5500]\t Training Loss 0.3443\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [1500][5500]\t Training Loss 0.6141\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [1550][5500]\t Training Loss 0.0153\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1600][5500]\t Training Loss 0.1758\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [1650][5500]\t Training Loss 0.0181\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1700][5500]\t Training Loss 0.0203\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1750][5500]\t Training Loss 0.3390\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [1800][5500]\t Training Loss 0.0593\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1850][5500]\t Training Loss 0.1486\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1900][5500]\t Training Loss 0.3036\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [1950][5500]\t Training Loss 0.9892\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [2000][5500]\t Training Loss 1.0197\t Accuracy 0.7000\n",
      "Epoch [1][10]\t Batch [2050][5500]\t Training Loss 0.1134\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [2100][5500]\t Training Loss 0.0832\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [2150][5500]\t Training Loss 0.1532\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [2200][5500]\t Training Loss 0.0939\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [2250][5500]\t Training Loss 0.3671\t Accuracy 0.7000\n",
      "Epoch [1][10]\t Batch [2300][5500]\t Training Loss 0.2444\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [2350][5500]\t Training Loss 0.0216\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [2400][5500]\t Training Loss 1.5761\t Accuracy 0.7000\n",
      "Epoch [1][10]\t Batch [2450][5500]\t Training Loss 0.1456\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [2500][5500]\t Training Loss 0.2793\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [2550][5500]\t Training Loss 0.3528\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [2600][5500]\t Training Loss 0.1244\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [2650][5500]\t Training Loss 0.4275\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [2700][5500]\t Training Loss 0.0576\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [2750][5500]\t Training Loss 0.3896\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [2800][5500]\t Training Loss 0.8344\t Accuracy 0.6000\n",
      "Epoch [1][10]\t Batch [2850][5500]\t Training Loss 0.5327\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [2900][5500]\t Training Loss 0.2300\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [2950][5500]\t Training Loss 0.5165\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [3000][5500]\t Training Loss 0.2431\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [3050][5500]\t Training Loss 0.1178\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [3100][5500]\t Training Loss 0.3596\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [3150][5500]\t Training Loss 0.0637\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [3200][5500]\t Training Loss 0.3710\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [3250][5500]\t Training Loss 0.0488\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [3300][5500]\t Training Loss 0.2072\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [3350][5500]\t Training Loss 0.3736\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [3400][5500]\t Training Loss 0.1610\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [3450][5500]\t Training Loss 0.3182\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [3500][5500]\t Training Loss 0.5989\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [3550][5500]\t Training Loss 0.2873\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [3600][5500]\t Training Loss 0.1155\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [3650][5500]\t Training Loss 0.5672\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [3700][5500]\t Training Loss 0.1388\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [3750][5500]\t Training Loss 0.3690\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [3800][5500]\t Training Loss 0.5770\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [3850][5500]\t Training Loss 0.0642\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [3900][5500]\t Training Loss 0.2560\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [3950][5500]\t Training Loss 0.1124\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [4000][5500]\t Training Loss 0.4781\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4050][5500]\t Training Loss 0.1761\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4100][5500]\t Training Loss 0.2879\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4150][5500]\t Training Loss 1.1174\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [4200][5500]\t Training Loss 0.1566\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [4250][5500]\t Training Loss 0.1430\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4300][5500]\t Training Loss 0.2352\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4350][5500]\t Training Loss 0.6094\t Accuracy 0.7000\n",
      "Epoch [1][10]\t Batch [4400][5500]\t Training Loss 0.5975\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [4450][5500]\t Training Loss 0.2972\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4500][5500]\t Training Loss 0.4211\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [4550][5500]\t Training Loss 0.1874\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4600][5500]\t Training Loss 0.7606\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4650][5500]\t Training Loss 0.1140\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [4700][5500]\t Training Loss 0.3495\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4750][5500]\t Training Loss 0.1078\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [4800][5500]\t Training Loss 0.4919\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [4850][5500]\t Training Loss 0.0220\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [4900][5500]\t Training Loss 0.7916\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [4950][5500]\t Training Loss 0.1367\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [5000][5500]\t Training Loss 0.0581\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [5050][5500]\t Training Loss 0.6807\t Accuracy 0.7000\n",
      "Epoch [1][10]\t Batch [5100][5500]\t Training Loss 0.2141\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [5150][5500]\t Training Loss 0.1557\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [5200][5500]\t Training Loss 1.2687\t Accuracy 0.8000\n",
      "Epoch [1][10]\t Batch [5250][5500]\t Training Loss 0.7510\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [5300][5500]\t Training Loss 0.1283\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [5350][5500]\t Training Loss 0.6461\t Accuracy 0.7000\n",
      "Epoch [1][10]\t Batch [5400][5500]\t Training Loss 0.1079\t Accuracy 1.0000\n",
      "Epoch [1][10]\t Batch [5450][5500]\t Training Loss 0.3976\t Accuracy 0.9000\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3066\t Average training accuracy 0.9127\n",
      "Epoch [1]\t Average validation loss 0.2393\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [2][10]\t Batch [0][5500]\t Training Loss 0.4429\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [50][5500]\t Training Loss 0.2627\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [100][5500]\t Training Loss 0.5226\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [150][5500]\t Training Loss 0.0716\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [200][5500]\t Training Loss 0.0273\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [250][5500]\t Training Loss 0.5619\t Accuracy 0.7000\n",
      "Epoch [2][10]\t Batch [300][5500]\t Training Loss 0.0650\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [350][5500]\t Training Loss 0.6216\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [400][5500]\t Training Loss 0.0467\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [450][5500]\t Training Loss 0.1377\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [500][5500]\t Training Loss 0.0779\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [550][5500]\t Training Loss 0.1618\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [600][5500]\t Training Loss 0.1965\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [650][5500]\t Training Loss 0.3397\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [700][5500]\t Training Loss 0.0618\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [750][5500]\t Training Loss 0.3619\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [800][5500]\t Training Loss 0.2014\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [850][5500]\t Training Loss 0.0526\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [900][5500]\t Training Loss 0.3025\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [950][5500]\t Training Loss 0.1073\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [1000][5500]\t Training Loss 0.1896\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1050][5500]\t Training Loss 0.0997\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [1100][5500]\t Training Loss 0.4709\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1150][5500]\t Training Loss 0.2183\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1200][5500]\t Training Loss 0.0783\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [1250][5500]\t Training Loss 0.2553\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1300][5500]\t Training Loss 0.1138\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [1350][5500]\t Training Loss 0.3513\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1400][5500]\t Training Loss 0.3256\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1450][5500]\t Training Loss 1.0943\t Accuracy 0.7000\n",
      "Epoch [2][10]\t Batch [1500][5500]\t Training Loss 0.0154\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [1550][5500]\t Training Loss 0.0657\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [1600][5500]\t Training Loss 0.8886\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1650][5500]\t Training Loss 1.4320\t Accuracy 0.7000\n",
      "Epoch [2][10]\t Batch [1700][5500]\t Training Loss 0.0706\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [1750][5500]\t Training Loss 0.3674\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1800][5500]\t Training Loss 1.0053\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1850][5500]\t Training Loss 0.2927\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1900][5500]\t Training Loss 0.1760\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [1950][5500]\t Training Loss 0.2321\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [2000][5500]\t Training Loss 0.0957\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2050][5500]\t Training Loss 0.0763\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2100][5500]\t Training Loss 0.5664\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [2150][5500]\t Training Loss 0.1623\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [2200][5500]\t Training Loss 0.1014\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2250][5500]\t Training Loss 0.1023\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2300][5500]\t Training Loss 0.2012\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [2350][5500]\t Training Loss 0.4156\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [2400][5500]\t Training Loss 0.7722\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [2450][5500]\t Training Loss 0.0527\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2500][5500]\t Training Loss 0.4378\t Accuracy 0.7000\n",
      "Epoch [2][10]\t Batch [2550][5500]\t Training Loss 0.8196\t Accuracy 0.7000\n",
      "Epoch [2][10]\t Batch [2600][5500]\t Training Loss 0.0943\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2650][5500]\t Training Loss 0.1732\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2700][5500]\t Training Loss 0.2355\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2750][5500]\t Training Loss 0.4576\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [2800][5500]\t Training Loss 0.2727\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [2850][5500]\t Training Loss 0.0996\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [2900][5500]\t Training Loss 0.1865\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [2950][5500]\t Training Loss 0.0344\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3000][5500]\t Training Loss 0.1434\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [3050][5500]\t Training Loss 0.1646\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3100][5500]\t Training Loss 0.6752\t Accuracy 0.7000\n",
      "Epoch [2][10]\t Batch [3150][5500]\t Training Loss 0.2208\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [3200][5500]\t Training Loss 0.4566\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [3250][5500]\t Training Loss 0.3576\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3300][5500]\t Training Loss 0.1655\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [3350][5500]\t Training Loss 0.1976\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [3400][5500]\t Training Loss 0.0615\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3450][5500]\t Training Loss 1.2364\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [3500][5500]\t Training Loss 0.0700\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3550][5500]\t Training Loss 0.0814\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3600][5500]\t Training Loss 0.8669\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [3650][5500]\t Training Loss 0.1833\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [3700][5500]\t Training Loss 0.6492\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [3750][5500]\t Training Loss 0.0796\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3800][5500]\t Training Loss 0.1429\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3850][5500]\t Training Loss 0.1816\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3900][5500]\t Training Loss 0.0439\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [3950][5500]\t Training Loss 0.3919\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [4000][5500]\t Training Loss 0.0141\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [4050][5500]\t Training Loss 0.0915\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [4100][5500]\t Training Loss 0.2648\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [4150][5500]\t Training Loss 0.6860\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [4200][5500]\t Training Loss 0.3485\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [4250][5500]\t Training Loss 0.0600\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [4300][5500]\t Training Loss 0.6139\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [4350][5500]\t Training Loss 0.1075\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [4400][5500]\t Training Loss 0.0610\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [4450][5500]\t Training Loss 0.1919\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [4500][5500]\t Training Loss 1.3967\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [4550][5500]\t Training Loss 0.1214\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [4600][5500]\t Training Loss 0.0498\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [4650][5500]\t Training Loss 0.1480\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [4700][5500]\t Training Loss 0.2273\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [4750][5500]\t Training Loss 0.1773\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [4800][5500]\t Training Loss 0.3023\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [4850][5500]\t Training Loss 0.3279\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [4900][5500]\t Training Loss 0.0265\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [4950][5500]\t Training Loss 0.2582\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [5000][5500]\t Training Loss 0.1473\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [5050][5500]\t Training Loss 0.9118\t Accuracy 0.5000\n",
      "Epoch [2][10]\t Batch [5100][5500]\t Training Loss 0.4638\t Accuracy 0.7000\n",
      "Epoch [2][10]\t Batch [5150][5500]\t Training Loss 0.0158\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [5200][5500]\t Training Loss 0.1446\t Accuracy 1.0000\n",
      "Epoch [2][10]\t Batch [5250][5500]\t Training Loss 0.5973\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [5300][5500]\t Training Loss 0.5730\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [5350][5500]\t Training Loss 1.0773\t Accuracy 0.8000\n",
      "Epoch [2][10]\t Batch [5400][5500]\t Training Loss 0.9442\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [5450][5500]\t Training Loss 0.0802\t Accuracy 1.0000\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2943\t Average training accuracy 0.9174\n",
      "Epoch [2]\t Average validation loss 0.2324\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [3][10]\t Batch [0][5500]\t Training Loss 0.1693\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [50][5500]\t Training Loss 0.6826\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [100][5500]\t Training Loss 0.0505\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [150][5500]\t Training Loss 0.1751\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [200][5500]\t Training Loss 0.2964\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [250][5500]\t Training Loss 0.1370\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [300][5500]\t Training Loss 1.0092\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [350][5500]\t Training Loss 0.1099\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [400][5500]\t Training Loss 0.0396\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [450][5500]\t Training Loss 0.2728\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [500][5500]\t Training Loss 0.0323\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [550][5500]\t Training Loss 0.2023\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [600][5500]\t Training Loss 0.0181\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [650][5500]\t Training Loss 0.0635\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [700][5500]\t Training Loss 0.5169\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [750][5500]\t Training Loss 0.1466\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [800][5500]\t Training Loss 0.1310\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [850][5500]\t Training Loss 0.1627\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [900][5500]\t Training Loss 0.0359\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [950][5500]\t Training Loss 0.2817\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [1000][5500]\t Training Loss 0.0921\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1050][5500]\t Training Loss 0.9157\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [1100][5500]\t Training Loss 0.1454\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [1150][5500]\t Training Loss 0.3453\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [1200][5500]\t Training Loss 0.6272\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [1250][5500]\t Training Loss 0.0446\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1300][5500]\t Training Loss 0.3639\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [1350][5500]\t Training Loss 0.0268\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1400][5500]\t Training Loss 0.1095\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1450][5500]\t Training Loss 0.3707\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [1500][5500]\t Training Loss 0.0902\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1550][5500]\t Training Loss 1.4180\t Accuracy 0.7000\n",
      "Epoch [3][10]\t Batch [1600][5500]\t Training Loss 0.3743\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [1650][5500]\t Training Loss 0.1346\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1700][5500]\t Training Loss 0.0852\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1750][5500]\t Training Loss 0.3669\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [1800][5500]\t Training Loss 0.0676\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1850][5500]\t Training Loss 0.0908\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [1900][5500]\t Training Loss 0.2997\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [1950][5500]\t Training Loss 0.6142\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [2000][5500]\t Training Loss 0.1908\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2050][5500]\t Training Loss 0.7006\t Accuracy 0.6000\n",
      "Epoch [3][10]\t Batch [2100][5500]\t Training Loss 0.1017\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2150][5500]\t Training Loss 0.3231\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [2200][5500]\t Training Loss 1.1636\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [2250][5500]\t Training Loss 0.0597\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2300][5500]\t Training Loss 0.1218\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2350][5500]\t Training Loss 0.5063\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [2400][5500]\t Training Loss 0.0632\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2450][5500]\t Training Loss 0.2686\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [2500][5500]\t Training Loss 0.3291\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [2550][5500]\t Training Loss 0.1519\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2600][5500]\t Training Loss 0.1878\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [2650][5500]\t Training Loss 0.0350\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2700][5500]\t Training Loss 0.1047\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2750][5500]\t Training Loss 0.6083\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [2800][5500]\t Training Loss 0.0199\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [2850][5500]\t Training Loss 0.4051\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [2900][5500]\t Training Loss 0.1522\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [2950][5500]\t Training Loss 0.1160\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [3000][5500]\t Training Loss 0.2734\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [3050][5500]\t Training Loss 0.1328\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [3100][5500]\t Training Loss 0.2142\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [3150][5500]\t Training Loss 0.7793\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [3200][5500]\t Training Loss 0.0246\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [3250][5500]\t Training Loss 1.1256\t Accuracy 0.7000\n",
      "Epoch [3][10]\t Batch [3300][5500]\t Training Loss 0.3955\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [3350][5500]\t Training Loss 0.8292\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [3400][5500]\t Training Loss 0.5639\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [3450][5500]\t Training Loss 0.2074\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [3500][5500]\t Training Loss 0.0156\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [3550][5500]\t Training Loss 0.0381\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [3600][5500]\t Training Loss 0.3007\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [3650][5500]\t Training Loss 0.3196\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [3700][5500]\t Training Loss 0.7579\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [3750][5500]\t Training Loss 0.0726\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [3800][5500]\t Training Loss 0.4132\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [3850][5500]\t Training Loss 0.2274\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [3900][5500]\t Training Loss 0.4753\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [3950][5500]\t Training Loss 0.0214\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [4000][5500]\t Training Loss 0.5223\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4050][5500]\t Training Loss 0.5263\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [4100][5500]\t Training Loss 0.0739\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [4150][5500]\t Training Loss 0.1245\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4200][5500]\t Training Loss 0.2789\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4250][5500]\t Training Loss 0.1406\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4300][5500]\t Training Loss 0.8906\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [4350][5500]\t Training Loss 0.1066\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [4400][5500]\t Training Loss 0.6063\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [4450][5500]\t Training Loss 1.8659\t Accuracy 0.7000\n",
      "Epoch [3][10]\t Batch [4500][5500]\t Training Loss 0.2298\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4550][5500]\t Training Loss 0.4783\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4600][5500]\t Training Loss 0.9030\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4650][5500]\t Training Loss 0.0425\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [4700][5500]\t Training Loss 0.7623\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4750][5500]\t Training Loss 0.6402\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [4800][5500]\t Training Loss 0.2043\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4850][5500]\t Training Loss 0.2338\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [4900][5500]\t Training Loss 0.0884\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [4950][5500]\t Training Loss 0.4469\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [5000][5500]\t Training Loss 0.1205\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [5050][5500]\t Training Loss 0.2910\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [5100][5500]\t Training Loss 0.0276\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [5150][5500]\t Training Loss 0.0593\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [5200][5500]\t Training Loss 0.1728\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [5250][5500]\t Training Loss 0.7379\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [5300][5500]\t Training Loss 0.0627\t Accuracy 1.0000\n",
      "Epoch [3][10]\t Batch [5350][5500]\t Training Loss 0.2579\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [5400][5500]\t Training Loss 0.4109\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [5450][5500]\t Training Loss 0.1979\t Accuracy 0.9000\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2873\t Average training accuracy 0.9197\n",
      "Epoch [3]\t Average validation loss 0.2356\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [4][10]\t Batch [0][5500]\t Training Loss 0.0185\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [50][5500]\t Training Loss 0.0584\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [100][5500]\t Training Loss 0.0297\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [150][5500]\t Training Loss 0.0972\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [200][5500]\t Training Loss 0.3056\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [250][5500]\t Training Loss 0.3241\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [300][5500]\t Training Loss 0.3012\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [350][5500]\t Training Loss 0.5572\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [400][5500]\t Training Loss 0.6521\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [450][5500]\t Training Loss 0.2749\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [500][5500]\t Training Loss 0.0158\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [550][5500]\t Training Loss 0.2557\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [600][5500]\t Training Loss 0.2575\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [650][5500]\t Training Loss 0.1520\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [700][5500]\t Training Loss 0.2490\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [750][5500]\t Training Loss 0.1205\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [800][5500]\t Training Loss 0.1495\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [850][5500]\t Training Loss 0.1805\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [900][5500]\t Training Loss 0.3992\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [950][5500]\t Training Loss 0.8013\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [1000][5500]\t Training Loss 0.3914\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [1050][5500]\t Training Loss 0.1295\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1100][5500]\t Training Loss 0.0611\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1150][5500]\t Training Loss 0.0909\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1200][5500]\t Training Loss 0.1633\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1250][5500]\t Training Loss 0.0144\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1300][5500]\t Training Loss 0.2252\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [1350][5500]\t Training Loss 0.2768\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [1400][5500]\t Training Loss 0.1008\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1450][5500]\t Training Loss 0.0535\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1500][5500]\t Training Loss 0.2404\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [1550][5500]\t Training Loss 0.0600\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1600][5500]\t Training Loss 1.2835\t Accuracy 0.6000\n",
      "Epoch [4][10]\t Batch [1650][5500]\t Training Loss 0.3181\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [1700][5500]\t Training Loss 0.1050\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1750][5500]\t Training Loss 0.0319\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1800][5500]\t Training Loss 0.0761\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1850][5500]\t Training Loss 0.2514\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [1900][5500]\t Training Loss 0.0575\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [1950][5500]\t Training Loss 0.1903\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [2000][5500]\t Training Loss 0.0382\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2050][5500]\t Training Loss 0.2924\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [2100][5500]\t Training Loss 0.0149\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2150][5500]\t Training Loss 0.0485\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2200][5500]\t Training Loss 0.0978\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2250][5500]\t Training Loss 0.0566\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2300][5500]\t Training Loss 0.2287\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [2350][5500]\t Training Loss 0.2192\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2400][5500]\t Training Loss 0.4306\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [2450][5500]\t Training Loss 0.1415\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2500][5500]\t Training Loss 0.3154\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [2550][5500]\t Training Loss 0.0493\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2600][5500]\t Training Loss 0.4807\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [2650][5500]\t Training Loss 0.2159\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [2700][5500]\t Training Loss 0.0198\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2750][5500]\t Training Loss 0.1533\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [2800][5500]\t Training Loss 0.8305\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [2850][5500]\t Training Loss 0.2187\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [2900][5500]\t Training Loss 0.1641\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [2950][5500]\t Training Loss 0.1030\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3000][5500]\t Training Loss 1.6146\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [3050][5500]\t Training Loss 0.3487\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [3100][5500]\t Training Loss 1.3362\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [3150][5500]\t Training Loss 0.0339\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3200][5500]\t Training Loss 0.2645\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [3250][5500]\t Training Loss 0.1027\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3300][5500]\t Training Loss 0.3808\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [3350][5500]\t Training Loss 0.0595\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3400][5500]\t Training Loss 0.4372\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [3450][5500]\t Training Loss 0.1150\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3500][5500]\t Training Loss 0.1509\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3550][5500]\t Training Loss 0.0920\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3600][5500]\t Training Loss 0.1207\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3650][5500]\t Training Loss 0.5490\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [3700][5500]\t Training Loss 0.1531\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [3750][5500]\t Training Loss 0.3451\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [3800][5500]\t Training Loss 0.0691\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3850][5500]\t Training Loss 0.0425\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3900][5500]\t Training Loss 0.1345\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [3950][5500]\t Training Loss 0.2155\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4000][5500]\t Training Loss 0.3738\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4050][5500]\t Training Loss 0.0706\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [4100][5500]\t Training Loss 0.7006\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4150][5500]\t Training Loss 0.1790\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4200][5500]\t Training Loss 0.2545\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4250][5500]\t Training Loss 0.3644\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4300][5500]\t Training Loss 0.0148\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [4350][5500]\t Training Loss 0.7861\t Accuracy 0.7000\n",
      "Epoch [4][10]\t Batch [4400][5500]\t Training Loss 0.4723\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4450][5500]\t Training Loss 0.2501\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4500][5500]\t Training Loss 0.4739\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4550][5500]\t Training Loss 0.1177\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [4600][5500]\t Training Loss 0.3586\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [4650][5500]\t Training Loss 0.0610\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [4700][5500]\t Training Loss 0.3201\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4750][5500]\t Training Loss 0.2548\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [4800][5500]\t Training Loss 0.5012\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [4850][5500]\t Training Loss 0.0438\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [4900][5500]\t Training Loss 0.4165\t Accuracy 0.7000\n",
      "Epoch [4][10]\t Batch [4950][5500]\t Training Loss 0.1103\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [5000][5500]\t Training Loss 0.9888\t Accuracy 0.7000\n",
      "Epoch [4][10]\t Batch [5050][5500]\t Training Loss 0.4168\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [5100][5500]\t Training Loss 0.1250\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [5150][5500]\t Training Loss 0.2353\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [5200][5500]\t Training Loss 0.1858\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [5250][5500]\t Training Loss 2.6637\t Accuracy 0.8000\n",
      "Epoch [4][10]\t Batch [5300][5500]\t Training Loss 0.5156\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [5350][5500]\t Training Loss 0.0632\t Accuracy 1.0000\n",
      "Epoch [4][10]\t Batch [5400][5500]\t Training Loss 0.3618\t Accuracy 0.9000\n",
      "Epoch [4][10]\t Batch [5450][5500]\t Training Loss 0.0698\t Accuracy 1.0000\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2829\t Average training accuracy 0.9206\n",
      "Epoch [4]\t Average validation loss 0.2336\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [5][10]\t Batch [0][5500]\t Training Loss 0.1457\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [50][5500]\t Training Loss 0.1175\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [100][5500]\t Training Loss 0.0950\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [150][5500]\t Training Loss 0.3219\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [200][5500]\t Training Loss 0.1604\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [250][5500]\t Training Loss 0.0152\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [300][5500]\t Training Loss 0.7722\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [350][5500]\t Training Loss 0.8049\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [400][5500]\t Training Loss 0.4705\t Accuracy 0.7000\n",
      "Epoch [5][10]\t Batch [450][5500]\t Training Loss 0.1077\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [500][5500]\t Training Loss 0.0542\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [550][5500]\t Training Loss 0.4033\t Accuracy 0.7000\n",
      "Epoch [5][10]\t Batch [600][5500]\t Training Loss 0.4327\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [650][5500]\t Training Loss 0.4507\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [700][5500]\t Training Loss 0.3384\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [750][5500]\t Training Loss 0.0297\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [800][5500]\t Training Loss 0.3251\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [850][5500]\t Training Loss 1.9016\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [900][5500]\t Training Loss 0.0240\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [950][5500]\t Training Loss 0.1029\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1000][5500]\t Training Loss 0.0116\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1050][5500]\t Training Loss 0.1063\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1100][5500]\t Training Loss 0.0638\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1150][5500]\t Training Loss 0.2058\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [1200][5500]\t Training Loss 0.3196\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [1250][5500]\t Training Loss 0.0358\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1300][5500]\t Training Loss 0.1381\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1350][5500]\t Training Loss 0.1158\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [1400][5500]\t Training Loss 0.1887\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [1450][5500]\t Training Loss 0.0311\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1500][5500]\t Training Loss 0.0638\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1550][5500]\t Training Loss 0.0678\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1600][5500]\t Training Loss 0.1533\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1650][5500]\t Training Loss 0.2755\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [1700][5500]\t Training Loss 0.0580\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1750][5500]\t Training Loss 0.3054\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [1800][5500]\t Training Loss 0.0831\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1850][5500]\t Training Loss 0.2093\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [1900][5500]\t Training Loss 0.1433\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [1950][5500]\t Training Loss 0.5942\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [2000][5500]\t Training Loss 0.2353\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [2050][5500]\t Training Loss 0.1402\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2100][5500]\t Training Loss 0.0161\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2150][5500]\t Training Loss 0.0768\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2200][5500]\t Training Loss 0.3050\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [2250][5500]\t Training Loss 0.0556\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2300][5500]\t Training Loss 0.1698\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2350][5500]\t Training Loss 0.0766\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2400][5500]\t Training Loss 0.2489\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [2450][5500]\t Training Loss 1.0294\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [2500][5500]\t Training Loss 0.4065\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [2550][5500]\t Training Loss 0.2122\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [2600][5500]\t Training Loss 0.0358\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2650][5500]\t Training Loss 0.5082\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [2700][5500]\t Training Loss 0.2120\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [2750][5500]\t Training Loss 0.1882\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2800][5500]\t Training Loss 0.0493\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2850][5500]\t Training Loss 0.5638\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [2900][5500]\t Training Loss 0.1726\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [2950][5500]\t Training Loss 0.2126\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [3000][5500]\t Training Loss 0.2538\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [3050][5500]\t Training Loss 0.5605\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [3100][5500]\t Training Loss 0.2521\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3150][5500]\t Training Loss 0.0751\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3200][5500]\t Training Loss 0.3166\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [3250][5500]\t Training Loss 0.1814\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3300][5500]\t Training Loss 0.0232\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3350][5500]\t Training Loss 0.3467\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [3400][5500]\t Training Loss 0.1745\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [3450][5500]\t Training Loss 0.1836\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [3500][5500]\t Training Loss 0.0149\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3550][5500]\t Training Loss 0.0751\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3600][5500]\t Training Loss 0.2828\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [3650][5500]\t Training Loss 0.4480\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [3700][5500]\t Training Loss 0.0532\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3750][5500]\t Training Loss 0.2643\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [3800][5500]\t Training Loss 0.1403\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3850][5500]\t Training Loss 0.8940\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [3900][5500]\t Training Loss 0.1704\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [3950][5500]\t Training Loss 0.0300\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4000][5500]\t Training Loss 0.2579\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [4050][5500]\t Training Loss 0.0891\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4100][5500]\t Training Loss 0.1441\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4150][5500]\t Training Loss 0.0558\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4200][5500]\t Training Loss 0.1830\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4250][5500]\t Training Loss 0.0384\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4300][5500]\t Training Loss 0.0446\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4350][5500]\t Training Loss 0.5066\t Accuracy 0.7000\n",
      "Epoch [5][10]\t Batch [4400][5500]\t Training Loss 0.0432\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4450][5500]\t Training Loss 0.6498\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [4500][5500]\t Training Loss 0.0722\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4550][5500]\t Training Loss 0.0123\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4600][5500]\t Training Loss 0.6180\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [4650][5500]\t Training Loss 0.0505\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4700][5500]\t Training Loss 0.2630\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [4750][5500]\t Training Loss 0.0643\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4800][5500]\t Training Loss 0.8055\t Accuracy 0.6000\n",
      "Epoch [5][10]\t Batch [4850][5500]\t Training Loss 0.0840\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [4900][5500]\t Training Loss 0.1770\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [4950][5500]\t Training Loss 0.1410\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [5000][5500]\t Training Loss 0.0310\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [5050][5500]\t Training Loss 0.1162\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [5100][5500]\t Training Loss 0.3159\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [5150][5500]\t Training Loss 0.2093\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [5200][5500]\t Training Loss 0.1185\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [5250][5500]\t Training Loss 1.5937\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [5300][5500]\t Training Loss 0.0507\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [5350][5500]\t Training Loss 0.1357\t Accuracy 1.0000\n",
      "Epoch [5][10]\t Batch [5400][5500]\t Training Loss 0.1083\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [5450][5500]\t Training Loss 0.7978\t Accuracy 0.9000\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2803\t Average training accuracy 0.9219\n",
      "Epoch [5]\t Average validation loss 0.2287\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [6][10]\t Batch [0][5500]\t Training Loss 0.4534\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [50][5500]\t Training Loss 0.1554\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [100][5500]\t Training Loss 0.0811\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [150][5500]\t Training Loss 1.0758\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [200][5500]\t Training Loss 0.3134\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [250][5500]\t Training Loss 1.0656\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [300][5500]\t Training Loss 0.0486\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [350][5500]\t Training Loss 0.2204\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [400][5500]\t Training Loss 0.8068\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [450][5500]\t Training Loss 0.0580\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [500][5500]\t Training Loss 0.0493\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [550][5500]\t Training Loss 0.5733\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [600][5500]\t Training Loss 1.0997\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [650][5500]\t Training Loss 0.2195\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [700][5500]\t Training Loss 0.1003\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [750][5500]\t Training Loss 0.1260\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [800][5500]\t Training Loss 0.9667\t Accuracy 0.6000\n",
      "Epoch [6][10]\t Batch [850][5500]\t Training Loss 0.0480\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [900][5500]\t Training Loss 0.5145\t Accuracy 0.7000\n",
      "Epoch [6][10]\t Batch [950][5500]\t Training Loss 0.3400\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [1000][5500]\t Training Loss 0.2651\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [1050][5500]\t Training Loss 0.0659\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1100][5500]\t Training Loss 0.2470\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [1150][5500]\t Training Loss 0.0896\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1200][5500]\t Training Loss 0.1462\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [1250][5500]\t Training Loss 0.6172\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [1300][5500]\t Training Loss 0.2940\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [1350][5500]\t Training Loss 0.2793\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [1400][5500]\t Training Loss 0.0784\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1450][5500]\t Training Loss 0.2656\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [1500][5500]\t Training Loss 0.1710\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [1550][5500]\t Training Loss 0.0937\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1600][5500]\t Training Loss 0.0465\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1650][5500]\t Training Loss 0.1834\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [1700][5500]\t Training Loss 0.0580\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1750][5500]\t Training Loss 0.1061\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1800][5500]\t Training Loss 0.0760\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1850][5500]\t Training Loss 0.2058\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1900][5500]\t Training Loss 0.0720\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [1950][5500]\t Training Loss 0.1247\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2000][5500]\t Training Loss 0.1610\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2050][5500]\t Training Loss 0.0118\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [2100][5500]\t Training Loss 0.0558\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [2150][5500]\t Training Loss 0.7061\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [2200][5500]\t Training Loss 0.0293\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [2250][5500]\t Training Loss 0.5900\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2300][5500]\t Training Loss 0.0781\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [2350][5500]\t Training Loss 0.1335\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [2400][5500]\t Training Loss 0.1247\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2450][5500]\t Training Loss 0.0473\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [2500][5500]\t Training Loss 0.4679\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2550][5500]\t Training Loss 0.5103\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [2600][5500]\t Training Loss 0.2059\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2650][5500]\t Training Loss 0.0645\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [2700][5500]\t Training Loss 0.0260\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [2750][5500]\t Training Loss 0.3678\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2800][5500]\t Training Loss 0.3623\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2850][5500]\t Training Loss 0.3453\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2900][5500]\t Training Loss 0.1538\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [2950][5500]\t Training Loss 0.0611\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3000][5500]\t Training Loss 0.0755\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3050][5500]\t Training Loss 0.0770\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3100][5500]\t Training Loss 0.0145\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3150][5500]\t Training Loss 0.3694\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [3200][5500]\t Training Loss 0.0525\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3250][5500]\t Training Loss 0.2678\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [3300][5500]\t Training Loss 0.5538\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [3350][5500]\t Training Loss 0.3633\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [3400][5500]\t Training Loss 0.2076\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [3450][5500]\t Training Loss 0.3786\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [3500][5500]\t Training Loss 0.0530\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3550][5500]\t Training Loss 0.5284\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [3600][5500]\t Training Loss 0.0394\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3650][5500]\t Training Loss 0.0773\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3700][5500]\t Training Loss 1.3702\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [3750][5500]\t Training Loss 0.1517\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3800][5500]\t Training Loss 0.3094\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [3850][5500]\t Training Loss 0.1399\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [3900][5500]\t Training Loss 0.1968\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [3950][5500]\t Training Loss 0.2073\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [4000][5500]\t Training Loss 0.9177\t Accuracy 0.7000\n",
      "Epoch [6][10]\t Batch [4050][5500]\t Training Loss 0.1084\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [4100][5500]\t Training Loss 0.1544\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [4150][5500]\t Training Loss 0.4847\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [4200][5500]\t Training Loss 0.0344\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [4250][5500]\t Training Loss 0.1243\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [4300][5500]\t Training Loss 0.1506\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [4350][5500]\t Training Loss 0.5793\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [4400][5500]\t Training Loss 0.4429\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [4450][5500]\t Training Loss 0.3045\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [4500][5500]\t Training Loss 0.0425\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [4550][5500]\t Training Loss 0.1237\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [4600][5500]\t Training Loss 0.0144\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [4650][5500]\t Training Loss 0.1736\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [4700][5500]\t Training Loss 0.7092\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [4750][5500]\t Training Loss 0.5668\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [4800][5500]\t Training Loss 0.1462\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [4850][5500]\t Training Loss 0.6257\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [4900][5500]\t Training Loss 0.4161\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [4950][5500]\t Training Loss 0.3108\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [5000][5500]\t Training Loss 0.2928\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [5050][5500]\t Training Loss 0.3226\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [5100][5500]\t Training Loss 0.1383\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [5150][5500]\t Training Loss 0.2317\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [5200][5500]\t Training Loss 0.0336\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [5250][5500]\t Training Loss 0.4341\t Accuracy 0.8000\n",
      "Epoch [6][10]\t Batch [5300][5500]\t Training Loss 0.5139\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [5350][5500]\t Training Loss 0.1550\t Accuracy 0.9000\n",
      "Epoch [6][10]\t Batch [5400][5500]\t Training Loss 0.0552\t Accuracy 1.0000\n",
      "Epoch [6][10]\t Batch [5450][5500]\t Training Loss 0.5240\t Accuracy 0.9000\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2772\t Average training accuracy 0.9215\n",
      "Epoch [6]\t Average validation loss 0.2318\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [7][10]\t Batch [0][5500]\t Training Loss 0.4470\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [50][5500]\t Training Loss 0.2604\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [100][5500]\t Training Loss 0.0700\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [150][5500]\t Training Loss 0.1743\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [200][5500]\t Training Loss 0.1031\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [250][5500]\t Training Loss 0.1298\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [300][5500]\t Training Loss 0.2391\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [350][5500]\t Training Loss 0.0525\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [400][5500]\t Training Loss 0.1089\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [450][5500]\t Training Loss 0.0100\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [500][5500]\t Training Loss 0.1667\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [550][5500]\t Training Loss 0.0367\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [600][5500]\t Training Loss 0.0202\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [650][5500]\t Training Loss 0.0535\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [700][5500]\t Training Loss 0.0884\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [750][5500]\t Training Loss 0.2350\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [800][5500]\t Training Loss 0.0930\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [850][5500]\t Training Loss 0.0724\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [900][5500]\t Training Loss 0.3984\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [950][5500]\t Training Loss 0.5894\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [1000][5500]\t Training Loss 0.0400\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [1050][5500]\t Training Loss 0.8255\t Accuracy 0.7000\n",
      "Epoch [7][10]\t Batch [1100][5500]\t Training Loss 0.8295\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [1150][5500]\t Training Loss 0.3212\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [1200][5500]\t Training Loss 0.0468\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [1250][5500]\t Training Loss 0.0963\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [1300][5500]\t Training Loss 0.4794\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [1350][5500]\t Training Loss 0.2429\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [1400][5500]\t Training Loss 0.1185\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [1450][5500]\t Training Loss 0.0923\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [1500][5500]\t Training Loss 0.5434\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [1550][5500]\t Training Loss 0.0980\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [1600][5500]\t Training Loss 0.2544\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [1650][5500]\t Training Loss 0.1987\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [1700][5500]\t Training Loss 0.2244\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [1750][5500]\t Training Loss 0.4805\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [1800][5500]\t Training Loss 0.1321\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [1850][5500]\t Training Loss 0.2757\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [1900][5500]\t Training Loss 0.1908\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [1950][5500]\t Training Loss 0.2160\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [2000][5500]\t Training Loss 0.0586\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2050][5500]\t Training Loss 0.1345\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2100][5500]\t Training Loss 0.0614\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2150][5500]\t Training Loss 0.1626\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [2200][5500]\t Training Loss 0.3170\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [2250][5500]\t Training Loss 0.2937\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [2300][5500]\t Training Loss 0.1223\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2350][5500]\t Training Loss 0.0291\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2400][5500]\t Training Loss 0.0217\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2450][5500]\t Training Loss 0.1446\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [2500][5500]\t Training Loss 0.0143\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2550][5500]\t Training Loss 0.0160\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2600][5500]\t Training Loss 0.1006\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2650][5500]\t Training Loss 0.0571\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2700][5500]\t Training Loss 0.1979\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [2750][5500]\t Training Loss 0.1527\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2800][5500]\t Training Loss 0.0481\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2850][5500]\t Training Loss 0.1799\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [2900][5500]\t Training Loss 0.1405\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [2950][5500]\t Training Loss 0.0471\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3000][5500]\t Training Loss 0.1739\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3050][5500]\t Training Loss 0.0344\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3100][5500]\t Training Loss 0.5639\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [3150][5500]\t Training Loss 0.0697\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3200][5500]\t Training Loss 1.5297\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [3250][5500]\t Training Loss 0.0775\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3300][5500]\t Training Loss 0.0312\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3350][5500]\t Training Loss 0.0950\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3400][5500]\t Training Loss 0.1404\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [3450][5500]\t Training Loss 0.3812\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [3500][5500]\t Training Loss 0.4491\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [3550][5500]\t Training Loss 0.1066\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [3600][5500]\t Training Loss 0.0124\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3650][5500]\t Training Loss 0.6751\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [3700][5500]\t Training Loss 0.1468\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [3750][5500]\t Training Loss 0.0376\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3800][5500]\t Training Loss 0.0321\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3850][5500]\t Training Loss 0.1035\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [3900][5500]\t Training Loss 0.1844\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [3950][5500]\t Training Loss 0.1134\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [4000][5500]\t Training Loss 0.1030\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [4050][5500]\t Training Loss 0.0117\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [4100][5500]\t Training Loss 0.4602\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [4150][5500]\t Training Loss 0.0616\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [4200][5500]\t Training Loss 0.0609\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [4250][5500]\t Training Loss 0.1455\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4300][5500]\t Training Loss 0.9097\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4350][5500]\t Training Loss 0.0127\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [4400][5500]\t Training Loss 0.3277\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [4450][5500]\t Training Loss 0.2138\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4500][5500]\t Training Loss 0.3252\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4550][5500]\t Training Loss 0.1856\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4600][5500]\t Training Loss 0.2693\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4650][5500]\t Training Loss 0.0987\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4700][5500]\t Training Loss 0.1879\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4750][5500]\t Training Loss 0.9209\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [4800][5500]\t Training Loss 0.1123\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [4850][5500]\t Training Loss 0.4226\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4900][5500]\t Training Loss 0.1406\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [4950][5500]\t Training Loss 0.5766\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [5000][5500]\t Training Loss 0.1236\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [5050][5500]\t Training Loss 0.0398\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [5100][5500]\t Training Loss 0.0480\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [5150][5500]\t Training Loss 0.0091\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [5200][5500]\t Training Loss 0.4366\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [5250][5500]\t Training Loss 0.0157\t Accuracy 1.0000\n",
      "Epoch [7][10]\t Batch [5300][5500]\t Training Loss 0.1617\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [5350][5500]\t Training Loss 0.7474\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [5400][5500]\t Training Loss 0.7874\t Accuracy 0.8000\n",
      "Epoch [7][10]\t Batch [5450][5500]\t Training Loss 0.5316\t Accuracy 0.8000\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2744\t Average training accuracy 0.9238\n",
      "Epoch [7]\t Average validation loss 0.2278\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [8][10]\t Batch [0][5500]\t Training Loss 0.1318\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [50][5500]\t Training Loss 0.0967\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [100][5500]\t Training Loss 0.0947\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [150][5500]\t Training Loss 0.0933\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [200][5500]\t Training Loss 0.1742\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [250][5500]\t Training Loss 0.1375\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [300][5500]\t Training Loss 0.2379\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [350][5500]\t Training Loss 0.1805\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [400][5500]\t Training Loss 0.1465\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [450][5500]\t Training Loss 0.2519\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [500][5500]\t Training Loss 0.8417\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [550][5500]\t Training Loss 0.5592\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [600][5500]\t Training Loss 0.0366\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [650][5500]\t Training Loss 0.2873\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [700][5500]\t Training Loss 0.4233\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [750][5500]\t Training Loss 0.5198\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [800][5500]\t Training Loss 0.0966\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [850][5500]\t Training Loss 0.1583\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [900][5500]\t Training Loss 0.8908\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [950][5500]\t Training Loss 0.2134\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [1000][5500]\t Training Loss 0.2958\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [1050][5500]\t Training Loss 0.0656\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1100][5500]\t Training Loss 0.1294\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [1150][5500]\t Training Loss 0.0216\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1200][5500]\t Training Loss 0.5005\t Accuracy 0.7000\n",
      "Epoch [8][10]\t Batch [1250][5500]\t Training Loss 0.0459\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1300][5500]\t Training Loss 0.0254\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1350][5500]\t Training Loss 0.0270\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1400][5500]\t Training Loss 0.0805\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1450][5500]\t Training Loss 0.0884\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1500][5500]\t Training Loss 0.0377\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1550][5500]\t Training Loss 0.1874\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [1600][5500]\t Training Loss 0.4939\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [1650][5500]\t Training Loss 0.1296\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [1700][5500]\t Training Loss 0.0371\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1750][5500]\t Training Loss 0.0461\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1800][5500]\t Training Loss 0.0614\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1850][5500]\t Training Loss 0.0123\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [1900][5500]\t Training Loss 0.9297\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [1950][5500]\t Training Loss 0.2303\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [2000][5500]\t Training Loss 0.0437\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2050][5500]\t Training Loss 0.0267\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2100][5500]\t Training Loss 0.1229\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2150][5500]\t Training Loss 0.6637\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [2200][5500]\t Training Loss 0.0504\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2250][5500]\t Training Loss 0.0056\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2300][5500]\t Training Loss 0.3307\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [2350][5500]\t Training Loss 0.0561\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2400][5500]\t Training Loss 0.9209\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [2450][5500]\t Training Loss 0.0170\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2500][5500]\t Training Loss 0.0534\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2550][5500]\t Training Loss 0.3040\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [2600][5500]\t Training Loss 0.9699\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [2650][5500]\t Training Loss 0.1313\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [2700][5500]\t Training Loss 0.5331\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [2750][5500]\t Training Loss 0.1389\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [2800][5500]\t Training Loss 0.0628\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2850][5500]\t Training Loss 0.0925\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [2900][5500]\t Training Loss 1.0117\t Accuracy 0.7000\n",
      "Epoch [8][10]\t Batch [2950][5500]\t Training Loss 0.0294\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3000][5500]\t Training Loss 0.3025\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3050][5500]\t Training Loss 0.1328\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3100][5500]\t Training Loss 0.2955\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [3150][5500]\t Training Loss 0.1977\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3200][5500]\t Training Loss 0.0135\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3250][5500]\t Training Loss 1.0882\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [3300][5500]\t Training Loss 0.6272\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [3350][5500]\t Training Loss 0.1141\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [3400][5500]\t Training Loss 1.4223\t Accuracy 0.7000\n",
      "Epoch [8][10]\t Batch [3450][5500]\t Training Loss 0.1605\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [3500][5500]\t Training Loss 0.3167\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [3550][5500]\t Training Loss 0.0369\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3600][5500]\t Training Loss 0.7786\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [3650][5500]\t Training Loss 0.1114\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3700][5500]\t Training Loss 0.3432\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [3750][5500]\t Training Loss 0.0190\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3800][5500]\t Training Loss 0.0923\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [3850][5500]\t Training Loss 0.1097\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [3900][5500]\t Training Loss 0.4599\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [3950][5500]\t Training Loss 0.0870\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4000][5500]\t Training Loss 0.0615\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4050][5500]\t Training Loss 0.6882\t Accuracy 0.8000\n",
      "Epoch [8][10]\t Batch [4100][5500]\t Training Loss 0.0671\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4150][5500]\t Training Loss 0.0435\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4200][5500]\t Training Loss 0.3861\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [4250][5500]\t Training Loss 0.1477\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4300][5500]\t Training Loss 0.1819\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [4350][5500]\t Training Loss 0.0445\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4400][5500]\t Training Loss 0.1329\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4450][5500]\t Training Loss 0.2870\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [4500][5500]\t Training Loss 0.2807\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [4550][5500]\t Training Loss 0.2858\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [4600][5500]\t Training Loss 0.0858\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4650][5500]\t Training Loss 0.3241\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [4700][5500]\t Training Loss 0.0360\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4750][5500]\t Training Loss 0.1340\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [4800][5500]\t Training Loss 0.0611\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4850][5500]\t Training Loss 0.0935\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4900][5500]\t Training Loss 0.0625\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [4950][5500]\t Training Loss 0.2205\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [5000][5500]\t Training Loss 0.1770\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [5050][5500]\t Training Loss 0.0637\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [5100][5500]\t Training Loss 0.4158\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [5150][5500]\t Training Loss 0.3112\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [5200][5500]\t Training Loss 0.0515\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [5250][5500]\t Training Loss 0.2492\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [5300][5500]\t Training Loss 0.1168\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [5350][5500]\t Training Loss 0.3903\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [5400][5500]\t Training Loss 0.0769\t Accuracy 1.0000\n",
      "Epoch [8][10]\t Batch [5450][5500]\t Training Loss 0.1969\t Accuracy 0.9000\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2717\t Average training accuracy 0.9228\n",
      "Epoch [8]\t Average validation loss 0.2325\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [9][10]\t Batch [0][5500]\t Training Loss 0.1238\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [50][5500]\t Training Loss 0.1446\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [100][5500]\t Training Loss 0.5333\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [150][5500]\t Training Loss 0.0553\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [200][5500]\t Training Loss 0.0483\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [250][5500]\t Training Loss 0.3099\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [300][5500]\t Training Loss 0.0271\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [350][5500]\t Training Loss 0.5493\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [400][5500]\t Training Loss 0.1645\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [450][5500]\t Training Loss 0.1073\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [500][5500]\t Training Loss 0.0346\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [550][5500]\t Training Loss 0.8499\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [600][5500]\t Training Loss 0.3690\t Accuracy 0.7000\n",
      "Epoch [9][10]\t Batch [650][5500]\t Training Loss 0.0319\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [700][5500]\t Training Loss 0.3589\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [750][5500]\t Training Loss 0.6183\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [800][5500]\t Training Loss 0.1445\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [850][5500]\t Training Loss 0.1657\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [900][5500]\t Training Loss 0.0822\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [950][5500]\t Training Loss 0.1424\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [1000][5500]\t Training Loss 0.2675\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [1050][5500]\t Training Loss 0.1347\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [1100][5500]\t Training Loss 0.1468\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [1150][5500]\t Training Loss 2.0153\t Accuracy 0.6000\n",
      "Epoch [9][10]\t Batch [1200][5500]\t Training Loss 0.9505\t Accuracy 0.6000\n",
      "Epoch [9][10]\t Batch [1250][5500]\t Training Loss 0.1147\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [1300][5500]\t Training Loss 0.0784\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [1350][5500]\t Training Loss 0.4022\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [1400][5500]\t Training Loss 0.1225\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [1450][5500]\t Training Loss 0.1054\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [1500][5500]\t Training Loss 0.2723\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [1550][5500]\t Training Loss 0.8457\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [1600][5500]\t Training Loss 0.7249\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [1650][5500]\t Training Loss 0.1525\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [1700][5500]\t Training Loss 0.3636\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [1750][5500]\t Training Loss 0.6879\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [1800][5500]\t Training Loss 0.3529\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [1850][5500]\t Training Loss 0.0497\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [1900][5500]\t Training Loss 0.3498\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [1950][5500]\t Training Loss 1.0849\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [2000][5500]\t Training Loss 0.0706\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2050][5500]\t Training Loss 0.8872\t Accuracy 0.7000\n",
      "Epoch [9][10]\t Batch [2100][5500]\t Training Loss 0.0426\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2150][5500]\t Training Loss 0.1384\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [2200][5500]\t Training Loss 0.1785\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2250][5500]\t Training Loss 0.2367\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [2300][5500]\t Training Loss 0.3631\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [2350][5500]\t Training Loss 0.1114\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2400][5500]\t Training Loss 0.0846\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2450][5500]\t Training Loss 0.0247\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2500][5500]\t Training Loss 0.0534\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2550][5500]\t Training Loss 0.1684\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [2600][5500]\t Training Loss 0.0483\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2650][5500]\t Training Loss 0.2141\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [2700][5500]\t Training Loss 0.0957\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2750][5500]\t Training Loss 0.2588\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [2800][5500]\t Training Loss 0.0535\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [2850][5500]\t Training Loss 0.3392\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [2900][5500]\t Training Loss 0.4381\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [2950][5500]\t Training Loss 0.1546\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [3000][5500]\t Training Loss 0.0417\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [3050][5500]\t Training Loss 0.1035\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [3100][5500]\t Training Loss 0.1997\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [3150][5500]\t Training Loss 0.0304\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [3200][5500]\t Training Loss 0.0530\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [3250][5500]\t Training Loss 0.5600\t Accuracy 0.7000\n",
      "Epoch [9][10]\t Batch [3300][5500]\t Training Loss 0.3415\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [3350][5500]\t Training Loss 0.0924\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [3400][5500]\t Training Loss 0.5657\t Accuracy 0.7000\n",
      "Epoch [9][10]\t Batch [3450][5500]\t Training Loss 0.1434\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [3500][5500]\t Training Loss 0.0479\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [3550][5500]\t Training Loss 0.3704\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [3600][5500]\t Training Loss 0.6432\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [3650][5500]\t Training Loss 0.1363\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [3700][5500]\t Training Loss 0.1881\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [3750][5500]\t Training Loss 1.7780\t Accuracy 0.7000\n",
      "Epoch [9][10]\t Batch [3800][5500]\t Training Loss 0.2740\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [3850][5500]\t Training Loss 0.1935\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [3900][5500]\t Training Loss 0.6168\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [3950][5500]\t Training Loss 1.0113\t Accuracy 0.7000\n",
      "Epoch [9][10]\t Batch [4000][5500]\t Training Loss 0.1126\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [4050][5500]\t Training Loss 0.2602\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [4100][5500]\t Training Loss 0.0996\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4150][5500]\t Training Loss 0.0584\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4200][5500]\t Training Loss 0.1817\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4250][5500]\t Training Loss 0.0679\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4300][5500]\t Training Loss 0.1008\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4350][5500]\t Training Loss 0.1811\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [4400][5500]\t Training Loss 0.0207\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4450][5500]\t Training Loss 0.0243\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4500][5500]\t Training Loss 0.0533\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4550][5500]\t Training Loss 0.0281\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4600][5500]\t Training Loss 0.5637\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [4650][5500]\t Training Loss 0.0273\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4700][5500]\t Training Loss 0.1615\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [4750][5500]\t Training Loss 0.1530\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [4800][5500]\t Training Loss 0.8518\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [4850][5500]\t Training Loss 0.0690\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4900][5500]\t Training Loss 0.0777\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [4950][5500]\t Training Loss 0.0690\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [5000][5500]\t Training Loss 0.2814\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [5050][5500]\t Training Loss 0.3336\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [5100][5500]\t Training Loss 0.6620\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [5150][5500]\t Training Loss 0.4219\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [5200][5500]\t Training Loss 0.1655\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [5250][5500]\t Training Loss 1.1647\t Accuracy 0.8000\n",
      "Epoch [9][10]\t Batch [5300][5500]\t Training Loss 0.0349\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [5350][5500]\t Training Loss 0.0816\t Accuracy 1.0000\n",
      "Epoch [9][10]\t Batch [5400][5500]\t Training Loss 0.1903\t Accuracy 0.9000\n",
      "Epoch [9][10]\t Batch [5450][5500]\t Training Loss 0.1413\t Accuracy 0.9000\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2694\t Average training accuracy 0.9243\n",
      "Epoch [9]\t Average validation loss 0.2238\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.3365\t Accuracy 0.1300\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 0.8338\t Accuracy 0.7700\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 0.5614\t Accuracy 0.8900\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.5253\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.5118\t Accuracy 0.8600\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.3733\t Accuracy 0.9000\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.5917\t Accuracy 0.8100\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.4864\t Accuracy 0.8900\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.4181\t Accuracy 0.8800\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.3866\t Accuracy 0.9100\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.3537\t Accuracy 0.9200\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5768\t Average training accuracy 0.8428\n",
      "Epoch [0]\t Average validation loss 0.3177\t Average validation accuracy 0.9168\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.3516\t Accuracy 0.9100\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.3411\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.2566\t Accuracy 0.9400\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.4048\t Accuracy 0.8400\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.4314\t Accuracy 0.8900\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.3712\t Accuracy 0.9000\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.2449\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.2805\t Accuracy 0.9300\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.4257\t Accuracy 0.8700\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.3455\t Accuracy 0.9200\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.4612\t Accuracy 0.8800\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3737\t Average training accuracy 0.8965\n",
      "Epoch [1]\t Average validation loss 0.2815\t Average validation accuracy 0.9232\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.3879\t Accuracy 0.9000\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.4581\t Accuracy 0.8500\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.3957\t Accuracy 0.8900\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.3112\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.4554\t Accuracy 0.8600\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.3957\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.4230\t Accuracy 0.8700\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.4020\t Accuracy 0.9100\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.3384\t Accuracy 0.9200\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.2915\t Accuracy 0.9400\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.1867\t Accuracy 0.9500\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3433\t Average training accuracy 0.9043\n",
      "Epoch [2]\t Average validation loss 0.2651\t Average validation accuracy 0.9282\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.1492\t Accuracy 0.9700\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.2124\t Accuracy 0.9300\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.2615\t Accuracy 0.9200\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.4954\t Accuracy 0.8800\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.3116\t Accuracy 0.8800\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.2292\t Accuracy 0.9400\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.2072\t Accuracy 0.9500\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.3871\t Accuracy 0.9000\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.2504\t Accuracy 0.9400\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3674\t Accuracy 0.9100\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.3281\t Accuracy 0.9000\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3273\t Average training accuracy 0.9087\n",
      "Epoch [3]\t Average validation loss 0.2562\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.3644\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.1775\t Accuracy 0.9500\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.3910\t Accuracy 0.8900\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.3771\t Accuracy 0.8700\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.2495\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.3561\t Accuracy 0.9400\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.2468\t Accuracy 0.9100\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.3655\t Accuracy 0.8800\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.3146\t Accuracy 0.9200\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.2558\t Accuracy 0.9300\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.2848\t Accuracy 0.9400\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3171\t Average training accuracy 0.9115\n",
      "Epoch [4]\t Average validation loss 0.2501\t Average validation accuracy 0.9294\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.3281\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.3043\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.3319\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.2858\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.1867\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.2369\t Accuracy 0.9400\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.3605\t Accuracy 0.9100\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.3936\t Accuracy 0.9300\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.3388\t Accuracy 0.9000\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.2896\t Accuracy 0.9200\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.3544\t Accuracy 0.9300\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3100\t Average training accuracy 0.9136\n",
      "Epoch [5]\t Average validation loss 0.2458\t Average validation accuracy 0.9302\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.3720\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.3434\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.1722\t Accuracy 0.9600\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.3701\t Accuracy 0.9100\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.2698\t Accuracy 0.9300\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.2449\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.3518\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.2390\t Accuracy 0.9200\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.2514\t Accuracy 0.9500\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.1494\t Accuracy 0.9600\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.2685\t Accuracy 0.9200\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3042\t Average training accuracy 0.9149\n",
      "Epoch [6]\t Average validation loss 0.2424\t Average validation accuracy 0.9316\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.3681\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.3172\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.3315\t Accuracy 0.9000\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.2233\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.2880\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.2550\t Accuracy 0.9300\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.3145\t Accuracy 0.9100\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.2818\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.2466\t Accuracy 0.9200\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.2162\t Accuracy 0.9500\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.1832\t Accuracy 0.9500\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2996\t Average training accuracy 0.9170\n",
      "Epoch [7]\t Average validation loss 0.2411\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.2284\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.2041\t Accuracy 0.9600\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.2154\t Accuracy 0.9400\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.3922\t Accuracy 0.8800\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.2964\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.3833\t Accuracy 0.9100\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.2753\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.1935\t Accuracy 0.9700\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.3138\t Accuracy 0.9200\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.2540\t Accuracy 0.9000\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.2358\t Accuracy 0.9400\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2959\t Average training accuracy 0.9172\n",
      "Epoch [8]\t Average validation loss 0.2368\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.2490\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.2779\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.3111\t Accuracy 0.9500\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.4469\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.2059\t Accuracy 0.9500\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.2898\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.3902\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.2746\t Accuracy 0.9100\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.2981\t Accuracy 0.9300\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.2330\t Accuracy 0.9500\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.2836\t Accuracy 0.9100\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2928\t Average training accuracy 0.9183\n",
      "Epoch [9]\t Average validation loss 0.2350\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [0][10]\t Batch [0][55]\t Training Loss 2.4620\t Accuracy 0.0850\n",
      "Epoch [0][10]\t Batch [50][55]\t Training Loss 0.8524\t Accuracy 0.7900\n",
      "\n",
      "Epoch [0]\t Average training loss 1.4031\t Average training accuracy 0.5971\n",
      "Epoch [0]\t Average validation loss 0.6991\t Average validation accuracy 0.8522\n",
      "\n",
      "Epoch [1][10]\t Batch [0][55]\t Training Loss 0.8294\t Accuracy 0.7940\n",
      "Epoch [1][10]\t Batch [50][55]\t Training Loss 0.6247\t Accuracy 0.8420\n",
      "\n",
      "Epoch [1]\t Average training loss 0.6831\t Average training accuracy 0.8295\n",
      "Epoch [1]\t Average validation loss 0.5045\t Average validation accuracy 0.8864\n",
      "\n",
      "Epoch [2][10]\t Batch [0][55]\t Training Loss 0.6204\t Accuracy 0.8470\n",
      "Epoch [2][10]\t Batch [50][55]\t Training Loss 0.5000\t Accuracy 0.8790\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5658\t Average training accuracy 0.8552\n",
      "Epoch [2]\t Average validation loss 0.4350\t Average validation accuracy 0.8968\n",
      "\n",
      "Epoch [3][10]\t Batch [0][55]\t Training Loss 0.5116\t Accuracy 0.8640\n",
      "Epoch [3][10]\t Batch [50][55]\t Training Loss 0.4791\t Accuracy 0.8730\n",
      "\n",
      "Epoch [3]\t Average training loss 0.5110\t Average training accuracy 0.8667\n",
      "Epoch [3]\t Average validation loss 0.3970\t Average validation accuracy 0.9050\n",
      "\n",
      "Epoch [4][10]\t Batch [0][55]\t Training Loss 0.4972\t Accuracy 0.8660\n",
      "Epoch [4][10]\t Batch [50][55]\t Training Loss 0.4546\t Accuracy 0.8780\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4775\t Average training accuracy 0.8729\n",
      "Epoch [4]\t Average validation loss 0.3725\t Average validation accuracy 0.9100\n",
      "\n",
      "Epoch [5][10]\t Batch [0][55]\t Training Loss 0.4595\t Accuracy 0.8870\n",
      "Epoch [5][10]\t Batch [50][55]\t Training Loss 0.4359\t Accuracy 0.8940\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4540\t Average training accuracy 0.8785\n",
      "Epoch [5]\t Average validation loss 0.3549\t Average validation accuracy 0.9126\n",
      "\n",
      "Epoch [6][10]\t Batch [0][55]\t Training Loss 0.4304\t Accuracy 0.8850\n",
      "Epoch [6][10]\t Batch [50][55]\t Training Loss 0.4297\t Accuracy 0.8900\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4366\t Average training accuracy 0.8824\n",
      "Epoch [6]\t Average validation loss 0.3419\t Average validation accuracy 0.9148\n",
      "\n",
      "Epoch [7][10]\t Batch [0][55]\t Training Loss 0.4201\t Accuracy 0.8900\n",
      "Epoch [7][10]\t Batch [50][55]\t Training Loss 0.4532\t Accuracy 0.8600\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4230\t Average training accuracy 0.8854\n",
      "Epoch [7]\t Average validation loss 0.3315\t Average validation accuracy 0.9154\n",
      "\n",
      "Epoch [8][10]\t Batch [0][55]\t Training Loss 0.4149\t Accuracy 0.8850\n",
      "Epoch [8][10]\t Batch [50][55]\t Training Loss 0.4374\t Accuracy 0.8890\n",
      "\n",
      "Epoch [8]\t Average training loss 0.4119\t Average training accuracy 0.8877\n",
      "Epoch [8]\t Average validation loss 0.3232\t Average validation accuracy 0.9172\n",
      "\n",
      "Epoch [9][10]\t Batch [0][55]\t Training Loss 0.4224\t Accuracy 0.8780\n",
      "Epoch [9][10]\t Batch [50][55]\t Training Loss 0.3606\t Accuracy 0.8990\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4026\t Average training accuracy 0.8902\n",
      "Epoch [9]\t Average validation loss 0.3164\t Average validation accuracy 0.9178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train with different batch_size\n",
    "\n",
    "batch_sizes = [10, 100, 1000]\n",
    "loss6, acc6 = [], []\n",
    "for size in batch_sizes:\n",
    "    cfg = {\n",
    "        'data_root': 'data',\n",
    "        'max_epoch': 10,\n",
    "        'batch_size': size,\n",
    "        'learning_rate': 0.01,\n",
    "        'momentum': 0.9,\n",
    "        'display_freq': 50,\n",
    "    }\n",
    "\n",
    "    runner = Solver(cfg)\n",
    "    loss, acc = runner.train()\n",
    "    loss6.append(loss)\n",
    "    acc6.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({\n",
    "    \"batch_size=10\": [loss6[0], acc6[0]],\n",
    "    \"batch_size=100\": [loss6[1], acc6[1]],\n",
    "    \"batch_size=1000\": [loss6[2], acc6[2]],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][100]\t Batch [0][550]\t Training Loss 2.6707\t Accuracy 0.0400\n",
      "Epoch [0][100]\t Batch [50][550]\t Training Loss 0.8267\t Accuracy 0.7800\n",
      "Epoch [0][100]\t Batch [100][550]\t Training Loss 0.6069\t Accuracy 0.8500\n",
      "Epoch [0][100]\t Batch [150][550]\t Training Loss 0.6902\t Accuracy 0.8200\n",
      "Epoch [0][100]\t Batch [200][550]\t Training Loss 0.4599\t Accuracy 0.9200\n",
      "Epoch [0][100]\t Batch [250][550]\t Training Loss 0.4870\t Accuracy 0.8600\n",
      "Epoch [0][100]\t Batch [300][550]\t Training Loss 0.4930\t Accuracy 0.8400\n",
      "Epoch [0][100]\t Batch [350][550]\t Training Loss 0.3985\t Accuracy 0.8700\n",
      "Epoch [0][100]\t Batch [400][550]\t Training Loss 0.3277\t Accuracy 0.9400\n",
      "Epoch [0][100]\t Batch [450][550]\t Training Loss 0.5044\t Accuracy 0.8700\n",
      "Epoch [0][100]\t Batch [500][550]\t Training Loss 0.4172\t Accuracy 0.9100\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5870\t Average training accuracy 0.8384\n",
      "Epoch [0]\t Average validation loss 0.3177\t Average validation accuracy 0.9166\n",
      "\n",
      "Epoch [1][100]\t Batch [0][550]\t Training Loss 0.4164\t Accuracy 0.8600\n",
      "Epoch [1][100]\t Batch [50][550]\t Training Loss 0.3970\t Accuracy 0.8900\n",
      "Epoch [1][100]\t Batch [100][550]\t Training Loss 0.3554\t Accuracy 0.9000\n",
      "Epoch [1][100]\t Batch [150][550]\t Training Loss 0.2587\t Accuracy 0.9200\n",
      "Epoch [1][100]\t Batch [200][550]\t Training Loss 0.5517\t Accuracy 0.8300\n",
      "Epoch [1][100]\t Batch [250][550]\t Training Loss 0.3344\t Accuracy 0.9200\n",
      "Epoch [1][100]\t Batch [300][550]\t Training Loss 0.3050\t Accuracy 0.9300\n",
      "Epoch [1][100]\t Batch [350][550]\t Training Loss 0.3154\t Accuracy 0.8900\n",
      "Epoch [1][100]\t Batch [400][550]\t Training Loss 0.3649\t Accuracy 0.8700\n",
      "Epoch [1][100]\t Batch [450][550]\t Training Loss 0.4400\t Accuracy 0.8600\n",
      "Epoch [1][100]\t Batch [500][550]\t Training Loss 0.2711\t Accuracy 0.9300\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3736\t Average training accuracy 0.8955\n",
      "Epoch [1]\t Average validation loss 0.2792\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [2][100]\t Batch [0][550]\t Training Loss 0.3493\t Accuracy 0.9000\n",
      "Epoch [2][100]\t Batch [50][550]\t Training Loss 0.2599\t Accuracy 0.9200\n",
      "Epoch [2][100]\t Batch [100][550]\t Training Loss 0.3001\t Accuracy 0.9300\n",
      "Epoch [2][100]\t Batch [150][550]\t Training Loss 0.3323\t Accuracy 0.8800\n",
      "Epoch [2][100]\t Batch [200][550]\t Training Loss 0.4633\t Accuracy 0.8900\n",
      "Epoch [2][100]\t Batch [250][550]\t Training Loss 0.2476\t Accuracy 0.9300\n",
      "Epoch [2][100]\t Batch [300][550]\t Training Loss 0.3890\t Accuracy 0.8700\n",
      "Epoch [2][100]\t Batch [350][550]\t Training Loss 0.3139\t Accuracy 0.9300\n",
      "Epoch [2][100]\t Batch [400][550]\t Training Loss 0.2557\t Accuracy 0.9300\n",
      "Epoch [2][100]\t Batch [450][550]\t Training Loss 0.1952\t Accuracy 0.9600\n",
      "Epoch [2][100]\t Batch [500][550]\t Training Loss 0.3313\t Accuracy 0.9000\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3434\t Average training accuracy 0.9041\n",
      "Epoch [2]\t Average validation loss 0.2657\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [3][100]\t Batch [0][550]\t Training Loss 0.4689\t Accuracy 0.8800\n",
      "Epoch [3][100]\t Batch [50][550]\t Training Loss 0.2579\t Accuracy 0.9000\n",
      "Epoch [3][100]\t Batch [100][550]\t Training Loss 0.3377\t Accuracy 0.9100\n",
      "Epoch [3][100]\t Batch [150][550]\t Training Loss 0.3669\t Accuracy 0.9100\n",
      "Epoch [3][100]\t Batch [200][550]\t Training Loss 0.1954\t Accuracy 0.9400\n",
      "Epoch [3][100]\t Batch [250][550]\t Training Loss 0.2742\t Accuracy 0.9100\n",
      "Epoch [3][100]\t Batch [300][550]\t Training Loss 0.4478\t Accuracy 0.8900\n",
      "Epoch [3][100]\t Batch [350][550]\t Training Loss 0.2869\t Accuracy 0.9200\n",
      "Epoch [3][100]\t Batch [400][550]\t Training Loss 0.3042\t Accuracy 0.9100\n",
      "Epoch [3][100]\t Batch [450][550]\t Training Loss 0.2898\t Accuracy 0.9200\n",
      "Epoch [3][100]\t Batch [500][550]\t Training Loss 0.3030\t Accuracy 0.9500\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3273\t Average training accuracy 0.9090\n",
      "Epoch [3]\t Average validation loss 0.2551\t Average validation accuracy 0.9282\n",
      "\n",
      "Epoch [4][100]\t Batch [0][550]\t Training Loss 0.3374\t Accuracy 0.9100\n",
      "Epoch [4][100]\t Batch [50][550]\t Training Loss 0.2549\t Accuracy 0.9400\n",
      "Epoch [4][100]\t Batch [100][550]\t Training Loss 0.5773\t Accuracy 0.8600\n",
      "Epoch [4][100]\t Batch [150][550]\t Training Loss 0.1960\t Accuracy 0.9600\n",
      "Epoch [4][100]\t Batch [200][550]\t Training Loss 0.3111\t Accuracy 0.9200\n",
      "Epoch [4][100]\t Batch [250][550]\t Training Loss 0.4923\t Accuracy 0.9100\n",
      "Epoch [4][100]\t Batch [300][550]\t Training Loss 0.3046\t Accuracy 0.9000\n",
      "Epoch [4][100]\t Batch [350][550]\t Training Loss 0.3255\t Accuracy 0.9000\n",
      "Epoch [4][100]\t Batch [400][550]\t Training Loss 0.3812\t Accuracy 0.8700\n",
      "Epoch [4][100]\t Batch [450][550]\t Training Loss 0.3950\t Accuracy 0.8800\n",
      "Epoch [4][100]\t Batch [500][550]\t Training Loss 0.2450\t Accuracy 0.9200\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3170\t Average training accuracy 0.9116\n",
      "Epoch [4]\t Average validation loss 0.2503\t Average validation accuracy 0.9308\n",
      "\n",
      "Epoch [5][100]\t Batch [0][550]\t Training Loss 0.2886\t Accuracy 0.9300\n",
      "Epoch [5][100]\t Batch [50][550]\t Training Loss 0.5299\t Accuracy 0.8600\n",
      "Epoch [5][100]\t Batch [100][550]\t Training Loss 0.2226\t Accuracy 0.9200\n",
      "Epoch [5][100]\t Batch [150][550]\t Training Loss 0.3978\t Accuracy 0.8900\n",
      "Epoch [5][100]\t Batch [200][550]\t Training Loss 0.1815\t Accuracy 0.9600\n",
      "Epoch [5][100]\t Batch [250][550]\t Training Loss 0.2704\t Accuracy 0.9200\n",
      "Epoch [5][100]\t Batch [300][550]\t Training Loss 0.3174\t Accuracy 0.9000\n",
      "Epoch [5][100]\t Batch [350][550]\t Training Loss 0.3322\t Accuracy 0.9100\n",
      "Epoch [5][100]\t Batch [400][550]\t Training Loss 0.3834\t Accuracy 0.9100\n",
      "Epoch [5][100]\t Batch [450][550]\t Training Loss 0.2721\t Accuracy 0.9100\n",
      "Epoch [5][100]\t Batch [500][550]\t Training Loss 0.2502\t Accuracy 0.9400\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3101\t Average training accuracy 0.9137\n",
      "Epoch [5]\t Average validation loss 0.2457\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [6][100]\t Batch [0][550]\t Training Loss 0.2655\t Accuracy 0.9100\n",
      "Epoch [6][100]\t Batch [50][550]\t Training Loss 0.2617\t Accuracy 0.9400\n",
      "Epoch [6][100]\t Batch [100][550]\t Training Loss 0.3658\t Accuracy 0.8800\n",
      "Epoch [6][100]\t Batch [150][550]\t Training Loss 0.1772\t Accuracy 0.9600\n",
      "Epoch [6][100]\t Batch [200][550]\t Training Loss 0.2732\t Accuracy 0.9300\n",
      "Epoch [6][100]\t Batch [250][550]\t Training Loss 0.2970\t Accuracy 0.9000\n",
      "Epoch [6][100]\t Batch [300][550]\t Training Loss 0.2807\t Accuracy 0.9200\n",
      "Epoch [6][100]\t Batch [350][550]\t Training Loss 0.3124\t Accuracy 0.9100\n",
      "Epoch [6][100]\t Batch [400][550]\t Training Loss 0.2115\t Accuracy 0.9300\n",
      "Epoch [6][100]\t Batch [450][550]\t Training Loss 0.5680\t Accuracy 0.8900\n",
      "Epoch [6][100]\t Batch [500][550]\t Training Loss 0.3108\t Accuracy 0.8900\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3041\t Average training accuracy 0.9150\n",
      "Epoch [6]\t Average validation loss 0.2428\t Average validation accuracy 0.9322\n",
      "\n",
      "Epoch [7][100]\t Batch [0][550]\t Training Loss 0.2590\t Accuracy 0.9000\n",
      "Epoch [7][100]\t Batch [50][550]\t Training Loss 0.2796\t Accuracy 0.9100\n",
      "Epoch [7][100]\t Batch [100][550]\t Training Loss 0.3025\t Accuracy 0.9000\n",
      "Epoch [7][100]\t Batch [150][550]\t Training Loss 0.2759\t Accuracy 0.9200\n",
      "Epoch [7][100]\t Batch [200][550]\t Training Loss 0.2808\t Accuracy 0.9200\n",
      "Epoch [7][100]\t Batch [250][550]\t Training Loss 0.4695\t Accuracy 0.8700\n",
      "Epoch [7][100]\t Batch [300][550]\t Training Loss 0.3896\t Accuracy 0.9300\n",
      "Epoch [7][100]\t Batch [350][550]\t Training Loss 0.3426\t Accuracy 0.9100\n",
      "Epoch [7][100]\t Batch [400][550]\t Training Loss 0.3684\t Accuracy 0.9100\n",
      "Epoch [7][100]\t Batch [450][550]\t Training Loss 0.2279\t Accuracy 0.9500\n",
      "Epoch [7][100]\t Batch [500][550]\t Training Loss 0.2118\t Accuracy 0.9300\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2996\t Average training accuracy 0.9165\n",
      "Epoch [7]\t Average validation loss 0.2407\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [8][100]\t Batch [0][550]\t Training Loss 0.2182\t Accuracy 0.9200\n",
      "Epoch [8][100]\t Batch [50][550]\t Training Loss 0.3619\t Accuracy 0.9500\n",
      "Epoch [8][100]\t Batch [100][550]\t Training Loss 0.3562\t Accuracy 0.8800\n",
      "Epoch [8][100]\t Batch [150][550]\t Training Loss 0.3574\t Accuracy 0.8600\n",
      "Epoch [8][100]\t Batch [200][550]\t Training Loss 0.4107\t Accuracy 0.8900\n",
      "Epoch [8][100]\t Batch [250][550]\t Training Loss 0.3653\t Accuracy 0.8800\n",
      "Epoch [8][100]\t Batch [300][550]\t Training Loss 0.2663\t Accuracy 0.9200\n",
      "Epoch [8][100]\t Batch [350][550]\t Training Loss 0.2585\t Accuracy 0.9300\n",
      "Epoch [8][100]\t Batch [400][550]\t Training Loss 0.4974\t Accuracy 0.8500\n",
      "Epoch [8][100]\t Batch [450][550]\t Training Loss 0.3393\t Accuracy 0.8800\n",
      "Epoch [8][100]\t Batch [500][550]\t Training Loss 0.3914\t Accuracy 0.8700\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2959\t Average training accuracy 0.9175\n",
      "Epoch [8]\t Average validation loss 0.2380\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [9][100]\t Batch [0][550]\t Training Loss 0.2809\t Accuracy 0.9300\n",
      "Epoch [9][100]\t Batch [50][550]\t Training Loss 0.3173\t Accuracy 0.9300\n",
      "Epoch [9][100]\t Batch [100][550]\t Training Loss 0.3752\t Accuracy 0.8600\n",
      "Epoch [9][100]\t Batch [150][550]\t Training Loss 0.4681\t Accuracy 0.8700\n",
      "Epoch [9][100]\t Batch [200][550]\t Training Loss 0.3538\t Accuracy 0.8900\n",
      "Epoch [9][100]\t Batch [250][550]\t Training Loss 0.2448\t Accuracy 0.9100\n",
      "Epoch [9][100]\t Batch [300][550]\t Training Loss 0.3050\t Accuracy 0.9400\n",
      "Epoch [9][100]\t Batch [350][550]\t Training Loss 0.1616\t Accuracy 0.9600\n",
      "Epoch [9][100]\t Batch [400][550]\t Training Loss 0.3385\t Accuracy 0.9200\n",
      "Epoch [9][100]\t Batch [450][550]\t Training Loss 0.2877\t Accuracy 0.9300\n",
      "Epoch [9][100]\t Batch [500][550]\t Training Loss 0.3263\t Accuracy 0.9500\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2927\t Average training accuracy 0.9187\n",
      "Epoch [9]\t Average validation loss 0.2356\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [10][100]\t Batch [0][550]\t Training Loss 0.3961\t Accuracy 0.8600\n",
      "Epoch [10][100]\t Batch [50][550]\t Training Loss 0.2162\t Accuracy 0.9000\n",
      "Epoch [10][100]\t Batch [100][550]\t Training Loss 0.2168\t Accuracy 0.9400\n",
      "Epoch [10][100]\t Batch [150][550]\t Training Loss 0.4031\t Accuracy 0.8800\n",
      "Epoch [10][100]\t Batch [200][550]\t Training Loss 0.2674\t Accuracy 0.9400\n",
      "Epoch [10][100]\t Batch [250][550]\t Training Loss 0.2875\t Accuracy 0.9000\n",
      "Epoch [10][100]\t Batch [300][550]\t Training Loss 0.2336\t Accuracy 0.9600\n",
      "Epoch [10][100]\t Batch [350][550]\t Training Loss 0.2384\t Accuracy 0.9400\n",
      "Epoch [10][100]\t Batch [400][550]\t Training Loss 0.2744\t Accuracy 0.9000\n",
      "Epoch [10][100]\t Batch [450][550]\t Training Loss 0.2890\t Accuracy 0.9300\n",
      "Epoch [10][100]\t Batch [500][550]\t Training Loss 0.3494\t Accuracy 0.9100\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2899\t Average training accuracy 0.9192\n",
      "Epoch [10]\t Average validation loss 0.2348\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [11][100]\t Batch [0][550]\t Training Loss 0.2311\t Accuracy 0.9300\n",
      "Epoch [11][100]\t Batch [50][550]\t Training Loss 0.2025\t Accuracy 0.9800\n",
      "Epoch [11][100]\t Batch [100][550]\t Training Loss 0.3214\t Accuracy 0.9300\n",
      "Epoch [11][100]\t Batch [150][550]\t Training Loss 0.2845\t Accuracy 0.9300\n",
      "Epoch [11][100]\t Batch [200][550]\t Training Loss 0.3716\t Accuracy 0.9000\n",
      "Epoch [11][100]\t Batch [250][550]\t Training Loss 0.3260\t Accuracy 0.9200\n",
      "Epoch [11][100]\t Batch [300][550]\t Training Loss 0.4610\t Accuracy 0.8800\n",
      "Epoch [11][100]\t Batch [350][550]\t Training Loss 0.5832\t Accuracy 0.8800\n",
      "Epoch [11][100]\t Batch [400][550]\t Training Loss 0.2099\t Accuracy 0.9400\n",
      "Epoch [11][100]\t Batch [450][550]\t Training Loss 0.2617\t Accuracy 0.9200\n",
      "Epoch [11][100]\t Batch [500][550]\t Training Loss 0.3774\t Accuracy 0.8900\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2874\t Average training accuracy 0.9199\n",
      "Epoch [11]\t Average validation loss 0.2320\t Average validation accuracy 0.9368\n",
      "\n",
      "Epoch [12][100]\t Batch [0][550]\t Training Loss 0.2958\t Accuracy 0.9000\n",
      "Epoch [12][100]\t Batch [50][550]\t Training Loss 0.3287\t Accuracy 0.8600\n",
      "Epoch [12][100]\t Batch [100][550]\t Training Loss 0.1796\t Accuracy 0.9600\n",
      "Epoch [12][100]\t Batch [150][550]\t Training Loss 0.2004\t Accuracy 0.9400\n",
      "Epoch [12][100]\t Batch [200][550]\t Training Loss 0.2842\t Accuracy 0.9100\n",
      "Epoch [12][100]\t Batch [250][550]\t Training Loss 0.4177\t Accuracy 0.8900\n",
      "Epoch [12][100]\t Batch [300][550]\t Training Loss 0.2541\t Accuracy 0.9400\n",
      "Epoch [12][100]\t Batch [350][550]\t Training Loss 0.2476\t Accuracy 0.9400\n",
      "Epoch [12][100]\t Batch [400][550]\t Training Loss 0.2720\t Accuracy 0.9200\n",
      "Epoch [12][100]\t Batch [450][550]\t Training Loss 0.2612\t Accuracy 0.9400\n",
      "Epoch [12][100]\t Batch [500][550]\t Training Loss 0.2894\t Accuracy 0.8800\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2856\t Average training accuracy 0.9201\n",
      "Epoch [12]\t Average validation loss 0.2322\t Average validation accuracy 0.9368\n",
      "\n",
      "Epoch [13][100]\t Batch [0][550]\t Training Loss 0.3031\t Accuracy 0.9200\n",
      "Epoch [13][100]\t Batch [50][550]\t Training Loss 0.2015\t Accuracy 0.9500\n",
      "Epoch [13][100]\t Batch [100][550]\t Training Loss 0.3082\t Accuracy 0.9100\n",
      "Epoch [13][100]\t Batch [150][550]\t Training Loss 0.3062\t Accuracy 0.9400\n",
      "Epoch [13][100]\t Batch [200][550]\t Training Loss 0.1538\t Accuracy 0.9700\n",
      "Epoch [13][100]\t Batch [250][550]\t Training Loss 0.1715\t Accuracy 0.9700\n",
      "Epoch [13][100]\t Batch [300][550]\t Training Loss 0.3278\t Accuracy 0.9300\n",
      "Epoch [13][100]\t Batch [350][550]\t Training Loss 0.5548\t Accuracy 0.8700\n",
      "Epoch [13][100]\t Batch [400][550]\t Training Loss 0.5277\t Accuracy 0.8500\n",
      "Epoch [13][100]\t Batch [450][550]\t Training Loss 0.2541\t Accuracy 0.9200\n",
      "Epoch [13][100]\t Batch [500][550]\t Training Loss 0.2726\t Accuracy 0.9300\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2837\t Average training accuracy 0.9211\n",
      "Epoch [13]\t Average validation loss 0.2305\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [14][100]\t Batch [0][550]\t Training Loss 0.2136\t Accuracy 0.9800\n",
      "Epoch [14][100]\t Batch [50][550]\t Training Loss 0.3634\t Accuracy 0.9100\n",
      "Epoch [14][100]\t Batch [100][550]\t Training Loss 0.3092\t Accuracy 0.8900\n",
      "Epoch [14][100]\t Batch [150][550]\t Training Loss 0.5125\t Accuracy 0.8600\n",
      "Epoch [14][100]\t Batch [200][550]\t Training Loss 0.3479\t Accuracy 0.9100\n",
      "Epoch [14][100]\t Batch [250][550]\t Training Loss 0.3477\t Accuracy 0.8700\n",
      "Epoch [14][100]\t Batch [300][550]\t Training Loss 0.2368\t Accuracy 0.9300\n",
      "Epoch [14][100]\t Batch [350][550]\t Training Loss 0.1716\t Accuracy 0.9700\n",
      "Epoch [14][100]\t Batch [400][550]\t Training Loss 0.2338\t Accuracy 0.9300\n",
      "Epoch [14][100]\t Batch [450][550]\t Training Loss 0.1935\t Accuracy 0.9400\n",
      "Epoch [14][100]\t Batch [500][550]\t Training Loss 0.2844\t Accuracy 0.9200\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2820\t Average training accuracy 0.9212\n",
      "Epoch [14]\t Average validation loss 0.2304\t Average validation accuracy 0.9370\n",
      "\n",
      "Epoch [15][100]\t Batch [0][550]\t Training Loss 0.3593\t Accuracy 0.9300\n",
      "Epoch [15][100]\t Batch [50][550]\t Training Loss 0.3911\t Accuracy 0.8900\n",
      "Epoch [15][100]\t Batch [100][550]\t Training Loss 0.2257\t Accuracy 0.9200\n",
      "Epoch [15][100]\t Batch [150][550]\t Training Loss 0.1698\t Accuracy 0.9500\n",
      "Epoch [15][100]\t Batch [200][550]\t Training Loss 0.1588\t Accuracy 0.9500\n",
      "Epoch [15][100]\t Batch [250][550]\t Training Loss 0.2563\t Accuracy 0.9100\n",
      "Epoch [15][100]\t Batch [300][550]\t Training Loss 0.2252\t Accuracy 0.9100\n",
      "Epoch [15][100]\t Batch [350][550]\t Training Loss 0.2026\t Accuracy 0.9500\n",
      "Epoch [15][100]\t Batch [400][550]\t Training Loss 0.2603\t Accuracy 0.9000\n",
      "Epoch [15][100]\t Batch [450][550]\t Training Loss 0.1740\t Accuracy 0.9500\n",
      "Epoch [15][100]\t Batch [500][550]\t Training Loss 0.2297\t Accuracy 0.9300\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2804\t Average training accuracy 0.9221\n",
      "Epoch [15]\t Average validation loss 0.2284\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [16][100]\t Batch [0][550]\t Training Loss 0.3109\t Accuracy 0.9400\n",
      "Epoch [16][100]\t Batch [50][550]\t Training Loss 0.3408\t Accuracy 0.9000\n",
      "Epoch [16][100]\t Batch [100][550]\t Training Loss 0.2630\t Accuracy 0.9100\n",
      "Epoch [16][100]\t Batch [150][550]\t Training Loss 0.2537\t Accuracy 0.9300\n",
      "Epoch [16][100]\t Batch [200][550]\t Training Loss 0.1818\t Accuracy 0.9400\n",
      "Epoch [16][100]\t Batch [250][550]\t Training Loss 0.3266\t Accuracy 0.9200\n",
      "Epoch [16][100]\t Batch [300][550]\t Training Loss 0.3435\t Accuracy 0.8900\n",
      "Epoch [16][100]\t Batch [350][550]\t Training Loss 0.1837\t Accuracy 0.9700\n",
      "Epoch [16][100]\t Batch [400][550]\t Training Loss 0.3002\t Accuracy 0.9200\n",
      "Epoch [16][100]\t Batch [450][550]\t Training Loss 0.4660\t Accuracy 0.9000\n",
      "Epoch [16][100]\t Batch [500][550]\t Training Loss 0.4774\t Accuracy 0.8700\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2790\t Average training accuracy 0.9220\n",
      "Epoch [16]\t Average validation loss 0.2280\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [17][100]\t Batch [0][550]\t Training Loss 0.2334\t Accuracy 0.9500\n",
      "Epoch [17][100]\t Batch [50][550]\t Training Loss 0.2818\t Accuracy 0.9300\n",
      "Epoch [17][100]\t Batch [100][550]\t Training Loss 0.4472\t Accuracy 0.8900\n",
      "Epoch [17][100]\t Batch [150][550]\t Training Loss 0.3748\t Accuracy 0.9000\n",
      "Epoch [17][100]\t Batch [200][550]\t Training Loss 0.2159\t Accuracy 0.9300\n",
      "Epoch [17][100]\t Batch [250][550]\t Training Loss 0.1675\t Accuracy 0.9600\n",
      "Epoch [17][100]\t Batch [300][550]\t Training Loss 0.3315\t Accuracy 0.9000\n",
      "Epoch [17][100]\t Batch [350][550]\t Training Loss 0.1562\t Accuracy 0.9600\n",
      "Epoch [17][100]\t Batch [400][550]\t Training Loss 0.2924\t Accuracy 0.9200\n",
      "Epoch [17][100]\t Batch [450][550]\t Training Loss 0.3038\t Accuracy 0.8900\n",
      "Epoch [17][100]\t Batch [500][550]\t Training Loss 0.1730\t Accuracy 0.9600\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2777\t Average training accuracy 0.9228\n",
      "Epoch [17]\t Average validation loss 0.2272\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [18][100]\t Batch [0][550]\t Training Loss 0.2559\t Accuracy 0.9300\n",
      "Epoch [18][100]\t Batch [50][550]\t Training Loss 0.2563\t Accuracy 0.9100\n",
      "Epoch [18][100]\t Batch [100][550]\t Training Loss 0.2659\t Accuracy 0.9600\n",
      "Epoch [18][100]\t Batch [150][550]\t Training Loss 0.2148\t Accuracy 0.8900\n",
      "Epoch [18][100]\t Batch [200][550]\t Training Loss 0.2176\t Accuracy 0.9200\n",
      "Epoch [18][100]\t Batch [250][550]\t Training Loss 0.2779\t Accuracy 0.8900\n",
      "Epoch [18][100]\t Batch [300][550]\t Training Loss 0.2343\t Accuracy 0.9500\n",
      "Epoch [18][100]\t Batch [350][550]\t Training Loss 0.2962\t Accuracy 0.9200\n",
      "Epoch [18][100]\t Batch [400][550]\t Training Loss 0.2411\t Accuracy 0.9200\n",
      "Epoch [18][100]\t Batch [450][550]\t Training Loss 0.1562\t Accuracy 0.9500\n",
      "Epoch [18][100]\t Batch [500][550]\t Training Loss 0.1769\t Accuracy 0.9500\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2764\t Average training accuracy 0.9231\n",
      "Epoch [18]\t Average validation loss 0.2268\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [19][100]\t Batch [0][550]\t Training Loss 0.2724\t Accuracy 0.9000\n",
      "Epoch [19][100]\t Batch [50][550]\t Training Loss 0.1825\t Accuracy 0.9400\n",
      "Epoch [19][100]\t Batch [100][550]\t Training Loss 0.1979\t Accuracy 0.9400\n",
      "Epoch [19][100]\t Batch [150][550]\t Training Loss 0.3326\t Accuracy 0.9100\n",
      "Epoch [19][100]\t Batch [200][550]\t Training Loss 0.3087\t Accuracy 0.9300\n",
      "Epoch [19][100]\t Batch [250][550]\t Training Loss 0.2811\t Accuracy 0.8900\n",
      "Epoch [19][100]\t Batch [300][550]\t Training Loss 0.2196\t Accuracy 0.9600\n",
      "Epoch [19][100]\t Batch [350][550]\t Training Loss 0.3605\t Accuracy 0.8600\n",
      "Epoch [19][100]\t Batch [400][550]\t Training Loss 0.2903\t Accuracy 0.9200\n",
      "Epoch [19][100]\t Batch [450][550]\t Training Loss 0.2323\t Accuracy 0.9700\n",
      "Epoch [19][100]\t Batch [500][550]\t Training Loss 0.1623\t Accuracy 0.9600\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2753\t Average training accuracy 0.9231\n",
      "Epoch [19]\t Average validation loss 0.2265\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [20][100]\t Batch [0][550]\t Training Loss 0.2248\t Accuracy 0.9500\n",
      "Epoch [20][100]\t Batch [50][550]\t Training Loss 0.2165\t Accuracy 0.9400\n",
      "Epoch [20][100]\t Batch [100][550]\t Training Loss 0.2531\t Accuracy 0.9300\n",
      "Epoch [20][100]\t Batch [150][550]\t Training Loss 0.1415\t Accuracy 0.9500\n",
      "Epoch [20][100]\t Batch [200][550]\t Training Loss 0.2691\t Accuracy 0.9300\n",
      "Epoch [20][100]\t Batch [250][550]\t Training Loss 0.2621\t Accuracy 0.9500\n",
      "Epoch [20][100]\t Batch [300][550]\t Training Loss 0.2163\t Accuracy 0.9400\n",
      "Epoch [20][100]\t Batch [350][550]\t Training Loss 0.2014\t Accuracy 0.9400\n",
      "Epoch [20][100]\t Batch [400][550]\t Training Loss 0.3198\t Accuracy 0.9100\n",
      "Epoch [20][100]\t Batch [450][550]\t Training Loss 0.3707\t Accuracy 0.9100\n",
      "Epoch [20][100]\t Batch [500][550]\t Training Loss 0.2031\t Accuracy 0.9700\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2742\t Average training accuracy 0.9240\n",
      "Epoch [20]\t Average validation loss 0.2256\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [21][100]\t Batch [0][550]\t Training Loss 0.2558\t Accuracy 0.9300\n",
      "Epoch [21][100]\t Batch [50][550]\t Training Loss 0.2469\t Accuracy 0.9100\n",
      "Epoch [21][100]\t Batch [100][550]\t Training Loss 0.1674\t Accuracy 0.9600\n",
      "Epoch [21][100]\t Batch [150][550]\t Training Loss 0.3273\t Accuracy 0.9200\n",
      "Epoch [21][100]\t Batch [200][550]\t Training Loss 0.3242\t Accuracy 0.9000\n",
      "Epoch [21][100]\t Batch [250][550]\t Training Loss 0.3600\t Accuracy 0.9000\n",
      "Epoch [21][100]\t Batch [300][550]\t Training Loss 0.3741\t Accuracy 0.8900\n",
      "Epoch [21][100]\t Batch [350][550]\t Training Loss 0.2382\t Accuracy 0.9600\n",
      "Epoch [21][100]\t Batch [400][550]\t Training Loss 0.1230\t Accuracy 0.9600\n",
      "Epoch [21][100]\t Batch [450][550]\t Training Loss 0.2319\t Accuracy 0.9300\n",
      "Epoch [21][100]\t Batch [500][550]\t Training Loss 0.2851\t Accuracy 0.9300\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2731\t Average training accuracy 0.9237\n",
      "Epoch [21]\t Average validation loss 0.2254\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [22][100]\t Batch [0][550]\t Training Loss 0.1748\t Accuracy 0.9600\n",
      "Epoch [22][100]\t Batch [50][550]\t Training Loss 0.2187\t Accuracy 0.9500\n",
      "Epoch [22][100]\t Batch [100][550]\t Training Loss 0.1834\t Accuracy 0.9300\n",
      "Epoch [22][100]\t Batch [150][550]\t Training Loss 0.3896\t Accuracy 0.9000\n",
      "Epoch [22][100]\t Batch [200][550]\t Training Loss 0.2196\t Accuracy 0.9200\n",
      "Epoch [22][100]\t Batch [250][550]\t Training Loss 0.3050\t Accuracy 0.8900\n",
      "Epoch [22][100]\t Batch [300][550]\t Training Loss 0.2417\t Accuracy 0.9300\n",
      "Epoch [22][100]\t Batch [350][550]\t Training Loss 0.3442\t Accuracy 0.9100\n",
      "Epoch [22][100]\t Batch [400][550]\t Training Loss 0.3345\t Accuracy 0.9100\n",
      "Epoch [22][100]\t Batch [450][550]\t Training Loss 0.3426\t Accuracy 0.9100\n",
      "Epoch [22][100]\t Batch [500][550]\t Training Loss 0.3559\t Accuracy 0.9300\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2724\t Average training accuracy 0.9242\n",
      "Epoch [22]\t Average validation loss 0.2258\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [23][100]\t Batch [0][550]\t Training Loss 0.3308\t Accuracy 0.9200\n",
      "Epoch [23][100]\t Batch [50][550]\t Training Loss 0.2358\t Accuracy 0.9300\n",
      "Epoch [23][100]\t Batch [100][550]\t Training Loss 0.2810\t Accuracy 0.9300\n",
      "Epoch [23][100]\t Batch [150][550]\t Training Loss 0.3288\t Accuracy 0.9000\n",
      "Epoch [23][100]\t Batch [200][550]\t Training Loss 0.2259\t Accuracy 0.9600\n",
      "Epoch [23][100]\t Batch [250][550]\t Training Loss 0.2206\t Accuracy 0.9400\n",
      "Epoch [23][100]\t Batch [300][550]\t Training Loss 0.2174\t Accuracy 0.9500\n",
      "Epoch [23][100]\t Batch [350][550]\t Training Loss 0.3735\t Accuracy 0.9000\n",
      "Epoch [23][100]\t Batch [400][550]\t Training Loss 0.1153\t Accuracy 0.9600\n",
      "Epoch [23][100]\t Batch [450][550]\t Training Loss 0.3571\t Accuracy 0.9200\n",
      "Epoch [23][100]\t Batch [500][550]\t Training Loss 0.1633\t Accuracy 0.9600\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2713\t Average training accuracy 0.9245\n",
      "Epoch [23]\t Average validation loss 0.2247\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [24][100]\t Batch [0][550]\t Training Loss 0.2036\t Accuracy 0.9400\n",
      "Epoch [24][100]\t Batch [50][550]\t Training Loss 0.3225\t Accuracy 0.9100\n",
      "Epoch [24][100]\t Batch [100][550]\t Training Loss 0.2077\t Accuracy 0.9400\n",
      "Epoch [24][100]\t Batch [150][550]\t Training Loss 0.3998\t Accuracy 0.9000\n",
      "Epoch [24][100]\t Batch [200][550]\t Training Loss 0.4237\t Accuracy 0.9200\n",
      "Epoch [24][100]\t Batch [250][550]\t Training Loss 0.3347\t Accuracy 0.9000\n",
      "Epoch [24][100]\t Batch [300][550]\t Training Loss 0.5399\t Accuracy 0.8800\n",
      "Epoch [24][100]\t Batch [350][550]\t Training Loss 0.2577\t Accuracy 0.9200\n",
      "Epoch [24][100]\t Batch [400][550]\t Training Loss 0.1954\t Accuracy 0.9300\n",
      "Epoch [24][100]\t Batch [450][550]\t Training Loss 0.2724\t Accuracy 0.9500\n",
      "Epoch [24][100]\t Batch [500][550]\t Training Loss 0.1913\t Accuracy 0.9500\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2706\t Average training accuracy 0.9248\n",
      "Epoch [24]\t Average validation loss 0.2248\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [25][100]\t Batch [0][550]\t Training Loss 0.2848\t Accuracy 0.9100\n",
      "Epoch [25][100]\t Batch [50][550]\t Training Loss 0.2527\t Accuracy 0.9300\n",
      "Epoch [25][100]\t Batch [100][550]\t Training Loss 0.2669\t Accuracy 0.9300\n",
      "Epoch [25][100]\t Batch [150][550]\t Training Loss 0.2274\t Accuracy 0.9400\n",
      "Epoch [25][100]\t Batch [200][550]\t Training Loss 0.1288\t Accuracy 0.9600\n",
      "Epoch [25][100]\t Batch [250][550]\t Training Loss 0.2531\t Accuracy 0.9300\n",
      "Epoch [25][100]\t Batch [300][550]\t Training Loss 0.1973\t Accuracy 0.9700\n",
      "Epoch [25][100]\t Batch [350][550]\t Training Loss 0.5689\t Accuracy 0.8700\n",
      "Epoch [25][100]\t Batch [400][550]\t Training Loss 0.3400\t Accuracy 0.9000\n",
      "Epoch [25][100]\t Batch [450][550]\t Training Loss 0.4634\t Accuracy 0.9000\n",
      "Epoch [25][100]\t Batch [500][550]\t Training Loss 0.3960\t Accuracy 0.8800\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2699\t Average training accuracy 0.9244\n",
      "Epoch [25]\t Average validation loss 0.2236\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [26][100]\t Batch [0][550]\t Training Loss 0.2859\t Accuracy 0.9400\n",
      "Epoch [26][100]\t Batch [50][550]\t Training Loss 0.1799\t Accuracy 0.9500\n",
      "Epoch [26][100]\t Batch [100][550]\t Training Loss 0.1505\t Accuracy 0.9400\n",
      "Epoch [26][100]\t Batch [150][550]\t Training Loss 0.3786\t Accuracy 0.9200\n",
      "Epoch [26][100]\t Batch [200][550]\t Training Loss 0.2789\t Accuracy 0.9200\n",
      "Epoch [26][100]\t Batch [250][550]\t Training Loss 0.2068\t Accuracy 0.9100\n",
      "Epoch [26][100]\t Batch [300][550]\t Training Loss 0.2030\t Accuracy 0.9200\n",
      "Epoch [26][100]\t Batch [350][550]\t Training Loss 0.3202\t Accuracy 0.9100\n",
      "Epoch [26][100]\t Batch [400][550]\t Training Loss 0.2576\t Accuracy 0.9400\n",
      "Epoch [26][100]\t Batch [450][550]\t Training Loss 0.2742\t Accuracy 0.9100\n",
      "Epoch [26][100]\t Batch [500][550]\t Training Loss 0.4290\t Accuracy 0.8900\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2690\t Average training accuracy 0.9255\n",
      "Epoch [26]\t Average validation loss 0.2233\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [27][100]\t Batch [0][550]\t Training Loss 0.1743\t Accuracy 0.9300\n",
      "Epoch [27][100]\t Batch [50][550]\t Training Loss 0.2210\t Accuracy 0.9300\n",
      "Epoch [27][100]\t Batch [100][550]\t Training Loss 0.1316\t Accuracy 0.9600\n",
      "Epoch [27][100]\t Batch [150][550]\t Training Loss 0.1519\t Accuracy 0.9600\n",
      "Epoch [27][100]\t Batch [200][550]\t Training Loss 0.2388\t Accuracy 0.9500\n",
      "Epoch [27][100]\t Batch [250][550]\t Training Loss 0.3280\t Accuracy 0.8900\n",
      "Epoch [27][100]\t Batch [300][550]\t Training Loss 0.2077\t Accuracy 0.9400\n",
      "Epoch [27][100]\t Batch [350][550]\t Training Loss 0.3376\t Accuracy 0.8900\n",
      "Epoch [27][100]\t Batch [400][550]\t Training Loss 0.2214\t Accuracy 0.9600\n",
      "Epoch [27][100]\t Batch [450][550]\t Training Loss 0.1714\t Accuracy 0.9500\n",
      "Epoch [27][100]\t Batch [500][550]\t Training Loss 0.2123\t Accuracy 0.9200\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2683\t Average training accuracy 0.9254\n",
      "Epoch [27]\t Average validation loss 0.2230\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [28][100]\t Batch [0][550]\t Training Loss 0.3139\t Accuracy 0.9000\n",
      "Epoch [28][100]\t Batch [50][550]\t Training Loss 0.1948\t Accuracy 0.9500\n",
      "Epoch [28][100]\t Batch [100][550]\t Training Loss 0.2289\t Accuracy 0.9400\n",
      "Epoch [28][100]\t Batch [150][550]\t Training Loss 0.2373\t Accuracy 0.9500\n",
      "Epoch [28][100]\t Batch [200][550]\t Training Loss 0.3920\t Accuracy 0.8700\n",
      "Epoch [28][100]\t Batch [250][550]\t Training Loss 0.1733\t Accuracy 0.9600\n",
      "Epoch [28][100]\t Batch [300][550]\t Training Loss 0.2851\t Accuracy 0.9400\n",
      "Epoch [28][100]\t Batch [350][550]\t Training Loss 0.2239\t Accuracy 0.9400\n",
      "Epoch [28][100]\t Batch [400][550]\t Training Loss 0.1737\t Accuracy 0.9600\n",
      "Epoch [28][100]\t Batch [450][550]\t Training Loss 0.3250\t Accuracy 0.9000\n",
      "Epoch [28][100]\t Batch [500][550]\t Training Loss 0.1558\t Accuracy 0.9700\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2677\t Average training accuracy 0.9255\n",
      "Epoch [28]\t Average validation loss 0.2235\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [29][100]\t Batch [0][550]\t Training Loss 0.3415\t Accuracy 0.8800\n",
      "Epoch [29][100]\t Batch [50][550]\t Training Loss 0.1684\t Accuracy 0.9700\n",
      "Epoch [29][100]\t Batch [100][550]\t Training Loss 0.3332\t Accuracy 0.9300\n",
      "Epoch [29][100]\t Batch [150][550]\t Training Loss 0.2338\t Accuracy 0.9100\n",
      "Epoch [29][100]\t Batch [200][550]\t Training Loss 0.4462\t Accuracy 0.9200\n",
      "Epoch [29][100]\t Batch [250][550]\t Training Loss 0.4693\t Accuracy 0.8700\n",
      "Epoch [29][100]\t Batch [300][550]\t Training Loss 0.3313\t Accuracy 0.9100\n",
      "Epoch [29][100]\t Batch [350][550]\t Training Loss 0.2789\t Accuracy 0.9600\n",
      "Epoch [29][100]\t Batch [400][550]\t Training Loss 0.3156\t Accuracy 0.8700\n",
      "Epoch [29][100]\t Batch [450][550]\t Training Loss 0.2470\t Accuracy 0.8800\n",
      "Epoch [29][100]\t Batch [500][550]\t Training Loss 0.3259\t Accuracy 0.9000\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2670\t Average training accuracy 0.9255\n",
      "Epoch [29]\t Average validation loss 0.2226\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [30][100]\t Batch [0][550]\t Training Loss 0.2499\t Accuracy 0.9300\n",
      "Epoch [30][100]\t Batch [50][550]\t Training Loss 0.3920\t Accuracy 0.9200\n",
      "Epoch [30][100]\t Batch [100][550]\t Training Loss 0.5184\t Accuracy 0.8800\n",
      "Epoch [30][100]\t Batch [150][550]\t Training Loss 0.2745\t Accuracy 0.9100\n",
      "Epoch [30][100]\t Batch [200][550]\t Training Loss 0.2522\t Accuracy 0.9200\n",
      "Epoch [30][100]\t Batch [250][550]\t Training Loss 0.2804\t Accuracy 0.9200\n",
      "Epoch [30][100]\t Batch [300][550]\t Training Loss 0.3849\t Accuracy 0.9200\n",
      "Epoch [30][100]\t Batch [350][550]\t Training Loss 0.1782\t Accuracy 0.9600\n",
      "Epoch [30][100]\t Batch [400][550]\t Training Loss 0.2246\t Accuracy 0.9700\n",
      "Epoch [30][100]\t Batch [450][550]\t Training Loss 0.3429\t Accuracy 0.9300\n",
      "Epoch [30][100]\t Batch [500][550]\t Training Loss 0.2109\t Accuracy 0.9400\n",
      "\n",
      "Epoch [30]\t Average training loss 0.2665\t Average training accuracy 0.9262\n",
      "Epoch [30]\t Average validation loss 0.2222\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [31][100]\t Batch [0][550]\t Training Loss 0.2838\t Accuracy 0.9200\n",
      "Epoch [31][100]\t Batch [50][550]\t Training Loss 0.3245\t Accuracy 0.9200\n",
      "Epoch [31][100]\t Batch [100][550]\t Training Loss 0.2106\t Accuracy 0.9200\n",
      "Epoch [31][100]\t Batch [150][550]\t Training Loss 0.3047\t Accuracy 0.9100\n",
      "Epoch [31][100]\t Batch [200][550]\t Training Loss 0.3640\t Accuracy 0.8800\n",
      "Epoch [31][100]\t Batch [250][550]\t Training Loss 0.1904\t Accuracy 0.9500\n",
      "Epoch [31][100]\t Batch [300][550]\t Training Loss 0.2169\t Accuracy 0.9400\n",
      "Epoch [31][100]\t Batch [350][550]\t Training Loss 0.2431\t Accuracy 0.9400\n",
      "Epoch [31][100]\t Batch [400][550]\t Training Loss 0.2018\t Accuracy 0.9400\n",
      "Epoch [31][100]\t Batch [450][550]\t Training Loss 0.4466\t Accuracy 0.8900\n",
      "Epoch [31][100]\t Batch [500][550]\t Training Loss 0.1225\t Accuracy 0.9700\n",
      "\n",
      "Epoch [31]\t Average training loss 0.2656\t Average training accuracy 0.9261\n",
      "Epoch [31]\t Average validation loss 0.2211\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [32][100]\t Batch [0][550]\t Training Loss 0.4407\t Accuracy 0.8800\n",
      "Epoch [32][100]\t Batch [50][550]\t Training Loss 0.1784\t Accuracy 0.9600\n",
      "Epoch [32][100]\t Batch [100][550]\t Training Loss 0.2396\t Accuracy 0.9300\n",
      "Epoch [32][100]\t Batch [150][550]\t Training Loss 0.3900\t Accuracy 0.9100\n",
      "Epoch [32][100]\t Batch [200][550]\t Training Loss 0.4653\t Accuracy 0.8800\n",
      "Epoch [32][100]\t Batch [250][550]\t Training Loss 0.1655\t Accuracy 0.9200\n",
      "Epoch [32][100]\t Batch [300][550]\t Training Loss 0.1893\t Accuracy 0.9300\n",
      "Epoch [32][100]\t Batch [350][550]\t Training Loss 0.3931\t Accuracy 0.9200\n",
      "Epoch [32][100]\t Batch [400][550]\t Training Loss 0.2895\t Accuracy 0.9100\n",
      "Epoch [32][100]\t Batch [450][550]\t Training Loss 0.2207\t Accuracy 0.9400\n",
      "Epoch [32][100]\t Batch [500][550]\t Training Loss 0.3450\t Accuracy 0.9100\n",
      "\n",
      "Epoch [32]\t Average training loss 0.2653\t Average training accuracy 0.9264\n",
      "Epoch [32]\t Average validation loss 0.2218\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [33][100]\t Batch [0][550]\t Training Loss 0.1022\t Accuracy 0.9800\n",
      "Epoch [33][100]\t Batch [50][550]\t Training Loss 0.3318\t Accuracy 0.9100\n",
      "Epoch [33][100]\t Batch [100][550]\t Training Loss 0.2354\t Accuracy 0.9300\n",
      "Epoch [33][100]\t Batch [150][550]\t Training Loss 0.2957\t Accuracy 0.9100\n",
      "Epoch [33][100]\t Batch [200][550]\t Training Loss 0.1500\t Accuracy 0.9600\n",
      "Epoch [33][100]\t Batch [250][550]\t Training Loss 0.1814\t Accuracy 0.9700\n",
      "Epoch [33][100]\t Batch [300][550]\t Training Loss 0.2381\t Accuracy 0.8900\n",
      "Epoch [33][100]\t Batch [350][550]\t Training Loss 0.3102\t Accuracy 0.9200\n",
      "Epoch [33][100]\t Batch [400][550]\t Training Loss 0.3119\t Accuracy 0.9200\n",
      "Epoch [33][100]\t Batch [450][550]\t Training Loss 0.2435\t Accuracy 0.9200\n",
      "Epoch [33][100]\t Batch [500][550]\t Training Loss 0.4378\t Accuracy 0.8800\n",
      "\n",
      "Epoch [33]\t Average training loss 0.2647\t Average training accuracy 0.9267\n",
      "Epoch [33]\t Average validation loss 0.2213\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [34][100]\t Batch [0][550]\t Training Loss 0.2443\t Accuracy 0.9300\n",
      "Epoch [34][100]\t Batch [50][550]\t Training Loss 0.3813\t Accuracy 0.8600\n",
      "Epoch [34][100]\t Batch [100][550]\t Training Loss 0.3489\t Accuracy 0.9300\n",
      "Epoch [34][100]\t Batch [150][550]\t Training Loss 0.3930\t Accuracy 0.8800\n",
      "Epoch [34][100]\t Batch [200][550]\t Training Loss 0.1916\t Accuracy 0.9400\n",
      "Epoch [34][100]\t Batch [250][550]\t Training Loss 0.2714\t Accuracy 0.9000\n",
      "Epoch [34][100]\t Batch [300][550]\t Training Loss 0.2512\t Accuracy 0.9300\n",
      "Epoch [34][100]\t Batch [350][550]\t Training Loss 0.3288\t Accuracy 0.9400\n",
      "Epoch [34][100]\t Batch [400][550]\t Training Loss 0.3041\t Accuracy 0.9000\n",
      "Epoch [34][100]\t Batch [450][550]\t Training Loss 0.1752\t Accuracy 0.9200\n",
      "Epoch [34][100]\t Batch [500][550]\t Training Loss 0.4568\t Accuracy 0.8700\n",
      "\n",
      "Epoch [34]\t Average training loss 0.2642\t Average training accuracy 0.9266\n",
      "Epoch [34]\t Average validation loss 0.2211\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [35][100]\t Batch [0][550]\t Training Loss 0.2037\t Accuracy 0.9300\n",
      "Epoch [35][100]\t Batch [50][550]\t Training Loss 0.1165\t Accuracy 0.9700\n",
      "Epoch [35][100]\t Batch [100][550]\t Training Loss 0.2418\t Accuracy 0.9300\n",
      "Epoch [35][100]\t Batch [150][550]\t Training Loss 0.2193\t Accuracy 0.9500\n",
      "Epoch [35][100]\t Batch [200][550]\t Training Loss 0.2639\t Accuracy 0.8900\n",
      "Epoch [35][100]\t Batch [250][550]\t Training Loss 0.2243\t Accuracy 0.9300\n",
      "Epoch [35][100]\t Batch [300][550]\t Training Loss 0.2051\t Accuracy 0.9300\n",
      "Epoch [35][100]\t Batch [350][550]\t Training Loss 0.2705\t Accuracy 0.9100\n",
      "Epoch [35][100]\t Batch [400][550]\t Training Loss 0.2460\t Accuracy 0.9200\n",
      "Epoch [35][100]\t Batch [450][550]\t Training Loss 0.2811\t Accuracy 0.9300\n",
      "Epoch [35][100]\t Batch [500][550]\t Training Loss 0.2764\t Accuracy 0.8800\n",
      "\n",
      "Epoch [35]\t Average training loss 0.2635\t Average training accuracy 0.9266\n",
      "Epoch [35]\t Average validation loss 0.2227\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [36][100]\t Batch [0][550]\t Training Loss 0.2794\t Accuracy 0.9300\n",
      "Epoch [36][100]\t Batch [50][550]\t Training Loss 0.1698\t Accuracy 0.9500\n",
      "Epoch [36][100]\t Batch [100][550]\t Training Loss 0.3246\t Accuracy 0.9400\n",
      "Epoch [36][100]\t Batch [150][550]\t Training Loss 0.2918\t Accuracy 0.9200\n",
      "Epoch [36][100]\t Batch [200][550]\t Training Loss 0.2763\t Accuracy 0.9200\n",
      "Epoch [36][100]\t Batch [250][550]\t Training Loss 0.5004\t Accuracy 0.8600\n",
      "Epoch [36][100]\t Batch [300][550]\t Training Loss 0.3671\t Accuracy 0.8800\n",
      "Epoch [36][100]\t Batch [350][550]\t Training Loss 0.3110\t Accuracy 0.8800\n",
      "Epoch [36][100]\t Batch [400][550]\t Training Loss 0.1229\t Accuracy 0.9700\n",
      "Epoch [36][100]\t Batch [450][550]\t Training Loss 0.1761\t Accuracy 0.9400\n",
      "Epoch [36][100]\t Batch [500][550]\t Training Loss 0.2902\t Accuracy 0.9600\n",
      "\n",
      "Epoch [36]\t Average training loss 0.2634\t Average training accuracy 0.9266\n",
      "Epoch [36]\t Average validation loss 0.2216\t Average validation accuracy 0.9380\n",
      "\n",
      "Epoch [37][100]\t Batch [0][550]\t Training Loss 0.3388\t Accuracy 0.9300\n",
      "Epoch [37][100]\t Batch [50][550]\t Training Loss 0.1969\t Accuracy 0.9500\n",
      "Epoch [37][100]\t Batch [100][550]\t Training Loss 0.2698\t Accuracy 0.9600\n",
      "Epoch [37][100]\t Batch [150][550]\t Training Loss 0.2706\t Accuracy 0.9100\n",
      "Epoch [37][100]\t Batch [200][550]\t Training Loss 0.2495\t Accuracy 0.9100\n",
      "Epoch [37][100]\t Batch [250][550]\t Training Loss 0.2034\t Accuracy 0.9400\n",
      "Epoch [37][100]\t Batch [300][550]\t Training Loss 0.3024\t Accuracy 0.9300\n",
      "Epoch [37][100]\t Batch [350][550]\t Training Loss 0.3253\t Accuracy 0.9100\n",
      "Epoch [37][100]\t Batch [400][550]\t Training Loss 0.3465\t Accuracy 0.9100\n",
      "Epoch [37][100]\t Batch [450][550]\t Training Loss 0.1884\t Accuracy 0.9600\n",
      "Epoch [37][100]\t Batch [500][550]\t Training Loss 0.1917\t Accuracy 0.9400\n",
      "\n",
      "Epoch [37]\t Average training loss 0.2627\t Average training accuracy 0.9272\n",
      "Epoch [37]\t Average validation loss 0.2212\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [38][100]\t Batch [0][550]\t Training Loss 0.2634\t Accuracy 0.9200\n",
      "Epoch [38][100]\t Batch [50][550]\t Training Loss 0.4003\t Accuracy 0.8900\n",
      "Epoch [38][100]\t Batch [100][550]\t Training Loss 0.3925\t Accuracy 0.8900\n",
      "Epoch [38][100]\t Batch [150][550]\t Training Loss 0.5833\t Accuracy 0.9100\n",
      "Epoch [38][100]\t Batch [200][550]\t Training Loss 0.2471\t Accuracy 0.9600\n",
      "Epoch [38][100]\t Batch [250][550]\t Training Loss 0.3375\t Accuracy 0.9100\n",
      "Epoch [38][100]\t Batch [300][550]\t Training Loss 0.1192\t Accuracy 0.9600\n",
      "Epoch [38][100]\t Batch [350][550]\t Training Loss 0.3361\t Accuracy 0.9400\n",
      "Epoch [38][100]\t Batch [400][550]\t Training Loss 0.3929\t Accuracy 0.8900\n",
      "Epoch [38][100]\t Batch [450][550]\t Training Loss 0.1521\t Accuracy 0.9700\n",
      "Epoch [38][100]\t Batch [500][550]\t Training Loss 0.1962\t Accuracy 0.9400\n",
      "\n",
      "Epoch [38]\t Average training loss 0.2623\t Average training accuracy 0.9269\n",
      "Epoch [38]\t Average validation loss 0.2207\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [39][100]\t Batch [0][550]\t Training Loss 0.2897\t Accuracy 0.9300\n",
      "Epoch [39][100]\t Batch [50][550]\t Training Loss 0.1413\t Accuracy 0.9600\n",
      "Epoch [39][100]\t Batch [100][550]\t Training Loss 0.1628\t Accuracy 0.9400\n",
      "Epoch [39][100]\t Batch [150][550]\t Training Loss 0.2092\t Accuracy 0.9200\n",
      "Epoch [39][100]\t Batch [200][550]\t Training Loss 0.2834\t Accuracy 0.9100\n",
      "Epoch [39][100]\t Batch [250][550]\t Training Loss 0.1485\t Accuracy 0.9700\n",
      "Epoch [39][100]\t Batch [300][550]\t Training Loss 0.2685\t Accuracy 0.9000\n",
      "Epoch [39][100]\t Batch [350][550]\t Training Loss 0.3763\t Accuracy 0.8800\n",
      "Epoch [39][100]\t Batch [400][550]\t Training Loss 0.4713\t Accuracy 0.8700\n",
      "Epoch [39][100]\t Batch [450][550]\t Training Loss 0.1514\t Accuracy 0.9400\n",
      "Epoch [39][100]\t Batch [500][550]\t Training Loss 0.1756\t Accuracy 0.9500\n",
      "\n",
      "Epoch [39]\t Average training loss 0.2618\t Average training accuracy 0.9275\n",
      "Epoch [39]\t Average validation loss 0.2214\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [40][100]\t Batch [0][550]\t Training Loss 0.2118\t Accuracy 0.9500\n",
      "Epoch [40][100]\t Batch [50][550]\t Training Loss 0.4107\t Accuracy 0.8800\n",
      "Epoch [40][100]\t Batch [100][550]\t Training Loss 0.2296\t Accuracy 0.9500\n",
      "Epoch [40][100]\t Batch [150][550]\t Training Loss 0.2273\t Accuracy 0.9400\n",
      "Epoch [40][100]\t Batch [200][550]\t Training Loss 0.2027\t Accuracy 0.9400\n",
      "Epoch [40][100]\t Batch [250][550]\t Training Loss 0.2202\t Accuracy 0.9300\n",
      "Epoch [40][100]\t Batch [300][550]\t Training Loss 0.2943\t Accuracy 0.9100\n",
      "Epoch [40][100]\t Batch [350][550]\t Training Loss 0.2537\t Accuracy 0.9500\n",
      "Epoch [40][100]\t Batch [400][550]\t Training Loss 0.2013\t Accuracy 0.9400\n",
      "Epoch [40][100]\t Batch [450][550]\t Training Loss 0.2637\t Accuracy 0.9100\n",
      "Epoch [40][100]\t Batch [500][550]\t Training Loss 0.4409\t Accuracy 0.8900\n",
      "\n",
      "Epoch [40]\t Average training loss 0.2614\t Average training accuracy 0.9275\n",
      "Epoch [40]\t Average validation loss 0.2209\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [41][100]\t Batch [0][550]\t Training Loss 0.2604\t Accuracy 0.9400\n",
      "Epoch [41][100]\t Batch [50][550]\t Training Loss 0.2503\t Accuracy 0.9100\n",
      "Epoch [41][100]\t Batch [100][550]\t Training Loss 0.1609\t Accuracy 0.9600\n",
      "Epoch [41][100]\t Batch [150][550]\t Training Loss 0.3598\t Accuracy 0.8900\n",
      "Epoch [41][100]\t Batch [200][550]\t Training Loss 0.3817\t Accuracy 0.8800\n",
      "Epoch [41][100]\t Batch [250][550]\t Training Loss 0.2417\t Accuracy 0.9500\n",
      "Epoch [41][100]\t Batch [300][550]\t Training Loss 0.1600\t Accuracy 0.9600\n",
      "Epoch [41][100]\t Batch [350][550]\t Training Loss 0.3583\t Accuracy 0.9000\n",
      "Epoch [41][100]\t Batch [400][550]\t Training Loss 0.4062\t Accuracy 0.9300\n",
      "Epoch [41][100]\t Batch [450][550]\t Training Loss 0.4980\t Accuracy 0.9000\n",
      "Epoch [41][100]\t Batch [500][550]\t Training Loss 0.2400\t Accuracy 0.9300\n",
      "\n",
      "Epoch [41]\t Average training loss 0.2610\t Average training accuracy 0.9278\n",
      "Epoch [41]\t Average validation loss 0.2219\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [42][100]\t Batch [0][550]\t Training Loss 0.3112\t Accuracy 0.9200\n",
      "Epoch [42][100]\t Batch [50][550]\t Training Loss 0.2348\t Accuracy 0.9300\n",
      "Epoch [42][100]\t Batch [100][550]\t Training Loss 0.2530\t Accuracy 0.8900\n",
      "Epoch [42][100]\t Batch [150][550]\t Training Loss 0.3814\t Accuracy 0.8900\n",
      "Epoch [42][100]\t Batch [200][550]\t Training Loss 0.3076\t Accuracy 0.9400\n",
      "Epoch [42][100]\t Batch [250][550]\t Training Loss 0.3014\t Accuracy 0.8800\n",
      "Epoch [42][100]\t Batch [300][550]\t Training Loss 0.1020\t Accuracy 0.9800\n",
      "Epoch [42][100]\t Batch [350][550]\t Training Loss 0.1401\t Accuracy 0.9600\n",
      "Epoch [42][100]\t Batch [400][550]\t Training Loss 0.1859\t Accuracy 0.9500\n",
      "Epoch [42][100]\t Batch [450][550]\t Training Loss 0.2131\t Accuracy 0.9500\n",
      "Epoch [42][100]\t Batch [500][550]\t Training Loss 0.1526\t Accuracy 0.9600\n",
      "\n",
      "Epoch [42]\t Average training loss 0.2606\t Average training accuracy 0.9277\n",
      "Epoch [42]\t Average validation loss 0.2205\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [43][100]\t Batch [0][550]\t Training Loss 0.2131\t Accuracy 0.9300\n",
      "Epoch [43][100]\t Batch [50][550]\t Training Loss 0.3111\t Accuracy 0.9200\n",
      "Epoch [43][100]\t Batch [100][550]\t Training Loss 0.2584\t Accuracy 0.9200\n",
      "Epoch [43][100]\t Batch [150][550]\t Training Loss 0.3430\t Accuracy 0.8700\n",
      "Epoch [43][100]\t Batch [200][550]\t Training Loss 0.2135\t Accuracy 0.9400\n",
      "Epoch [43][100]\t Batch [250][550]\t Training Loss 0.2878\t Accuracy 0.9200\n",
      "Epoch [43][100]\t Batch [300][550]\t Training Loss 0.2398\t Accuracy 0.9600\n",
      "Epoch [43][100]\t Batch [350][550]\t Training Loss 0.2579\t Accuracy 0.9300\n",
      "Epoch [43][100]\t Batch [400][550]\t Training Loss 0.1734\t Accuracy 0.9600\n",
      "Epoch [43][100]\t Batch [450][550]\t Training Loss 0.2253\t Accuracy 0.9500\n",
      "Epoch [43][100]\t Batch [500][550]\t Training Loss 0.2485\t Accuracy 0.9000\n",
      "\n",
      "Epoch [43]\t Average training loss 0.2601\t Average training accuracy 0.9276\n",
      "Epoch [43]\t Average validation loss 0.2206\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [44][100]\t Batch [0][550]\t Training Loss 0.1682\t Accuracy 0.9700\n",
      "Epoch [44][100]\t Batch [50][550]\t Training Loss 0.3352\t Accuracy 0.9200\n",
      "Epoch [44][100]\t Batch [100][550]\t Training Loss 0.3947\t Accuracy 0.9400\n",
      "Epoch [44][100]\t Batch [150][550]\t Training Loss 0.1936\t Accuracy 0.9400\n",
      "Epoch [44][100]\t Batch [200][550]\t Training Loss 0.2267\t Accuracy 0.9100\n",
      "Epoch [44][100]\t Batch [250][550]\t Training Loss 0.3449\t Accuracy 0.8800\n",
      "Epoch [44][100]\t Batch [300][550]\t Training Loss 0.3091\t Accuracy 0.9200\n",
      "Epoch [44][100]\t Batch [350][550]\t Training Loss 0.2116\t Accuracy 0.9500\n",
      "Epoch [44][100]\t Batch [400][550]\t Training Loss 0.3394\t Accuracy 0.9000\n",
      "Epoch [44][100]\t Batch [450][550]\t Training Loss 0.1800\t Accuracy 0.9700\n",
      "Epoch [44][100]\t Batch [500][550]\t Training Loss 0.2629\t Accuracy 0.9400\n",
      "\n",
      "Epoch [44]\t Average training loss 0.2597\t Average training accuracy 0.9279\n",
      "Epoch [44]\t Average validation loss 0.2202\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [45][100]\t Batch [0][550]\t Training Loss 0.4473\t Accuracy 0.9200\n",
      "Epoch [45][100]\t Batch [50][550]\t Training Loss 0.3121\t Accuracy 0.8900\n",
      "Epoch [45][100]\t Batch [100][550]\t Training Loss 0.4047\t Accuracy 0.9200\n",
      "Epoch [45][100]\t Batch [150][550]\t Training Loss 0.2248\t Accuracy 0.9100\n",
      "Epoch [45][100]\t Batch [200][550]\t Training Loss 0.1933\t Accuracy 0.9500\n",
      "Epoch [45][100]\t Batch [250][550]\t Training Loss 0.1508\t Accuracy 0.9600\n",
      "Epoch [45][100]\t Batch [300][550]\t Training Loss 0.3876\t Accuracy 0.9100\n",
      "Epoch [45][100]\t Batch [350][550]\t Training Loss 0.4351\t Accuracy 0.9000\n",
      "Epoch [45][100]\t Batch [400][550]\t Training Loss 0.2531\t Accuracy 0.9300\n",
      "Epoch [45][100]\t Batch [450][550]\t Training Loss 0.1784\t Accuracy 0.9300\n",
      "Epoch [45][100]\t Batch [500][550]\t Training Loss 0.4679\t Accuracy 0.8900\n",
      "\n",
      "Epoch [45]\t Average training loss 0.2593\t Average training accuracy 0.9281\n",
      "Epoch [45]\t Average validation loss 0.2198\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [46][100]\t Batch [0][550]\t Training Loss 0.1938\t Accuracy 0.9200\n",
      "Epoch [46][100]\t Batch [50][550]\t Training Loss 0.1224\t Accuracy 0.9700\n",
      "Epoch [46][100]\t Batch [100][550]\t Training Loss 0.1983\t Accuracy 0.9400\n",
      "Epoch [46][100]\t Batch [150][550]\t Training Loss 0.4284\t Accuracy 0.9000\n",
      "Epoch [46][100]\t Batch [200][550]\t Training Loss 0.3428\t Accuracy 0.8800\n",
      "Epoch [46][100]\t Batch [250][550]\t Training Loss 0.3195\t Accuracy 0.8900\n",
      "Epoch [46][100]\t Batch [300][550]\t Training Loss 0.4086\t Accuracy 0.9000\n",
      "Epoch [46][100]\t Batch [350][550]\t Training Loss 0.1325\t Accuracy 0.9300\n",
      "Epoch [46][100]\t Batch [400][550]\t Training Loss 0.2205\t Accuracy 0.9100\n",
      "Epoch [46][100]\t Batch [450][550]\t Training Loss 0.1780\t Accuracy 0.9500\n",
      "Epoch [46][100]\t Batch [500][550]\t Training Loss 0.1770\t Accuracy 0.9600\n",
      "\n",
      "Epoch [46]\t Average training loss 0.2591\t Average training accuracy 0.9285\n",
      "Epoch [46]\t Average validation loss 0.2205\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [47][100]\t Batch [0][550]\t Training Loss 0.3920\t Accuracy 0.9000\n",
      "Epoch [47][100]\t Batch [50][550]\t Training Loss 0.4063\t Accuracy 0.8700\n",
      "Epoch [47][100]\t Batch [100][550]\t Training Loss 0.2187\t Accuracy 0.9500\n",
      "Epoch [47][100]\t Batch [150][550]\t Training Loss 0.2722\t Accuracy 0.9100\n",
      "Epoch [47][100]\t Batch [200][550]\t Training Loss 0.2479\t Accuracy 0.9100\n",
      "Epoch [47][100]\t Batch [250][550]\t Training Loss 0.1880\t Accuracy 0.9400\n",
      "Epoch [47][100]\t Batch [300][550]\t Training Loss 0.2470\t Accuracy 0.9300\n",
      "Epoch [47][100]\t Batch [350][550]\t Training Loss 0.2239\t Accuracy 0.9300\n",
      "Epoch [47][100]\t Batch [400][550]\t Training Loss 0.4018\t Accuracy 0.8900\n",
      "Epoch [47][100]\t Batch [450][550]\t Training Loss 0.2533\t Accuracy 0.8900\n",
      "Epoch [47][100]\t Batch [500][550]\t Training Loss 0.2633\t Accuracy 0.9200\n",
      "\n",
      "Epoch [47]\t Average training loss 0.2587\t Average training accuracy 0.9284\n",
      "Epoch [47]\t Average validation loss 0.2197\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [48][100]\t Batch [0][550]\t Training Loss 0.2881\t Accuracy 0.9300\n",
      "Epoch [48][100]\t Batch [50][550]\t Training Loss 0.2884\t Accuracy 0.9200\n",
      "Epoch [48][100]\t Batch [100][550]\t Training Loss 0.3688\t Accuracy 0.8800\n",
      "Epoch [48][100]\t Batch [150][550]\t Training Loss 0.2234\t Accuracy 0.9300\n",
      "Epoch [48][100]\t Batch [200][550]\t Training Loss 0.3073\t Accuracy 0.8900\n",
      "Epoch [48][100]\t Batch [250][550]\t Training Loss 0.2078\t Accuracy 0.9200\n",
      "Epoch [48][100]\t Batch [300][550]\t Training Loss 0.4666\t Accuracy 0.8500\n",
      "Epoch [48][100]\t Batch [350][550]\t Training Loss 0.3218\t Accuracy 0.9300\n",
      "Epoch [48][100]\t Batch [400][550]\t Training Loss 0.2614\t Accuracy 0.9200\n",
      "Epoch [48][100]\t Batch [450][550]\t Training Loss 0.3727\t Accuracy 0.9100\n",
      "Epoch [48][100]\t Batch [500][550]\t Training Loss 0.2030\t Accuracy 0.9300\n",
      "\n",
      "Epoch [48]\t Average training loss 0.2583\t Average training accuracy 0.9282\n",
      "Epoch [48]\t Average validation loss 0.2195\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [49][100]\t Batch [0][550]\t Training Loss 0.2705\t Accuracy 0.9400\n",
      "Epoch [49][100]\t Batch [50][550]\t Training Loss 0.1604\t Accuracy 0.9300\n",
      "Epoch [49][100]\t Batch [100][550]\t Training Loss 0.4766\t Accuracy 0.8800\n",
      "Epoch [49][100]\t Batch [150][550]\t Training Loss 0.2173\t Accuracy 0.9400\n",
      "Epoch [49][100]\t Batch [200][550]\t Training Loss 0.1994\t Accuracy 0.9500\n",
      "Epoch [49][100]\t Batch [250][550]\t Training Loss 0.2079\t Accuracy 0.9300\n",
      "Epoch [49][100]\t Batch [300][550]\t Training Loss 0.2088\t Accuracy 0.9300\n",
      "Epoch [49][100]\t Batch [350][550]\t Training Loss 0.2592\t Accuracy 0.9100\n",
      "Epoch [49][100]\t Batch [400][550]\t Training Loss 0.1428\t Accuracy 0.9700\n",
      "Epoch [49][100]\t Batch [450][550]\t Training Loss 0.1508\t Accuracy 0.9400\n",
      "Epoch [49][100]\t Batch [500][550]\t Training Loss 0.3024\t Accuracy 0.8900\n",
      "\n",
      "Epoch [49]\t Average training loss 0.2579\t Average training accuracy 0.9287\n",
      "Epoch [49]\t Average validation loss 0.2191\t Average validation accuracy 0.9410\n",
      "\n",
      "Epoch [50][100]\t Batch [0][550]\t Training Loss 0.2214\t Accuracy 0.9400\n",
      "Epoch [50][100]\t Batch [50][550]\t Training Loss 0.2498\t Accuracy 0.9400\n",
      "Epoch [50][100]\t Batch [100][550]\t Training Loss 0.2344\t Accuracy 0.9300\n",
      "Epoch [50][100]\t Batch [150][550]\t Training Loss 0.2269\t Accuracy 0.9200\n",
      "Epoch [50][100]\t Batch [200][550]\t Training Loss 0.2043\t Accuracy 0.9300\n",
      "Epoch [50][100]\t Batch [250][550]\t Training Loss 0.3172\t Accuracy 0.9500\n",
      "Epoch [50][100]\t Batch [300][550]\t Training Loss 0.2997\t Accuracy 0.8900\n",
      "Epoch [50][100]\t Batch [350][550]\t Training Loss 0.3261\t Accuracy 0.8600\n",
      "Epoch [50][100]\t Batch [400][550]\t Training Loss 0.2808\t Accuracy 0.9500\n",
      "Epoch [50][100]\t Batch [450][550]\t Training Loss 0.2656\t Accuracy 0.9000\n",
      "Epoch [50][100]\t Batch [500][550]\t Training Loss 0.2191\t Accuracy 0.9200\n",
      "\n",
      "Epoch [50]\t Average training loss 0.2576\t Average training accuracy 0.9285\n",
      "Epoch [50]\t Average validation loss 0.2203\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [51][100]\t Batch [0][550]\t Training Loss 0.1795\t Accuracy 0.9500\n",
      "Epoch [51][100]\t Batch [50][550]\t Training Loss 0.1951\t Accuracy 0.9300\n",
      "Epoch [51][100]\t Batch [100][550]\t Training Loss 0.1153\t Accuracy 0.9800\n",
      "Epoch [51][100]\t Batch [150][550]\t Training Loss 0.1695\t Accuracy 0.9300\n",
      "Epoch [51][100]\t Batch [200][550]\t Training Loss 0.1772\t Accuracy 0.9300\n",
      "Epoch [51][100]\t Batch [250][550]\t Training Loss 0.2334\t Accuracy 0.9400\n",
      "Epoch [51][100]\t Batch [300][550]\t Training Loss 0.2361\t Accuracy 0.9300\n",
      "Epoch [51][100]\t Batch [350][550]\t Training Loss 0.2400\t Accuracy 0.9200\n",
      "Epoch [51][100]\t Batch [400][550]\t Training Loss 0.1193\t Accuracy 0.9600\n",
      "Epoch [51][100]\t Batch [450][550]\t Training Loss 0.2271\t Accuracy 0.9100\n",
      "Epoch [51][100]\t Batch [500][550]\t Training Loss 0.3223\t Accuracy 0.9300\n",
      "\n",
      "Epoch [51]\t Average training loss 0.2571\t Average training accuracy 0.9285\n",
      "Epoch [51]\t Average validation loss 0.2192\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [52][100]\t Batch [0][550]\t Training Loss 0.1573\t Accuracy 0.9500\n",
      "Epoch [52][100]\t Batch [50][550]\t Training Loss 0.4539\t Accuracy 0.9000\n",
      "Epoch [52][100]\t Batch [100][550]\t Training Loss 0.2919\t Accuracy 0.9200\n",
      "Epoch [52][100]\t Batch [150][550]\t Training Loss 0.2406\t Accuracy 0.9600\n",
      "Epoch [52][100]\t Batch [200][550]\t Training Loss 0.3457\t Accuracy 0.8900\n",
      "Epoch [52][100]\t Batch [250][550]\t Training Loss 0.1545\t Accuracy 0.9500\n",
      "Epoch [52][100]\t Batch [300][550]\t Training Loss 0.1956\t Accuracy 0.9400\n",
      "Epoch [52][100]\t Batch [350][550]\t Training Loss 0.1938\t Accuracy 0.9300\n",
      "Epoch [52][100]\t Batch [400][550]\t Training Loss 0.3083\t Accuracy 0.9400\n",
      "Epoch [52][100]\t Batch [450][550]\t Training Loss 0.3759\t Accuracy 0.8800\n",
      "Epoch [52][100]\t Batch [500][550]\t Training Loss 0.1628\t Accuracy 0.9500\n",
      "\n",
      "Epoch [52]\t Average training loss 0.2571\t Average training accuracy 0.9281\n",
      "Epoch [52]\t Average validation loss 0.2203\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [53][100]\t Batch [0][550]\t Training Loss 0.3824\t Accuracy 0.9000\n",
      "Epoch [53][100]\t Batch [50][550]\t Training Loss 0.2386\t Accuracy 0.9100\n",
      "Epoch [53][100]\t Batch [100][550]\t Training Loss 0.2865\t Accuracy 0.9200\n",
      "Epoch [53][100]\t Batch [150][550]\t Training Loss 0.2716\t Accuracy 0.9400\n",
      "Epoch [53][100]\t Batch [200][550]\t Training Loss 0.2867\t Accuracy 0.8900\n",
      "Epoch [53][100]\t Batch [250][550]\t Training Loss 0.3088\t Accuracy 0.9200\n",
      "Epoch [53][100]\t Batch [300][550]\t Training Loss 0.3866\t Accuracy 0.8900\n",
      "Epoch [53][100]\t Batch [350][550]\t Training Loss 0.2322\t Accuracy 0.9100\n",
      "Epoch [53][100]\t Batch [400][550]\t Training Loss 0.2698\t Accuracy 0.9400\n",
      "Epoch [53][100]\t Batch [450][550]\t Training Loss 0.4376\t Accuracy 0.9200\n",
      "Epoch [53][100]\t Batch [500][550]\t Training Loss 0.2983\t Accuracy 0.9400\n",
      "\n",
      "Epoch [53]\t Average training loss 0.2567\t Average training accuracy 0.9289\n",
      "Epoch [53]\t Average validation loss 0.2203\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [54][100]\t Batch [0][550]\t Training Loss 0.2172\t Accuracy 0.9500\n",
      "Epoch [54][100]\t Batch [50][550]\t Training Loss 0.2607\t Accuracy 0.9600\n",
      "Epoch [54][100]\t Batch [100][550]\t Training Loss 0.1184\t Accuracy 0.9700\n",
      "Epoch [54][100]\t Batch [150][550]\t Training Loss 0.2697\t Accuracy 0.9200\n",
      "Epoch [54][100]\t Batch [200][550]\t Training Loss 0.4074\t Accuracy 0.9200\n",
      "Epoch [54][100]\t Batch [250][550]\t Training Loss 0.2673\t Accuracy 0.9100\n",
      "Epoch [54][100]\t Batch [300][550]\t Training Loss 0.2637\t Accuracy 0.9400\n",
      "Epoch [54][100]\t Batch [350][550]\t Training Loss 0.2358\t Accuracy 0.9600\n",
      "Epoch [54][100]\t Batch [400][550]\t Training Loss 0.1497\t Accuracy 0.9600\n",
      "Epoch [54][100]\t Batch [450][550]\t Training Loss 0.2733\t Accuracy 0.9000\n",
      "Epoch [54][100]\t Batch [500][550]\t Training Loss 0.2097\t Accuracy 0.9300\n",
      "\n",
      "Epoch [54]\t Average training loss 0.2564\t Average training accuracy 0.9292\n",
      "Epoch [54]\t Average validation loss 0.2198\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [55][100]\t Batch [0][550]\t Training Loss 0.2610\t Accuracy 0.9400\n",
      "Epoch [55][100]\t Batch [50][550]\t Training Loss 0.3232\t Accuracy 0.9000\n",
      "Epoch [55][100]\t Batch [100][550]\t Training Loss 0.2582\t Accuracy 0.9200\n",
      "Epoch [55][100]\t Batch [150][550]\t Training Loss 0.1730\t Accuracy 0.9400\n",
      "Epoch [55][100]\t Batch [200][550]\t Training Loss 0.2831\t Accuracy 0.9200\n",
      "Epoch [55][100]\t Batch [250][550]\t Training Loss 0.3271\t Accuracy 0.9000\n",
      "Epoch [55][100]\t Batch [300][550]\t Training Loss 0.2479\t Accuracy 0.9300\n",
      "Epoch [55][100]\t Batch [350][550]\t Training Loss 0.2240\t Accuracy 0.9200\n",
      "Epoch [55][100]\t Batch [400][550]\t Training Loss 0.2894\t Accuracy 0.9100\n",
      "Epoch [55][100]\t Batch [450][550]\t Training Loss 0.3249\t Accuracy 0.8900\n",
      "Epoch [55][100]\t Batch [500][550]\t Training Loss 0.2393\t Accuracy 0.9300\n",
      "\n",
      "Epoch [55]\t Average training loss 0.2563\t Average training accuracy 0.9284\n",
      "Epoch [55]\t Average validation loss 0.2194\t Average validation accuracy 0.9392\n",
      "\n",
      "Epoch [56][100]\t Batch [0][550]\t Training Loss 0.2561\t Accuracy 0.9200\n",
      "Epoch [56][100]\t Batch [50][550]\t Training Loss 0.2906\t Accuracy 0.9000\n",
      "Epoch [56][100]\t Batch [100][550]\t Training Loss 0.2546\t Accuracy 0.9100\n",
      "Epoch [56][100]\t Batch [150][550]\t Training Loss 0.1656\t Accuracy 0.9700\n",
      "Epoch [56][100]\t Batch [200][550]\t Training Loss 0.1684\t Accuracy 0.9600\n",
      "Epoch [56][100]\t Batch [250][550]\t Training Loss 0.2146\t Accuracy 0.9300\n",
      "Epoch [56][100]\t Batch [300][550]\t Training Loss 0.2931\t Accuracy 0.9300\n",
      "Epoch [56][100]\t Batch [350][550]\t Training Loss 0.1910\t Accuracy 0.9400\n",
      "Epoch [56][100]\t Batch [400][550]\t Training Loss 0.2851\t Accuracy 0.9500\n",
      "Epoch [56][100]\t Batch [450][550]\t Training Loss 0.2949\t Accuracy 0.9200\n",
      "Epoch [56][100]\t Batch [500][550]\t Training Loss 0.2593\t Accuracy 0.9000\n",
      "\n",
      "Epoch [56]\t Average training loss 0.2560\t Average training accuracy 0.9296\n",
      "Epoch [56]\t Average validation loss 0.2194\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [57][100]\t Batch [0][550]\t Training Loss 0.4853\t Accuracy 0.8800\n",
      "Epoch [57][100]\t Batch [50][550]\t Training Loss 0.1877\t Accuracy 0.9300\n",
      "Epoch [57][100]\t Batch [100][550]\t Training Loss 0.1426\t Accuracy 0.9500\n",
      "Epoch [57][100]\t Batch [150][550]\t Training Loss 0.2779\t Accuracy 0.9300\n",
      "Epoch [57][100]\t Batch [200][550]\t Training Loss 0.1277\t Accuracy 0.9500\n",
      "Epoch [57][100]\t Batch [250][550]\t Training Loss 0.3978\t Accuracy 0.8600\n",
      "Epoch [57][100]\t Batch [300][550]\t Training Loss 0.2662\t Accuracy 0.9200\n",
      "Epoch [57][100]\t Batch [350][550]\t Training Loss 0.2665\t Accuracy 0.9300\n",
      "Epoch [57][100]\t Batch [400][550]\t Training Loss 0.2184\t Accuracy 0.9400\n",
      "Epoch [57][100]\t Batch [450][550]\t Training Loss 0.3006\t Accuracy 0.9100\n",
      "Epoch [57][100]\t Batch [500][550]\t Training Loss 0.2990\t Accuracy 0.9500\n",
      "\n",
      "Epoch [57]\t Average training loss 0.2555\t Average training accuracy 0.9296\n",
      "Epoch [57]\t Average validation loss 0.2191\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [58][100]\t Batch [0][550]\t Training Loss 0.1789\t Accuracy 0.9500\n",
      "Epoch [58][100]\t Batch [50][550]\t Training Loss 0.6142\t Accuracy 0.8800\n",
      "Epoch [58][100]\t Batch [100][550]\t Training Loss 0.1667\t Accuracy 0.9700\n",
      "Epoch [58][100]\t Batch [150][550]\t Training Loss 0.2725\t Accuracy 0.9600\n",
      "Epoch [58][100]\t Batch [200][550]\t Training Loss 0.0849\t Accuracy 0.9800\n",
      "Epoch [58][100]\t Batch [250][550]\t Training Loss 0.2784\t Accuracy 0.9300\n",
      "Epoch [58][100]\t Batch [300][550]\t Training Loss 0.1567\t Accuracy 0.9600\n",
      "Epoch [58][100]\t Batch [350][550]\t Training Loss 0.2647\t Accuracy 0.9000\n",
      "Epoch [58][100]\t Batch [400][550]\t Training Loss 0.2079\t Accuracy 0.9300\n",
      "Epoch [58][100]\t Batch [450][550]\t Training Loss 0.4367\t Accuracy 0.9000\n",
      "Epoch [58][100]\t Batch [500][550]\t Training Loss 0.2372\t Accuracy 0.9500\n",
      "\n",
      "Epoch [58]\t Average training loss 0.2551\t Average training accuracy 0.9291\n",
      "Epoch [58]\t Average validation loss 0.2194\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [59][100]\t Batch [0][550]\t Training Loss 0.2718\t Accuracy 0.9300\n",
      "Epoch [59][100]\t Batch [50][550]\t Training Loss 0.1814\t Accuracy 0.9700\n",
      "Epoch [59][100]\t Batch [100][550]\t Training Loss 0.1307\t Accuracy 0.9600\n",
      "Epoch [59][100]\t Batch [150][550]\t Training Loss 0.2981\t Accuracy 0.9400\n",
      "Epoch [59][100]\t Batch [200][550]\t Training Loss 0.2927\t Accuracy 0.9100\n",
      "Epoch [59][100]\t Batch [250][550]\t Training Loss 0.2019\t Accuracy 0.9600\n",
      "Epoch [59][100]\t Batch [300][550]\t Training Loss 0.3144\t Accuracy 0.9300\n",
      "Epoch [59][100]\t Batch [350][550]\t Training Loss 0.1091\t Accuracy 0.9700\n",
      "Epoch [59][100]\t Batch [400][550]\t Training Loss 0.1975\t Accuracy 0.9300\n",
      "Epoch [59][100]\t Batch [450][550]\t Training Loss 0.3044\t Accuracy 0.9300\n",
      "Epoch [59][100]\t Batch [500][550]\t Training Loss 0.1783\t Accuracy 0.9400\n",
      "\n",
      "Epoch [59]\t Average training loss 0.2551\t Average training accuracy 0.9294\n",
      "Epoch [59]\t Average validation loss 0.2194\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [60][100]\t Batch [0][550]\t Training Loss 0.2725\t Accuracy 0.9100\n",
      "Epoch [60][100]\t Batch [50][550]\t Training Loss 0.3620\t Accuracy 0.9400\n",
      "Epoch [60][100]\t Batch [100][550]\t Training Loss 0.2144\t Accuracy 0.9200\n",
      "Epoch [60][100]\t Batch [150][550]\t Training Loss 0.2723\t Accuracy 0.9300\n",
      "Epoch [60][100]\t Batch [200][550]\t Training Loss 0.1691\t Accuracy 0.9400\n",
      "Epoch [60][100]\t Batch [250][550]\t Training Loss 0.2109\t Accuracy 0.9500\n",
      "Epoch [60][100]\t Batch [300][550]\t Training Loss 0.2387\t Accuracy 0.9300\n",
      "Epoch [60][100]\t Batch [350][550]\t Training Loss 0.3464\t Accuracy 0.8700\n",
      "Epoch [60][100]\t Batch [400][550]\t Training Loss 0.1586\t Accuracy 0.9700\n",
      "Epoch [60][100]\t Batch [450][550]\t Training Loss 0.2234\t Accuracy 0.9600\n",
      "Epoch [60][100]\t Batch [500][550]\t Training Loss 0.3084\t Accuracy 0.9600\n",
      "\n",
      "Epoch [60]\t Average training loss 0.2548\t Average training accuracy 0.9292\n",
      "Epoch [60]\t Average validation loss 0.2196\t Average validation accuracy 0.9420\n",
      "\n",
      "Epoch [61][100]\t Batch [0][550]\t Training Loss 0.2428\t Accuracy 0.9200\n",
      "Epoch [61][100]\t Batch [50][550]\t Training Loss 0.1401\t Accuracy 0.9700\n",
      "Epoch [61][100]\t Batch [100][550]\t Training Loss 0.2146\t Accuracy 0.9700\n",
      "Epoch [61][100]\t Batch [150][550]\t Training Loss 0.1497\t Accuracy 0.9600\n",
      "Epoch [61][100]\t Batch [200][550]\t Training Loss 0.4667\t Accuracy 0.9100\n",
      "Epoch [61][100]\t Batch [250][550]\t Training Loss 0.2773\t Accuracy 0.9200\n",
      "Epoch [61][100]\t Batch [300][550]\t Training Loss 0.2837\t Accuracy 0.9200\n",
      "Epoch [61][100]\t Batch [350][550]\t Training Loss 0.2547\t Accuracy 0.9100\n",
      "Epoch [61][100]\t Batch [400][550]\t Training Loss 0.1729\t Accuracy 0.9500\n",
      "Epoch [61][100]\t Batch [450][550]\t Training Loss 0.3597\t Accuracy 0.8700\n",
      "Epoch [61][100]\t Batch [500][550]\t Training Loss 0.2428\t Accuracy 0.9100\n",
      "\n",
      "Epoch [61]\t Average training loss 0.2546\t Average training accuracy 0.9295\n",
      "Epoch [61]\t Average validation loss 0.2196\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [62][100]\t Batch [0][550]\t Training Loss 0.1388\t Accuracy 0.9400\n",
      "Epoch [62][100]\t Batch [50][550]\t Training Loss 0.2519\t Accuracy 0.9200\n",
      "Epoch [62][100]\t Batch [100][550]\t Training Loss 0.1561\t Accuracy 0.9500\n",
      "Epoch [62][100]\t Batch [150][550]\t Training Loss 0.1364\t Accuracy 0.9700\n",
      "Epoch [62][100]\t Batch [200][550]\t Training Loss 0.6240\t Accuracy 0.9000\n",
      "Epoch [62][100]\t Batch [250][550]\t Training Loss 0.2272\t Accuracy 0.9500\n",
      "Epoch [62][100]\t Batch [300][550]\t Training Loss 0.3085\t Accuracy 0.9400\n",
      "Epoch [62][100]\t Batch [350][550]\t Training Loss 0.2038\t Accuracy 0.9200\n",
      "Epoch [62][100]\t Batch [400][550]\t Training Loss 0.1466\t Accuracy 0.9600\n",
      "Epoch [62][100]\t Batch [450][550]\t Training Loss 0.2346\t Accuracy 0.9200\n",
      "Epoch [62][100]\t Batch [500][550]\t Training Loss 0.2300\t Accuracy 0.9200\n",
      "\n",
      "Epoch [62]\t Average training loss 0.2543\t Average training accuracy 0.9299\n",
      "Epoch [62]\t Average validation loss 0.2192\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [63][100]\t Batch [0][550]\t Training Loss 0.3857\t Accuracy 0.8800\n",
      "Epoch [63][100]\t Batch [50][550]\t Training Loss 0.1651\t Accuracy 0.9500\n",
      "Epoch [63][100]\t Batch [100][550]\t Training Loss 0.3617\t Accuracy 0.9400\n",
      "Epoch [63][100]\t Batch [150][550]\t Training Loss 0.1814\t Accuracy 0.9200\n",
      "Epoch [63][100]\t Batch [200][550]\t Training Loss 0.1234\t Accuracy 0.9500\n",
      "Epoch [63][100]\t Batch [250][550]\t Training Loss 0.4898\t Accuracy 0.8300\n",
      "Epoch [63][100]\t Batch [300][550]\t Training Loss 0.2175\t Accuracy 0.9300\n",
      "Epoch [63][100]\t Batch [350][550]\t Training Loss 0.1488\t Accuracy 0.9700\n",
      "Epoch [63][100]\t Batch [400][550]\t Training Loss 0.1440\t Accuracy 0.9700\n",
      "Epoch [63][100]\t Batch [450][550]\t Training Loss 0.4319\t Accuracy 0.8500\n",
      "Epoch [63][100]\t Batch [500][550]\t Training Loss 0.2166\t Accuracy 0.8900\n",
      "\n",
      "Epoch [63]\t Average training loss 0.2541\t Average training accuracy 0.9301\n",
      "Epoch [63]\t Average validation loss 0.2189\t Average validation accuracy 0.9422\n",
      "\n",
      "Epoch [64][100]\t Batch [0][550]\t Training Loss 0.2704\t Accuracy 0.9300\n",
      "Epoch [64][100]\t Batch [50][550]\t Training Loss 0.2691\t Accuracy 0.9100\n",
      "Epoch [64][100]\t Batch [100][550]\t Training Loss 0.2800\t Accuracy 0.9300\n",
      "Epoch [64][100]\t Batch [150][550]\t Training Loss 0.1713\t Accuracy 0.9400\n",
      "Epoch [64][100]\t Batch [200][550]\t Training Loss 0.3888\t Accuracy 0.9300\n",
      "Epoch [64][100]\t Batch [250][550]\t Training Loss 0.3115\t Accuracy 0.9100\n",
      "Epoch [64][100]\t Batch [300][550]\t Training Loss 0.2107\t Accuracy 0.9400\n",
      "Epoch [64][100]\t Batch [350][550]\t Training Loss 0.2412\t Accuracy 0.9100\n",
      "Epoch [64][100]\t Batch [400][550]\t Training Loss 0.2153\t Accuracy 0.9500\n",
      "Epoch [64][100]\t Batch [450][550]\t Training Loss 0.1589\t Accuracy 0.9400\n",
      "Epoch [64][100]\t Batch [500][550]\t Training Loss 0.2788\t Accuracy 0.9100\n",
      "\n",
      "Epoch [64]\t Average training loss 0.2537\t Average training accuracy 0.9297\n",
      "Epoch [64]\t Average validation loss 0.2182\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [65][100]\t Batch [0][550]\t Training Loss 0.3075\t Accuracy 0.9300\n",
      "Epoch [65][100]\t Batch [50][550]\t Training Loss 0.1688\t Accuracy 0.9500\n",
      "Epoch [65][100]\t Batch [100][550]\t Training Loss 0.2219\t Accuracy 0.9400\n",
      "Epoch [65][100]\t Batch [150][550]\t Training Loss 0.1386\t Accuracy 0.9600\n",
      "Epoch [65][100]\t Batch [200][550]\t Training Loss 0.3462\t Accuracy 0.8900\n",
      "Epoch [65][100]\t Batch [250][550]\t Training Loss 0.2692\t Accuracy 0.9200\n",
      "Epoch [65][100]\t Batch [300][550]\t Training Loss 0.1734\t Accuracy 0.9700\n",
      "Epoch [65][100]\t Batch [350][550]\t Training Loss 0.2302\t Accuracy 0.9300\n",
      "Epoch [65][100]\t Batch [400][550]\t Training Loss 0.1850\t Accuracy 0.9500\n",
      "Epoch [65][100]\t Batch [450][550]\t Training Loss 0.2268\t Accuracy 0.9400\n",
      "Epoch [65][100]\t Batch [500][550]\t Training Loss 0.2701\t Accuracy 0.9500\n",
      "\n",
      "Epoch [65]\t Average training loss 0.2536\t Average training accuracy 0.9298\n",
      "Epoch [65]\t Average validation loss 0.2187\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [66][100]\t Batch [0][550]\t Training Loss 0.3382\t Accuracy 0.8900\n",
      "Epoch [66][100]\t Batch [50][550]\t Training Loss 0.2213\t Accuracy 0.9300\n",
      "Epoch [66][100]\t Batch [100][550]\t Training Loss 0.1964\t Accuracy 0.9400\n",
      "Epoch [66][100]\t Batch [150][550]\t Training Loss 0.2430\t Accuracy 0.9700\n",
      "Epoch [66][100]\t Batch [200][550]\t Training Loss 0.2182\t Accuracy 0.9400\n",
      "Epoch [66][100]\t Batch [250][550]\t Training Loss 0.2856\t Accuracy 0.9300\n",
      "Epoch [66][100]\t Batch [300][550]\t Training Loss 0.1841\t Accuracy 0.9600\n",
      "Epoch [66][100]\t Batch [350][550]\t Training Loss 0.1822\t Accuracy 0.9700\n",
      "Epoch [66][100]\t Batch [400][550]\t Training Loss 0.1664\t Accuracy 0.9600\n",
      "Epoch [66][100]\t Batch [450][550]\t Training Loss 0.4076\t Accuracy 0.9000\n",
      "Epoch [66][100]\t Batch [500][550]\t Training Loss 0.1902\t Accuracy 0.9500\n",
      "\n",
      "Epoch [66]\t Average training loss 0.2533\t Average training accuracy 0.9300\n",
      "Epoch [66]\t Average validation loss 0.2184\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [67][100]\t Batch [0][550]\t Training Loss 0.2404\t Accuracy 0.9400\n",
      "Epoch [67][100]\t Batch [50][550]\t Training Loss 0.1130\t Accuracy 0.9700\n",
      "Epoch [67][100]\t Batch [100][550]\t Training Loss 0.1571\t Accuracy 0.9600\n",
      "Epoch [67][100]\t Batch [150][550]\t Training Loss 0.1798\t Accuracy 0.9500\n",
      "Epoch [67][100]\t Batch [200][550]\t Training Loss 0.2207\t Accuracy 0.9400\n",
      "Epoch [67][100]\t Batch [250][550]\t Training Loss 0.3263\t Accuracy 0.9000\n",
      "Epoch [67][100]\t Batch [300][550]\t Training Loss 0.4035\t Accuracy 0.8900\n",
      "Epoch [67][100]\t Batch [350][550]\t Training Loss 0.2517\t Accuracy 0.9200\n",
      "Epoch [67][100]\t Batch [400][550]\t Training Loss 0.2009\t Accuracy 0.9300\n",
      "Epoch [67][100]\t Batch [450][550]\t Training Loss 0.2071\t Accuracy 0.9400\n",
      "Epoch [67][100]\t Batch [500][550]\t Training Loss 0.3381\t Accuracy 0.9100\n",
      "\n",
      "Epoch [67]\t Average training loss 0.2530\t Average training accuracy 0.9300\n",
      "Epoch [67]\t Average validation loss 0.2189\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [68][100]\t Batch [0][550]\t Training Loss 0.3245\t Accuracy 0.9000\n",
      "Epoch [68][100]\t Batch [50][550]\t Training Loss 0.2294\t Accuracy 0.9300\n",
      "Epoch [68][100]\t Batch [100][550]\t Training Loss 0.1942\t Accuracy 0.9300\n",
      "Epoch [68][100]\t Batch [150][550]\t Training Loss 0.1541\t Accuracy 0.9700\n",
      "Epoch [68][100]\t Batch [200][550]\t Training Loss 0.1858\t Accuracy 0.9500\n",
      "Epoch [68][100]\t Batch [250][550]\t Training Loss 0.3042\t Accuracy 0.9200\n",
      "Epoch [68][100]\t Batch [300][550]\t Training Loss 0.4389\t Accuracy 0.9000\n",
      "Epoch [68][100]\t Batch [350][550]\t Training Loss 0.1404\t Accuracy 0.9800\n",
      "Epoch [68][100]\t Batch [400][550]\t Training Loss 0.3664\t Accuracy 0.8900\n",
      "Epoch [68][100]\t Batch [450][550]\t Training Loss 0.1895\t Accuracy 0.9400\n",
      "Epoch [68][100]\t Batch [500][550]\t Training Loss 0.2623\t Accuracy 0.9400\n",
      "\n",
      "Epoch [68]\t Average training loss 0.2525\t Average training accuracy 0.9300\n",
      "Epoch [68]\t Average validation loss 0.2187\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [69][100]\t Batch [0][550]\t Training Loss 0.1173\t Accuracy 0.9800\n",
      "Epoch [69][100]\t Batch [50][550]\t Training Loss 0.1940\t Accuracy 0.9500\n",
      "Epoch [69][100]\t Batch [100][550]\t Training Loss 0.2754\t Accuracy 0.9300\n",
      "Epoch [69][100]\t Batch [150][550]\t Training Loss 0.2136\t Accuracy 0.9200\n",
      "Epoch [69][100]\t Batch [200][550]\t Training Loss 0.2288\t Accuracy 0.9200\n",
      "Epoch [69][100]\t Batch [250][550]\t Training Loss 0.2248\t Accuracy 0.9400\n",
      "Epoch [69][100]\t Batch [300][550]\t Training Loss 0.2091\t Accuracy 0.9500\n",
      "Epoch [69][100]\t Batch [350][550]\t Training Loss 0.2252\t Accuracy 0.9200\n",
      "Epoch [69][100]\t Batch [400][550]\t Training Loss 0.1617\t Accuracy 0.9500\n",
      "Epoch [69][100]\t Batch [450][550]\t Training Loss 0.1879\t Accuracy 0.9400\n",
      "Epoch [69][100]\t Batch [500][550]\t Training Loss 0.4558\t Accuracy 0.8700\n",
      "\n",
      "Epoch [69]\t Average training loss 0.2528\t Average training accuracy 0.9299\n",
      "Epoch [69]\t Average validation loss 0.2188\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [70][100]\t Batch [0][550]\t Training Loss 0.3074\t Accuracy 0.9100\n",
      "Epoch [70][100]\t Batch [50][550]\t Training Loss 0.1646\t Accuracy 0.9600\n",
      "Epoch [70][100]\t Batch [100][550]\t Training Loss 0.3447\t Accuracy 0.8900\n",
      "Epoch [70][100]\t Batch [150][550]\t Training Loss 0.2893\t Accuracy 0.8800\n",
      "Epoch [70][100]\t Batch [200][550]\t Training Loss 0.2609\t Accuracy 0.9300\n",
      "Epoch [70][100]\t Batch [250][550]\t Training Loss 0.1603\t Accuracy 0.9500\n",
      "Epoch [70][100]\t Batch [300][550]\t Training Loss 0.3625\t Accuracy 0.9300\n",
      "Epoch [70][100]\t Batch [350][550]\t Training Loss 0.2516\t Accuracy 0.9200\n",
      "Epoch [70][100]\t Batch [400][550]\t Training Loss 0.2312\t Accuracy 0.9200\n",
      "Epoch [70][100]\t Batch [450][550]\t Training Loss 0.2784\t Accuracy 0.9000\n",
      "Epoch [70][100]\t Batch [500][550]\t Training Loss 0.3598\t Accuracy 0.9300\n",
      "\n",
      "Epoch [70]\t Average training loss 0.2525\t Average training accuracy 0.9300\n",
      "Epoch [70]\t Average validation loss 0.2184\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [71][100]\t Batch [0][550]\t Training Loss 0.6005\t Accuracy 0.8500\n",
      "Epoch [71][100]\t Batch [50][550]\t Training Loss 0.2478\t Accuracy 0.9400\n",
      "Epoch [71][100]\t Batch [100][550]\t Training Loss 0.3401\t Accuracy 0.9100\n",
      "Epoch [71][100]\t Batch [150][550]\t Training Loss 0.2146\t Accuracy 0.9100\n",
      "Epoch [71][100]\t Batch [200][550]\t Training Loss 0.2750\t Accuracy 0.8900\n",
      "Epoch [71][100]\t Batch [250][550]\t Training Loss 0.1806\t Accuracy 0.9800\n",
      "Epoch [71][100]\t Batch [300][550]\t Training Loss 0.2339\t Accuracy 0.9400\n",
      "Epoch [71][100]\t Batch [350][550]\t Training Loss 0.2282\t Accuracy 0.9600\n",
      "Epoch [71][100]\t Batch [400][550]\t Training Loss 0.2260\t Accuracy 0.9500\n",
      "Epoch [71][100]\t Batch [450][550]\t Training Loss 0.5173\t Accuracy 0.9200\n",
      "Epoch [71][100]\t Batch [500][550]\t Training Loss 0.1809\t Accuracy 0.9300\n",
      "\n",
      "Epoch [71]\t Average training loss 0.2521\t Average training accuracy 0.9300\n",
      "Epoch [71]\t Average validation loss 0.2196\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [72][100]\t Batch [0][550]\t Training Loss 0.3135\t Accuracy 0.9200\n",
      "Epoch [72][100]\t Batch [50][550]\t Training Loss 0.1825\t Accuracy 0.9600\n",
      "Epoch [72][100]\t Batch [100][550]\t Training Loss 0.1301\t Accuracy 0.9800\n",
      "Epoch [72][100]\t Batch [150][550]\t Training Loss 0.3285\t Accuracy 0.9100\n",
      "Epoch [72][100]\t Batch [200][550]\t Training Loss 0.2013\t Accuracy 0.9500\n",
      "Epoch [72][100]\t Batch [250][550]\t Training Loss 0.2565\t Accuracy 0.9700\n",
      "Epoch [72][100]\t Batch [300][550]\t Training Loss 0.2062\t Accuracy 0.9400\n",
      "Epoch [72][100]\t Batch [350][550]\t Training Loss 0.1711\t Accuracy 0.9400\n",
      "Epoch [72][100]\t Batch [400][550]\t Training Loss 0.2448\t Accuracy 0.9600\n",
      "Epoch [72][100]\t Batch [450][550]\t Training Loss 0.1671\t Accuracy 0.9600\n",
      "Epoch [72][100]\t Batch [500][550]\t Training Loss 0.1925\t Accuracy 0.9600\n",
      "\n",
      "Epoch [72]\t Average training loss 0.2519\t Average training accuracy 0.9304\n",
      "Epoch [72]\t Average validation loss 0.2196\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [73][100]\t Batch [0][550]\t Training Loss 0.3094\t Accuracy 0.9100\n",
      "Epoch [73][100]\t Batch [50][550]\t Training Loss 0.1261\t Accuracy 0.9500\n",
      "Epoch [73][100]\t Batch [100][550]\t Training Loss 0.1496\t Accuracy 0.9600\n",
      "Epoch [73][100]\t Batch [150][550]\t Training Loss 0.3550\t Accuracy 0.9100\n",
      "Epoch [73][100]\t Batch [200][550]\t Training Loss 0.4046\t Accuracy 0.9200\n",
      "Epoch [73][100]\t Batch [250][550]\t Training Loss 0.1834\t Accuracy 0.9500\n",
      "Epoch [73][100]\t Batch [300][550]\t Training Loss 0.2841\t Accuracy 0.9300\n",
      "Epoch [73][100]\t Batch [350][550]\t Training Loss 0.2665\t Accuracy 0.9200\n",
      "Epoch [73][100]\t Batch [400][550]\t Training Loss 0.1537\t Accuracy 0.9500\n",
      "Epoch [73][100]\t Batch [450][550]\t Training Loss 0.3313\t Accuracy 0.9200\n",
      "Epoch [73][100]\t Batch [500][550]\t Training Loss 0.2614\t Accuracy 0.9300\n",
      "\n",
      "Epoch [73]\t Average training loss 0.2520\t Average training accuracy 0.9303\n",
      "Epoch [73]\t Average validation loss 0.2182\t Average validation accuracy 0.9414\n",
      "\n",
      "Epoch [74][100]\t Batch [0][550]\t Training Loss 0.3157\t Accuracy 0.9200\n",
      "Epoch [74][100]\t Batch [50][550]\t Training Loss 0.2780\t Accuracy 0.8900\n",
      "Epoch [74][100]\t Batch [100][550]\t Training Loss 0.1792\t Accuracy 0.9400\n",
      "Epoch [74][100]\t Batch [150][550]\t Training Loss 0.4554\t Accuracy 0.8700\n",
      "Epoch [74][100]\t Batch [200][550]\t Training Loss 0.4721\t Accuracy 0.9100\n",
      "Epoch [74][100]\t Batch [250][550]\t Training Loss 0.1594\t Accuracy 0.9700\n",
      "Epoch [74][100]\t Batch [300][550]\t Training Loss 0.3459\t Accuracy 0.8900\n",
      "Epoch [74][100]\t Batch [350][550]\t Training Loss 0.1774\t Accuracy 0.9300\n",
      "Epoch [74][100]\t Batch [400][550]\t Training Loss 0.3311\t Accuracy 0.9000\n",
      "Epoch [74][100]\t Batch [450][550]\t Training Loss 0.1589\t Accuracy 0.9600\n",
      "Epoch [74][100]\t Batch [500][550]\t Training Loss 0.2740\t Accuracy 0.9300\n",
      "\n",
      "Epoch [74]\t Average training loss 0.2514\t Average training accuracy 0.9309\n",
      "Epoch [74]\t Average validation loss 0.2199\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [75][100]\t Batch [0][550]\t Training Loss 0.3126\t Accuracy 0.8900\n",
      "Epoch [75][100]\t Batch [50][550]\t Training Loss 0.2218\t Accuracy 0.9200\n",
      "Epoch [75][100]\t Batch [100][550]\t Training Loss 0.2157\t Accuracy 0.9400\n",
      "Epoch [75][100]\t Batch [150][550]\t Training Loss 0.3966\t Accuracy 0.9000\n",
      "Epoch [75][100]\t Batch [200][550]\t Training Loss 0.3377\t Accuracy 0.8900\n",
      "Epoch [75][100]\t Batch [250][550]\t Training Loss 0.2407\t Accuracy 0.9500\n",
      "Epoch [75][100]\t Batch [300][550]\t Training Loss 0.3684\t Accuracy 0.8900\n",
      "Epoch [75][100]\t Batch [350][550]\t Training Loss 0.2777\t Accuracy 0.9500\n",
      "Epoch [75][100]\t Batch [400][550]\t Training Loss 0.1998\t Accuracy 0.9600\n",
      "Epoch [75][100]\t Batch [450][550]\t Training Loss 0.1851\t Accuracy 0.9300\n",
      "Epoch [75][100]\t Batch [500][550]\t Training Loss 0.2836\t Accuracy 0.9300\n",
      "\n",
      "Epoch [75]\t Average training loss 0.2514\t Average training accuracy 0.9300\n",
      "Epoch [75]\t Average validation loss 0.2185\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [76][100]\t Batch [0][550]\t Training Loss 0.2728\t Accuracy 0.9000\n",
      "Epoch [76][100]\t Batch [50][550]\t Training Loss 0.2789\t Accuracy 0.9400\n",
      "Epoch [76][100]\t Batch [100][550]\t Training Loss 0.1779\t Accuracy 0.9600\n",
      "Epoch [76][100]\t Batch [150][550]\t Training Loss 0.3320\t Accuracy 0.8900\n",
      "Epoch [76][100]\t Batch [200][550]\t Training Loss 0.4062\t Accuracy 0.9100\n",
      "Epoch [76][100]\t Batch [250][550]\t Training Loss 0.1366\t Accuracy 0.9600\n",
      "Epoch [76][100]\t Batch [300][550]\t Training Loss 0.3435\t Accuracy 0.9100\n",
      "Epoch [76][100]\t Batch [350][550]\t Training Loss 0.2210\t Accuracy 0.9400\n",
      "Epoch [76][100]\t Batch [400][550]\t Training Loss 0.2623\t Accuracy 0.9300\n",
      "Epoch [76][100]\t Batch [450][550]\t Training Loss 0.2597\t Accuracy 0.9300\n",
      "Epoch [76][100]\t Batch [500][550]\t Training Loss 0.1966\t Accuracy 0.9300\n",
      "\n",
      "Epoch [76]\t Average training loss 0.2512\t Average training accuracy 0.9304\n",
      "Epoch [76]\t Average validation loss 0.2195\t Average validation accuracy 0.9414\n",
      "\n",
      "Epoch [77][100]\t Batch [0][550]\t Training Loss 0.2514\t Accuracy 0.9400\n",
      "Epoch [77][100]\t Batch [50][550]\t Training Loss 0.1820\t Accuracy 0.9400\n",
      "Epoch [77][100]\t Batch [100][550]\t Training Loss 0.2368\t Accuracy 0.9500\n",
      "Epoch [77][100]\t Batch [150][550]\t Training Loss 0.2264\t Accuracy 0.9300\n",
      "Epoch [77][100]\t Batch [200][550]\t Training Loss 0.2957\t Accuracy 0.9500\n",
      "Epoch [77][100]\t Batch [250][550]\t Training Loss 0.1625\t Accuracy 0.9500\n",
      "Epoch [77][100]\t Batch [300][550]\t Training Loss 0.2731\t Accuracy 0.9000\n",
      "Epoch [77][100]\t Batch [350][550]\t Training Loss 0.2502\t Accuracy 0.9300\n",
      "Epoch [77][100]\t Batch [400][550]\t Training Loss 0.2291\t Accuracy 0.9500\n",
      "Epoch [77][100]\t Batch [450][550]\t Training Loss 0.2442\t Accuracy 0.9100\n",
      "Epoch [77][100]\t Batch [500][550]\t Training Loss 0.3058\t Accuracy 0.9400\n",
      "\n",
      "Epoch [77]\t Average training loss 0.2510\t Average training accuracy 0.9306\n",
      "Epoch [77]\t Average validation loss 0.2189\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [78][100]\t Batch [0][550]\t Training Loss 0.1680\t Accuracy 0.9400\n",
      "Epoch [78][100]\t Batch [50][550]\t Training Loss 0.1742\t Accuracy 0.9400\n",
      "Epoch [78][100]\t Batch [100][550]\t Training Loss 0.2687\t Accuracy 0.9100\n",
      "Epoch [78][100]\t Batch [150][550]\t Training Loss 0.1319\t Accuracy 0.9600\n",
      "Epoch [78][100]\t Batch [200][550]\t Training Loss 0.2882\t Accuracy 0.9300\n",
      "Epoch [78][100]\t Batch [250][550]\t Training Loss 0.2606\t Accuracy 0.9200\n",
      "Epoch [78][100]\t Batch [300][550]\t Training Loss 0.2581\t Accuracy 0.9000\n",
      "Epoch [78][100]\t Batch [350][550]\t Training Loss 0.3352\t Accuracy 0.8900\n",
      "Epoch [78][100]\t Batch [400][550]\t Training Loss 0.2234\t Accuracy 0.9500\n",
      "Epoch [78][100]\t Batch [450][550]\t Training Loss 0.2910\t Accuracy 0.9200\n",
      "Epoch [78][100]\t Batch [500][550]\t Training Loss 0.1809\t Accuracy 0.9300\n",
      "\n",
      "Epoch [78]\t Average training loss 0.2509\t Average training accuracy 0.9307\n",
      "Epoch [78]\t Average validation loss 0.2186\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [79][100]\t Batch [0][550]\t Training Loss 0.1901\t Accuracy 0.9200\n",
      "Epoch [79][100]\t Batch [50][550]\t Training Loss 0.3707\t Accuracy 0.9000\n",
      "Epoch [79][100]\t Batch [100][550]\t Training Loss 0.2487\t Accuracy 0.9400\n",
      "Epoch [79][100]\t Batch [150][550]\t Training Loss 0.2419\t Accuracy 0.9400\n",
      "Epoch [79][100]\t Batch [200][550]\t Training Loss 0.1802\t Accuracy 0.9500\n",
      "Epoch [79][100]\t Batch [250][550]\t Training Loss 0.2643\t Accuracy 0.9100\n",
      "Epoch [79][100]\t Batch [300][550]\t Training Loss 0.3482\t Accuracy 0.9200\n",
      "Epoch [79][100]\t Batch [350][550]\t Training Loss 0.1662\t Accuracy 0.9300\n",
      "Epoch [79][100]\t Batch [400][550]\t Training Loss 0.3838\t Accuracy 0.8800\n",
      "Epoch [79][100]\t Batch [450][550]\t Training Loss 0.2165\t Accuracy 0.9300\n",
      "Epoch [79][100]\t Batch [500][550]\t Training Loss 0.1137\t Accuracy 0.9800\n",
      "\n",
      "Epoch [79]\t Average training loss 0.2506\t Average training accuracy 0.9311\n",
      "Epoch [79]\t Average validation loss 0.2188\t Average validation accuracy 0.9414\n",
      "\n",
      "Epoch [80][100]\t Batch [0][550]\t Training Loss 0.1523\t Accuracy 0.9500\n",
      "Epoch [80][100]\t Batch [50][550]\t Training Loss 0.1697\t Accuracy 0.9400\n",
      "Epoch [80][100]\t Batch [100][550]\t Training Loss 0.0873\t Accuracy 1.0000\n",
      "Epoch [80][100]\t Batch [150][550]\t Training Loss 0.1960\t Accuracy 0.9200\n",
      "Epoch [80][100]\t Batch [200][550]\t Training Loss 0.3147\t Accuracy 0.9200\n",
      "Epoch [80][100]\t Batch [250][550]\t Training Loss 0.3795\t Accuracy 0.9200\n",
      "Epoch [80][100]\t Batch [300][550]\t Training Loss 0.2339\t Accuracy 0.9500\n",
      "Epoch [80][100]\t Batch [350][550]\t Training Loss 0.2668\t Accuracy 0.9500\n",
      "Epoch [80][100]\t Batch [400][550]\t Training Loss 0.2749\t Accuracy 0.8900\n",
      "Epoch [80][100]\t Batch [450][550]\t Training Loss 0.2786\t Accuracy 0.9000\n",
      "Epoch [80][100]\t Batch [500][550]\t Training Loss 0.2014\t Accuracy 0.9700\n",
      "\n",
      "Epoch [80]\t Average training loss 0.2505\t Average training accuracy 0.9308\n",
      "Epoch [80]\t Average validation loss 0.2182\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [81][100]\t Batch [0][550]\t Training Loss 0.2427\t Accuracy 0.9100\n",
      "Epoch [81][100]\t Batch [50][550]\t Training Loss 0.2224\t Accuracy 0.9500\n",
      "Epoch [81][100]\t Batch [100][550]\t Training Loss 0.3575\t Accuracy 0.9300\n",
      "Epoch [81][100]\t Batch [150][550]\t Training Loss 0.3495\t Accuracy 0.9400\n",
      "Epoch [81][100]\t Batch [200][550]\t Training Loss 0.2636\t Accuracy 0.9000\n",
      "Epoch [81][100]\t Batch [250][550]\t Training Loss 0.2855\t Accuracy 0.9000\n",
      "Epoch [81][100]\t Batch [300][550]\t Training Loss 0.1901\t Accuracy 0.9200\n",
      "Epoch [81][100]\t Batch [350][550]\t Training Loss 0.3987\t Accuracy 0.8800\n",
      "Epoch [81][100]\t Batch [400][550]\t Training Loss 0.1991\t Accuracy 0.9600\n",
      "Epoch [81][100]\t Batch [450][550]\t Training Loss 0.2769\t Accuracy 0.9400\n",
      "Epoch [81][100]\t Batch [500][550]\t Training Loss 0.3087\t Accuracy 0.9300\n",
      "\n",
      "Epoch [81]\t Average training loss 0.2505\t Average training accuracy 0.9307\n",
      "Epoch [81]\t Average validation loss 0.2191\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [82][100]\t Batch [0][550]\t Training Loss 0.1779\t Accuracy 0.9100\n",
      "Epoch [82][100]\t Batch [50][550]\t Training Loss 0.2825\t Accuracy 0.9100\n",
      "Epoch [82][100]\t Batch [100][550]\t Training Loss 0.3833\t Accuracy 0.9100\n",
      "Epoch [82][100]\t Batch [150][550]\t Training Loss 0.1533\t Accuracy 0.9700\n",
      "Epoch [82][100]\t Batch [200][550]\t Training Loss 0.1748\t Accuracy 0.9400\n",
      "Epoch [82][100]\t Batch [250][550]\t Training Loss 0.4697\t Accuracy 0.9100\n",
      "Epoch [82][100]\t Batch [300][550]\t Training Loss 0.2115\t Accuracy 0.9700\n",
      "Epoch [82][100]\t Batch [350][550]\t Training Loss 0.4089\t Accuracy 0.9000\n",
      "Epoch [82][100]\t Batch [400][550]\t Training Loss 0.1606\t Accuracy 0.9300\n",
      "Epoch [82][100]\t Batch [450][550]\t Training Loss 0.1570\t Accuracy 0.9500\n",
      "Epoch [82][100]\t Batch [500][550]\t Training Loss 0.1484\t Accuracy 0.9500\n",
      "\n",
      "Epoch [82]\t Average training loss 0.2500\t Average training accuracy 0.9307\n",
      "Epoch [82]\t Average validation loss 0.2188\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [83][100]\t Batch [0][550]\t Training Loss 0.2010\t Accuracy 0.9600\n",
      "Epoch [83][100]\t Batch [50][550]\t Training Loss 0.2285\t Accuracy 0.9400\n",
      "Epoch [83][100]\t Batch [100][550]\t Training Loss 0.1608\t Accuracy 0.9400\n",
      "Epoch [83][100]\t Batch [150][550]\t Training Loss 0.2072\t Accuracy 0.9500\n",
      "Epoch [83][100]\t Batch [200][550]\t Training Loss 0.2469\t Accuracy 0.9100\n",
      "Epoch [83][100]\t Batch [250][550]\t Training Loss 0.2661\t Accuracy 0.9200\n",
      "Epoch [83][100]\t Batch [300][550]\t Training Loss 0.2195\t Accuracy 0.9600\n",
      "Epoch [83][100]\t Batch [350][550]\t Training Loss 0.1883\t Accuracy 0.9500\n",
      "Epoch [83][100]\t Batch [400][550]\t Training Loss 0.3638\t Accuracy 0.9000\n",
      "Epoch [83][100]\t Batch [450][550]\t Training Loss 0.2610\t Accuracy 0.9200\n",
      "Epoch [83][100]\t Batch [500][550]\t Training Loss 0.3392\t Accuracy 0.8800\n",
      "\n",
      "Epoch [83]\t Average training loss 0.2498\t Average training accuracy 0.9306\n",
      "Epoch [83]\t Average validation loss 0.2192\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [84][100]\t Batch [0][550]\t Training Loss 0.1622\t Accuracy 0.9500\n",
      "Epoch [84][100]\t Batch [50][550]\t Training Loss 0.2575\t Accuracy 0.9400\n",
      "Epoch [84][100]\t Batch [100][550]\t Training Loss 0.1844\t Accuracy 0.9300\n",
      "Epoch [84][100]\t Batch [150][550]\t Training Loss 0.2172\t Accuracy 0.9200\n",
      "Epoch [84][100]\t Batch [200][550]\t Training Loss 0.4266\t Accuracy 0.8900\n",
      "Epoch [84][100]\t Batch [250][550]\t Training Loss 0.2087\t Accuracy 0.9300\n",
      "Epoch [84][100]\t Batch [300][550]\t Training Loss 0.3719\t Accuracy 0.9000\n",
      "Epoch [84][100]\t Batch [350][550]\t Training Loss 0.2090\t Accuracy 0.9500\n",
      "Epoch [84][100]\t Batch [400][550]\t Training Loss 0.2180\t Accuracy 0.9100\n",
      "Epoch [84][100]\t Batch [450][550]\t Training Loss 0.2663\t Accuracy 0.9300\n",
      "Epoch [84][100]\t Batch [500][550]\t Training Loss 0.3077\t Accuracy 0.9200\n",
      "\n",
      "Epoch [84]\t Average training loss 0.2497\t Average training accuracy 0.9307\n",
      "Epoch [84]\t Average validation loss 0.2192\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [85][100]\t Batch [0][550]\t Training Loss 0.4260\t Accuracy 0.8900\n",
      "Epoch [85][100]\t Batch [50][550]\t Training Loss 0.4255\t Accuracy 0.8700\n",
      "Epoch [85][100]\t Batch [100][550]\t Training Loss 0.3261\t Accuracy 0.9300\n",
      "Epoch [85][100]\t Batch [150][550]\t Training Loss 0.1794\t Accuracy 0.9700\n",
      "Epoch [85][100]\t Batch [200][550]\t Training Loss 0.1271\t Accuracy 0.9800\n",
      "Epoch [85][100]\t Batch [250][550]\t Training Loss 0.2305\t Accuracy 0.9200\n",
      "Epoch [85][100]\t Batch [300][550]\t Training Loss 0.2207\t Accuracy 0.9500\n",
      "Epoch [85][100]\t Batch [350][550]\t Training Loss 0.1928\t Accuracy 0.9500\n",
      "Epoch [85][100]\t Batch [400][550]\t Training Loss 0.1870\t Accuracy 0.9500\n",
      "Epoch [85][100]\t Batch [450][550]\t Training Loss 0.1528\t Accuracy 0.9700\n",
      "Epoch [85][100]\t Batch [500][550]\t Training Loss 0.3274\t Accuracy 0.9200\n",
      "\n",
      "Epoch [85]\t Average training loss 0.2495\t Average training accuracy 0.9307\n",
      "Epoch [85]\t Average validation loss 0.2194\t Average validation accuracy 0.9428\n",
      "\n",
      "Epoch [86][100]\t Batch [0][550]\t Training Loss 0.2428\t Accuracy 0.9500\n",
      "Epoch [86][100]\t Batch [50][550]\t Training Loss 0.1245\t Accuracy 0.9600\n",
      "Epoch [86][100]\t Batch [100][550]\t Training Loss 0.3015\t Accuracy 0.9400\n",
      "Epoch [86][100]\t Batch [150][550]\t Training Loss 0.2150\t Accuracy 0.9300\n",
      "Epoch [86][100]\t Batch [200][550]\t Training Loss 0.1897\t Accuracy 0.9100\n",
      "Epoch [86][100]\t Batch [250][550]\t Training Loss 0.3537\t Accuracy 0.9300\n",
      "Epoch [86][100]\t Batch [300][550]\t Training Loss 0.3145\t Accuracy 0.8900\n",
      "Epoch [86][100]\t Batch [350][550]\t Training Loss 0.2155\t Accuracy 0.9500\n",
      "Epoch [86][100]\t Batch [400][550]\t Training Loss 0.1334\t Accuracy 0.9700\n",
      "Epoch [86][100]\t Batch [450][550]\t Training Loss 0.1876\t Accuracy 0.9500\n",
      "Epoch [86][100]\t Batch [500][550]\t Training Loss 0.2053\t Accuracy 0.9300\n",
      "\n",
      "Epoch [86]\t Average training loss 0.2493\t Average training accuracy 0.9310\n",
      "Epoch [86]\t Average validation loss 0.2185\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [87][100]\t Batch [0][550]\t Training Loss 0.2230\t Accuracy 0.9300\n",
      "Epoch [87][100]\t Batch [50][550]\t Training Loss 0.2926\t Accuracy 0.9000\n",
      "Epoch [87][100]\t Batch [100][550]\t Training Loss 0.3470\t Accuracy 0.9500\n",
      "Epoch [87][100]\t Batch [150][550]\t Training Loss 0.1706\t Accuracy 0.9400\n",
      "Epoch [87][100]\t Batch [200][550]\t Training Loss 0.1992\t Accuracy 0.9400\n",
      "Epoch [87][100]\t Batch [250][550]\t Training Loss 0.2624\t Accuracy 0.9100\n",
      "Epoch [87][100]\t Batch [300][550]\t Training Loss 0.2166\t Accuracy 0.9200\n",
      "Epoch [87][100]\t Batch [350][550]\t Training Loss 0.3427\t Accuracy 0.9300\n",
      "Epoch [87][100]\t Batch [400][550]\t Training Loss 0.1988\t Accuracy 0.9600\n",
      "Epoch [87][100]\t Batch [450][550]\t Training Loss 0.2149\t Accuracy 0.9500\n",
      "Epoch [87][100]\t Batch [500][550]\t Training Loss 0.1964\t Accuracy 0.9400\n",
      "\n",
      "Epoch [87]\t Average training loss 0.2491\t Average training accuracy 0.9315\n",
      "Epoch [87]\t Average validation loss 0.2178\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [88][100]\t Batch [0][550]\t Training Loss 0.1127\t Accuracy 0.9800\n",
      "Epoch [88][100]\t Batch [50][550]\t Training Loss 0.2426\t Accuracy 0.9300\n",
      "Epoch [88][100]\t Batch [100][550]\t Training Loss 0.4326\t Accuracy 0.8900\n",
      "Epoch [88][100]\t Batch [150][550]\t Training Loss 0.3185\t Accuracy 0.9400\n",
      "Epoch [88][100]\t Batch [200][550]\t Training Loss 0.1285\t Accuracy 0.9500\n",
      "Epoch [88][100]\t Batch [250][550]\t Training Loss 0.2025\t Accuracy 0.9400\n",
      "Epoch [88][100]\t Batch [300][550]\t Training Loss 0.1798\t Accuracy 0.9500\n",
      "Epoch [88][100]\t Batch [350][550]\t Training Loss 0.2947\t Accuracy 0.9200\n",
      "Epoch [88][100]\t Batch [400][550]\t Training Loss 0.2100\t Accuracy 0.9200\n",
      "Epoch [88][100]\t Batch [450][550]\t Training Loss 0.2437\t Accuracy 0.9200\n",
      "Epoch [88][100]\t Batch [500][550]\t Training Loss 0.3748\t Accuracy 0.9000\n",
      "\n",
      "Epoch [88]\t Average training loss 0.2492\t Average training accuracy 0.9315\n",
      "Epoch [88]\t Average validation loss 0.2191\t Average validation accuracy 0.9414\n",
      "\n",
      "Epoch [89][100]\t Batch [0][550]\t Training Loss 0.2414\t Accuracy 0.9200\n",
      "Epoch [89][100]\t Batch [50][550]\t Training Loss 0.2022\t Accuracy 0.9200\n",
      "Epoch [89][100]\t Batch [100][550]\t Training Loss 0.1593\t Accuracy 0.9500\n",
      "Epoch [89][100]\t Batch [150][550]\t Training Loss 0.2363\t Accuracy 0.9300\n",
      "Epoch [89][100]\t Batch [200][550]\t Training Loss 0.2267\t Accuracy 0.9500\n",
      "Epoch [89][100]\t Batch [250][550]\t Training Loss 0.3171\t Accuracy 0.8800\n",
      "Epoch [89][100]\t Batch [300][550]\t Training Loss 0.3724\t Accuracy 0.8800\n",
      "Epoch [89][100]\t Batch [350][550]\t Training Loss 0.2717\t Accuracy 0.8800\n",
      "Epoch [89][100]\t Batch [400][550]\t Training Loss 0.2076\t Accuracy 0.9200\n",
      "Epoch [89][100]\t Batch [450][550]\t Training Loss 0.2519\t Accuracy 0.9200\n",
      "Epoch [89][100]\t Batch [500][550]\t Training Loss 0.1895\t Accuracy 0.9500\n",
      "\n",
      "Epoch [89]\t Average training loss 0.2488\t Average training accuracy 0.9310\n",
      "Epoch [89]\t Average validation loss 0.2183\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [90][100]\t Batch [0][550]\t Training Loss 0.2554\t Accuracy 0.9000\n",
      "Epoch [90][100]\t Batch [50][550]\t Training Loss 0.2762\t Accuracy 0.9200\n",
      "Epoch [90][100]\t Batch [100][550]\t Training Loss 0.2923\t Accuracy 0.9300\n",
      "Epoch [90][100]\t Batch [150][550]\t Training Loss 0.2195\t Accuracy 0.9300\n",
      "Epoch [90][100]\t Batch [200][550]\t Training Loss 0.4815\t Accuracy 0.8800\n",
      "Epoch [90][100]\t Batch [250][550]\t Training Loss 0.1706\t Accuracy 0.9600\n",
      "Epoch [90][100]\t Batch [300][550]\t Training Loss 0.2041\t Accuracy 0.9600\n",
      "Epoch [90][100]\t Batch [350][550]\t Training Loss 0.2366\t Accuracy 0.8900\n",
      "Epoch [90][100]\t Batch [400][550]\t Training Loss 0.2053\t Accuracy 0.9400\n",
      "Epoch [90][100]\t Batch [450][550]\t Training Loss 0.3890\t Accuracy 0.8700\n",
      "Epoch [90][100]\t Batch [500][550]\t Training Loss 0.1639\t Accuracy 0.9500\n",
      "\n",
      "Epoch [90]\t Average training loss 0.2488\t Average training accuracy 0.9312\n",
      "Epoch [90]\t Average validation loss 0.2186\t Average validation accuracy 0.9414\n",
      "\n",
      "Epoch [91][100]\t Batch [0][550]\t Training Loss 0.2709\t Accuracy 0.9200\n",
      "Epoch [91][100]\t Batch [50][550]\t Training Loss 0.3037\t Accuracy 0.9600\n",
      "Epoch [91][100]\t Batch [100][550]\t Training Loss 0.1460\t Accuracy 0.9600\n",
      "Epoch [91][100]\t Batch [150][550]\t Training Loss 0.3482\t Accuracy 0.8900\n",
      "Epoch [91][100]\t Batch [200][550]\t Training Loss 0.1442\t Accuracy 0.9700\n",
      "Epoch [91][100]\t Batch [250][550]\t Training Loss 0.2145\t Accuracy 0.9400\n",
      "Epoch [91][100]\t Batch [300][550]\t Training Loss 0.2780\t Accuracy 0.9200\n",
      "Epoch [91][100]\t Batch [350][550]\t Training Loss 0.1716\t Accuracy 0.9300\n",
      "Epoch [91][100]\t Batch [400][550]\t Training Loss 0.3099\t Accuracy 0.9100\n",
      "Epoch [91][100]\t Batch [450][550]\t Training Loss 0.3456\t Accuracy 0.9100\n",
      "Epoch [91][100]\t Batch [500][550]\t Training Loss 0.2700\t Accuracy 0.9200\n",
      "\n",
      "Epoch [91]\t Average training loss 0.2485\t Average training accuracy 0.9315\n",
      "Epoch [91]\t Average validation loss 0.2182\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [92][100]\t Batch [0][550]\t Training Loss 0.3123\t Accuracy 0.9400\n",
      "Epoch [92][100]\t Batch [50][550]\t Training Loss 0.2286\t Accuracy 0.9300\n",
      "Epoch [92][100]\t Batch [100][550]\t Training Loss 0.2426\t Accuracy 0.9300\n",
      "Epoch [92][100]\t Batch [150][550]\t Training Loss 0.3702\t Accuracy 0.9300\n",
      "Epoch [92][100]\t Batch [200][550]\t Training Loss 0.2127\t Accuracy 0.9700\n",
      "Epoch [92][100]\t Batch [250][550]\t Training Loss 0.4469\t Accuracy 0.8900\n",
      "Epoch [92][100]\t Batch [300][550]\t Training Loss 0.1044\t Accuracy 0.9700\n",
      "Epoch [92][100]\t Batch [350][550]\t Training Loss 0.2437\t Accuracy 0.9100\n",
      "Epoch [92][100]\t Batch [400][550]\t Training Loss 0.1305\t Accuracy 0.9500\n",
      "Epoch [92][100]\t Batch [450][550]\t Training Loss 0.3341\t Accuracy 0.9100\n",
      "Epoch [92][100]\t Batch [500][550]\t Training Loss 0.3230\t Accuracy 0.9200\n",
      "\n",
      "Epoch [92]\t Average training loss 0.2485\t Average training accuracy 0.9314\n",
      "Epoch [92]\t Average validation loss 0.2180\t Average validation accuracy 0.9418\n",
      "\n",
      "Epoch [93][100]\t Batch [0][550]\t Training Loss 0.4255\t Accuracy 0.9200\n",
      "Epoch [93][100]\t Batch [50][550]\t Training Loss 0.2567\t Accuracy 0.9200\n",
      "Epoch [93][100]\t Batch [100][550]\t Training Loss 0.3601\t Accuracy 0.9200\n",
      "Epoch [93][100]\t Batch [150][550]\t Training Loss 0.2529\t Accuracy 0.9300\n",
      "Epoch [93][100]\t Batch [200][550]\t Training Loss 0.1130\t Accuracy 0.9900\n",
      "Epoch [93][100]\t Batch [250][550]\t Training Loss 0.4909\t Accuracy 0.9000\n",
      "Epoch [93][100]\t Batch [300][550]\t Training Loss 0.2380\t Accuracy 0.9300\n",
      "Epoch [93][100]\t Batch [350][550]\t Training Loss 0.3393\t Accuracy 0.8800\n",
      "Epoch [93][100]\t Batch [400][550]\t Training Loss 0.3173\t Accuracy 0.8900\n",
      "Epoch [93][100]\t Batch [450][550]\t Training Loss 0.2479\t Accuracy 0.9100\n",
      "Epoch [93][100]\t Batch [500][550]\t Training Loss 0.3370\t Accuracy 0.9300\n",
      "\n",
      "Epoch [93]\t Average training loss 0.2483\t Average training accuracy 0.9319\n",
      "Epoch [93]\t Average validation loss 0.2181\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [94][100]\t Batch [0][550]\t Training Loss 0.4078\t Accuracy 0.9000\n",
      "Epoch [94][100]\t Batch [50][550]\t Training Loss 0.3611\t Accuracy 0.9000\n",
      "Epoch [94][100]\t Batch [100][550]\t Training Loss 0.2447\t Accuracy 0.9200\n",
      "Epoch [94][100]\t Batch [150][550]\t Training Loss 0.3610\t Accuracy 0.9100\n",
      "Epoch [94][100]\t Batch [200][550]\t Training Loss 0.3476\t Accuracy 0.8700\n",
      "Epoch [94][100]\t Batch [250][550]\t Training Loss 0.2350\t Accuracy 0.9400\n",
      "Epoch [94][100]\t Batch [300][550]\t Training Loss 0.3006\t Accuracy 0.8900\n",
      "Epoch [94][100]\t Batch [350][550]\t Training Loss 0.2110\t Accuracy 0.9500\n",
      "Epoch [94][100]\t Batch [400][550]\t Training Loss 0.2281\t Accuracy 0.9400\n",
      "Epoch [94][100]\t Batch [450][550]\t Training Loss 0.1668\t Accuracy 0.9500\n",
      "Epoch [94][100]\t Batch [500][550]\t Training Loss 0.2444\t Accuracy 0.9200\n",
      "\n",
      "Epoch [94]\t Average training loss 0.2480\t Average training accuracy 0.9317\n",
      "Epoch [94]\t Average validation loss 0.2178\t Average validation accuracy 0.9428\n",
      "\n",
      "Epoch [95][100]\t Batch [0][550]\t Training Loss 0.2343\t Accuracy 0.9300\n",
      "Epoch [95][100]\t Batch [50][550]\t Training Loss 0.1798\t Accuracy 0.9400\n",
      "Epoch [95][100]\t Batch [100][550]\t Training Loss 0.1479\t Accuracy 0.9700\n",
      "Epoch [95][100]\t Batch [150][550]\t Training Loss 0.1510\t Accuracy 0.9600\n",
      "Epoch [95][100]\t Batch [200][550]\t Training Loss 0.2250\t Accuracy 0.9300\n",
      "Epoch [95][100]\t Batch [250][550]\t Training Loss 0.2272\t Accuracy 0.9300\n",
      "Epoch [95][100]\t Batch [300][550]\t Training Loss 0.2636\t Accuracy 0.9300\n",
      "Epoch [95][100]\t Batch [350][550]\t Training Loss 0.1739\t Accuracy 0.9400\n",
      "Epoch [95][100]\t Batch [400][550]\t Training Loss 0.3297\t Accuracy 0.8900\n",
      "Epoch [95][100]\t Batch [450][550]\t Training Loss 0.2749\t Accuracy 0.9100\n",
      "Epoch [95][100]\t Batch [500][550]\t Training Loss 0.1766\t Accuracy 0.9700\n",
      "\n",
      "Epoch [95]\t Average training loss 0.2480\t Average training accuracy 0.9313\n",
      "Epoch [95]\t Average validation loss 0.2192\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [96][100]\t Batch [0][550]\t Training Loss 0.1922\t Accuracy 0.9400\n",
      "Epoch [96][100]\t Batch [50][550]\t Training Loss 0.3998\t Accuracy 0.9000\n",
      "Epoch [96][100]\t Batch [100][550]\t Training Loss 0.1714\t Accuracy 0.9600\n",
      "Epoch [96][100]\t Batch [150][550]\t Training Loss 0.1903\t Accuracy 0.9500\n",
      "Epoch [96][100]\t Batch [200][550]\t Training Loss 0.1997\t Accuracy 0.9500\n",
      "Epoch [96][100]\t Batch [250][550]\t Training Loss 0.1250\t Accuracy 0.9600\n",
      "Epoch [96][100]\t Batch [300][550]\t Training Loss 0.2102\t Accuracy 0.9100\n",
      "Epoch [96][100]\t Batch [350][550]\t Training Loss 0.1837\t Accuracy 0.9400\n",
      "Epoch [96][100]\t Batch [400][550]\t Training Loss 0.2549\t Accuracy 0.9200\n",
      "Epoch [96][100]\t Batch [450][550]\t Training Loss 0.2468\t Accuracy 0.9600\n",
      "Epoch [96][100]\t Batch [500][550]\t Training Loss 0.2617\t Accuracy 0.9300\n",
      "\n",
      "Epoch [96]\t Average training loss 0.2478\t Average training accuracy 0.9317\n",
      "Epoch [96]\t Average validation loss 0.2190\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [97][100]\t Batch [0][550]\t Training Loss 0.2082\t Accuracy 0.9400\n",
      "Epoch [97][100]\t Batch [50][550]\t Training Loss 0.2308\t Accuracy 0.9600\n",
      "Epoch [97][100]\t Batch [100][550]\t Training Loss 0.2325\t Accuracy 0.9500\n",
      "Epoch [97][100]\t Batch [150][550]\t Training Loss 0.2065\t Accuracy 0.9600\n",
      "Epoch [97][100]\t Batch [200][550]\t Training Loss 0.1236\t Accuracy 0.9600\n",
      "Epoch [97][100]\t Batch [250][550]\t Training Loss 0.1750\t Accuracy 0.9400\n",
      "Epoch [97][100]\t Batch [300][550]\t Training Loss 0.1947\t Accuracy 0.8900\n",
      "Epoch [97][100]\t Batch [350][550]\t Training Loss 0.2360\t Accuracy 0.9300\n",
      "Epoch [97][100]\t Batch [400][550]\t Training Loss 0.1418\t Accuracy 0.9400\n",
      "Epoch [97][100]\t Batch [450][550]\t Training Loss 0.1502\t Accuracy 0.9400\n",
      "Epoch [97][100]\t Batch [500][550]\t Training Loss 0.4325\t Accuracy 0.8800\n",
      "\n",
      "Epoch [97]\t Average training loss 0.2477\t Average training accuracy 0.9318\n",
      "Epoch [97]\t Average validation loss 0.2192\t Average validation accuracy 0.9418\n",
      "\n",
      "Epoch [98][100]\t Batch [0][550]\t Training Loss 0.4135\t Accuracy 0.8900\n",
      "Epoch [98][100]\t Batch [50][550]\t Training Loss 0.3204\t Accuracy 0.9100\n",
      "Epoch [98][100]\t Batch [100][550]\t Training Loss 0.3799\t Accuracy 0.9300\n",
      "Epoch [98][100]\t Batch [150][550]\t Training Loss 0.3370\t Accuracy 0.9100\n",
      "Epoch [98][100]\t Batch [200][550]\t Training Loss 0.2993\t Accuracy 0.9300\n",
      "Epoch [98][100]\t Batch [250][550]\t Training Loss 0.2180\t Accuracy 0.9200\n",
      "Epoch [98][100]\t Batch [300][550]\t Training Loss 0.2136\t Accuracy 0.9100\n",
      "Epoch [98][100]\t Batch [350][550]\t Training Loss 0.1422\t Accuracy 0.9600\n",
      "Epoch [98][100]\t Batch [400][550]\t Training Loss 0.1861\t Accuracy 0.9600\n",
      "Epoch [98][100]\t Batch [450][550]\t Training Loss 0.1233\t Accuracy 0.9500\n",
      "Epoch [98][100]\t Batch [500][550]\t Training Loss 0.2931\t Accuracy 0.9100\n",
      "\n",
      "Epoch [98]\t Average training loss 0.2474\t Average training accuracy 0.9317\n",
      "Epoch [98]\t Average validation loss 0.2182\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [99][100]\t Batch [0][550]\t Training Loss 0.1849\t Accuracy 0.9400\n",
      "Epoch [99][100]\t Batch [50][550]\t Training Loss 0.3071\t Accuracy 0.9500\n",
      "Epoch [99][100]\t Batch [100][550]\t Training Loss 0.4432\t Accuracy 0.8600\n",
      "Epoch [99][100]\t Batch [150][550]\t Training Loss 0.4927\t Accuracy 0.8800\n",
      "Epoch [99][100]\t Batch [200][550]\t Training Loss 0.6305\t Accuracy 0.8800\n",
      "Epoch [99][100]\t Batch [250][550]\t Training Loss 0.2448\t Accuracy 0.9000\n",
      "Epoch [99][100]\t Batch [300][550]\t Training Loss 0.1784\t Accuracy 0.9300\n",
      "Epoch [99][100]\t Batch [350][550]\t Training Loss 0.2566\t Accuracy 0.9100\n",
      "Epoch [99][100]\t Batch [400][550]\t Training Loss 0.2873\t Accuracy 0.9300\n",
      "Epoch [99][100]\t Batch [450][550]\t Training Loss 0.2431\t Accuracy 0.9400\n",
      "Epoch [99][100]\t Batch [500][550]\t Training Loss 0.1203\t Accuracy 0.9600\n",
      "\n",
      "Epoch [99]\t Average training loss 0.2473\t Average training accuracy 0.9316\n",
      "Epoch [99]\t Average validation loss 0.2187\t Average validation accuracy 0.9396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train with different max_epoch size\n",
    "max_epoch = 100\n",
    "cfg = {\n",
    "    'data_root': 'data',\n",
    "    'max_epoch': max_epoch,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'display_freq': 50,\n",
    "}\n",
    "\n",
    "runner = Solver(cfg)\n",
    "loss7, acc7 = runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_epoch = 100\n",
    "plot_graph({\n",
    "    \"x\": { \"max_epoch\": [i for i in range(max_epoch)]},\n",
    "    \"y\": [\n",
    "        { \"loss\": loss7 }, \n",
    "        { \"accuracy\": acc7 }\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
