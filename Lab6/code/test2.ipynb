{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/DL Lab/Lab6/code')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lzYhPXv98cGO","executionInfo":{"status":"ok","timestamp":1652019974296,"user_tz":-480,"elapsed":21410,"user":{"displayName":"SI YUAN BIAN","userId":"02322420278915835023"}},"outputId":"613b955c-3991-4e76-8956-680bcfeaf724"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install Pillow\n","!pip install scipy==1.1.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suL5J5xY8k5f","executionInfo":{"status":"ok","timestamp":1651989729103,"user_tz":-480,"elapsed":11260,"user":{"displayName":"SI YUAN BIAN","userId":"02322420278915835023"}},"outputId":"5540f48b-7175-43e1-c3c8-6cce048bc828"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","Collecting scipy==1.1.0\n","  Downloading scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n","\u001b[K     |████████████████████████████████| 31.2 MB 9.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.21.6)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n","jax 0.3.8 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed scipy-1.1.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iZ5i3l-vHb3p"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","from nltk.translate.bleu_score import corpus_bleu\n","from tqdm import tqdm\n","from datasets import *\n","from utils import *"]},{"cell_type":"markdown","source":["### Test model with attention"],"metadata":{"id":"qOGkeGr0Re1m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"irlXJVnAHb3t"},"outputs":[],"source":["# Parameters\n","data_folder = '/content/drive/MyDrive/Colab Notebooks/DL Lab/Lab6/data/coco2014'  # folder with data files saved by create_input_files.py\n","data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n","checkpoint = '/content/drive/MyDrive/Colab Notebooks/DL Lab/Lab6/code/saved_checkpoints/attention/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n","word_map_file = '/content/drive/MyDrive/Colab Notebooks/DL Lab/Lab6/data/coco2014/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json'\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","cudnn.benchmark = True\n","beam_size = 1\n","attention = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfWiw4U6Hb3u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651999564216,"user_tz":-480,"elapsed":3939,"user":{"displayName":"SI YUAN BIAN","userId":"02322420278915835023"}},"outputId":"3b9bf1d9-2cd5-46f4-ea7d-31124344b944"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecoderWithAttention(\n","  (attention): Attention(\n","    (f_att1): Linear(in_features=2048, out_features=512, bias=True)\n","    (f_att2): Linear(in_features=512, out_features=512, bias=True)\n","    (f_att3): Linear(in_features=512, out_features=1, bias=True)\n","  )\n","  (embedding): Embedding(9490, 512)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (decode_step): LSTMCell(2560, 512)\n","  (init_h): Linear(in_features=2048, out_features=512, bias=True)\n","  (init_c): Linear(in_features=2048, out_features=512, bias=True)\n","  (f_beta): Linear(in_features=512, out_features=2048, bias=True)\n","  (fc): Linear(in_features=512, out_features=9490, bias=True)\n",")"]},"metadata":{},"execution_count":4}],"source":["# Load model\n","checkpoint = torch.load(checkpoint)\n","encoder = checkpoint['encoder']\n","encoder = encoder.to(device)\n","encoder.eval()\n","decoder = checkpoint['decoder']\n","decoder = decoder.to(device)\n","decoder.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jb8fXeWVHb3v"},"outputs":[],"source":["# Load word map (word2ix)\n","with open(word_map_file, 'r') as j:\n","    word_map = json.load(j)\n","rev_word_map = {v: k for k, v in word_map.items()}\n","vocab_size = len(word_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lhUlOrxHb3v"},"outputs":[],"source":["# Normalization transform\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eC_d954rHb3w"},"outputs":[],"source":["# DataLoader\n","loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n","    batch_size=1, shuffle=False, num_workers=1, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCTd2vnsHb3w"},"outputs":[],"source":["# Lists to store references (true captions), and hypothesis (prediction) for each image\n","# If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n","# references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n","references = list()\n","hypotheses = list()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKIqp64UHb3x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651990609578,"user_tz":-480,"elapsed":756450,"user":{"displayName":"SI YUAN BIAN","userId":"02322420278915835023"}},"outputId":"c990e6a6-e168-4777-895a-e7a19a9c69e2"},"outputs":[{"output_type":"stream","name":"stderr","text":["EVALUATING AT BEAM SIZE 1:   0%|          | 0/25000 [00:00<?, ?it/s]/content/drive/MyDrive/Colab Notebooks/DL Lab/Lab6/code/models.py:203: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  alpha = F.softmax(e)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","EVALUATING AT BEAM SIZE 1: 100%|██████████| 25000/25000 [12:36<00:00, 33.06it/s]\n"]}],"source":["# For each image\n","for i, (image, caps, caplens, allcaps) in enumerate(tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n","    k = beam_size\n","\n","    # Tensor to store top k previous words at each step; now they're just <start>\n","    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n","\n","    # Tensor to store top k sequences; now they're just <start>\n","    seqs = k_prev_words  # (k, 1)\n","\n","    # Tensor to store top k sequences' scores; now they're just 0\n","    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n","\n","    # Lists to store completed sequences and scores\n","    complete_seqs = list()\n","    complete_seqs_scores = list()\n","    \n","    # Move to GPU device, if available\n","    image = image.to(device)  # (1, 3, 256, 256)\n","    \n","    # Encode\n","    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n","\n","    # Flatten encoding\n","    # We'll treat the problem as having a batch size of k\n","    if attention:\n","        encoder_dim = encoder_out.size(3)\n","        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n","        num_pixels = encoder_out.size(1)\n","        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n","    else:\n","        encoder_out = encoder_out.reshape(1, -1)\n","        encoder_dim = encoder_out.size(1)\n","        encoder_out = encoder_out.expand(k, encoder_dim)\n","    \n","    # Start decoding\n","    step = 1\n","    if attention:\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = decoder.init_h(mean_encoder_out)  # (1, decoder_dim)\n","        c = decoder.init_c(mean_encoder_out)\n","    else:\n","        init_input = decoder.bn(decoder.init(encoder_out))\n","        h, c = decoder.decode_step(init_input)  # (batch_size_t, decoder_dim)\n","    \n","    smoth_wrong = False\n","    \n","    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n","    while True:\n","        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n","        if attention:\n","            scores, _, h, c = decoder.one_step(embeddings, encoder_out, h, c)\n","        else:\n","            scores, h, c = decoder.one_step(embeddings, h, c)\n","        scores = F.log_softmax(scores, dim=1)\n","        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n","        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n","        if step == 1:\n","            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n","        else:\n","            # Unroll and find top scores, and their unrolled indices\n","            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n","        # Convert unrolled indices to actual indices of scores\n","        prev_word_inds = top_k_words // vocab_size  # (s)\n","        next_word_inds = top_k_words % vocab_size  # (s)\n","        # Add new words to sequences\n","        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n","        # Which sequences are incomplete (didn't reach <end>)?\n","        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n","                           next_word != word_map['<end>']]\n","        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n","        # Set aside complete sequences\n","        if len(complete_inds) > 0:\n","            complete_seqs.extend(seqs[complete_inds].tolist())\n","            complete_seqs_scores.extend(top_k_scores[complete_inds])\n","        k -= len(complete_inds)  # reduce beam length accordingly\n","        # Proceed with incomplete sequences\n","        if k == 0:\n","            break\n","        seqs = seqs[incomplete_inds]\n","        h = h[prev_word_inds[incomplete_inds]]\n","        c = c[prev_word_inds[incomplete_inds]]\n","        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n","        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n","        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n","        # Break if things have been going on too long\n","        if step > 50:\n","            smoth_wrong = True\n","            break\n","        step += 1\n","    if not smoth_wrong:\n","        i = complete_seqs_scores.index(max(complete_seqs_scores))\n","        seq = complete_seqs[i]\n","    else:\n","        seq = seqs[0][:20]\n","    # References\n","    img_caps = allcaps[0].tolist()\n","    img_captions = list(\n","        map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n","            img_caps))  # remove <start> and pads\n","    references.append(img_captions)\n","    # Hypotheses\n","    hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n","    assert len(references) == len(hypotheses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wM_0otENHb33","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651990616096,"user_tz":-480,"elapsed":6539,"user":{"displayName":"SI YUAN BIAN","userId":"02322420278915835023"}},"outputId":"9f5ae83f-d070-44c6-bf0f-6904a085336a"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.2782035814507714\n"]}],"source":["# Calculate BLEU-4 scores\n","bleu4 = corpus_bleu(references, hypotheses)\n","print(bleu4)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"test2.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}