{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例2: 构建自己的多层感知机: MNIST 手写数字识别\n",
    "\n",
    "### 本案例要求如下\n",
    "- #### 实现SGD优化器 (`./optimizer.py`)\n",
    "- #### 实现全连接层FCLayer前向和后向计算 (`layers/fc_layer.py`)\n",
    "- #### 实现激活层SigmoidLayer前向和后向计算 (`layers/sigmoid_layer.py`)\n",
    "- #### 实现激活层ReLULayer前向和后向计算 (`layers/relu_layer.py`)\n",
    "- #### 实现损失层EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### 实现损失层SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc, plot_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # 归一化处理\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # 将标签变为one-hot编码\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 使用欧式距离损失训练多层感知机(MLP with Euclidean Loss)\n",
    "第一部分将使用欧式距离损失训练多层感知机. \n",
    "分别使用**Sigmoid**激活函数和**ReLU**激活函数.\n",
    "### TODO\n",
    "执行以下代码之前，请完成 **./optimizer.py** 和 **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 使用欧式距离损失和Sigmoid激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用欧式距离损失和Sigmoid激活函数.\n",
    "\n",
    "### TODO\n",
    "执行以下代码之前，请完成 **layers/fc_layer.py** 和 **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.1634\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.2468\t Accuracy 0.1137\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.9298\t Accuracy 0.1278\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8132\t Accuracy 0.1409\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.7475\t Accuracy 0.1519\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.7027\t Accuracy 0.1649\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.6687\t Accuracy 0.1803\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.6439\t Accuracy 0.1914\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.6229\t Accuracy 0.2044\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.6053\t Accuracy 0.2158\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.5897\t Accuracy 0.2298\n",
      "\n",
      "Epoch [0]\t Average training loss 0.5755\t Average training accuracy 0.2452\n",
      "Epoch [0]\t Average validation loss 0.4195\t Average validation accuracy 0.4198\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4114\t Accuracy 0.3900\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4206\t Accuracy 0.4112\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4177\t Accuracy 0.4232\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4161\t Accuracy 0.4277\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4129\t Accuracy 0.4354\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4082\t Accuracy 0.4474\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.4036\t Accuracy 0.4567\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.4008\t Accuracy 0.4618\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.3973\t Accuracy 0.4698\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.3944\t Accuracy 0.4761\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.3915\t Accuracy 0.4832\n",
      "\n",
      "Epoch [1]\t Average training loss 0.3880\t Average training accuracy 0.4915\n",
      "Epoch [1]\t Average validation loss 0.3427\t Average validation accuracy 0.6128\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3330\t Accuracy 0.6200\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3480\t Accuracy 0.5824\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3472\t Accuracy 0.5870\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3480\t Accuracy 0.5857\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3471\t Accuracy 0.5896\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3446\t Accuracy 0.5957\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3423\t Accuracy 0.6013\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3415\t Accuracy 0.6038\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3398\t Accuracy 0.6080\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3386\t Accuracy 0.6098\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3373\t Accuracy 0.6129\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3354\t Average training accuracy 0.6176\n",
      "Epoch [2]\t Average validation loss 0.3059\t Average validation accuracy 0.6962\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.2968\t Accuracy 0.7800\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3130\t Accuracy 0.6655\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3129\t Accuracy 0.6686\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3145\t Accuracy 0.6638\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3144\t Accuracy 0.6655\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3129\t Accuracy 0.6692\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3116\t Accuracy 0.6717\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3115\t Accuracy 0.6731\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3105\t Accuracy 0.6758\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3100\t Accuracy 0.6771\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3093\t Accuracy 0.6788\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3081\t Average training accuracy 0.6820\n",
      "Epoch [3]\t Average validation loss 0.2852\t Average validation accuracy 0.7476\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2768\t Accuracy 0.8200\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.2933\t Accuracy 0.7153\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.2934\t Accuracy 0.7164\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.2954\t Accuracy 0.7110\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.2956\t Accuracy 0.7114\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.2946\t Accuracy 0.7129\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.2937\t Accuracy 0.7141\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.2940\t Accuracy 0.7152\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.2935\t Accuracy 0.7168\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.2932\t Accuracy 0.7172\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.2929\t Accuracy 0.7181\n",
      "\n",
      "Epoch [4]\t Average training loss 0.2921\t Average training accuracy 0.7206\n",
      "Epoch [4]\t Average validation loss 0.2726\t Average validation accuracy 0.7770\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2649\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2812\t Accuracy 0.7431\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2814\t Accuracy 0.7449\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2836\t Accuracy 0.7397\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2840\t Accuracy 0.7392\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2832\t Accuracy 0.7405\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2827\t Accuracy 0.7409\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2832\t Accuracy 0.7415\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2828\t Accuracy 0.7426\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2828\t Accuracy 0.7429\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2827\t Accuracy 0.7436\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2820\t Average training accuracy 0.7455\n",
      "Epoch [5]\t Average validation loss 0.2648\t Average validation accuracy 0.8002\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2576\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2735\t Accuracy 0.7588\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2738\t Accuracy 0.7623\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2761\t Accuracy 0.7569\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2766\t Accuracy 0.7561\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2760\t Accuracy 0.7570\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2757\t Accuracy 0.7576\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2764\t Accuracy 0.7578\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2761\t Accuracy 0.7587\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2762\t Accuracy 0.7592\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2762\t Accuracy 0.7597\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2757\t Average training accuracy 0.7614\n",
      "Epoch [6]\t Average validation loss 0.2599\t Average validation accuracy 0.8132\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2532\t Accuracy 0.8300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2687\t Accuracy 0.7745\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2691\t Accuracy 0.7755\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2714\t Accuracy 0.7691\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2720\t Accuracy 0.7684\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2715\t Accuracy 0.7694\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2713\t Accuracy 0.7695\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2721\t Accuracy 0.7696\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2719\t Accuracy 0.7706\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2721\t Accuracy 0.7708\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2722\t Accuracy 0.7709\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2718\t Average training accuracy 0.7725\n",
      "Epoch [7]\t Average validation loss 0.2571\t Average validation accuracy 0.8224\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2507\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2659\t Accuracy 0.7859\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2663\t Accuracy 0.7849\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2686\t Accuracy 0.7789\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2693\t Accuracy 0.7785\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2689\t Accuracy 0.7788\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2687\t Accuracy 0.7789\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2696\t Accuracy 0.7788\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2695\t Accuracy 0.7800\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2698\t Accuracy 0.7803\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2699\t Accuracy 0.7802\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2696\t Average training accuracy 0.7816\n",
      "Epoch [8]\t Average validation loss 0.2557\t Average validation accuracy 0.8298\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2497\t Accuracy 0.8300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2646\t Accuracy 0.7924\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2649\t Accuracy 0.7910\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2672\t Accuracy 0.7862\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2679\t Accuracy 0.7854\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2676\t Accuracy 0.7855\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2675\t Accuracy 0.7856\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2685\t Accuracy 0.7852\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2684\t Accuracy 0.7866\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2687\t Accuracy 0.7869\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2689\t Accuracy 0.7866\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2686\t Average training accuracy 0.7878\n",
      "Epoch [9]\t Average validation loss 0.2555\t Average validation accuracy 0.8368\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2497\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2642\t Accuracy 0.7971\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2646\t Accuracy 0.7951\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2668\t Accuracy 0.7899\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2675\t Accuracy 0.7897\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2673\t Accuracy 0.7901\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2673\t Accuracy 0.7903\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2683\t Accuracy 0.7899\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2683\t Accuracy 0.7911\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2686\t Accuracy 0.7913\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2688\t Accuracy 0.7910\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2686\t Average training accuracy 0.7922\n",
      "Epoch [10]\t Average validation loss 0.2561\t Average validation accuracy 0.8400\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2505\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2647\t Accuracy 0.8018\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2651\t Accuracy 0.7988\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2673\t Accuracy 0.7934\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2680\t Accuracy 0.7934\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2678\t Accuracy 0.7934\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2678\t Accuracy 0.7935\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2688\t Accuracy 0.7932\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2688\t Accuracy 0.7946\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2692\t Accuracy 0.7947\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2694\t Accuracy 0.7945\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2692\t Average training accuracy 0.7955\n",
      "Epoch [11]\t Average validation loss 0.2573\t Average validation accuracy 0.8442\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2519\t Accuracy 0.8300\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2658\t Accuracy 0.8035\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2661\t Accuracy 0.8015\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2683\t Accuracy 0.7954\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2690\t Accuracy 0.7957\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2689\t Accuracy 0.7955\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2689\t Accuracy 0.7957\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2699\t Accuracy 0.7955\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2700\t Accuracy 0.7967\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2704\t Accuracy 0.7969\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2706\t Accuracy 0.7965\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2704\t Average training accuracy 0.7977\n",
      "Epoch [12]\t Average validation loss 0.2590\t Average validation accuracy 0.8472\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2538\t Accuracy 0.8300\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2673\t Accuracy 0.8043\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2677\t Accuracy 0.8028\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2698\t Accuracy 0.7968\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2705\t Accuracy 0.7974\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2704\t Accuracy 0.7972\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2704\t Accuracy 0.7977\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2715\t Accuracy 0.7974\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2716\t Accuracy 0.7986\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2720\t Accuracy 0.7986\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2722\t Accuracy 0.7981\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2721\t Average training accuracy 0.7992\n",
      "Epoch [13]\t Average validation loss 0.2611\t Average validation accuracy 0.8486\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2560\t Accuracy 0.8300\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2693\t Accuracy 0.8045\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2696\t Accuracy 0.8039\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2717\t Accuracy 0.7977\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2724\t Accuracy 0.7983\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2723\t Accuracy 0.7980\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2724\t Accuracy 0.7987\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2735\t Accuracy 0.7981\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2735\t Accuracy 0.7993\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2739\t Accuracy 0.7993\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2742\t Accuracy 0.7988\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2741\t Average training accuracy 0.7998\n",
      "Epoch [14]\t Average validation loss 0.2636\t Average validation accuracy 0.8494\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2586\t Accuracy 0.8200\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2715\t Accuracy 0.8039\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2718\t Accuracy 0.8036\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2739\t Accuracy 0.7976\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2746\t Accuracy 0.7982\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2745\t Accuracy 0.7982\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2746\t Accuracy 0.7990\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2757\t Accuracy 0.7985\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2758\t Accuracy 0.7998\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2762\t Accuracy 0.7998\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2764\t Accuracy 0.7993\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2764\t Average training accuracy 0.8002\n",
      "Epoch [15]\t Average validation loss 0.2663\t Average validation accuracy 0.8492\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2614\t Accuracy 0.8100\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2740\t Accuracy 0.8035\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2743\t Accuracy 0.8032\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2763\t Accuracy 0.7975\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2770\t Accuracy 0.7983\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2770\t Accuracy 0.7985\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2770\t Accuracy 0.7993\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2781\t Accuracy 0.7987\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2782\t Accuracy 0.8001\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2787\t Accuracy 0.7999\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2789\t Accuracy 0.7993\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2788\t Average training accuracy 0.8004\n",
      "Epoch [16]\t Average validation loss 0.2691\t Average validation accuracy 0.8496\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2644\t Accuracy 0.8100\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2767\t Accuracy 0.8041\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2770\t Accuracy 0.8036\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2789\t Accuracy 0.7977\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2797\t Accuracy 0.7982\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2796\t Accuracy 0.7983\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2797\t Accuracy 0.7991\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2808\t Accuracy 0.7985\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2809\t Accuracy 0.8001\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2813\t Accuracy 0.7998\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2816\t Accuracy 0.7992\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2815\t Average training accuracy 0.8002\n",
      "Epoch [17]\t Average validation loss 0.2722\t Average validation accuracy 0.8500\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2675\t Accuracy 0.8100\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2795\t Accuracy 0.8027\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2798\t Accuracy 0.8028\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2816\t Accuracy 0.7975\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2824\t Accuracy 0.7976\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2824\t Accuracy 0.7978\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2824\t Accuracy 0.7986\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2835\t Accuracy 0.7979\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2836\t Accuracy 0.7994\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2841\t Accuracy 0.7992\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2843\t Accuracy 0.7986\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2843\t Average training accuracy 0.7995\n",
      "Epoch [18]\t Average validation loss 0.2753\t Average validation accuracy 0.8484\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2707\t Accuracy 0.8100\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2824\t Accuracy 0.8016\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2827\t Accuracy 0.8018\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2845\t Accuracy 0.7967\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2852\t Accuracy 0.7966\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2852\t Accuracy 0.7969\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2853\t Accuracy 0.7978\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2864\t Accuracy 0.7972\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2865\t Accuracy 0.7986\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2869\t Accuracy 0.7985\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2872\t Accuracy 0.7978\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2871\t Average training accuracy 0.7987\n",
      "Epoch [19]\t Average validation loss 0.2785\t Average validation accuracy 0.8472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8130.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 使用欧式距离损失和ReLU激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用欧式距离损失和ReLU激活函数.\n",
    "\n",
    "### TODO\n",
    "执行以下代码之前，请完成 **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# 使用FCLayer和ReLULayer构建多层感知机\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.8844\t Accuracy 0.1200\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.2832\t Accuracy 0.0845\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.8672\t Accuracy 0.0891\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.6585\t Accuracy 0.1005\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.5195\t Accuracy 0.1113\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.4174\t Accuracy 0.1239\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.3340\t Accuracy 0.1382\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.2658\t Accuracy 0.1526\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.2092\t Accuracy 0.1669\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.1611\t Accuracy 0.1807\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 1.1188\t Accuracy 0.1942\n",
      "\n",
      "Epoch [0]\t Average training loss 1.0809\t Average training accuracy 0.2080\n",
      "Epoch [0]\t Average validation loss 0.6688\t Average validation accuracy 0.3626\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.6579\t Accuracy 0.4100\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.6611\t Accuracy 0.3702\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.6492\t Accuracy 0.3807\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.6412\t Accuracy 0.3877\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.6288\t Accuracy 0.3943\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.6182\t Accuracy 0.4037\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.6068\t Accuracy 0.4118\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.5974\t Accuracy 0.4173\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.5882\t Accuracy 0.4254\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.5802\t Accuracy 0.4318\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.5724\t Accuracy 0.4381\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5643\t Average training accuracy 0.4447\n",
      "Epoch [1]\t Average validation loss 0.4679\t Average validation accuracy 0.5436\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.4588\t Accuracy 0.5800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.4679\t Accuracy 0.5306\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.4628\t Accuracy 0.5350\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.4616\t Accuracy 0.5380\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.4561\t Accuracy 0.5433\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.4517\t Accuracy 0.5482\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.4465\t Accuracy 0.5515\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.4428\t Accuracy 0.5532\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.4386\t Accuracy 0.5571\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.4350\t Accuracy 0.5604\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.4315\t Accuracy 0.5642\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4275\t Average training accuracy 0.5675\n",
      "Epoch [2]\t Average validation loss 0.3751\t Average validation accuracy 0.6418\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3667\t Accuracy 0.6300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3781\t Accuracy 0.6233\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3751\t Accuracy 0.6266\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3758\t Accuracy 0.6264\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3728\t Accuracy 0.6300\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3704\t Accuracy 0.6326\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3674\t Accuracy 0.6347\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3657\t Accuracy 0.6347\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3632\t Accuracy 0.6372\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3614\t Accuracy 0.6395\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3594\t Accuracy 0.6425\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3570\t Average training accuracy 0.6450\n",
      "Epoch [3]\t Average validation loss 0.3213\t Average validation accuracy 0.7048\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.3133\t Accuracy 0.6900\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3258\t Accuracy 0.6918\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3239\t Accuracy 0.6915\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3255\t Accuracy 0.6905\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3236\t Accuracy 0.6919\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3222\t Accuracy 0.6939\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3202\t Accuracy 0.6948\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3195\t Accuracy 0.6946\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3180\t Accuracy 0.6960\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3169\t Accuracy 0.6975\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3158\t Accuracy 0.6993\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3141\t Average training accuracy 0.7011\n",
      "Epoch [4]\t Average validation loss 0.2864\t Average validation accuracy 0.7500\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2792\t Accuracy 0.7600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2921\t Accuracy 0.7376\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.2909\t Accuracy 0.7376\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.2928\t Accuracy 0.7355\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.2917\t Accuracy 0.7367\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.2907\t Accuracy 0.7365\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.2894\t Accuracy 0.7365\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.2893\t Accuracy 0.7355\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.2882\t Accuracy 0.7367\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.2876\t Accuracy 0.7376\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.2869\t Accuracy 0.7389\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2857\t Average training accuracy 0.7403\n",
      "Epoch [5]\t Average validation loss 0.2624\t Average validation accuracy 0.7812\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2557\t Accuracy 0.8100\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2689\t Accuracy 0.7688\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2681\t Accuracy 0.7703\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.2703\t Accuracy 0.7661\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.2696\t Accuracy 0.7664\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.2689\t Accuracy 0.7656\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.2680\t Accuracy 0.7663\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.2682\t Accuracy 0.7649\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.2674\t Accuracy 0.7656\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.2671\t Accuracy 0.7661\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.2667\t Accuracy 0.7667\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2657\t Average training accuracy 0.7679\n",
      "Epoch [6]\t Average validation loss 0.2452\t Average validation accuracy 0.8050\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2387\t Accuracy 0.8200\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2522\t Accuracy 0.7898\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2517\t Accuracy 0.7915\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2540\t Accuracy 0.7882\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2536\t Accuracy 0.7875\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2532\t Accuracy 0.7860\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2525\t Accuracy 0.7864\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2529\t Accuracy 0.7855\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2523\t Accuracy 0.7863\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2522\t Accuracy 0.7865\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2519\t Accuracy 0.7870\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2512\t Average training accuracy 0.7879\n",
      "Epoch [7]\t Average validation loss 0.2324\t Average validation accuracy 0.8226\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2266\t Accuracy 0.8400\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2399\t Accuracy 0.8047\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2396\t Accuracy 0.8065\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2420\t Accuracy 0.8025\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2418\t Accuracy 0.8025\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2415\t Accuracy 0.8014\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2409\t Accuracy 0.8017\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2416\t Accuracy 0.8019\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2411\t Accuracy 0.8028\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2412\t Accuracy 0.8025\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2410\t Accuracy 0.8030\n",
      "\n",
      "Epoch [8]\t Average training loss 0.2404\t Average training accuracy 0.8036\n",
      "Epoch [8]\t Average validation loss 0.2228\t Average validation accuracy 0.8386\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2175\t Accuracy 0.8800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2306\t Accuracy 0.8200\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2305\t Accuracy 0.8201\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2330\t Accuracy 0.8153\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2329\t Accuracy 0.8148\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2326\t Accuracy 0.8137\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2323\t Accuracy 0.8142\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2330\t Accuracy 0.8143\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2327\t Accuracy 0.8147\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2328\t Accuracy 0.8143\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2327\t Accuracy 0.8147\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2322\t Average training accuracy 0.8153\n",
      "Epoch [9]\t Average validation loss 0.2154\t Average validation accuracy 0.8492\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2107\t Accuracy 0.9100\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2236\t Accuracy 0.8267\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2236\t Accuracy 0.8292\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2261\t Accuracy 0.8246\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2262\t Accuracy 0.8238\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2259\t Accuracy 0.8224\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2257\t Accuracy 0.8231\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2265\t Accuracy 0.8232\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2263\t Accuracy 0.8237\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2265\t Accuracy 0.8234\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2265\t Accuracy 0.8237\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2260\t Average training accuracy 0.8243\n",
      "Epoch [10]\t Average validation loss 0.2098\t Average validation accuracy 0.8608\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2056\t Accuracy 0.9100\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2183\t Accuracy 0.8361\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2183\t Accuracy 0.8374\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2209\t Accuracy 0.8326\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2210\t Accuracy 0.8318\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2208\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2206\t Accuracy 0.8306\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2216\t Accuracy 0.8308\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2214\t Accuracy 0.8312\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2216\t Accuracy 0.8310\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2217\t Accuracy 0.8312\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2213\t Average training accuracy 0.8315\n",
      "Epoch [11]\t Average validation loss 0.2055\t Average validation accuracy 0.8696\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2016\t Accuracy 0.9100\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2142\t Accuracy 0.8435\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2143\t Accuracy 0.8427\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2169\t Accuracy 0.8378\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2171\t Accuracy 0.8370\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2169\t Accuracy 0.8354\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2168\t Accuracy 0.8359\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2178\t Accuracy 0.8361\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2177\t Accuracy 0.8369\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2180\t Accuracy 0.8367\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2180\t Accuracy 0.8366\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2177\t Average training accuracy 0.8367\n",
      "Epoch [12]\t Average validation loss 0.2023\t Average validation accuracy 0.8732\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1986\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2111\t Accuracy 0.8492\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2113\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2139\t Accuracy 0.8422\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2142\t Accuracy 0.8417\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2140\t Accuracy 0.8402\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2140\t Accuracy 0.8409\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2150\t Accuracy 0.8408\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2149\t Accuracy 0.8415\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2152\t Accuracy 0.8411\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2153\t Accuracy 0.8410\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2150\t Average training accuracy 0.8411\n",
      "Epoch [13]\t Average validation loss 0.1999\t Average validation accuracy 0.8770\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1964\t Accuracy 0.9000\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2089\t Accuracy 0.8522\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2090\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2116\t Accuracy 0.8453\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2120\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2119\t Accuracy 0.8439\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2119\t Accuracy 0.8442\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2129\t Accuracy 0.8440\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2128\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2132\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2133\t Accuracy 0.8443\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2131\t Average training accuracy 0.8442\n",
      "Epoch [14]\t Average validation loss 0.1982\t Average validation accuracy 0.8808\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1947\t Accuracy 0.9000\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2072\t Accuracy 0.8543\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2075\t Accuracy 0.8543\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2101\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2105\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2104\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2104\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2115\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2114\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2118\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2120\t Accuracy 0.8469\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2117\t Average training accuracy 0.8466\n",
      "Epoch [15]\t Average validation loss 0.1971\t Average validation accuracy 0.8848\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1937\t Accuracy 0.9000\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2062\t Accuracy 0.8563\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2064\t Accuracy 0.8564\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2091\t Accuracy 0.8507\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2095\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2094\t Accuracy 0.8491\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2095\t Accuracy 0.8494\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2106\t Accuracy 0.8489\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2106\t Accuracy 0.8499\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2110\t Accuracy 0.8495\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2112\t Accuracy 0.8490\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2109\t Average training accuracy 0.8486\n",
      "Epoch [16]\t Average validation loss 0.1965\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1933\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2056\t Accuracy 0.8575\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2059\t Accuracy 0.8592\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2085\t Accuracy 0.8532\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2090\t Accuracy 0.8521\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2089\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2090\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2102\t Accuracy 0.8504\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2101\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2106\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2108\t Accuracy 0.8506\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2106\t Average training accuracy 0.8501\n",
      "Epoch [17]\t Average validation loss 0.1962\t Average validation accuracy 0.8882\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1932\t Accuracy 0.8800\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2054\t Accuracy 0.8586\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2057\t Accuracy 0.8603\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2083\t Accuracy 0.8542\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2089\t Accuracy 0.8532\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2088\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2089\t Accuracy 0.8517\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2101\t Accuracy 0.8513\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2101\t Accuracy 0.8525\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2105\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2107\t Accuracy 0.8514\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2105\t Average training accuracy 0.8510\n",
      "Epoch [18]\t Average validation loss 0.1963\t Average validation accuracy 0.8898\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1934\t Accuracy 0.8800\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2055\t Accuracy 0.8602\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2058\t Accuracy 0.8617\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2084\t Accuracy 0.8552\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2090\t Accuracy 0.8539\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2089\t Accuracy 0.8524\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2091\t Accuracy 0.8520\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2103\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2103\t Accuracy 0.8527\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2107\t Accuracy 0.8522\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2110\t Accuracy 0.8516\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2108\t Average training accuracy 0.8512\n",
      "Epoch [19]\t Average validation loss 0.1966\t Average validation accuracy 0.8912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8633.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAArsklEQVR4nO3deZxU1Zn/8c/T1Rt0Nwg0CNJAtwoqCLKJGmXRGFyiIokGTX5xjHHchkySmUnUTCZxXGIcjZnJxMQQozHRaDLGhRgSMC7gLktwAURRQFtAaJRVen9+f9zqpmmqqqu761ZV09/361Wvu9a5TxfFeeqce++55u6IiIi0lpPpAEREJDspQYiISExKECIiEpMShIiIxKQEISIiMeVmOoD2Ki0t9fLy8kyHISLSpSxdurTK3fu35z1dLkGUl5ezZMmSTIchItKlmNn69r5HXUwiIhKTEoSIiMSkBCEiIjF1uXMQItL91NXVUVlZSXV1daZDyXqFhYWUlZWRl5fX6bKUIEQk61VWVlJSUkJ5eTlmlulwspa7s3XrViorK6moqOh0eepiEpGsV11dTb9+/ZQc2mBm9OvXL2UtLSUIEekSlBySk8rPSQlCRERiUoIQEUnCTTfdxKhRoxgzZgxjx47l5Zdf5tJLL2XlypWhHvfMM89k27Zt+62/7rrruO2220I9tk5Si8gBZeKNT1C1q3a/9aXF+Sz57mc6VOaLL77I448/zrJlyygoKKCqqora2lruuuuuzobbpnnz5oV+jHjUghCRA0qs5JBofTI2btxIaWkpBQUFAJSWlnLIIYcwbdq05qF/fvWrXzFixAimTZvGP/7jPzJ79mwALr74Yq688kpOPvlkDj30UBYuXMgll1zCUUcdxcUXX9x8jAceeIDRo0dz9NFHc/XVVzevLy8vp6qqCghaMUcccQSnnnoqq1ev7vDfkyy1IESkS/nPP61g5YYdHXrvrF+8GHP9yEN68f2zR8V93/Tp07n++usZMWIEp556KrNmzWLq1KnN2zds2MANN9zAsmXLKCkp4ZRTTuGYY45p3v7xxx/z1FNPMXfuXM4++2yef/557rrrLo499liWL1/OgAEDuPrqq1m6dCl9+vRh+vTpPProo5x77rnNZSxdupQHH3yQv//979TX1zN+/HgmTJjQoc8hWWpBiIi0obi4mKVLlzJnzhz69+/PrFmz+PWvf928/ZVXXmHq1Kn07duXvLw8zj///H3ef/bZZ2NmjB49moMPPpjRo0eTk5PDqFGjWLduHYsXL2batGn079+f3NxcvvSlL7Fo0aJ9ynj22WeZOXMmPXv2pFevXpxzzjmh/91qQYhIl5Lolz5A+TV/jrvt95ef0OHjRiIRpk2bxrRp0xg9ejT33ntv8zZ3T/jepq6pnJyc5vmm5fr6enJzk6uK032pr1oQIiJtWL16NW+//Xbz8vLlyxk2bFjz8qRJk1i4cCEff/wx9fX1/PGPf2xX+ccddxwLFy6kqqqKhoYGHnjggX26sACmTJnCI488wp49e9i5cyd/+tOfOvdHJUEtCBE5oJQW58e9iqmjdu3axde+9jW2bdtGbm4uhx9+OHPmzOG8884DYPDgwXznO9/huOOO45BDDmHkyJH07t076fIHDRrEzTffzMknn4y7c+aZZzJjxox99hk/fjyzZs1i7NixDBs2jMmTJ3f470mWtdU0yjYTJ050PTBIpHtZtWoVRx11VKbDSGjXrl0UFxdTX1/PzJkzueSSS5g5c2ZGYon1eZnZUnef2J5yQu1iMrPTzWy1ma0xs2tibP+WmS2Pvt4wswYz6xtmTCIiYbjuuusYO3YsRx99NBUVFftcgdRVhdbFZGYR4A7gM0AlsNjM5rp7822H7n4rcGt0/7OBb7r7R2HFJCISlrDvas6EMFsQk4A17v6uu9cCDwIzEux/IfBAiPGIiEg7hJkgBgPvt1iujK7bj5n1BE4H2nfqX0REQhNmgoh1wW68M+JnA8/H614ys8vMbImZLdmyZUvKAhQRkfjCTBCVwJAWy2XAhjj7XkCC7iV3n+PuE919Yv/+/VMYooiIxBNmglgMDDezCjPLJ0gCc1vvZGa9ganAYyHGIiISuuLi4kyHkFKhXcXk7vVmNhuYD0SAu919hZldEd1+Z3TXmcACd98dViwi0o3cOhx2b95/fdEA+Nbb+69vJ3fH3cnJOfAHogj1L3T3ee4+wt0Pc/ebouvubJEccPdfu/sFYcYhIt1IrOSQaH0S1q1bx1FHHcVVV13F+PHjueGGGzj22GMZM2YM3//+9/fb/5lnnuGss85qXp49e/Y+g/t1FRpqQ0S6lr9cA5te79h77/ls7PUDR8MZP0z41tWrV3PPPfdw7rnn8tBDD/HKK6/g7pxzzjksWrSIKVOmdCymLHbgt5FERFJg2LBhHH/88SxYsIAFCxYwbtw4xo8fz5tvvrnPQH4Hku7Tggi5X1JE0qSNX/pcl2CQvK/EHwq8LUVFRUBwDuLaa6/l8ssvj7tvbm4ujY2NzcvV1dUdPm4mdZ8WRAj9kiLS/Zx22mncfffd7Nq1C4APPviAzZv3rUeGDRvGypUrqampYfv27Tz55JOZCLXTuk8LQkS6h6IB8XsLUmD69OmsWrWKE04IHj5UXFzMfffdx4ABe8sfMmQIX/jCFxgzZgzDhw9n3LhxKTl2unWf4b4TNTuv297xgEQkdF1huO9s0iWG+xYRka5LCUJERGLqPgkiXv9jivolRSRcXa07PFNS+Tl1n5PULS9l/dPX4Y2H4dtrIdJ9PgKRrqqwsJCtW7fSr18/zGINFC0QJIetW7dSWFiYkvK6Z+1YMQWW/ho2vgplEzIdjYi0oaysjMrKSjTcf9sKCwspKytLSVndM0GUTw6maxcqQYh0AXl5eVRUVGQ6jG6n+5yDaKl4AAwYCWsXZToSEZGs1T0TBATdTO+9BPU1mY5ERCQrde8EUb8HKjtw052ISDfQfRPEsBPBctTNJCISR/dNED0OgkHHKEGIiMTRfRMEBN1MlYuhVk87FRFpTQmisS44WS0iIvvo3gli6AmQk6tuJhGRGLp3gsgvgrJjlSBERGLo3gkCgm6mjcthz7ZMRyIiklWUICqmgDfC+hcyHYmISFZRgig7FnIL1c0kItJKqAnCzE43s9VmtsbMromzzzQzW25mK8xsYZjxxJRbAEOPV4IQEWkltARhZhHgDuAMYCRwoZmNbLXPQcDPgHPcfRRwfljxJFQxBTavgF0aSlhEpEmYLYhJwBp3f9fda4EHgRmt9vki8LC7vwfg7ptDjCe+iqnBdN2zGTm8iEg2CjNBDAbeb7FcGV3X0gigj5k9Y2ZLzeyiWAWZ2WVmtsTMloTywJBBYyG/RN1MIiIthJkgYj0XsPXDUnOBCcBngdOA/zCzEfu9yX2Ou09094n9+/dPfaSRXCg/US0IEZEWwkwQlcCQFstlwIYY+/zV3Xe7exWwCDgmxJjiq5gCW9fA9g8ycngRkWwTZoJYDAw3swozywcuAOa22ucxYLKZ5ZpZT+A4YFWIMcVXMSWYqhUhIgKEmCDcvR6YDcwnqPT/4O4rzOwKM7sius8q4K/Aa8ArwF3u/kZYMSU0YBT06KvzECIiUblhFu7u84B5rdbd2Wr5VuDWMONISk4OVEwOEoQ7WKxTKCIi3YfupG6pYgpsfx8+XpvpSEREMk4JoqWm+yHUzSQiogSxj36HQ8kgJQgREZQg9mUWdDM1nYcQEenGlCBaK58Mu7fAljczHYmISEYpQbTWdD+EuplEpJtTgmitzzA4aJgShIh0e0oQsVRMCe6obmzIdCQiIhmjBBFLxVSo3g6bXst0JCIiGaMEEUvF5GCqbiYR6caUIGIpGQilRyhBiEi3pgQRT8UUWP8i1NdmOhIRkYxQgoinYgrU7YYNyzIdiYhIRihBxFN+EmDqZhKRbksJIp6efWHgaCUIEem2lCASqZgC778MdXsyHYmISNopQSRSMRUaaoMkISLSzShBJDLsBLCIuplEpFtSgkikoAQGT1CCEJFuSQmiLRVT4INlUL0j05GIiKSVEkRbKqaAN8B7L2Y6EhGRtFKCaMuQSRApUDeTiHQ7ShBtyesRJIm1CzMdiYhIWoWaIMzsdDNbbWZrzOyaGNunmdl2M1sefX0vzHg6rGIqbHodPvko05GIiKRNaAnCzCLAHcAZwEjgQjMbGWPXZ919bPR1fVjxdErTY0jXPZvZOERE0ijMFsQkYI27v+vutcCDwIwQjxeeweMhr0jnIUSkWwkzQQwG3m+xXBld19oJZvaqmf3FzEbFKsjMLjOzJWa2ZMuWLWHEmlgkD4Z9SglCRLqVMBOExVjnrZaXAcPc/Rjgf4FHYxXk7nPcfaK7T+zfv39qo0xWxRSoegt2bMzM8UVE0izMBFEJDGmxXAZsaLmDu+9w913R+XlAnpmVhhhTx+k8hIh0M2EmiMXAcDOrMLN84AJgbssdzGygmVl0flI0nq0hxtRxA0dD4UHqZhKRbiM3rILdvd7MZgPzgQhwt7uvMLMrotvvBM4DrjSzemAPcIG7t+6Gyg45keAhQkoQItJNhJYgoLnbaF6rdXe2mP8p8NMwY0ipiqnw5uPw8TroU57paEREQqU7qdujYnIwXavzECJy4FOCaI/+R0JRf3UziUi3EGoX0wHnthGwewu8/ofg1aRoAHzr7czFJSISArUg2mP35vatFxHpwpQgREQkJiUIERGJSQlCRERi6jYnqSfe+ARVu2r3W19anM+S734mAxGJiGS3pFoQZlZkZjnR+RFmdo6Z5YUbWmrFSg6J1sdUNKB960VEurBkWxCLgMlm1gd4ElgCzAK+FFZgWan1payLboOnboDz78lMPCIiIUr2HIS5+yfA54D/dfeZBE+J695O+CfoVQbzvwONjZmORkQkpZJOEGZ2AkGL4c/Rdd3m/EVceT3g09+Dja/ue+OciMgBINkE8Q3gWuCR6IishwJPhxZVVzL6fDhkHDx5PdR+kuloRERSJqkE4e4L3f0cd78lerK6yt3/OeTYUqq0OL9d65OWkwPTb4IdH8BLd3SuLBGRLJJUN5GZ/Q64AmgAlgK9zex2d781zOBSqfWlrJfeu4SX3t3K/G9M6Xzh5SfCkWfBc/8N4y6CkoM7X6aISIYl28U00t13AOcSPN9hKPDlsIJKh2vOOJI9dQ385MkUDbJ36n9CfTU8c3NqyhMRybBkE0Re9L6Hc4HH3L0OyM4nvyXp8AHFXDhpCPe//B7vbtnV+QJLD4djL4Vl98LmVZ0vT0Qkw5JNEL8A1gFFwCIzGwbsCCuodPn6p0dQkJvDLX99MzUFTvk25JfAgv9ITXkiIhmU7Enqn7j7YHc/0wPrgZNDji10/UsKuGLqYcxf8SGL133U+QKL+sGUf4M1T8A7T3W+PBGRDEp2qI3eZna7mS2Jvn5E0Jro8i6dfCgH9yrgpj+vwj0FvWbHXQ4HDYP534XGhs6XJyKSIcl2Md0N7AS+EH3tAA6I8SV65Ef41+lHsPz9bfz59Y2dLzC3AE69DjavgOX3d748EZEMSTZBHObu33f3d6Ov/wQODTOwdPr8+DKOHFjCf/11NTX1KfjVP2omlE2Cp26EmhScABcRyYBkE8QeMzupacHMTgT2hBNS+kVyjGvPPIr3PvqE+156r/MFmsFpN8GuD+GFn3S+PBGRDEg2QVwB3GFm68xsHfBT4PK23mRmp5vZajNbY2bXJNjvWDNrMLPzkown5aaO6M/k4aX871Nvs31PXecLHDIpaEk8/xPYsaHz5YmIpFmyVzG96u7HAGOAMe4+Djgl0XvMLALcAZxBMPLrhWa23wiw0f1uAea3M/aUu/aMo9i+p46fPb0mNQWeeh14Azx1U2rKExFJo3Y9ctTdd0TvqAb4lzZ2nwSsiZ6zqAUeBGbE2O9rwB+Bze2JJQwjD+nF58eXcc/z63j/oxQMvNenPLiqafn9sPG1zpcnIpJGnXkmtbWxfTDwfovlyui6vQWYDQZmAnd2Io6U+tfpI8jJgdsWrE5NgZP/FXocBAu+C6m4jFZEJE06kyDaqu1iJZDW7/lv4Gp3T3jpkJld1nQPxpYtW9oRYvsN6t2Dr55UwWPLN/Ba5bbOF9ijD0y9BtYuhLcXdL48EZE0SZggzGynme2I8doJHNJG2ZXAkBbLZUDrs7UTgQejJ77PA35mZue2Lsjd57j7RHef2L9//zYO23lXTD2MfkX5/GBeim6em3gJ9D0sGIKjob7z5YmIpEHCBOHuJe7eK8arxN3bGip8MTDczCrMLB+4AJjbqvwKdy9393LgIeAqd3+0439OapQU5vH1U4fz0rsf8dSbKTg1kpsPn7keqlbDsl93vjwRkTToTBdTQu5eD8wmuDppFfCH6NPorjCzK8I6bqpcOGkoh5YW8YN5q6hvSMHzpo/8LAw7EZ6+Gaq7/DiHItINhJYgANx9nruPcPfD3P2m6Lo73X2/k9LufrG7PxRmPO2RF8nh6jOO5J0tu/n9kvfbfkNbzGD6jfBJFTz3486XJyISsqSeKNddTR95MMeW9+HHT7zNjLGDKS7o5Mf1u1nB9Lnbg1eTogHwrRQ9uEhEJEVCbUF0dWbGd848iqpdNcxZ9G7nC9wd53xGvPUiIhmkBNGGcUP7cNaYQfxy0bt8uKM60+GIiKSNEkQSvn3akdQ3NnL7grcyHYqISNooQSRhaL+eXHRCOf+39H1Wb9qZ6XBERNJCCSJJXzvlcIoLcrn5L6vCOcDaZ8MpV0Skg3QVU5IO6plPQ6PzzOotlF/z5322lRbns+S7n2m7kKIBsU9IWyS4wumiR4NhwkVEsoASRDvsro09ZFTVrtrkCoh3KevOTXDPGXDfefAPc+GQsR0LUEQkhdTFlA1KBsJFc6GwF/x2Jny4MtMRiYgoQWSNg4YErYdIPvxmBlSl6KFFIiIdpASRTfoeGiQJb4TfnAMfr8t0RCLSjSlBpMgraz9KTUH9j4CLHoPa3XDvObD9g9SUKyLSTkoQ7VBanB9zfcTgi798id+9/F5qDjTwaPjyI7Dn46AlsfPD1JQrItIOlpIH4qTRxIkTfcmSJZkOYx/b99Txzw/8nYVvbeHLxw/je2ePJC+Sgtz73kvBSes+5fAPj0NRv86XKSLdkpktdfeJ7XmPWhAp0LtHHndffCyXTTmU3760ni//6mU+2p3kpa+JDD0eLnwQtr4D982EPds6X6aISJKUIFIkkhOM/Hr7F45h2XvbmHHHc7y5KQUPBjp0Ksy6L7j09f7zoEZDfYhIeihBpNjnxpfx+8uOp6aukc/97AXmr9jU+UJHTIfz74EPlsEDF0LtJ50vU0SkDToHEZIPd1Rz2W+W8Grldv71MyOYfcrhmFnnCn39IfjjV2Nv00OHRCQBnYPIIgf3KuT3l5/AzHGD+dETbzH7d3/nk9r6zhU6+rz42/TQIRFJMY3FFKLCvAi3f+EYjhxYwg//+iZrq3Yz56IJlPXpmenQRETapAQRMjPj8qmHMeLgEi759WJOuuXp/fZJejRYEZE0UhdTmpx85ADine1JejTYtlSn4KopEZEoJYgDyc8/Be/s30IREekIJYgs8dOn3ubjZG6uKxoQe32PPpBbCL89Fx7/F6jZldL4RKT7CfUchJmdDvwPEAHucvcftto+A7gBaATqgW+4+3NhxpStblvwFnc8/Q7nTyzjkhMrKC8tir1joktZ6/bAUzfCi3fAmidgxs+gYnI4AYvIAS+0FoSZRYA7gDOAkcCFZjay1W5PAse4+1jgEuCusOLJdn/9xmQ+O2YQD7zyHif/6Bku/+0Slqz7iHbdp5LXA067Cb7yF8jJhXvPgnnfDkaGFRFppzC7mCYBa9z9XXevBR4EZrTcwd13+d4asAjinsc9IMQbDba0OJ8jB/bitvOP4fmrT+GqaYfx0rsfcd6dLzLzZy/w59c2Ut/QmPyBhp0AVzwHx10Br/wCfn4irH8xRX+FiHQXod1JbWbnAae7+6XR5S8Dx7n77Fb7zQRuBgYAn3X3/WoyM7sMuAxg6NChE9avXx9KzNnkk9p6HlpayV3PruW9jz5hSN8eXHJiBT99ag1bY5yriHup7Lrn4NGrYNt7cPxV8On/CFoaItKtdORO6jATxPnAaa0SxCR3/1qc/acA33P3UxOV21WG2kiVhkbniZWb+OWza1m6/uOE+6774Wdjb6jZBX/7Piy+C/odDp9sDZ410ZqG6xA5YGXbUBuVwJAWy2XAhng7u/si4DAzKw0xpi4nkmOcfvQg/njlp3j4qk91rJCCYvjsj4In1dXXxE4OoOE6RGQfYSaIxcBwM6sws3zgAmBuyx3M7HCLjmBnZuOBfGBriDF1aeOH9km4/fo/reTpNzfHH/Pp0Glw5QupD0xEDkihXebq7vVmNhuYT3CZ693uvsLMrohuvxP4PHCRmdUBe4BZ3tWGl80i97+8nrufX0t+JIcJw/pw0vBSpgzvz6hDepGTEx1JtrBXZoMUkbSZeOMTzSM15A88fEJ73x/qfRDuPg+Y12rdnS3mbwFuCTOG7uTV709n8bqPePbtKp59u4pb56/m1vmr6dMzjxMPD5LFScNLOSRRIct+A2NmQW5BusIWkRhaVu4ttWfsts4O46PB+rqY0uL8uF+awrwIk4f3Z/Lw/gBs3lnN82uqmhPG469tBGBdYYIDzP0aPP0DOP5KmHAxFPYO4a8QOfB1toKPV7lX7arlk9p6dlbXs2NPHTuq69lRXceOPXXBuuq65m2dpQTRxbRn1NcBJYXMHFfGzHFluDtvfbiLZ9/ewpa/9aa/bd9v/y3emxXH/RcT3v8NJU98DxbdBhMvCZJFycB99k3FrxuRbBXmr/eqXbVs3VXDjup6dkYr853VdezYs7dy31md+NkxI783P+H2vIjRqzAvqTgTUYLoJsyMIwaWcMTAEsr//PP4Oy4C+CeOtjP4Zs5fOPn5n9D4wh1UDj2H2kmzGTriGArzIgm//CKZFHblvnlHNTuq69lVs7eC39Xil3vT+kQm3Pi3hNuL8iMJt199+pH06pFLSWEevQqDae8eufQqzKOkMI/CvBzMjPJr/pz4D22DEoTsY+G3prF6007e3nwEj206ifs2rObUbf/H59fNJX/dwzzROJHHis9nccFNcVsh8F6bx1EL5MDU2X/XsCv3NZt3Nlfiu5qmLeZ3RucTmfSDJxNuL8qPUFyYuGq97uyR9OoRVOYlhbmUFAaVe6/CPIoLc4nkJK7cr5x2WMLyU0UJQvYxrF8Rw/oVMX1U05px1DV8gcr311P/4p1MXXM/p+35NsR5vHZ/286Mnz7HwN6FDOxVyMG9CxnUu5CDexUyqHcPBvYqpEd+alogSjKpFXbl3Nn3P/3mZnbV1LM7Wqnvrmlgd22wHKxrYHdN4sr91NsXxd1WkJtDSWEuxQWJq8Ubzz26uVIvKcyjuCA6X7C3cgcSVvAXn1iR8BipEu+cZbKUILqhRCe6Y8mL5FBRXgHlt0DNd4MrneZ/J275vXrksbZqNy+8szVmX2qvNn5dvfjOVnpFm8u9e+ZRnJ+79zLdFrIlyWTDr+Z0V+4NjU51XQN76hrYU9vQPJ/ID+atYndNPZ/UNvBJbdM0qNT31DWwuybx+7/y68X7rSvIzaG4IJeeBRGK8tuu3H9y4ThKCnIpjiaCpsq9qCCXvMje28ISVe7/7/hhCY+RKu39fxpLy397u+Wspe2NQQmiG+rUr+uCEjjhnxImiN+eVwa9BwOwu6aeTTuq+XB7NZt2VLNxezUf7qjmNy/GH0/rwl++tM+yGZQU5NK7Z15zM7x3j8Qn4P628kMK8nIozItQkLv/tCA3mKYiyYT5q7k1d6eh0alvdOoaGqlvCKaJyvjbyg+pqW+kpr6B2vrG5vmaukZqG6LLbVTu0259ukUyCN7XXr99cT098yP0LIjQMy+o1HvmR+hb1JOi/Ag98nN54JX43ZOPXPUpigqCyrw4P3h/y0q9SaLK/ZxjEl7knVKdreCzoRWsBCGp9+ORMOQ4GHkuRSNncFj/wRzWv3ifXRIliN/943HNV3Ts2FPXfCnf9ub5OtZWJR7C/NLfdH68rlN+9Aw5ZkTMyMkxciwY+iTH9p1PZNYvXsQdGt1xgmmjBxV90/pEJtzwBLXRRFDf2EhdQ/vvI030WZhBYW6E/NzEgyocM+QgeuRFKIy+euRF6JGf07yuR36w7qv3xj/WqhtObzPWRAliXBsjCaRSqn+9d1VKEJJ6p3wXVjwG868NXmWTYNRMGDmjuWWRyKcOS244rkS/FP80+ySqo7+Sq+saqKnfO62pD34F19Q38N9/iz844chBvWiM/mJvdGhsdBq8xXx0uS2RHCPXgmRixj7THIMVG+I/S/z0oweSF8khL2LkRnLIywmmuREjLyc6jeTw3UffSPhZ5OfmBC2nvKD11LScm2NER7tJ+Hn+zwXj2vw7s4Uq99RRgpCOKRoQe3C/ogEw5VvBq2oNrHwkZrJYVngzfdm/Yqwiuaug2jK6LLkb/BIliJ9+cXxSZSSqWH9/+Qmdev9NM0cnFUOiBJHsZ5EKna2cVblnFyUI6ZhkhgUvPbxVsngUVjwK86+lb7y3sP+ls3GLT0FlIntlQ+Wsyj27KEFIepQeDlP+LXhtfQf+N8Gv8+2V0LuszSJTUZmkolLMhl/N2VC5y4EntAcGhaW7PTDogHVdG90eBw2D8pOC17AToU96Li0UOVB15IFBakFI9jn9h8GjUlfPg+X3B+t6DwkSRfmJQdLoUwG3jYh/HkRPxhPpNCUIyT7HXxm8GhthyypY9zysfw7W/A1eezDYp+SQ+E/A05PxRFJCCUIyI9FVUE1ycuDgUcHruMvAHbasDpLFuudhxcPxy/9wJZQOh0jnR7QU6a50DkK6rrbOY0QKYMCRMHA0DBwTTA8ete8zLm4drm4q6RZ0DkKkyed+CZteg02vw+q/wN/v27vtoGHRpDFa3VQiCShByIFpzBeCFwRdUzs3Bcniw9eD6abX4c02xsr/YBn0PRR6HJR4P7VC5AClBCFdVzLnMSAYcKjXoOA1Yvre9bW74QcJBm/75cnBtEdf6HdYkCz6Hgp9m+YroGdftULkgKUEIV1XZ3+d5xcl3j7rfvjo3ejrHVj/Arz2B6DFebvCgxKX4R4kqETUApEspQQhEs9RZ+2/rq4aPl63b+JYcnf8Mm4ug5JB0OsQ6DU4Om05Pzg1LRAlGQmBEoR0b8l2UzXJKwyujBpw5N51iRLE+ItgxwewYyOsXQQ7N4InfvbCPlbOhaL+UFQKPfsFLZacGENzq5tLQhBqgjCz04H/ASLAXe7+w1bbvwRcHV3cBVzp7q+GGZPIPsL+dX36zfsuNzbArs2wY0M0cWyAv14d+70Af/jyvssWCRJFU8IoKoWebQyP3tgAOZG2Y01FK0QtmQNKaAnCzCLAHcBngEpgsZnNdfeVLXZbC0x194/N7AxgDnBcWDGJhKI9rZCcyN4T5kwI1iVKEJcvgt1V8MnW6LQqmDbNb3wtmCZyfT8o6BXc/5HolagV0lAPkSSqi862ZLIlSWVDGSmOYcKgnAnJvWmvMFsQk4A17v4ugJk9CMwAmhOEu7/QYv+XgLaH8BTJNmH+Mh50THL7JbppcOq3oXr7vq9t66PzO6AmiSHWb+gHuYWQ1xPyi4MT/M2vFsuJvLUAcgsgr0cwzS3c+8qLTlPRVXaglBFmDEkKM0EMBt5vsVxJ4tbBV4G/xNpgZpcBlwEMHTo0VfGJZIf2ngdpr5PjPz8cCLqganbCLQlGzD3536F2V3BpcO3ufec/+WjvciK/O7/9sbf08xODoVNy8oJpvPlEnv5B0E2XkxOdRlpNo+sTeePh6JVpFkwtZ+98y2kiby2IzrS4Iq55VIskRrdY8Qh4Y/Ae92Aeb7GuMbly2hBmgoj1CcWM2MxOJkgQJ8Xa7u5zCLqfmDhxYtcaG0SkLalogXQmyeRE2r4ZcOq3k4sjUUvm0qegfg/UV0N9DdTtCab11XtfT14f//19yqGhFhrqglfdHmjYEcw31u1dn8jCW5L7OxJ56CudL6OzyfL/Lu58DEkIM0FUAkNaLJcBG1rvZGZjgLuAM9x9a4jxiBy4usIJ4LIkusATJYgL7k/uOImS1Pe3Bb+uGxuCq8n2mbZYf/tR8cu46qXor31vMW3cf90vT4lfxqVP0vwbep+f0k3rDOZMSxwD0daL5URbMtZiXXT641Hxy0hCmAliMTDczCqAD4ALgC+23MHMhgIPA19297dCjEVE2pKKrq6wu8s6y2xvl1JHDUiQPJJV1q4x88KJIQmhJQh3rzez2cB8gstc73b3FWZ2RXT7ncD3gH7Azyy427S+vaMNikiKpKIV0tkysiVJZUMZYcaQJA33LSLSDXRkuO8Yt2SKiIgoQYiISBxKECIiEpMShIiIxKQEISIiMSlBiIhITEoQIiISkxKEiIjEpAQhIiIxKUGIiEhMShAiIhKTEoSIiMSkBCEiIjEpQYiISExKECIiEpMShIiIxKQEISIiMSlBiIhITEoQIiISkxKEiIjEpAQhIiIxKUGIiEhMShAiIhJTqAnCzE43s9VmtsbMromx/Ugze9HMaszs38KMRURE2ic3rILNLALcAXwGqAQWm9lcd1/ZYrePgH8Gzg0rDhER6ZgwWxCTgDXu/q671wIPAjNa7uDum919MVAXYhwiItIBYSaIwcD7LZYro+tERKQLCDNBWIx13qGCzC4zsyVmtmTLli2dDEtERJIR2jkIghbDkBbLZcCGjhTk7nOAOQBmttPMVnc+vE4rBaoUA5AdcWRDDJAdcWRDDJAdcWRDDJAdcRzR3jeEmSAWA8PNrAL4ALgA+GIKyl3t7hNTUE6nmNmSTMeRDTFkSxzZEEO2xJENMWRLHNkQQ7bEYWZL2vue0BKEu9eb2WxgPhAB7nb3FWZ2RXT7nWY2EFgC9AIazewbwEh33xFWXCIikpwwWxC4+zxgXqt1d7aY30TQ9SQiIlmmK95JPSfTAURlQxzZEANkRxzZEANkRxzZEANkRxzZEANkRxztjsHcO3RhkYiIHOC6YgtCRETSQAlCRERi6lIJoq3B/9Jw/CFm9rSZrTKzFWb29XTH0CKWiJn93cwez2AMB5nZQ2b2ZvQzOSFDcXwz+u/xhpk9YGaFaTjm3Wa22czeaLGur5k9YWZvR6d9MhTHrdF/k9fM7BEzOyjdMbTY9m9m5mZWGmYMieIws69F640VZvZf6Y7BzMaa2Utmtjx6w++kkGOIWU916Pvp7l3iRXCp7DvAoUA+8CrBJbHpjGEQMD46XwK8le4YWsTyL8DvgMcz+G9yL3BpdD4fOCgDMQwG1gI9ost/AC5Ow3GnAOOBN1qs+y/gmuj8NcAtGYpjOpAbnb8l7DhixRBdP4TgMvf1QGmGPouTgb8BBdHlARmIYQFwRnT+TOCZkGOIWU915PvZlVoQbQ7+FzZ33+juy6LzO4FVZGB8KTMrAz4L3JXuY7eIoRfBf4ZfAbh7rbtvy1A4uUAPM8sFetLBO/bbw90XEYxG3NIMgqRJdHpuJuJw9wXuXh9dfImQLyWP81kA/Bj4Nh0cYidFcVwJ/NDda6L7bM5ADE5wrxdAb0L+fiaop9r9/exKCSKrBv8zs3JgHPByBg7/3wT/8RozcOwmhwJbgHuiXV13mVlRuoNw9w+A24D3gI3AdndfkO44og52943RuDYCAzIUR0uXAH9J90HN7BzgA3d/Nd3HbmUEMNnMXjazhWZ2bAZi+AZwq5m9T/BdvTZdB25VT7X7+9mVEkTKBv/rLDMrBv4IfMPTfNe3mZ0FbHb3pek8bgy5BE3pn7v7OGA3QbM1raL9qDOACuAQoMjM/l+648hGZvbvQD1wf5qP2xP4d+B76TxuHLlAH+B44FvAH8wsVl0SpiuBb7r7EOCbRFvdYUtFPdWVEkTKBv/rDDPLI/jQ73f3h9N9fOBE4BwzW0fQzXaKmd2XgTgqgUp3b2pBPUSQMNLtVGCtu29x9zrgYeBTGYgD4EMzGwQQnYbanZGImf0DcBbwJY92OqfRYQQJ+9Xo97QMWBYdWifdKoGHPfAKQas79BPmrfwDwfcS4P8IustDFaeeavf3sysliObB/8wsn2Dwv7npDCD6y+NXwCp3vz2dx27i7te6e5m7lxN8Bk+5e9p/MXswTMr7ZtY0QuSngZUJ3hKW94Djzaxn9N/n0wR9rpkwl6AyIDp9LBNBmNnpwNXAOe7+SbqP7+6vu/sAdy+Pfk8rCU6abkp3LMCjwCkAZjaC4GKKdI+qugGYGp0/BXg7zIMlqKfa//0M82x6CGfnzyQ4I/8O8O8ZOP5JBN1arwHLo68zM/h5TCOzVzGNJRhs8TWC/4h9MhTHfwJvAm8AvyV6xUrIx3yA4JxHHUEF+FWgH/AkQQXwJNA3Q3GsIThf1/QdvTPdMbTavo70XMUU67PIB+6LfjeWAadkIIaTgKUEV16+DEwIOYaY9VRHvp8aakNERGLqSl1MIiKSRkoQIiISkxKEiIjEpAQhIiIxKUGIiEhMShAirZhZQ3TkzaZXyu4QN7PyWKOeimSjUJ9JLdJF7XH3sZkOQiTT1IIQSZKZrTOzW8zslejr8Oj6YWb2ZPT5C0+a2dDo+oOjz2N4NfpqGgIkYma/jI7Vv8DMemTsjxJJQAlCZH89WnUxzWqxbYe7TwJ+SjCqLtH537j7GIKB8X4SXf8TYKG7H0MwTtWK6PrhwB3uPgrYBnw+1L9GpIN0J7VIK2a2y92LY6xfRzBUw7vRwdA2uXs/M6sCBrl7XXT9RncvNbMtQJlHn0UQLaMceMLdh0eXrwby3P3GNPxpIu2iFoRI+3ic+Xj7xFLTYr4BnQuULKUEIdI+s1pMX4zOv0Awsi7Al4DnovNPEjwLoOkZ4k1PFRPpEvTLRWR/PcxseYvlv7p706WuBWb2MsGPqwuj6/4ZuNvMvkXwlL2vRNd/HZhjZl8laClcSTDSp0iXoHMQIkmKnoOY6O7pfp6ASEaoi0lERGJSC0JERGJSC0JERGJSghARkZiUIEREJCYlCBERiUkJQkREYvr/78UaSxf9XTAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用Softmax交叉熵损失训练多层感知机(MLP with Softmax Cross-Entropy Loss)\n",
    "第二部分将使用Softmax交叉熵损失训练多层感知机. \n",
    "分别使用**Sigmoid**激活函数和**ReLU**激活函数.\n",
    "\n",
    "### TODO\n",
    "执行以下代码之前，请完成 **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 使用Softmax交叉熵损失和Sigmoid激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用Softmax交叉熵损失和Sigmoid激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4653\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4941\t Accuracy 0.0939\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4538\t Accuracy 0.0971\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.4275\t Accuracy 0.0996\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.4050\t Accuracy 0.1036\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.3837\t Accuracy 0.1114\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.3672\t Accuracy 0.1192\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.3523\t Accuracy 0.1268\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.3371\t Accuracy 0.1352\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.3236\t Accuracy 0.1438\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.3102\t Accuracy 0.1544\n",
      "\n",
      "Epoch [0]\t Average training loss 2.2981\t Average training accuracy 0.1655\n",
      "Epoch [0]\t Average validation loss 2.1662\t Average validation accuracy 0.2794\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.1413\t Accuracy 0.2900\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.1534\t Accuracy 0.3051\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.1434\t Accuracy 0.3204\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.1381\t Accuracy 0.3279\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.1321\t Accuracy 0.3343\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.1239\t Accuracy 0.3465\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.1175\t Accuracy 0.3562\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.1128\t Accuracy 0.3663\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.1057\t Accuracy 0.3759\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.0995\t Accuracy 0.3869\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.0927\t Accuracy 0.3970\n",
      "\n",
      "Epoch [1]\t Average training loss 2.0860\t Average training accuracy 0.4084\n",
      "Epoch [1]\t Average validation loss 2.0042\t Average validation accuracy 0.5338\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.9866\t Accuracy 0.5400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 2.0018\t Accuracy 0.5349\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.9940\t Accuracy 0.5457\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.9918\t Accuracy 0.5425\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.9888\t Accuracy 0.5448\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.9828\t Accuracy 0.5486\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.9782\t Accuracy 0.5531\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.9762\t Accuracy 0.5548\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.9712\t Accuracy 0.5582\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.9672\t Accuracy 0.5616\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.9624\t Accuracy 0.5648\n",
      "\n",
      "Epoch [2]\t Average training loss 1.9576\t Average training accuracy 0.5696\n",
      "Epoch [2]\t Average validation loss 1.8896\t Average validation accuracy 0.6284\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.8734\t Accuracy 0.6400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.8942\t Accuracy 0.6120\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.8877\t Accuracy 0.6227\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.8877\t Accuracy 0.6164\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.8866\t Accuracy 0.6177\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.8821\t Accuracy 0.6178\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.8787\t Accuracy 0.6189\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.8786\t Accuracy 0.6181\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.8749\t Accuracy 0.6202\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.8724\t Accuracy 0.6214\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.8691\t Accuracy 0.6228\n",
      "\n",
      "Epoch [3]\t Average training loss 1.8654\t Average training accuracy 0.6260\n",
      "Epoch [3]\t Average validation loss 1.8068\t Average validation accuracy 0.6708\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.7914\t Accuracy 0.6600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.8164\t Accuracy 0.6459\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.8110\t Accuracy 0.6552\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.8125\t Accuracy 0.6485\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.8126\t Accuracy 0.6498\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.8092\t Accuracy 0.6499\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.8067\t Accuracy 0.6505\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.8078\t Accuracy 0.6496\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.8051\t Accuracy 0.6508\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.8037\t Accuracy 0.6510\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.8014\t Accuracy 0.6521\n",
      "\n",
      "Epoch [4]\t Average training loss 1.7986\t Average training accuracy 0.6544\n",
      "Epoch [4]\t Average validation loss 1.7465\t Average validation accuracy 0.7006\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.7319\t Accuracy 0.6900\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.7598\t Accuracy 0.6708\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.7552\t Accuracy 0.6773\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.7577\t Accuracy 0.6712\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.7588\t Accuracy 0.6722\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.7562\t Accuracy 0.6716\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.7542\t Accuracy 0.6713\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.7563\t Accuracy 0.6701\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.7543\t Accuracy 0.6707\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.7537\t Accuracy 0.6709\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.7521\t Accuracy 0.6714\n",
      "\n",
      "Epoch [5]\t Average training loss 1.7499\t Average training accuracy 0.6731\n",
      "Epoch [5]\t Average validation loss 1.7026\t Average validation accuracy 0.7204\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.6887\t Accuracy 0.7200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.7185\t Accuracy 0.6886\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.7145\t Accuracy 0.6941\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.7178\t Accuracy 0.6872\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.7196\t Accuracy 0.6879\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.7175\t Accuracy 0.6862\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.7160\t Accuracy 0.6854\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.7187\t Accuracy 0.6844\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.7172\t Accuracy 0.6851\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.7172\t Accuracy 0.6852\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.7161\t Accuracy 0.6854\n",
      "\n",
      "Epoch [6]\t Average training loss 1.7144\t Average training accuracy 0.6868\n",
      "Epoch [6]\t Average validation loss 1.6706\t Average validation accuracy 0.7340\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.6575\t Accuracy 0.7500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.6884\t Accuracy 0.6980\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.6849\t Accuracy 0.7052\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.6888\t Accuracy 0.6981\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.6911\t Accuracy 0.6981\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.6894\t Accuracy 0.6960\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.6882\t Accuracy 0.6952\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.6914\t Accuracy 0.6946\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.6903\t Accuracy 0.6952\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.6907\t Accuracy 0.6953\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.6900\t Accuracy 0.6955\n",
      "\n",
      "Epoch [7]\t Average training loss 1.6887\t Average training accuracy 0.6968\n",
      "Epoch [7]\t Average validation loss 1.6475\t Average validation accuracy 0.7454\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.6351\t Accuracy 0.7400\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.6666\t Accuracy 0.7061\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.6636\t Accuracy 0.7115\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.6679\t Accuracy 0.7060\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.6706\t Accuracy 0.7055\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.6692\t Accuracy 0.7038\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.6682\t Accuracy 0.7031\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.6718\t Accuracy 0.7027\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.6710\t Accuracy 0.7030\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.6717\t Accuracy 0.7031\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.6712\t Accuracy 0.7032\n",
      "\n",
      "Epoch [8]\t Average training loss 1.6702\t Average training accuracy 0.7042\n",
      "Epoch [8]\t Average validation loss 1.6312\t Average validation accuracy 0.7514\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.6193\t Accuracy 0.7400\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.6512\t Accuracy 0.7131\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.6485\t Accuracy 0.7181\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.6530\t Accuracy 0.7121\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.6560\t Accuracy 0.7109\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.6549\t Accuracy 0.7091\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.6541\t Accuracy 0.7083\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.6579\t Accuracy 0.7077\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.6573\t Accuracy 0.7081\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.6582\t Accuracy 0.7081\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.6580\t Accuracy 0.7082\n",
      "\n",
      "Epoch [9]\t Average training loss 1.6572\t Average training accuracy 0.7094\n",
      "Epoch [9]\t Average validation loss 1.6199\t Average validation accuracy 0.7562\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.6086\t Accuracy 0.7500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.6405\t Accuracy 0.7180\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.6381\t Accuracy 0.7218\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.6428\t Accuracy 0.7160\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.6460\t Accuracy 0.7147\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.6451\t Accuracy 0.7130\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.6444\t Accuracy 0.7122\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.6484\t Accuracy 0.7116\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.6480\t Accuracy 0.7119\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.6491\t Accuracy 0.7119\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.6490\t Accuracy 0.7121\n",
      "\n",
      "Epoch [10]\t Average training loss 1.6484\t Average training accuracy 0.7130\n",
      "Epoch [10]\t Average validation loss 1.6124\t Average validation accuracy 0.7594\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.6016\t Accuracy 0.7700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.6333\t Accuracy 0.7216\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.6313\t Accuracy 0.7249\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.6361\t Accuracy 0.7189\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.6395\t Accuracy 0.7177\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.6386\t Accuracy 0.7158\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.6381\t Accuracy 0.7150\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.6422\t Accuracy 0.7144\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.6419\t Accuracy 0.7146\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.6431\t Accuracy 0.7146\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.6432\t Accuracy 0.7147\n",
      "\n",
      "Epoch [11]\t Average training loss 1.6427\t Average training accuracy 0.7156\n",
      "Epoch [11]\t Average validation loss 1.6079\t Average validation accuracy 0.7612\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.5975\t Accuracy 0.7700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.6290\t Accuracy 0.7224\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.6272\t Accuracy 0.7257\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.6321\t Accuracy 0.7199\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.6355\t Accuracy 0.7191\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.6348\t Accuracy 0.7175\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.6344\t Accuracy 0.7167\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.6385\t Accuracy 0.7163\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.6383\t Accuracy 0.7164\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.6396\t Accuracy 0.7163\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.6399\t Accuracy 0.7164\n",
      "\n",
      "Epoch [12]\t Average training loss 1.6394\t Average training accuracy 0.7173\n",
      "Epoch [12]\t Average validation loss 1.6056\t Average validation accuracy 0.7608\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.5956\t Accuracy 0.7600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.6267\t Accuracy 0.7243\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.6251\t Accuracy 0.7264\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.6300\t Accuracy 0.7207\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.6336\t Accuracy 0.7197\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.6330\t Accuracy 0.7184\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.6326\t Accuracy 0.7175\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.6368\t Accuracy 0.7168\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.6367\t Accuracy 0.7171\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.6380\t Accuracy 0.7171\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.6383\t Accuracy 0.7174\n",
      "\n",
      "Epoch [13]\t Average training loss 1.6380\t Average training accuracy 0.7181\n",
      "Epoch [13]\t Average validation loss 1.6050\t Average validation accuracy 0.7618\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.5953\t Accuracy 0.7600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.6260\t Accuracy 0.7249\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.6246\t Accuracy 0.7273\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.6295\t Accuracy 0.7215\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.6332\t Accuracy 0.7204\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.6326\t Accuracy 0.7192\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.6323\t Accuracy 0.7182\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.6365\t Accuracy 0.7174\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.6364\t Accuracy 0.7175\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.6379\t Accuracy 0.7175\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.6382\t Accuracy 0.7177\n",
      "\n",
      "Epoch [14]\t Average training loss 1.6379\t Average training accuracy 0.7185\n",
      "Epoch [14]\t Average validation loss 1.6057\t Average validation accuracy 0.7624\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.5963\t Accuracy 0.7600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.6265\t Accuracy 0.7243\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.6253\t Accuracy 0.7268\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.6302\t Accuracy 0.7209\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.6339\t Accuracy 0.7198\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.6334\t Accuracy 0.7191\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.6331\t Accuracy 0.7181\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.6373\t Accuracy 0.7174\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.6373\t Accuracy 0.7175\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.6388\t Accuracy 0.7175\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.6392\t Accuracy 0.7177\n",
      "\n",
      "Epoch [15]\t Average training loss 1.6389\t Average training accuracy 0.7186\n",
      "Epoch [15]\t Average validation loss 1.6074\t Average validation accuracy 0.7610\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.5981\t Accuracy 0.7500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.6280\t Accuracy 0.7237\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.6268\t Accuracy 0.7263\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.6318\t Accuracy 0.7203\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.6355\t Accuracy 0.7193\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.6350\t Accuracy 0.7188\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.6348\t Accuracy 0.7177\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.6390\t Accuracy 0.7171\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.6390\t Accuracy 0.7171\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.6405\t Accuracy 0.7171\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.6409\t Accuracy 0.7173\n",
      "\n",
      "Epoch [16]\t Average training loss 1.6407\t Average training accuracy 0.7182\n",
      "Epoch [16]\t Average validation loss 1.6097\t Average validation accuracy 0.7606\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.6007\t Accuracy 0.7500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.6301\t Accuracy 0.7243\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.6291\t Accuracy 0.7264\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.6339\t Accuracy 0.7200\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.6378\t Accuracy 0.7185\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.6373\t Accuracy 0.7183\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.6370\t Accuracy 0.7173\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.6412\t Accuracy 0.7164\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.6412\t Accuracy 0.7165\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.6427\t Accuracy 0.7166\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.6432\t Accuracy 0.7167\n",
      "\n",
      "Epoch [17]\t Average training loss 1.6430\t Average training accuracy 0.7175\n",
      "Epoch [17]\t Average validation loss 1.6126\t Average validation accuracy 0.7588\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.6037\t Accuracy 0.7500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.6326\t Accuracy 0.7235\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.6318\t Accuracy 0.7250\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.6366\t Accuracy 0.7189\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.6404\t Accuracy 0.7175\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.6400\t Accuracy 0.7170\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.6397\t Accuracy 0.7161\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.6439\t Accuracy 0.7156\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.6439\t Accuracy 0.7157\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.6454\t Accuracy 0.7157\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.6459\t Accuracy 0.7157\n",
      "\n",
      "Epoch [18]\t Average training loss 1.6458\t Average training accuracy 0.7163\n",
      "Epoch [18]\t Average validation loss 1.6158\t Average validation accuracy 0.7570\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.6071\t Accuracy 0.7500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.6355\t Accuracy 0.7212\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.6348\t Accuracy 0.7236\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.6396\t Accuracy 0.7174\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.6434\t Accuracy 0.7159\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.6430\t Accuracy 0.7155\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.6427\t Accuracy 0.7147\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.6469\t Accuracy 0.7141\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.6469\t Accuracy 0.7143\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.6484\t Accuracy 0.7143\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.6489\t Accuracy 0.7143\n",
      "\n",
      "Epoch [19]\t Average training loss 1.6488\t Average training accuracy 0.7148\n",
      "Epoch [19]\t Average validation loss 1.6193\t Average validation accuracy 0.7564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.7311.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 使用Softmax交叉熵损失和ReLU激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用Softmax交叉熵损失和ReLU激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5937\t Accuracy 0.0900\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.5514\t Accuracy 0.0692\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4865\t Accuracy 0.0827\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.4373\t Accuracy 0.1002\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3887\t Accuracy 0.1184\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.3497\t Accuracy 0.1361\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.3108\t Accuracy 0.1551\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.2779\t Accuracy 0.1726\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.2446\t Accuracy 0.1935\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2166\t Accuracy 0.2113\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.1871\t Accuracy 0.2303\n",
      "\n",
      "Epoch [0]\t Average training loss 2.1604\t Average training accuracy 0.2477\n",
      "Epoch [0]\t Average validation loss 1.8493\t Average validation accuracy 0.4560\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.8256\t Accuracy 0.4800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.8472\t Accuracy 0.4557\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.8229\t Accuracy 0.4670\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.8066\t Accuracy 0.4772\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.7885\t Accuracy 0.4903\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.7711\t Accuracy 0.5014\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.7523\t Accuracy 0.5112\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.7403\t Accuracy 0.5185\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.7232\t Accuracy 0.5279\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.7108\t Accuracy 0.5342\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.6953\t Accuracy 0.5427\n",
      "\n",
      "Epoch [1]\t Average training loss 1.6806\t Average training accuracy 0.5506\n",
      "Epoch [1]\t Average validation loss 1.4875\t Average validation accuracy 0.6546\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.4644\t Accuracy 0.7000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.5061\t Accuracy 0.6392\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.4903\t Accuracy 0.6424\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.4834\t Accuracy 0.6416\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.4739\t Accuracy 0.6470\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.4625\t Accuracy 0.6523\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.4501\t Accuracy 0.6547\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.4454\t Accuracy 0.6574\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.4341\t Accuracy 0.6620\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.4274\t Accuracy 0.6648\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.4176\t Accuracy 0.6694\n",
      "\n",
      "Epoch [2]\t Average training loss 1.4076\t Average training accuracy 0.6738\n",
      "Epoch [2]\t Average validation loss 1.2590\t Average validation accuracy 0.7448\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.2438\t Accuracy 0.7800\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.2912\t Accuracy 0.7163\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.2801\t Accuracy 0.7201\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.2786\t Accuracy 0.7172\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.2737\t Accuracy 0.7205\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.2658\t Accuracy 0.7225\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.2572\t Accuracy 0.7245\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.2566\t Accuracy 0.7249\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.2488\t Accuracy 0.7280\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.2452\t Accuracy 0.7296\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.2388\t Accuracy 0.7319\n",
      "\n",
      "Epoch [3]\t Average training loss 1.2318\t Average training accuracy 0.7346\n",
      "Epoch [3]\t Average validation loss 1.1094\t Average validation accuracy 0.7922\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.1014\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.1500\t Accuracy 0.7590\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.1424\t Accuracy 0.7631\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.1442\t Accuracy 0.7599\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.1419\t Accuracy 0.7616\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.1362\t Accuracy 0.7631\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.1301\t Accuracy 0.7640\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.1319\t Accuracy 0.7632\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.1263\t Accuracy 0.7654\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.1246\t Accuracy 0.7663\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.1205\t Accuracy 0.7672\n",
      "\n",
      "Epoch [4]\t Average training loss 1.1153\t Average training accuracy 0.7691\n",
      "Epoch [4]\t Average validation loss 1.0090\t Average validation accuracy 0.8206\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.0070\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.0549\t Accuracy 0.7839\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.0500\t Accuracy 0.7879\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.0538\t Accuracy 0.7846\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.0530\t Accuracy 0.7854\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.0487\t Accuracy 0.7862\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.0443\t Accuracy 0.7868\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.0475\t Accuracy 0.7859\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.0435\t Accuracy 0.7876\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.0428\t Accuracy 0.7882\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.0402\t Accuracy 0.7883\n",
      "\n",
      "Epoch [5]\t Average training loss 1.0362\t Average training accuracy 0.7895\n",
      "Epoch [5]\t Average validation loss 0.9400\t Average validation accuracy 0.8370\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.9427\t Accuracy 0.8600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.9892\t Accuracy 0.8022\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.9862\t Accuracy 0.8047\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.9910\t Accuracy 0.8021\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.9913\t Accuracy 0.8022\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.9880\t Accuracy 0.8024\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.9846\t Accuracy 0.8029\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.9888\t Accuracy 0.8022\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.9857\t Accuracy 0.8035\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.9858\t Accuracy 0.8038\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.9841\t Accuracy 0.8036\n",
      "\n",
      "Epoch [6]\t Average training loss 0.9809\t Average training accuracy 0.8045\n",
      "Epoch [6]\t Average validation loss 0.8914\t Average validation accuracy 0.8482\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8980\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.9426\t Accuracy 0.8157\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.9409\t Accuracy 0.8171\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.9465\t Accuracy 0.8136\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.9475\t Accuracy 0.8136\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.9447\t Accuracy 0.8141\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.9421\t Accuracy 0.8142\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.9469\t Accuracy 0.8133\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.9446\t Accuracy 0.8141\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.9450\t Accuracy 0.8144\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.9441\t Accuracy 0.8138\n",
      "\n",
      "Epoch [7]\t Average training loss 0.9415\t Average training accuracy 0.8145\n",
      "Epoch [7]\t Average validation loss 0.8564\t Average validation accuracy 0.8578\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8663\t Accuracy 0.8700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.9087\t Accuracy 0.8227\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.9081\t Accuracy 0.8243\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.9143\t Accuracy 0.8206\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.9158\t Accuracy 0.8208\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.9135\t Accuracy 0.8216\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.9114\t Accuracy 0.8220\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.9165\t Accuracy 0.8210\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.9147\t Accuracy 0.8216\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.9155\t Accuracy 0.8219\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.9150\t Accuracy 0.8216\n",
      "\n",
      "Epoch [8]\t Average training loss 0.9128\t Average training accuracy 0.8219\n",
      "Epoch [8]\t Average validation loss 0.8309\t Average validation accuracy 0.8676\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8439\t Accuracy 0.8900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8839\t Accuracy 0.8312\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8842\t Accuracy 0.8329\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8907\t Accuracy 0.8285\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8926\t Accuracy 0.8287\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8906\t Accuracy 0.8288\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8889\t Accuracy 0.8291\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8943\t Accuracy 0.8281\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8928\t Accuracy 0.8288\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8939\t Accuracy 0.8287\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8938\t Accuracy 0.8284\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8919\t Average training accuracy 0.8285\n",
      "Epoch [9]\t Average validation loss 0.8124\t Average validation accuracy 0.8714\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8281\t Accuracy 0.8900\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8654\t Accuracy 0.8353\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8665\t Accuracy 0.8376\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8733\t Accuracy 0.8336\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8756\t Accuracy 0.8331\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8738\t Accuracy 0.8334\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8724\t Accuracy 0.8340\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8780\t Accuracy 0.8333\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8768\t Accuracy 0.8339\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8780\t Accuracy 0.8337\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8782\t Accuracy 0.8330\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8766\t Average training accuracy 0.8331\n",
      "Epoch [10]\t Average validation loss 0.7989\t Average validation accuracy 0.8744\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8171\t Accuracy 0.8900\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8518\t Accuracy 0.8396\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8535\t Accuracy 0.8421\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8605\t Accuracy 0.8381\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8630\t Accuracy 0.8371\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8614\t Accuracy 0.8375\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8603\t Accuracy 0.8382\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8660\t Accuracy 0.8376\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8650\t Accuracy 0.8382\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8663\t Accuracy 0.8378\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8667\t Accuracy 0.8372\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8653\t Average training accuracy 0.8373\n",
      "Epoch [11]\t Average validation loss 0.7891\t Average validation accuracy 0.8774\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8092\t Accuracy 0.8900\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8417\t Accuracy 0.8424\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8438\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8511\t Accuracy 0.8414\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8538\t Accuracy 0.8403\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8523\t Accuracy 0.8411\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8514\t Accuracy 0.8418\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8572\t Accuracy 0.8410\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8564\t Accuracy 0.8416\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8578\t Accuracy 0.8413\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8583\t Accuracy 0.8408\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8571\t Average training accuracy 0.8409\n",
      "Epoch [12]\t Average validation loss 0.7820\t Average validation accuracy 0.8794\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8037\t Accuracy 0.9000\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8343\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8472\t Accuracy 0.8432\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8449\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8508\t Accuracy 0.8440\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8502\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8516\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8522\t Accuracy 0.8434\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8512\t Average training accuracy 0.8433\n",
      "Epoch [13]\t Average validation loss 0.7770\t Average validation accuracy 0.8816\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8001\t Accuracy 0.9000\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8290\t Accuracy 0.8490\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8318\t Accuracy 0.8502\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8394\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8425\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8411\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8404\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8464\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8458\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8481\t Accuracy 0.8455\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8471\t Average training accuracy 0.8455\n",
      "Epoch [14]\t Average validation loss 0.7736\t Average validation accuracy 0.8822\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7978\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8253\t Accuracy 0.8516\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8284\t Accuracy 0.8516\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8361\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8393\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8380\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8374\t Accuracy 0.8489\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8434\t Accuracy 0.8480\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8430\t Accuracy 0.8486\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8453\t Accuracy 0.8474\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8444\t Average training accuracy 0.8474\n",
      "Epoch [15]\t Average validation loss 0.7716\t Average validation accuracy 0.8836\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7966\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8229\t Accuracy 0.8537\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8262\t Accuracy 0.8538\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8340\t Accuracy 0.8503\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8373\t Accuracy 0.8497\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8361\t Accuracy 0.8504\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8355\t Accuracy 0.8510\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8416\t Accuracy 0.8499\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8412\t Accuracy 0.8504\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8428\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8436\t Accuracy 0.8491\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8428\t Average training accuracy 0.8491\n",
      "Epoch [16]\t Average validation loss 0.7706\t Average validation accuracy 0.8852\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7962\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8214\t Accuracy 0.8565\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8250\t Accuracy 0.8556\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8329\t Accuracy 0.8517\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8363\t Accuracy 0.8506\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8350\t Accuracy 0.8516\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8346\t Accuracy 0.8522\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8406\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8403\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8419\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8428\t Accuracy 0.8502\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8420\t Average training accuracy 0.8502\n",
      "Epoch [17]\t Average validation loss 0.7703\t Average validation accuracy 0.8854\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7964\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8207\t Accuracy 0.8571\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8245\t Accuracy 0.8564\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8324\t Accuracy 0.8528\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8359\t Accuracy 0.8520\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8346\t Accuracy 0.8528\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8342\t Accuracy 0.8533\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8403\t Accuracy 0.8522\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8401\t Accuracy 0.8526\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8417\t Accuracy 0.8522\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8426\t Accuracy 0.8514\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8419\t Average training accuracy 0.8514\n",
      "Epoch [18]\t Average validation loss 0.7706\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7973\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8206\t Accuracy 0.8584\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8245\t Accuracy 0.8571\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8324\t Accuracy 0.8535\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8360\t Accuracy 0.8526\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8347\t Accuracy 0.8533\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8344\t Accuracy 0.8539\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8405\t Accuracy 0.8525\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8403\t Accuracy 0.8529\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8419\t Accuracy 0.8526\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8428\t Accuracy 0.8518\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8421\t Average training accuracy 0.8518\n",
      "Epoch [19]\t Average validation loss 0.7712\t Average validation accuracy 0.8894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8633.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具有两层隐含层的多层感知机\n",
    "\n",
    "接下来，根据案例要求，还需要完成**构造具有两个隐含层的多层感知机，自行选取合适的激活函数和损失函数，与只有一个隐含层的结果相比较**.\n",
    "\n",
    "注意: 请在下方插入新的代码块，不要直接修改上面的代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer, SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "from layers import FCLayer, ReLULayer, SigmoidLayer \n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)\n",
    "\n",
    "model_2hidden = Network()\n",
    "model_2hidden.add(FCLayer(784, 128))\n",
    "model_2hidden.add(ReLULayer())\n",
    "model_2hidden.add(FCLayer(128, 64))\n",
    "model_2hidden.add(ReLULayer())\n",
    "model_2hidden.add(FCLayer(64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7220\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.6046\t Accuracy 0.1359\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4883\t Accuracy 0.1626\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.4184\t Accuracy 0.1773\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3581\t Accuracy 0.1921\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.3060\t Accuracy 0.2061\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2563\t Accuracy 0.2211\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.2176\t Accuracy 0.2335\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.1788\t Accuracy 0.2489\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.1439\t Accuracy 0.2640\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.1107\t Accuracy 0.2790\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0784\t Average training accuracy 0.2943\n",
      "Epoch [0]\t Average validation loss 1.7097\t Average validation accuracy 0.4616\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.6990\t Accuracy 0.5000\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.7157\t Accuracy 0.4663\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.6867\t Accuracy 0.4858\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.6699\t Accuracy 0.4952\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.6549\t Accuracy 0.5056\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.6364\t Accuracy 0.5169\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.6160\t Accuracy 0.5292\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.6040\t Accuracy 0.5352\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.5868\t Accuracy 0.5440\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.5719\t Accuracy 0.5526\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.5568\t Accuracy 0.5600\n",
      "\n",
      "Epoch [1]\t Average training loss 1.5402\t Average training accuracy 0.5687\n",
      "Epoch [1]\t Average validation loss 1.3241\t Average validation accuracy 0.6900\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.3005\t Accuracy 0.7200\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.3451\t Accuracy 0.6671\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.3286\t Accuracy 0.6743\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.3214\t Accuracy 0.6753\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.3152\t Accuracy 0.6768\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.3040\t Accuracy 0.6810\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.2917\t Accuracy 0.6857\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.2874\t Accuracy 0.6874\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.2773\t Accuracy 0.6913\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.2692\t Accuracy 0.6953\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.2608\t Accuracy 0.6984\n",
      "\n",
      "Epoch [2]\t Average training loss 1.2505\t Average training accuracy 0.7029\n",
      "Epoch [2]\t Average validation loss 1.0930\t Average validation accuracy 0.7832\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.0757\t Accuracy 0.8100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.1249\t Accuracy 0.7565\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.1166\t Accuracy 0.7558\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.1156\t Accuracy 0.7538\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.1142\t Accuracy 0.7536\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.1069\t Accuracy 0.7557\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.0996\t Accuracy 0.7579\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.0996\t Accuracy 0.7575\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.0937\t Accuracy 0.7593\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.0895\t Accuracy 0.7615\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.0849\t Accuracy 0.7628\n",
      "\n",
      "Epoch [3]\t Average training loss 1.0784\t Average training accuracy 0.7654\n",
      "Epoch [3]\t Average validation loss 0.9559\t Average validation accuracy 0.8238\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.9438\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.9934\t Accuracy 0.7986\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.9901\t Accuracy 0.7960\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.9929\t Accuracy 0.7921\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.9940\t Accuracy 0.7916\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.9890\t Accuracy 0.7919\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.9844\t Accuracy 0.7937\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.9868\t Accuracy 0.7932\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.9834\t Accuracy 0.7944\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.9814\t Accuracy 0.7960\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.9790\t Accuracy 0.7964\n",
      "\n",
      "Epoch [4]\t Average training loss 0.9747\t Average training accuracy 0.7980\n",
      "Epoch [4]\t Average validation loss 0.8734\t Average validation accuracy 0.8436\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8680\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.9136\t Accuracy 0.8208\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.9134\t Accuracy 0.8193\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.9186\t Accuracy 0.8132\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.9210\t Accuracy 0.8133\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.9172\t Accuracy 0.8144\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.9143\t Accuracy 0.8149\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.9181\t Accuracy 0.8142\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.9160\t Accuracy 0.8154\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.9153\t Accuracy 0.8162\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.9143\t Accuracy 0.8163\n",
      "\n",
      "Epoch [5]\t Average training loss 0.9113\t Average training accuracy 0.8173\n",
      "Epoch [5]\t Average validation loss 0.8226\t Average validation accuracy 0.8580\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8238\t Accuracy 0.8600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8647\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8664\t Accuracy 0.8294\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8728\t Accuracy 0.8246\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8759\t Accuracy 0.8253\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8728\t Accuracy 0.8260\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8709\t Accuracy 0.8265\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8755\t Accuracy 0.8258\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8742\t Accuracy 0.8272\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8743\t Accuracy 0.8278\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8741\t Accuracy 0.8279\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8720\t Average training accuracy 0.8283\n",
      "Epoch [6]\t Average validation loss 0.7912\t Average validation accuracy 0.8686\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.7988\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8342\t Accuracy 0.8390\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8378\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8442\t Accuracy 0.8326\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8333\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8450\t Accuracy 0.8341\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8437\t Accuracy 0.8344\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8488\t Accuracy 0.8338\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8480\t Accuracy 0.8349\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8486\t Accuracy 0.8354\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8489\t Accuracy 0.8352\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8474\t Average training accuracy 0.8354\n",
      "Epoch [7]\t Average validation loss 0.7718\t Average validation accuracy 0.8760\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.7849\t Accuracy 0.8600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8153\t Accuracy 0.8455\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8189\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8266\t Accuracy 0.8395\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8303\t Accuracy 0.8400\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8279\t Accuracy 0.8408\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8269\t Accuracy 0.8419\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8324\t Accuracy 0.8408\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8319\t Accuracy 0.8413\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8329\t Accuracy 0.8414\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8335\t Accuracy 0.8411\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8323\t Average training accuracy 0.8413\n",
      "Epoch [8]\t Average validation loss 0.7602\t Average validation accuracy 0.8804\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.7781\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8039\t Accuracy 0.8498\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8081\t Accuracy 0.8491\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8161\t Accuracy 0.8432\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8200\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8178\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8170\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8227\t Accuracy 0.8444\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8225\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8236\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8245\t Accuracy 0.8449\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8236\t Average training accuracy 0.8452\n",
      "Epoch [9]\t Average validation loss 0.7538\t Average validation accuracy 0.8844\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7757\t Accuracy 0.8700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.7976\t Accuracy 0.8520\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8023\t Accuracy 0.8517\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8105\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8145\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8124\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8119\t Accuracy 0.8493\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8176\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8176\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8189\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8199\t Accuracy 0.8481\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8192\t Average training accuracy 0.8484\n",
      "Epoch [10]\t Average validation loss 0.7514\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.7770\t Accuracy 0.8700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.7951\t Accuracy 0.8584\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8001\t Accuracy 0.8560\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8083\t Accuracy 0.8505\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8125\t Accuracy 0.8509\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8105\t Accuracy 0.8517\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8101\t Accuracy 0.8523\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8159\t Accuracy 0.8507\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8160\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8175\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8186\t Accuracy 0.8507\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8180\t Average training accuracy 0.8508\n",
      "Epoch [11]\t Average validation loss 0.7518\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.7806\t Accuracy 0.8700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.7953\t Accuracy 0.8592\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8005\t Accuracy 0.8570\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8088\t Accuracy 0.8514\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8130\t Accuracy 0.8524\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8111\t Accuracy 0.8531\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8108\t Accuracy 0.8538\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8168\t Accuracy 0.8522\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8169\t Accuracy 0.8528\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8185\t Accuracy 0.8528\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8196\t Accuracy 0.8523\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8193\t Average training accuracy 0.8523\n",
      "Epoch [12]\t Average validation loss 0.7544\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7854\t Accuracy 0.8600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.7975\t Accuracy 0.8600\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8028\t Accuracy 0.8576\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8112\t Accuracy 0.8521\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8155\t Accuracy 0.8529\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8137\t Accuracy 0.8538\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8135\t Accuracy 0.8544\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8194\t Accuracy 0.8530\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8197\t Accuracy 0.8536\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8213\t Accuracy 0.8535\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8225\t Accuracy 0.8531\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8222\t Average training accuracy 0.8531\n",
      "Epoch [13]\t Average validation loss 0.7585\t Average validation accuracy 0.8886\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7914\t Accuracy 0.8600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8012\t Accuracy 0.8616\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8066\t Accuracy 0.8589\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8149\t Accuracy 0.8540\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8193\t Accuracy 0.8544\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8175\t Accuracy 0.8549\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8174\t Accuracy 0.8554\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8234\t Accuracy 0.8541\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8236\t Accuracy 0.8544\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8252\t Accuracy 0.8544\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8265\t Accuracy 0.8540\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8263\t Average training accuracy 0.8539\n",
      "Epoch [14]\t Average validation loss 0.7634\t Average validation accuracy 0.8894\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7979\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8058\t Accuracy 0.8610\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8111\t Accuracy 0.8586\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8194\t Accuracy 0.8536\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8239\t Accuracy 0.8541\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8221\t Accuracy 0.8548\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8220\t Accuracy 0.8552\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8280\t Accuracy 0.8540\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8282\t Accuracy 0.8542\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8299\t Accuracy 0.8542\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8311\t Accuracy 0.8538\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8310\t Average training accuracy 0.8536\n",
      "Epoch [15]\t Average validation loss 0.7688\t Average validation accuracy 0.8894\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8049\t Accuracy 0.8400\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8108\t Accuracy 0.8614\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8162\t Accuracy 0.8594\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8244\t Accuracy 0.8542\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8289\t Accuracy 0.8546\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8270\t Accuracy 0.8551\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8270\t Accuracy 0.8555\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8329\t Accuracy 0.8544\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8332\t Accuracy 0.8546\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8348\t Accuracy 0.8545\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8361\t Accuracy 0.8540\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8359\t Average training accuracy 0.8537\n",
      "Epoch [16]\t Average validation loss 0.7744\t Average validation accuracy 0.8882\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8117\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8160\t Accuracy 0.8606\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8213\t Accuracy 0.8592\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8294\t Accuracy 0.8537\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8340\t Accuracy 0.8540\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8320\t Accuracy 0.8548\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8320\t Accuracy 0.8548\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8380\t Accuracy 0.8538\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8382\t Accuracy 0.8540\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8399\t Accuracy 0.8540\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8411\t Accuracy 0.8536\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8409\t Average training accuracy 0.8533\n",
      "Epoch [17]\t Average validation loss 0.7799\t Average validation accuracy 0.8866\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8184\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8211\t Accuracy 0.8610\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8264\t Accuracy 0.8590\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8344\t Accuracy 0.8532\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8389\t Accuracy 0.8534\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8370\t Accuracy 0.8542\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8370\t Accuracy 0.8543\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8429\t Accuracy 0.8532\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8431\t Accuracy 0.8534\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8448\t Accuracy 0.8533\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8459\t Accuracy 0.8530\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8459\t Average training accuracy 0.8526\n",
      "Epoch [18]\t Average validation loss 0.7852\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8249\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8262\t Accuracy 0.8594\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8313\t Accuracy 0.8584\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8392\t Accuracy 0.8529\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8438\t Accuracy 0.8530\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8418\t Accuracy 0.8538\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8419\t Accuracy 0.8540\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8477\t Accuracy 0.8529\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8480\t Accuracy 0.8531\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8496\t Accuracy 0.8530\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8507\t Accuracy 0.8526\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8507\t Average training accuracy 0.8522\n",
      "Epoch [19]\t Average validation loss 0.7904\t Average validation accuracy 0.8856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2hidden, model_2hidden_loss, model_2hidden_acc = train(model_2hidden, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8607.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model_2hidden, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nohidden = Network()\n",
    "model_nohidden.add(FCLayer(784, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单层模型 即softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5431\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4397\t Accuracy 0.1251\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4105\t Accuracy 0.1229\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3741\t Accuracy 0.1302\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3414\t Accuracy 0.1429\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.3136\t Accuracy 0.1529\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2861\t Accuracy 0.1632\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.2603\t Accuracy 0.1758\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.2363\t Accuracy 0.1879\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.2115\t Accuracy 0.2010\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.1882\t Accuracy 0.2148\n",
      "\n",
      "Epoch [0]\t Average training loss 2.1645\t Average training accuracy 0.2296\n",
      "Epoch [0]\t Average validation loss 1.8869\t Average validation accuracy 0.4138\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.9197\t Accuracy 0.4100\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.8828\t Accuracy 0.4139\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.8679\t Accuracy 0.4238\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.8515\t Accuracy 0.4384\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.8377\t Accuracy 0.4522\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.8229\t Accuracy 0.4645\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.8075\t Accuracy 0.4763\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.7962\t Accuracy 0.4838\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.7824\t Accuracy 0.4921\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.7691\t Accuracy 0.5014\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.7556\t Accuracy 0.5105\n",
      "\n",
      "Epoch [1]\t Average training loss 1.7410\t Average training accuracy 0.5202\n",
      "Epoch [1]\t Average validation loss 1.5470\t Average validation accuracy 0.6554\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.5721\t Accuracy 0.6600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.5636\t Accuracy 0.6325\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.5550\t Accuracy 0.6352\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.5476\t Accuracy 0.6354\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.5410\t Accuracy 0.6397\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.5316\t Accuracy 0.6431\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.5213\t Accuracy 0.6483\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.5168\t Accuracy 0.6502\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.5079\t Accuracy 0.6535\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.5003\t Accuracy 0.6573\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.4916\t Accuracy 0.6605\n",
      "\n",
      "Epoch [2]\t Average training loss 1.4818\t Average training accuracy 0.6657\n",
      "Epoch [2]\t Average validation loss 1.3286\t Average validation accuracy 0.7528\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.3532\t Accuracy 0.7500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.3584\t Accuracy 0.7155\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.3540\t Accuracy 0.7170\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.3518\t Accuracy 0.7156\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.3491\t Accuracy 0.7161\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.3429\t Accuracy 0.7181\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.3355\t Accuracy 0.7207\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.3349\t Accuracy 0.7212\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.3288\t Accuracy 0.7232\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.3244\t Accuracy 0.7253\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.3187\t Accuracy 0.7268\n",
      "\n",
      "Epoch [3]\t Average training loss 1.3117\t Average training accuracy 0.7301\n",
      "Epoch [3]\t Average validation loss 1.1818\t Average validation accuracy 0.8018\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.2084\t Accuracy 0.7900\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.2204\t Accuracy 0.7557\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.2187\t Accuracy 0.7572\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.2197\t Accuracy 0.7552\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.2191\t Accuracy 0.7560\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.2148\t Accuracy 0.7569\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.2093\t Accuracy 0.7575\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.2109\t Accuracy 0.7581\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.2065\t Accuracy 0.7599\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.2041\t Accuracy 0.7615\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.2002\t Accuracy 0.7619\n",
      "\n",
      "Epoch [4]\t Average training loss 1.1949\t Average training accuracy 0.7642\n",
      "Epoch [4]\t Average validation loss 1.0789\t Average validation accuracy 0.8264\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.1079\t Accuracy 0.8100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.1233\t Accuracy 0.7816\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.1234\t Accuracy 0.7813\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.1265\t Accuracy 0.7787\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.1272\t Accuracy 0.7788\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.1239\t Accuracy 0.7799\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.1196\t Accuracy 0.7805\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.1227\t Accuracy 0.7810\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.1194\t Accuracy 0.7821\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.1182\t Accuracy 0.7832\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.1155\t Accuracy 0.7836\n",
      "\n",
      "Epoch [5]\t Average training loss 1.1113\t Average training accuracy 0.7852\n",
      "Epoch [5]\t Average validation loss 1.0039\t Average validation accuracy 0.8422\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.0350\t Accuracy 0.8200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.0523\t Accuracy 0.7994\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.0537\t Accuracy 0.7993\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.0582\t Accuracy 0.7965\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.0596\t Accuracy 0.7967\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.0571\t Accuracy 0.7974\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.0536\t Accuracy 0.7976\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.0576\t Accuracy 0.7975\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.0551\t Accuracy 0.7983\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.0546\t Accuracy 0.7990\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.0527\t Accuracy 0.7989\n",
      "\n",
      "Epoch [6]\t Average training loss 1.0493\t Average training accuracy 0.8000\n",
      "Epoch [6]\t Average validation loss 0.9475\t Average validation accuracy 0.8570\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.9803\t Accuracy 0.8300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.9987\t Accuracy 0.8098\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.0010\t Accuracy 0.8105\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.0065\t Accuracy 0.8076\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.0085\t Accuracy 0.8077\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.0065\t Accuracy 0.8084\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.0035\t Accuracy 0.8086\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.0082\t Accuracy 0.8082\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.0061\t Accuracy 0.8088\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.0062\t Accuracy 0.8091\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.0049\t Accuracy 0.8088\n",
      "\n",
      "Epoch [7]\t Average training loss 1.0020\t Average training accuracy 0.8097\n",
      "Epoch [7]\t Average validation loss 0.9041\t Average validation accuracy 0.8624\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.9382\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.9573\t Accuracy 0.8206\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.9602\t Accuracy 0.8200\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.9664\t Accuracy 0.8170\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.9688\t Accuracy 0.8167\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.9671\t Accuracy 0.8171\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.9645\t Accuracy 0.8174\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.9696\t Accuracy 0.8164\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.9680\t Accuracy 0.8169\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.9685\t Accuracy 0.8171\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.9676\t Accuracy 0.8168\n",
      "\n",
      "Epoch [8]\t Average training loss 0.9651\t Average training accuracy 0.8173\n",
      "Epoch [8]\t Average validation loss 0.8700\t Average validation accuracy 0.8658\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.9050\t Accuracy 0.8300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.9245\t Accuracy 0.8296\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.9279\t Accuracy 0.8276\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.9348\t Accuracy 0.8237\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.9373\t Accuracy 0.8231\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.9359\t Accuracy 0.8235\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.9337\t Accuracy 0.8234\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.9391\t Accuracy 0.8227\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.9377\t Accuracy 0.8230\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.9385\t Accuracy 0.8231\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.9379\t Accuracy 0.8229\n",
      "\n",
      "Epoch [9]\t Average training loss 0.9357\t Average training accuracy 0.8233\n",
      "Epoch [9]\t Average validation loss 0.8427\t Average validation accuracy 0.8732\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8784\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8982\t Accuracy 0.8351\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.9020\t Accuracy 0.8327\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.9093\t Accuracy 0.8282\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.9120\t Accuracy 0.8279\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.9108\t Accuracy 0.8280\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.9088\t Accuracy 0.8279\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.9144\t Accuracy 0.8274\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.9133\t Accuracy 0.8278\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.9142\t Accuracy 0.8279\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.9139\t Accuracy 0.8274\n",
      "\n",
      "Epoch [10]\t Average training loss 0.9120\t Average training accuracy 0.8275\n",
      "Epoch [10]\t Average validation loss 0.8206\t Average validation accuracy 0.8750\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8567\t Accuracy 0.8600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8767\t Accuracy 0.8398\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8808\t Accuracy 0.8369\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8885\t Accuracy 0.8323\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8914\t Accuracy 0.8318\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8902\t Accuracy 0.8319\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8885\t Accuracy 0.8317\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8943\t Accuracy 0.8315\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8933\t Accuracy 0.8318\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8944\t Accuracy 0.8318\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8943\t Accuracy 0.8314\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8926\t Average training accuracy 0.8313\n",
      "Epoch [11]\t Average validation loss 0.8024\t Average validation accuracy 0.8780\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8390\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8591\t Accuracy 0.8431\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8635\t Accuracy 0.8405\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8714\t Accuracy 0.8364\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8744\t Accuracy 0.8358\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8733\t Accuracy 0.8358\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8718\t Accuracy 0.8360\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8777\t Accuracy 0.8356\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8768\t Accuracy 0.8358\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8781\t Accuracy 0.8357\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8781\t Accuracy 0.8351\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8765\t Average training accuracy 0.8350\n",
      "Epoch [12]\t Average validation loss 0.7874\t Average validation accuracy 0.8800\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8242\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8444\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8490\t Accuracy 0.8442\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8572\t Accuracy 0.8394\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8602\t Accuracy 0.8389\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8593\t Accuracy 0.8388\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8578\t Accuracy 0.8389\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8638\t Accuracy 0.8384\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8631\t Accuracy 0.8385\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8645\t Accuracy 0.8382\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8647\t Accuracy 0.8380\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8632\t Average training accuracy 0.8380\n",
      "Epoch [13]\t Average validation loss 0.7750\t Average validation accuracy 0.8832\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8119\t Accuracy 0.8900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8321\t Accuracy 0.8488\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8369\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8453\t Accuracy 0.8422\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8484\t Accuracy 0.8419\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8475\t Accuracy 0.8414\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8461\t Accuracy 0.8417\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8522\t Accuracy 0.8410\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8516\t Accuracy 0.8409\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8530\t Accuracy 0.8406\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8533\t Accuracy 0.8403\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8519\t Average training accuracy 0.8403\n",
      "Epoch [14]\t Average validation loss 0.7645\t Average validation accuracy 0.8846\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8015\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8217\t Accuracy 0.8516\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8267\t Accuracy 0.8493\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8352\t Accuracy 0.8443\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8384\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8375\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8363\t Accuracy 0.8440\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8424\t Accuracy 0.8432\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8419\t Accuracy 0.8432\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8434\t Accuracy 0.8429\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8438\t Accuracy 0.8426\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8425\t Average training accuracy 0.8425\n",
      "Epoch [15]\t Average validation loss 0.7556\t Average validation accuracy 0.8844\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7926\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8129\t Accuracy 0.8537\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8180\t Accuracy 0.8506\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8267\t Accuracy 0.8456\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8299\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8290\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8279\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8341\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8336\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8351\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8356\t Accuracy 0.8442\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8344\t Average training accuracy 0.8441\n",
      "Epoch [16]\t Average validation loss 0.7481\t Average validation accuracy 0.8866\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7851\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8054\t Accuracy 0.8547\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8106\t Accuracy 0.8517\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8195\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8226\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8218\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8208\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8270\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8266\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8281\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8287\t Accuracy 0.8455\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8275\t Average training accuracy 0.8453\n",
      "Epoch [17]\t Average validation loss 0.7418\t Average validation accuracy 0.8882\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7786\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.7990\t Accuracy 0.8569\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8043\t Accuracy 0.8534\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8132\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8164\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8157\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8147\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8209\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8205\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8221\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8228\t Accuracy 0.8464\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8216\t Average training accuracy 0.8463\n",
      "Epoch [18]\t Average validation loss 0.7363\t Average validation accuracy 0.8890\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7730\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.7935\t Accuracy 0.8569\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.7989\t Accuracy 0.8537\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8079\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8111\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8103\t Accuracy 0.8490\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8094\t Accuracy 0.8491\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8157\t Accuracy 0.8484\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8154\t Accuracy 0.8486\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8170\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8176\t Accuracy 0.8475\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8166\t Average training accuracy 0.8473\n",
      "Epoch [19]\t Average validation loss 0.7316\t Average validation accuracy 0.8898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_nohidden, model_nohidden_loss, model_nohidden_acc = train(model_nohidden, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8629.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model_nohidden, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'2 hidden layers': [model_2hidden_loss, model_2hidden_acc],\n",
    "                   '1 hidden layer': [relu_loss, relu_acc],\n",
    "                   'no hidden layer': [model_nohidden_loss, model_nohidden_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][55000]\t Training Loss 2.5450\t Accuracy 0.0000\n",
      "Epoch [0][20]\t Batch [50][55000]\t Training Loss 2.5936\t Accuracy 0.0392\n",
      "Epoch [0][20]\t Batch [100][55000]\t Training Loss 2.5607\t Accuracy 0.0990\n",
      "Epoch [0][20]\t Batch [150][55000]\t Training Loss 2.4948\t Accuracy 0.1126\n",
      "Epoch [0][20]\t Batch [200][55000]\t Training Loss 2.4117\t Accuracy 0.1443\n",
      "Epoch [0][20]\t Batch [250][55000]\t Training Loss 2.3514\t Accuracy 0.1554\n",
      "Epoch [0][20]\t Batch [300][55000]\t Training Loss 2.3195\t Accuracy 0.1794\n",
      "Epoch [0][20]\t Batch [350][55000]\t Training Loss 2.2729\t Accuracy 0.1966\n",
      "Epoch [0][20]\t Batch [400][55000]\t Training Loss 2.2276\t Accuracy 0.2244\n",
      "Epoch [0][20]\t Batch [450][55000]\t Training Loss 2.1829\t Accuracy 0.2528\n",
      "Epoch [0][20]\t Batch [500][55000]\t Training Loss 2.1550\t Accuracy 0.2635\n",
      "Epoch [0][20]\t Batch [550][55000]\t Training Loss 2.1372\t Accuracy 0.2686\n",
      "Epoch [0][20]\t Batch [600][55000]\t Training Loss 2.1040\t Accuracy 0.2845\n",
      "Epoch [0][20]\t Batch [650][55000]\t Training Loss 2.0905\t Accuracy 0.2919\n",
      "Epoch [0][20]\t Batch [700][55000]\t Training Loss 2.0563\t Accuracy 0.3138\n",
      "Epoch [0][20]\t Batch [750][55000]\t Training Loss 2.0291\t Accuracy 0.3302\n",
      "Epoch [0][20]\t Batch [800][55000]\t Training Loss 2.0045\t Accuracy 0.3458\n",
      "Epoch [0][20]\t Batch [850][55000]\t Training Loss 1.9763\t Accuracy 0.3666\n",
      "Epoch [0][20]\t Batch [900][55000]\t Training Loss 1.9547\t Accuracy 0.3829\n",
      "Epoch [0][20]\t Batch [950][55000]\t Training Loss 1.9357\t Accuracy 0.3922\n",
      "Epoch [0][20]\t Batch [1000][55000]\t Training Loss 1.9120\t Accuracy 0.4056\n",
      "Epoch [0][20]\t Batch [1050][55000]\t Training Loss 1.8992\t Accuracy 0.4129\n",
      "Epoch [0][20]\t Batch [1100][55000]\t Training Loss 1.8847\t Accuracy 0.4214\n",
      "Epoch [0][20]\t Batch [1150][55000]\t Training Loss 1.8711\t Accuracy 0.4240\n",
      "Epoch [0][20]\t Batch [1200][55000]\t Training Loss 1.8502\t Accuracy 0.4371\n",
      "Epoch [0][20]\t Batch [1250][55000]\t Training Loss 1.8306\t Accuracy 0.4460\n",
      "Epoch [0][20]\t Batch [1300][55000]\t Training Loss 1.8132\t Accuracy 0.4566\n",
      "Epoch [0][20]\t Batch [1350][55000]\t Training Loss 1.7932\t Accuracy 0.4693\n",
      "Epoch [0][20]\t Batch [1400][55000]\t Training Loss 1.7727\t Accuracy 0.4789\n",
      "Epoch [0][20]\t Batch [1450][55000]\t Training Loss 1.7605\t Accuracy 0.4866\n",
      "Epoch [0][20]\t Batch [1500][55000]\t Training Loss 1.7449\t Accuracy 0.4963\n",
      "Epoch [0][20]\t Batch [1550][55000]\t Training Loss 1.7309\t Accuracy 0.5035\n",
      "Epoch [0][20]\t Batch [1600][55000]\t Training Loss 1.7158\t Accuracy 0.5116\n",
      "Epoch [0][20]\t Batch [1650][55000]\t Training Loss 1.6973\t Accuracy 0.5215\n",
      "Epoch [0][20]\t Batch [1700][55000]\t Training Loss 1.6802\t Accuracy 0.5309\n",
      "Epoch [0][20]\t Batch [1750][55000]\t Training Loss 1.6616\t Accuracy 0.5397\n",
      "Epoch [0][20]\t Batch [1800][55000]\t Training Loss 1.6488\t Accuracy 0.5447\n",
      "Epoch [0][20]\t Batch [1850][55000]\t Training Loss 1.6330\t Accuracy 0.5527\n",
      "Epoch [0][20]\t Batch [1900][55000]\t Training Loss 1.6181\t Accuracy 0.5618\n",
      "Epoch [0][20]\t Batch [1950][55000]\t Training Loss 1.6041\t Accuracy 0.5674\n",
      "Epoch [0][20]\t Batch [2000][55000]\t Training Loss 1.5913\t Accuracy 0.5717\n",
      "Epoch [0][20]\t Batch [2050][55000]\t Training Loss 1.5804\t Accuracy 0.5753\n",
      "Epoch [0][20]\t Batch [2100][55000]\t Training Loss 1.5672\t Accuracy 0.5812\n",
      "Epoch [0][20]\t Batch [2150][55000]\t Training Loss 1.5536\t Accuracy 0.5867\n",
      "Epoch [0][20]\t Batch [2200][55000]\t Training Loss 1.5401\t Accuracy 0.5906\n",
      "Epoch [0][20]\t Batch [2250][55000]\t Training Loss 1.5285\t Accuracy 0.5948\n",
      "Epoch [0][20]\t Batch [2300][55000]\t Training Loss 1.5156\t Accuracy 0.6010\n",
      "Epoch [0][20]\t Batch [2350][55000]\t Training Loss 1.5037\t Accuracy 0.6070\n",
      "Epoch [0][20]\t Batch [2400][55000]\t Training Loss 1.4945\t Accuracy 0.6106\n",
      "Epoch [0][20]\t Batch [2450][55000]\t Training Loss 1.4877\t Accuracy 0.6136\n",
      "Epoch [0][20]\t Batch [2500][55000]\t Training Loss 1.4764\t Accuracy 0.6186\n",
      "Epoch [0][20]\t Batch [2550][55000]\t Training Loss 1.4652\t Accuracy 0.6233\n",
      "Epoch [0][20]\t Batch [2600][55000]\t Training Loss 1.4551\t Accuracy 0.6259\n",
      "Epoch [0][20]\t Batch [2650][55000]\t Training Loss 1.4448\t Accuracy 0.6296\n",
      "Epoch [0][20]\t Batch [2700][55000]\t Training Loss 1.4367\t Accuracy 0.6324\n",
      "Epoch [0][20]\t Batch [2750][55000]\t Training Loss 1.4296\t Accuracy 0.6340\n",
      "Epoch [0][20]\t Batch [2800][55000]\t Training Loss 1.4227\t Accuracy 0.6362\n",
      "Epoch [0][20]\t Batch [2850][55000]\t Training Loss 1.4134\t Accuracy 0.6398\n",
      "Epoch [0][20]\t Batch [2900][55000]\t Training Loss 1.4021\t Accuracy 0.6439\n",
      "Epoch [0][20]\t Batch [2950][55000]\t Training Loss 1.3950\t Accuracy 0.6462\n",
      "Epoch [0][20]\t Batch [3000][55000]\t Training Loss 1.3873\t Accuracy 0.6491\n",
      "Epoch [0][20]\t Batch [3050][55000]\t Training Loss 1.3808\t Accuracy 0.6519\n",
      "Epoch [0][20]\t Batch [3100][55000]\t Training Loss 1.3762\t Accuracy 0.6530\n",
      "Epoch [0][20]\t Batch [3150][55000]\t Training Loss 1.3687\t Accuracy 0.6557\n",
      "Epoch [0][20]\t Batch [3200][55000]\t Training Loss 1.3619\t Accuracy 0.6595\n",
      "Epoch [0][20]\t Batch [3250][55000]\t Training Loss 1.3546\t Accuracy 0.6623\n",
      "Epoch [0][20]\t Batch [3300][55000]\t Training Loss 1.3491\t Accuracy 0.6634\n",
      "Epoch [0][20]\t Batch [3350][55000]\t Training Loss 1.3414\t Accuracy 0.6664\n",
      "Epoch [0][20]\t Batch [3400][55000]\t Training Loss 1.3372\t Accuracy 0.6680\n",
      "Epoch [0][20]\t Batch [3450][55000]\t Training Loss 1.3311\t Accuracy 0.6708\n",
      "Epoch [0][20]\t Batch [3500][55000]\t Training Loss 1.3253\t Accuracy 0.6741\n",
      "Epoch [0][20]\t Batch [3550][55000]\t Training Loss 1.3217\t Accuracy 0.6756\n",
      "Epoch [0][20]\t Batch [3600][55000]\t Training Loss 1.3157\t Accuracy 0.6776\n",
      "Epoch [0][20]\t Batch [3650][55000]\t Training Loss 1.3080\t Accuracy 0.6806\n",
      "Epoch [0][20]\t Batch [3700][55000]\t Training Loss 1.3032\t Accuracy 0.6817\n",
      "Epoch [0][20]\t Batch [3750][55000]\t Training Loss 1.2974\t Accuracy 0.6841\n",
      "Epoch [0][20]\t Batch [3800][55000]\t Training Loss 1.2921\t Accuracy 0.6864\n",
      "Epoch [0][20]\t Batch [3850][55000]\t Training Loss 1.2867\t Accuracy 0.6884\n",
      "Epoch [0][20]\t Batch [3900][55000]\t Training Loss 1.2808\t Accuracy 0.6908\n",
      "Epoch [0][20]\t Batch [3950][55000]\t Training Loss 1.2755\t Accuracy 0.6932\n",
      "Epoch [0][20]\t Batch [4000][55000]\t Training Loss 1.2706\t Accuracy 0.6943\n",
      "Epoch [0][20]\t Batch [4050][55000]\t Training Loss 1.2644\t Accuracy 0.6969\n",
      "Epoch [0][20]\t Batch [4100][55000]\t Training Loss 1.2592\t Accuracy 0.6989\n",
      "Epoch [0][20]\t Batch [4150][55000]\t Training Loss 1.2549\t Accuracy 0.7003\n",
      "Epoch [0][20]\t Batch [4200][55000]\t Training Loss 1.2503\t Accuracy 0.7015\n",
      "Epoch [0][20]\t Batch [4250][55000]\t Training Loss 1.2447\t Accuracy 0.7036\n",
      "Epoch [0][20]\t Batch [4300][55000]\t Training Loss 1.2398\t Accuracy 0.7050\n",
      "Epoch [0][20]\t Batch [4350][55000]\t Training Loss 1.2363\t Accuracy 0.7063\n",
      "Epoch [0][20]\t Batch [4400][55000]\t Training Loss 1.2320\t Accuracy 0.7080\n",
      "Epoch [0][20]\t Batch [4450][55000]\t Training Loss 1.2280\t Accuracy 0.7097\n",
      "Epoch [0][20]\t Batch [4500][55000]\t Training Loss 1.2241\t Accuracy 0.7105\n",
      "Epoch [0][20]\t Batch [4550][55000]\t Training Loss 1.2183\t Accuracy 0.7128\n",
      "Epoch [0][20]\t Batch [4600][55000]\t Training Loss 1.2125\t Accuracy 0.7144\n",
      "Epoch [0][20]\t Batch [4650][55000]\t Training Loss 1.2081\t Accuracy 0.7158\n",
      "Epoch [0][20]\t Batch [4700][55000]\t Training Loss 1.2037\t Accuracy 0.7164\n",
      "Epoch [0][20]\t Batch [4750][55000]\t Training Loss 1.1989\t Accuracy 0.7186\n",
      "Epoch [0][20]\t Batch [4800][55000]\t Training Loss 1.1966\t Accuracy 0.7188\n",
      "Epoch [0][20]\t Batch [4850][55000]\t Training Loss 1.1945\t Accuracy 0.7194\n",
      "Epoch [0][20]\t Batch [4900][55000]\t Training Loss 1.1894\t Accuracy 0.7211\n",
      "Epoch [0][20]\t Batch [4950][55000]\t Training Loss 1.1862\t Accuracy 0.7223\n",
      "Epoch [0][20]\t Batch [5000][55000]\t Training Loss 1.1826\t Accuracy 0.7239\n",
      "Epoch [0][20]\t Batch [5050][55000]\t Training Loss 1.1788\t Accuracy 0.7250\n",
      "Epoch [0][20]\t Batch [5100][55000]\t Training Loss 1.1753\t Accuracy 0.7261\n",
      "Epoch [0][20]\t Batch [5150][55000]\t Training Loss 1.1722\t Accuracy 0.7268\n",
      "Epoch [0][20]\t Batch [5200][55000]\t Training Loss 1.1703\t Accuracy 0.7274\n",
      "Epoch [0][20]\t Batch [5250][55000]\t Training Loss 1.1660\t Accuracy 0.7290\n",
      "Epoch [0][20]\t Batch [5300][55000]\t Training Loss 1.1622\t Accuracy 0.7304\n",
      "Epoch [0][20]\t Batch [5350][55000]\t Training Loss 1.1595\t Accuracy 0.7309\n",
      "Epoch [0][20]\t Batch [5400][55000]\t Training Loss 1.1557\t Accuracy 0.7317\n",
      "Epoch [0][20]\t Batch [5450][55000]\t Training Loss 1.1520\t Accuracy 0.7331\n",
      "Epoch [0][20]\t Batch [5500][55000]\t Training Loss 1.1470\t Accuracy 0.7346\n",
      "Epoch [0][20]\t Batch [5550][55000]\t Training Loss 1.1440\t Accuracy 0.7354\n",
      "Epoch [0][20]\t Batch [5600][55000]\t Training Loss 1.1402\t Accuracy 0.7367\n",
      "Epoch [0][20]\t Batch [5650][55000]\t Training Loss 1.1379\t Accuracy 0.7372\n",
      "Epoch [0][20]\t Batch [5700][55000]\t Training Loss 1.1347\t Accuracy 0.7379\n",
      "Epoch [0][20]\t Batch [5750][55000]\t Training Loss 1.1326\t Accuracy 0.7381\n",
      "Epoch [0][20]\t Batch [5800][55000]\t Training Loss 1.1304\t Accuracy 0.7390\n",
      "Epoch [0][20]\t Batch [5850][55000]\t Training Loss 1.1279\t Accuracy 0.7400\n",
      "Epoch [0][20]\t Batch [5900][55000]\t Training Loss 1.1251\t Accuracy 0.7411\n",
      "Epoch [0][20]\t Batch [5950][55000]\t Training Loss 1.1223\t Accuracy 0.7424\n",
      "Epoch [0][20]\t Batch [6000][55000]\t Training Loss 1.1189\t Accuracy 0.7437\n",
      "Epoch [0][20]\t Batch [6050][55000]\t Training Loss 1.1147\t Accuracy 0.7452\n",
      "Epoch [0][20]\t Batch [6100][55000]\t Training Loss 1.1109\t Accuracy 0.7463\n",
      "Epoch [0][20]\t Batch [6150][55000]\t Training Loss 1.1065\t Accuracy 0.7475\n",
      "Epoch [0][20]\t Batch [6200][55000]\t Training Loss 1.1042\t Accuracy 0.7476\n",
      "Epoch [0][20]\t Batch [6250][55000]\t Training Loss 1.1007\t Accuracy 0.7490\n",
      "Epoch [0][20]\t Batch [6300][55000]\t Training Loss 1.0988\t Accuracy 0.7489\n",
      "Epoch [0][20]\t Batch [6350][55000]\t Training Loss 1.0957\t Accuracy 0.7503\n",
      "Epoch [0][20]\t Batch [6400][55000]\t Training Loss 1.0924\t Accuracy 0.7516\n",
      "Epoch [0][20]\t Batch [6450][55000]\t Training Loss 1.0895\t Accuracy 0.7523\n",
      "Epoch [0][20]\t Batch [6500][55000]\t Training Loss 1.0881\t Accuracy 0.7531\n",
      "Epoch [0][20]\t Batch [6550][55000]\t Training Loss 1.0847\t Accuracy 0.7542\n",
      "Epoch [0][20]\t Batch [6600][55000]\t Training Loss 1.0811\t Accuracy 0.7555\n",
      "Epoch [0][20]\t Batch [6650][55000]\t Training Loss 1.0779\t Accuracy 0.7566\n",
      "Epoch [0][20]\t Batch [6700][55000]\t Training Loss 1.0758\t Accuracy 0.7576\n",
      "Epoch [0][20]\t Batch [6750][55000]\t Training Loss 1.0738\t Accuracy 0.7589\n",
      "Epoch [0][20]\t Batch [6800][55000]\t Training Loss 1.0721\t Accuracy 0.7592\n",
      "Epoch [0][20]\t Batch [6850][55000]\t Training Loss 1.0723\t Accuracy 0.7593\n",
      "Epoch [0][20]\t Batch [6900][55000]\t Training Loss 1.0700\t Accuracy 0.7602\n",
      "Epoch [0][20]\t Batch [6950][55000]\t Training Loss 1.0687\t Accuracy 0.7599\n",
      "Epoch [0][20]\t Batch [7000][55000]\t Training Loss 1.0667\t Accuracy 0.7603\n",
      "Epoch [0][20]\t Batch [7050][55000]\t Training Loss 1.0653\t Accuracy 0.7605\n",
      "Epoch [0][20]\t Batch [7100][55000]\t Training Loss 1.0635\t Accuracy 0.7614\n",
      "Epoch [0][20]\t Batch [7150][55000]\t Training Loss 1.0617\t Accuracy 0.7623\n",
      "Epoch [0][20]\t Batch [7200][55000]\t Training Loss 1.0606\t Accuracy 0.7629\n",
      "Epoch [0][20]\t Batch [7250][55000]\t Training Loss 1.0611\t Accuracy 0.7627\n",
      "Epoch [0][20]\t Batch [7300][55000]\t Training Loss 1.0612\t Accuracy 0.7630\n",
      "Epoch [0][20]\t Batch [7350][55000]\t Training Loss 1.0611\t Accuracy 0.7633\n",
      "Epoch [0][20]\t Batch [7400][55000]\t Training Loss 1.0604\t Accuracy 0.7641\n",
      "Epoch [0][20]\t Batch [7450][55000]\t Training Loss 1.0587\t Accuracy 0.7649\n",
      "Epoch [0][20]\t Batch [7500][55000]\t Training Loss 1.0574\t Accuracy 0.7652\n",
      "Epoch [0][20]\t Batch [7550][55000]\t Training Loss 1.0562\t Accuracy 0.7656\n",
      "Epoch [0][20]\t Batch [7600][55000]\t Training Loss 1.0542\t Accuracy 0.7666\n",
      "Epoch [0][20]\t Batch [7650][55000]\t Training Loss 1.0533\t Accuracy 0.7671\n",
      "Epoch [0][20]\t Batch [7700][55000]\t Training Loss 1.0523\t Accuracy 0.7676\n",
      "Epoch [0][20]\t Batch [7750][55000]\t Training Loss 1.0508\t Accuracy 0.7679\n",
      "Epoch [0][20]\t Batch [7800][55000]\t Training Loss 1.0502\t Accuracy 0.7679\n",
      "Epoch [0][20]\t Batch [7850][55000]\t Training Loss 1.0487\t Accuracy 0.7684\n",
      "Epoch [0][20]\t Batch [7900][55000]\t Training Loss 1.0473\t Accuracy 0.7690\n",
      "Epoch [0][20]\t Batch [7950][55000]\t Training Loss 1.0457\t Accuracy 0.7695\n",
      "Epoch [0][20]\t Batch [8000][55000]\t Training Loss 1.0445\t Accuracy 0.7695\n",
      "Epoch [0][20]\t Batch [8050][55000]\t Training Loss 1.0435\t Accuracy 0.7701\n",
      "Epoch [0][20]\t Batch [8100][55000]\t Training Loss 1.0405\t Accuracy 0.7713\n",
      "Epoch [0][20]\t Batch [8150][55000]\t Training Loss 1.0401\t Accuracy 0.7709\n",
      "Epoch [0][20]\t Batch [8200][55000]\t Training Loss 1.0386\t Accuracy 0.7711\n",
      "Epoch [0][20]\t Batch [8250][55000]\t Training Loss 1.0387\t Accuracy 0.7712\n",
      "Epoch [0][20]\t Batch [8300][55000]\t Training Loss 1.0377\t Accuracy 0.7717\n",
      "Epoch [0][20]\t Batch [8350][55000]\t Training Loss 1.0368\t Accuracy 0.7720\n",
      "Epoch [0][20]\t Batch [8400][55000]\t Training Loss 1.0348\t Accuracy 0.7730\n",
      "Epoch [0][20]\t Batch [8450][55000]\t Training Loss 1.0348\t Accuracy 0.7730\n",
      "Epoch [0][20]\t Batch [8500][55000]\t Training Loss 1.0328\t Accuracy 0.7739\n",
      "Epoch [0][20]\t Batch [8550][55000]\t Training Loss 1.0304\t Accuracy 0.7750\n",
      "Epoch [0][20]\t Batch [8600][55000]\t Training Loss 1.0283\t Accuracy 0.7755\n",
      "Epoch [0][20]\t Batch [8650][55000]\t Training Loss 1.0271\t Accuracy 0.7756\n",
      "Epoch [0][20]\t Batch [8700][55000]\t Training Loss 1.0266\t Accuracy 0.7755\n",
      "Epoch [0][20]\t Batch [8750][55000]\t Training Loss 1.0269\t Accuracy 0.7753\n",
      "Epoch [0][20]\t Batch [8800][55000]\t Training Loss 1.0264\t Accuracy 0.7755\n",
      "Epoch [0][20]\t Batch [8850][55000]\t Training Loss 1.0251\t Accuracy 0.7762\n",
      "Epoch [0][20]\t Batch [8900][55000]\t Training Loss 1.0257\t Accuracy 0.7756\n",
      "Epoch [0][20]\t Batch [8950][55000]\t Training Loss 1.0242\t Accuracy 0.7759\n",
      "Epoch [0][20]\t Batch [9000][55000]\t Training Loss 1.0224\t Accuracy 0.7762\n",
      "Epoch [0][20]\t Batch [9050][55000]\t Training Loss 1.0204\t Accuracy 0.7770\n",
      "Epoch [0][20]\t Batch [9100][55000]\t Training Loss 1.0191\t Accuracy 0.7775\n",
      "Epoch [0][20]\t Batch [9150][55000]\t Training Loss 1.0184\t Accuracy 0.7779\n",
      "Epoch [0][20]\t Batch [9200][55000]\t Training Loss 1.0167\t Accuracy 0.7788\n",
      "Epoch [0][20]\t Batch [9250][55000]\t Training Loss 1.0160\t Accuracy 0.7793\n",
      "Epoch [0][20]\t Batch [9300][55000]\t Training Loss 1.0151\t Accuracy 0.7794\n",
      "Epoch [0][20]\t Batch [9350][55000]\t Training Loss 1.0140\t Accuracy 0.7796\n",
      "Epoch [0][20]\t Batch [9400][55000]\t Training Loss 1.0130\t Accuracy 0.7798\n",
      "Epoch [0][20]\t Batch [9450][55000]\t Training Loss 1.0122\t Accuracy 0.7802\n",
      "Epoch [0][20]\t Batch [9500][55000]\t Training Loss 1.0104\t Accuracy 0.7811\n",
      "Epoch [0][20]\t Batch [9550][55000]\t Training Loss 1.0092\t Accuracy 0.7814\n",
      "Epoch [0][20]\t Batch [9600][55000]\t Training Loss 1.0086\t Accuracy 0.7816\n",
      "Epoch [0][20]\t Batch [9650][55000]\t Training Loss 1.0074\t Accuracy 0.7820\n",
      "Epoch [0][20]\t Batch [9700][55000]\t Training Loss 1.0059\t Accuracy 0.7826\n",
      "Epoch [0][20]\t Batch [9750][55000]\t Training Loss 1.0040\t Accuracy 0.7832\n",
      "Epoch [0][20]\t Batch [9800][55000]\t Training Loss 1.0038\t Accuracy 0.7830\n",
      "Epoch [0][20]\t Batch [9850][55000]\t Training Loss 1.0021\t Accuracy 0.7838\n",
      "Epoch [0][20]\t Batch [9900][55000]\t Training Loss 1.0006\t Accuracy 0.7842\n",
      "Epoch [0][20]\t Batch [9950][55000]\t Training Loss 0.9989\t Accuracy 0.7848\n",
      "Epoch [0][20]\t Batch [10000][55000]\t Training Loss 0.9977\t Accuracy 0.7855\n",
      "Epoch [0][20]\t Batch [10050][55000]\t Training Loss 0.9967\t Accuracy 0.7856\n",
      "Epoch [0][20]\t Batch [10100][55000]\t Training Loss 0.9956\t Accuracy 0.7860\n",
      "Epoch [0][20]\t Batch [10150][55000]\t Training Loss 0.9945\t Accuracy 0.7865\n",
      "Epoch [0][20]\t Batch [10200][55000]\t Training Loss 0.9937\t Accuracy 0.7870\n",
      "Epoch [0][20]\t Batch [10250][55000]\t Training Loss 0.9928\t Accuracy 0.7869\n",
      "Epoch [0][20]\t Batch [10300][55000]\t Training Loss 0.9918\t Accuracy 0.7869\n",
      "Epoch [0][20]\t Batch [10350][55000]\t Training Loss 0.9899\t Accuracy 0.7877\n",
      "Epoch [0][20]\t Batch [10400][55000]\t Training Loss 0.9883\t Accuracy 0.7885\n",
      "Epoch [0][20]\t Batch [10450][55000]\t Training Loss 0.9873\t Accuracy 0.7887\n",
      "Epoch [0][20]\t Batch [10500][55000]\t Training Loss 0.9855\t Accuracy 0.7894\n",
      "Epoch [0][20]\t Batch [10550][55000]\t Training Loss 0.9841\t Accuracy 0.7901\n",
      "Epoch [0][20]\t Batch [10600][55000]\t Training Loss 0.9828\t Accuracy 0.7907\n",
      "Epoch [0][20]\t Batch [10650][55000]\t Training Loss 0.9817\t Accuracy 0.7912\n",
      "Epoch [0][20]\t Batch [10700][55000]\t Training Loss 0.9807\t Accuracy 0.7918\n",
      "Epoch [0][20]\t Batch [10750][55000]\t Training Loss 0.9803\t Accuracy 0.7918\n",
      "Epoch [0][20]\t Batch [10800][55000]\t Training Loss 0.9799\t Accuracy 0.7920\n",
      "Epoch [0][20]\t Batch [10850][55000]\t Training Loss 0.9783\t Accuracy 0.7926\n",
      "Epoch [0][20]\t Batch [10900][55000]\t Training Loss 0.9771\t Accuracy 0.7930\n",
      "Epoch [0][20]\t Batch [10950][55000]\t Training Loss 0.9760\t Accuracy 0.7931\n",
      "Epoch [0][20]\t Batch [11000][55000]\t Training Loss 0.9750\t Accuracy 0.7936\n",
      "Epoch [0][20]\t Batch [11050][55000]\t Training Loss 0.9735\t Accuracy 0.7940\n",
      "Epoch [0][20]\t Batch [11100][55000]\t Training Loss 0.9722\t Accuracy 0.7945\n",
      "Epoch [0][20]\t Batch [11150][55000]\t Training Loss 0.9715\t Accuracy 0.7951\n",
      "Epoch [0][20]\t Batch [11200][55000]\t Training Loss 0.9703\t Accuracy 0.7956\n",
      "Epoch [0][20]\t Batch [11250][55000]\t Training Loss 0.9700\t Accuracy 0.7957\n",
      "Epoch [0][20]\t Batch [11300][55000]\t Training Loss 0.9689\t Accuracy 0.7960\n",
      "Epoch [0][20]\t Batch [11350][55000]\t Training Loss 0.9676\t Accuracy 0.7963\n",
      "Epoch [0][20]\t Batch [11400][55000]\t Training Loss 0.9670\t Accuracy 0.7965\n",
      "Epoch [0][20]\t Batch [11450][55000]\t Training Loss 0.9659\t Accuracy 0.7969\n",
      "Epoch [0][20]\t Batch [11500][55000]\t Training Loss 0.9651\t Accuracy 0.7971\n",
      "Epoch [0][20]\t Batch [11550][55000]\t Training Loss 0.9644\t Accuracy 0.7973\n",
      "Epoch [0][20]\t Batch [11600][55000]\t Training Loss 0.9648\t Accuracy 0.7971\n",
      "Epoch [0][20]\t Batch [11650][55000]\t Training Loss 0.9646\t Accuracy 0.7972\n",
      "Epoch [0][20]\t Batch [11700][55000]\t Training Loss 0.9638\t Accuracy 0.7977\n",
      "Epoch [0][20]\t Batch [11750][55000]\t Training Loss 0.9642\t Accuracy 0.7971\n",
      "Epoch [0][20]\t Batch [11800][55000]\t Training Loss 0.9637\t Accuracy 0.7970\n",
      "Epoch [0][20]\t Batch [11850][55000]\t Training Loss 0.9630\t Accuracy 0.7973\n",
      "Epoch [0][20]\t Batch [11900][55000]\t Training Loss 0.9626\t Accuracy 0.7974\n",
      "Epoch [0][20]\t Batch [11950][55000]\t Training Loss 0.9621\t Accuracy 0.7978\n",
      "Epoch [0][20]\t Batch [12000][55000]\t Training Loss 0.9614\t Accuracy 0.7983\n",
      "Epoch [0][20]\t Batch [12050][55000]\t Training Loss 0.9605\t Accuracy 0.7987\n",
      "Epoch [0][20]\t Batch [12100][55000]\t Training Loss 0.9599\t Accuracy 0.7988\n",
      "Epoch [0][20]\t Batch [12150][55000]\t Training Loss 0.9587\t Accuracy 0.7994\n",
      "Epoch [0][20]\t Batch [12200][55000]\t Training Loss 0.9583\t Accuracy 0.7996\n",
      "Epoch [0][20]\t Batch [12250][55000]\t Training Loss 0.9575\t Accuracy 0.7998\n",
      "Epoch [0][20]\t Batch [12300][55000]\t Training Loss 0.9571\t Accuracy 0.7999\n",
      "Epoch [0][20]\t Batch [12350][55000]\t Training Loss 0.9568\t Accuracy 0.8001\n",
      "Epoch [0][20]\t Batch [12400][55000]\t Training Loss 0.9567\t Accuracy 0.8002\n",
      "Epoch [0][20]\t Batch [12450][55000]\t Training Loss 0.9562\t Accuracy 0.8003\n",
      "Epoch [0][20]\t Batch [12500][55000]\t Training Loss 0.9559\t Accuracy 0.8003\n",
      "Epoch [0][20]\t Batch [12550][55000]\t Training Loss 0.9553\t Accuracy 0.8004\n",
      "Epoch [0][20]\t Batch [12600][55000]\t Training Loss 0.9554\t Accuracy 0.8004\n",
      "Epoch [0][20]\t Batch [12650][55000]\t Training Loss 0.9553\t Accuracy 0.8001\n",
      "Epoch [0][20]\t Batch [12700][55000]\t Training Loss 0.9556\t Accuracy 0.8000\n",
      "Epoch [0][20]\t Batch [12750][55000]\t Training Loss 0.9546\t Accuracy 0.8006\n",
      "Epoch [0][20]\t Batch [12800][55000]\t Training Loss 0.9548\t Accuracy 0.8005\n",
      "Epoch [0][20]\t Batch [12850][55000]\t Training Loss 0.9545\t Accuracy 0.8006\n",
      "Epoch [0][20]\t Batch [12900][55000]\t Training Loss 0.9539\t Accuracy 0.8009\n",
      "Epoch [0][20]\t Batch [12950][55000]\t Training Loss 0.9539\t Accuracy 0.8008\n",
      "Epoch [0][20]\t Batch [13000][55000]\t Training Loss 0.9535\t Accuracy 0.8007\n",
      "Epoch [0][20]\t Batch [13050][55000]\t Training Loss 0.9537\t Accuracy 0.8004\n",
      "Epoch [0][20]\t Batch [13100][55000]\t Training Loss 0.9540\t Accuracy 0.8000\n",
      "Epoch [0][20]\t Batch [13150][55000]\t Training Loss 0.9539\t Accuracy 0.8000\n",
      "Epoch [0][20]\t Batch [13200][55000]\t Training Loss 0.9536\t Accuracy 0.8001\n",
      "Epoch [0][20]\t Batch [13250][55000]\t Training Loss 0.9526\t Accuracy 0.8005\n",
      "Epoch [0][20]\t Batch [13300][55000]\t Training Loss 0.9521\t Accuracy 0.8008\n",
      "Epoch [0][20]\t Batch [13350][55000]\t Training Loss 0.9520\t Accuracy 0.8010\n",
      "Epoch [0][20]\t Batch [13400][55000]\t Training Loss 0.9517\t Accuracy 0.8011\n",
      "Epoch [0][20]\t Batch [13450][55000]\t Training Loss 0.9507\t Accuracy 0.8016\n",
      "Epoch [0][20]\t Batch [13500][55000]\t Training Loss 0.9499\t Accuracy 0.8020\n",
      "Epoch [0][20]\t Batch [13550][55000]\t Training Loss 0.9490\t Accuracy 0.8022\n",
      "Epoch [0][20]\t Batch [13600][55000]\t Training Loss 0.9476\t Accuracy 0.8027\n",
      "Epoch [0][20]\t Batch [13650][55000]\t Training Loss 0.9470\t Accuracy 0.8030\n",
      "Epoch [0][20]\t Batch [13700][55000]\t Training Loss 0.9474\t Accuracy 0.8029\n",
      "Epoch [0][20]\t Batch [13750][55000]\t Training Loss 0.9476\t Accuracy 0.8028\n",
      "Epoch [0][20]\t Batch [13800][55000]\t Training Loss 0.9473\t Accuracy 0.8030\n",
      "Epoch [0][20]\t Batch [13850][55000]\t Training Loss 0.9468\t Accuracy 0.8032\n",
      "Epoch [0][20]\t Batch [13900][55000]\t Training Loss 0.9467\t Accuracy 0.8033\n",
      "Epoch [0][20]\t Batch [13950][55000]\t Training Loss 0.9466\t Accuracy 0.8030\n",
      "Epoch [0][20]\t Batch [14000][55000]\t Training Loss 0.9471\t Accuracy 0.8027\n",
      "Epoch [0][20]\t Batch [14050][55000]\t Training Loss 0.9468\t Accuracy 0.8028\n",
      "Epoch [0][20]\t Batch [14100][55000]\t Training Loss 0.9465\t Accuracy 0.8029\n",
      "Epoch [0][20]\t Batch [14150][55000]\t Training Loss 0.9464\t Accuracy 0.8031\n",
      "Epoch [0][20]\t Batch [14200][55000]\t Training Loss 0.9459\t Accuracy 0.8030\n",
      "Epoch [0][20]\t Batch [14250][55000]\t Training Loss 0.9457\t Accuracy 0.8031\n",
      "Epoch [0][20]\t Batch [14300][55000]\t Training Loss 0.9457\t Accuracy 0.8030\n",
      "Epoch [0][20]\t Batch [14350][55000]\t Training Loss 0.9458\t Accuracy 0.8029\n",
      "Epoch [0][20]\t Batch [14400][55000]\t Training Loss 0.9461\t Accuracy 0.8027\n",
      "Epoch [0][20]\t Batch [14450][55000]\t Training Loss 0.9460\t Accuracy 0.8028\n",
      "Epoch [0][20]\t Batch [14500][55000]\t Training Loss 0.9457\t Accuracy 0.8030\n",
      "Epoch [0][20]\t Batch [14550][55000]\t Training Loss 0.9461\t Accuracy 0.8030\n",
      "Epoch [0][20]\t Batch [14600][55000]\t Training Loss 0.9457\t Accuracy 0.8033\n",
      "Epoch [0][20]\t Batch [14650][55000]\t Training Loss 0.9464\t Accuracy 0.8029\n",
      "Epoch [0][20]\t Batch [14700][55000]\t Training Loss 0.9470\t Accuracy 0.8028\n",
      "Epoch [0][20]\t Batch [14750][55000]\t Training Loss 0.9473\t Accuracy 0.8027\n",
      "Epoch [0][20]\t Batch [14800][55000]\t Training Loss 0.9479\t Accuracy 0.8026\n",
      "Epoch [0][20]\t Batch [14850][55000]\t Training Loss 0.9482\t Accuracy 0.8025\n",
      "Epoch [0][20]\t Batch [14900][55000]\t Training Loss 0.9477\t Accuracy 0.8026\n",
      "Epoch [0][20]\t Batch [14950][55000]\t Training Loss 0.9474\t Accuracy 0.8027\n",
      "Epoch [0][20]\t Batch [15000][55000]\t Training Loss 0.9469\t Accuracy 0.8027\n",
      "Epoch [0][20]\t Batch [15050][55000]\t Training Loss 0.9462\t Accuracy 0.8031\n",
      "Epoch [0][20]\t Batch [15100][55000]\t Training Loss 0.9456\t Accuracy 0.8035\n",
      "Epoch [0][20]\t Batch [15150][55000]\t Training Loss 0.9456\t Accuracy 0.8036\n",
      "Epoch [0][20]\t Batch [15200][55000]\t Training Loss 0.9455\t Accuracy 0.8038\n",
      "Epoch [0][20]\t Batch [15250][55000]\t Training Loss 0.9451\t Accuracy 0.8041\n",
      "Epoch [0][20]\t Batch [15300][55000]\t Training Loss 0.9448\t Accuracy 0.8042\n",
      "Epoch [0][20]\t Batch [15350][55000]\t Training Loss 0.9441\t Accuracy 0.8048\n",
      "Epoch [0][20]\t Batch [15400][55000]\t Training Loss 0.9440\t Accuracy 0.8050\n",
      "Epoch [0][20]\t Batch [15450][55000]\t Training Loss 0.9438\t Accuracy 0.8052\n",
      "Epoch [0][20]\t Batch [15500][55000]\t Training Loss 0.9433\t Accuracy 0.8054\n",
      "Epoch [0][20]\t Batch [15550][55000]\t Training Loss 0.9429\t Accuracy 0.8056\n",
      "Epoch [0][20]\t Batch [15600][55000]\t Training Loss 0.9425\t Accuracy 0.8058\n",
      "Epoch [0][20]\t Batch [15650][55000]\t Training Loss 0.9420\t Accuracy 0.8061\n",
      "Epoch [0][20]\t Batch [15700][55000]\t Training Loss 0.9416\t Accuracy 0.8064\n",
      "Epoch [0][20]\t Batch [15750][55000]\t Training Loss 0.9422\t Accuracy 0.8062\n",
      "Epoch [0][20]\t Batch [15800][55000]\t Training Loss 0.9423\t Accuracy 0.8063\n",
      "Epoch [0][20]\t Batch [15850][55000]\t Training Loss 0.9423\t Accuracy 0.8062\n",
      "Epoch [0][20]\t Batch [15900][55000]\t Training Loss 0.9426\t Accuracy 0.8060\n",
      "Epoch [0][20]\t Batch [15950][55000]\t Training Loss 0.9424\t Accuracy 0.8061\n",
      "Epoch [0][20]\t Batch [16000][55000]\t Training Loss 0.9422\t Accuracy 0.8061\n",
      "Epoch [0][20]\t Batch [16050][55000]\t Training Loss 0.9428\t Accuracy 0.8058\n",
      "Epoch [0][20]\t Batch [16100][55000]\t Training Loss 0.9424\t Accuracy 0.8060\n",
      "Epoch [0][20]\t Batch [16150][55000]\t Training Loss 0.9420\t Accuracy 0.8063\n",
      "Epoch [0][20]\t Batch [16200][55000]\t Training Loss 0.9418\t Accuracy 0.8062\n",
      "Epoch [0][20]\t Batch [16250][55000]\t Training Loss 0.9413\t Accuracy 0.8062\n",
      "Epoch [0][20]\t Batch [16300][55000]\t Training Loss 0.9407\t Accuracy 0.8065\n",
      "Epoch [0][20]\t Batch [16350][55000]\t Training Loss 0.9400\t Accuracy 0.8067\n",
      "Epoch [0][20]\t Batch [16400][55000]\t Training Loss 0.9398\t Accuracy 0.8070\n",
      "Epoch [0][20]\t Batch [16450][55000]\t Training Loss 0.9393\t Accuracy 0.8073\n",
      "Epoch [0][20]\t Batch [16500][55000]\t Training Loss 0.9389\t Accuracy 0.8076\n",
      "Epoch [0][20]\t Batch [16550][55000]\t Training Loss 0.9382\t Accuracy 0.8079\n",
      "Epoch [0][20]\t Batch [16600][55000]\t Training Loss 0.9380\t Accuracy 0.8079\n",
      "Epoch [0][20]\t Batch [16650][55000]\t Training Loss 0.9377\t Accuracy 0.8081\n",
      "Epoch [0][20]\t Batch [16700][55000]\t Training Loss 0.9376\t Accuracy 0.8081\n",
      "Epoch [0][20]\t Batch [16750][55000]\t Training Loss 0.9372\t Accuracy 0.8084\n",
      "Epoch [0][20]\t Batch [16800][55000]\t Training Loss 0.9376\t Accuracy 0.8083\n",
      "Epoch [0][20]\t Batch [16850][55000]\t Training Loss 0.9378\t Accuracy 0.8084\n",
      "Epoch [0][20]\t Batch [16900][55000]\t Training Loss 0.9377\t Accuracy 0.8085\n",
      "Epoch [0][20]\t Batch [16950][55000]\t Training Loss 0.9375\t Accuracy 0.8085\n",
      "Epoch [0][20]\t Batch [17000][55000]\t Training Loss 0.9380\t Accuracy 0.8083\n",
      "Epoch [0][20]\t Batch [17050][55000]\t Training Loss 0.9376\t Accuracy 0.8084\n",
      "Epoch [0][20]\t Batch [17100][55000]\t Training Loss 0.9378\t Accuracy 0.8083\n",
      "Epoch [0][20]\t Batch [17150][55000]\t Training Loss 0.9374\t Accuracy 0.8085\n",
      "Epoch [0][20]\t Batch [17200][55000]\t Training Loss 0.9372\t Accuracy 0.8086\n",
      "Epoch [0][20]\t Batch [17250][55000]\t Training Loss 0.9375\t Accuracy 0.8085\n",
      "Epoch [0][20]\t Batch [17300][55000]\t Training Loss 0.9371\t Accuracy 0.8086\n",
      "Epoch [0][20]\t Batch [17350][55000]\t Training Loss 0.9362\t Accuracy 0.8090\n",
      "Epoch [0][20]\t Batch [17400][55000]\t Training Loss 0.9361\t Accuracy 0.8091\n",
      "Epoch [0][20]\t Batch [17450][55000]\t Training Loss 0.9359\t Accuracy 0.8092\n",
      "Epoch [0][20]\t Batch [17500][55000]\t Training Loss 0.9358\t Accuracy 0.8093\n",
      "Epoch [0][20]\t Batch [17550][55000]\t Training Loss 0.9363\t Accuracy 0.8091\n",
      "Epoch [0][20]\t Batch [17600][55000]\t Training Loss 0.9366\t Accuracy 0.8089\n",
      "Epoch [0][20]\t Batch [17650][55000]\t Training Loss 0.9366\t Accuracy 0.8092\n",
      "Epoch [0][20]\t Batch [17700][55000]\t Training Loss 0.9371\t Accuracy 0.8092\n",
      "Epoch [0][20]\t Batch [17750][55000]\t Training Loss 0.9372\t Accuracy 0.8093\n",
      "Epoch [0][20]\t Batch [17800][55000]\t Training Loss 0.9373\t Accuracy 0.8093\n",
      "Epoch [0][20]\t Batch [17850][55000]\t Training Loss 0.9374\t Accuracy 0.8093\n",
      "Epoch [0][20]\t Batch [17900][55000]\t Training Loss 0.9375\t Accuracy 0.8092\n",
      "Epoch [0][20]\t Batch [17950][55000]\t Training Loss 0.9370\t Accuracy 0.8093\n",
      "Epoch [0][20]\t Batch [18000][55000]\t Training Loss 0.9365\t Accuracy 0.8096\n",
      "Epoch [0][20]\t Batch [18050][55000]\t Training Loss 0.9365\t Accuracy 0.8097\n",
      "Epoch [0][20]\t Batch [18100][55000]\t Training Loss 0.9361\t Accuracy 0.8100\n",
      "Epoch [0][20]\t Batch [18150][55000]\t Training Loss 0.9355\t Accuracy 0.8101\n",
      "Epoch [0][20]\t Batch [18200][55000]\t Training Loss 0.9347\t Accuracy 0.8104\n",
      "Epoch [0][20]\t Batch [18250][55000]\t Training Loss 0.9345\t Accuracy 0.8105\n",
      "Epoch [0][20]\t Batch [18300][55000]\t Training Loss 0.9339\t Accuracy 0.8109\n",
      "Epoch [0][20]\t Batch [18350][55000]\t Training Loss 0.9335\t Accuracy 0.8112\n",
      "Epoch [0][20]\t Batch [18400][55000]\t Training Loss 0.9333\t Accuracy 0.8113\n",
      "Epoch [0][20]\t Batch [18450][55000]\t Training Loss 0.9336\t Accuracy 0.8112\n",
      "Epoch [0][20]\t Batch [18500][55000]\t Training Loss 0.9334\t Accuracy 0.8113\n",
      "Epoch [0][20]\t Batch [18550][55000]\t Training Loss 0.9329\t Accuracy 0.8117\n",
      "Epoch [0][20]\t Batch [18600][55000]\t Training Loss 0.9327\t Accuracy 0.8120\n",
      "Epoch [0][20]\t Batch [18650][55000]\t Training Loss 0.9323\t Accuracy 0.8123\n",
      "Epoch [0][20]\t Batch [18700][55000]\t Training Loss 0.9323\t Accuracy 0.8123\n",
      "Epoch [0][20]\t Batch [18750][55000]\t Training Loss 0.9322\t Accuracy 0.8124\n",
      "Epoch [0][20]\t Batch [18800][55000]\t Training Loss 0.9315\t Accuracy 0.8129\n",
      "Epoch [0][20]\t Batch [18850][55000]\t Training Loss 0.9316\t Accuracy 0.8130\n",
      "Epoch [0][20]\t Batch [18900][55000]\t Training Loss 0.9308\t Accuracy 0.8134\n",
      "Epoch [0][20]\t Batch [18950][55000]\t Training Loss 0.9304\t Accuracy 0.8137\n",
      "Epoch [0][20]\t Batch [19000][55000]\t Training Loss 0.9301\t Accuracy 0.8137\n",
      "Epoch [0][20]\t Batch [19050][55000]\t Training Loss 0.9302\t Accuracy 0.8138\n",
      "Epoch [0][20]\t Batch [19100][55000]\t Training Loss 0.9303\t Accuracy 0.8138\n",
      "Epoch [0][20]\t Batch [19150][55000]\t Training Loss 0.9302\t Accuracy 0.8138\n",
      "Epoch [0][20]\t Batch [19200][55000]\t Training Loss 0.9301\t Accuracy 0.8140\n",
      "Epoch [0][20]\t Batch [19250][55000]\t Training Loss 0.9298\t Accuracy 0.8140\n",
      "Epoch [0][20]\t Batch [19300][55000]\t Training Loss 0.9295\t Accuracy 0.8142\n",
      "Epoch [0][20]\t Batch [19350][55000]\t Training Loss 0.9294\t Accuracy 0.8141\n",
      "Epoch [0][20]\t Batch [19400][55000]\t Training Loss 0.9289\t Accuracy 0.8142\n",
      "Epoch [0][20]\t Batch [19450][55000]\t Training Loss 0.9285\t Accuracy 0.8142\n",
      "Epoch [0][20]\t Batch [19500][55000]\t Training Loss 0.9279\t Accuracy 0.8145\n",
      "Epoch [0][20]\t Batch [19550][55000]\t Training Loss 0.9277\t Accuracy 0.8145\n",
      "Epoch [0][20]\t Batch [19600][55000]\t Training Loss 0.9275\t Accuracy 0.8146\n",
      "Epoch [0][20]\t Batch [19650][55000]\t Training Loss 0.9269\t Accuracy 0.8149\n",
      "Epoch [0][20]\t Batch [19700][55000]\t Training Loss 0.9261\t Accuracy 0.8152\n",
      "Epoch [0][20]\t Batch [19750][55000]\t Training Loss 0.9253\t Accuracy 0.8157\n",
      "Epoch [0][20]\t Batch [19800][55000]\t Training Loss 0.9245\t Accuracy 0.8160\n",
      "Epoch [0][20]\t Batch [19850][55000]\t Training Loss 0.9244\t Accuracy 0.8159\n",
      "Epoch [0][20]\t Batch [19900][55000]\t Training Loss 0.9239\t Accuracy 0.8160\n",
      "Epoch [0][20]\t Batch [19950][55000]\t Training Loss 0.9240\t Accuracy 0.8159\n",
      "Epoch [0][20]\t Batch [20000][55000]\t Training Loss 0.9237\t Accuracy 0.8161\n",
      "Epoch [0][20]\t Batch [20050][55000]\t Training Loss 0.9240\t Accuracy 0.8158\n",
      "Epoch [0][20]\t Batch [20100][55000]\t Training Loss 0.9240\t Accuracy 0.8159\n",
      "Epoch [0][20]\t Batch [20150][55000]\t Training Loss 0.9236\t Accuracy 0.8161\n",
      "Epoch [0][20]\t Batch [20200][55000]\t Training Loss 0.9238\t Accuracy 0.8161\n",
      "Epoch [0][20]\t Batch [20250][55000]\t Training Loss 0.9237\t Accuracy 0.8162\n",
      "Epoch [0][20]\t Batch [20300][55000]\t Training Loss 0.9236\t Accuracy 0.8164\n",
      "Epoch [0][20]\t Batch [20350][55000]\t Training Loss 0.9235\t Accuracy 0.8166\n",
      "Epoch [0][20]\t Batch [20400][55000]\t Training Loss 0.9230\t Accuracy 0.8169\n",
      "Epoch [0][20]\t Batch [20450][55000]\t Training Loss 0.9224\t Accuracy 0.8170\n",
      "Epoch [0][20]\t Batch [20500][55000]\t Training Loss 0.9220\t Accuracy 0.8173\n",
      "Epoch [0][20]\t Batch [20550][55000]\t Training Loss 0.9218\t Accuracy 0.8174\n",
      "Epoch [0][20]\t Batch [20600][55000]\t Training Loss 0.9216\t Accuracy 0.8174\n",
      "Epoch [0][20]\t Batch [20650][55000]\t Training Loss 0.9211\t Accuracy 0.8175\n",
      "Epoch [0][20]\t Batch [20700][55000]\t Training Loss 0.9207\t Accuracy 0.8176\n",
      "Epoch [0][20]\t Batch [20750][55000]\t Training Loss 0.9205\t Accuracy 0.8176\n",
      "Epoch [0][20]\t Batch [20800][55000]\t Training Loss 0.9203\t Accuracy 0.8177\n",
      "Epoch [0][20]\t Batch [20850][55000]\t Training Loss 0.9201\t Accuracy 0.8178\n",
      "Epoch [0][20]\t Batch [20900][55000]\t Training Loss 0.9202\t Accuracy 0.8177\n",
      "Epoch [0][20]\t Batch [20950][55000]\t Training Loss 0.9205\t Accuracy 0.8176\n",
      "Epoch [0][20]\t Batch [21000][55000]\t Training Loss 0.9206\t Accuracy 0.8175\n",
      "Epoch [0][20]\t Batch [21050][55000]\t Training Loss 0.9206\t Accuracy 0.8175\n",
      "Epoch [0][20]\t Batch [21100][55000]\t Training Loss 0.9202\t Accuracy 0.8178\n",
      "Epoch [0][20]\t Batch [21150][55000]\t Training Loss 0.9200\t Accuracy 0.8178\n",
      "Epoch [0][20]\t Batch [21200][55000]\t Training Loss 0.9196\t Accuracy 0.8181\n",
      "Epoch [0][20]\t Batch [21250][55000]\t Training Loss 0.9193\t Accuracy 0.8183\n",
      "Epoch [0][20]\t Batch [21300][55000]\t Training Loss 0.9187\t Accuracy 0.8186\n",
      "Epoch [0][20]\t Batch [21350][55000]\t Training Loss 0.9187\t Accuracy 0.8187\n",
      "Epoch [0][20]\t Batch [21400][55000]\t Training Loss 0.9186\t Accuracy 0.8187\n",
      "Epoch [0][20]\t Batch [21450][55000]\t Training Loss 0.9186\t Accuracy 0.8187\n",
      "Epoch [0][20]\t Batch [21500][55000]\t Training Loss 0.9180\t Accuracy 0.8191\n",
      "Epoch [0][20]\t Batch [21550][55000]\t Training Loss 0.9177\t Accuracy 0.8192\n",
      "Epoch [0][20]\t Batch [21600][55000]\t Training Loss 0.9177\t Accuracy 0.8192\n",
      "Epoch [0][20]\t Batch [21650][55000]\t Training Loss 0.9176\t Accuracy 0.8194\n",
      "Epoch [0][20]\t Batch [21700][55000]\t Training Loss 0.9174\t Accuracy 0.8195\n",
      "Epoch [0][20]\t Batch [21750][55000]\t Training Loss 0.9172\t Accuracy 0.8197\n",
      "Epoch [0][20]\t Batch [21800][55000]\t Training Loss 0.9165\t Accuracy 0.8201\n",
      "Epoch [0][20]\t Batch [21850][55000]\t Training Loss 0.9160\t Accuracy 0.8205\n",
      "Epoch [0][20]\t Batch [21900][55000]\t Training Loss 0.9154\t Accuracy 0.8206\n",
      "Epoch [0][20]\t Batch [21950][55000]\t Training Loss 0.9147\t Accuracy 0.8206\n",
      "Epoch [0][20]\t Batch [22000][55000]\t Training Loss 0.9142\t Accuracy 0.8209\n",
      "Epoch [0][20]\t Batch [22050][55000]\t Training Loss 0.9137\t Accuracy 0.8210\n",
      "Epoch [0][20]\t Batch [22100][55000]\t Training Loss 0.9137\t Accuracy 0.8211\n",
      "Epoch [0][20]\t Batch [22150][55000]\t Training Loss 0.9140\t Accuracy 0.8209\n",
      "Epoch [0][20]\t Batch [22200][55000]\t Training Loss 0.9141\t Accuracy 0.8210\n",
      "Epoch [0][20]\t Batch [22250][55000]\t Training Loss 0.9140\t Accuracy 0.8211\n",
      "Epoch [0][20]\t Batch [22300][55000]\t Training Loss 0.9140\t Accuracy 0.8212\n",
      "Epoch [0][20]\t Batch [22350][55000]\t Training Loss 0.9136\t Accuracy 0.8214\n",
      "Epoch [0][20]\t Batch [22400][55000]\t Training Loss 0.9134\t Accuracy 0.8215\n",
      "Epoch [0][20]\t Batch [22450][55000]\t Training Loss 0.9132\t Accuracy 0.8216\n",
      "Epoch [0][20]\t Batch [22500][55000]\t Training Loss 0.9135\t Accuracy 0.8214\n",
      "Epoch [0][20]\t Batch [22550][55000]\t Training Loss 0.9141\t Accuracy 0.8211\n",
      "Epoch [0][20]\t Batch [22600][55000]\t Training Loss 0.9144\t Accuracy 0.8209\n",
      "Epoch [0][20]\t Batch [22650][55000]\t Training Loss 0.9145\t Accuracy 0.8209\n",
      "Epoch [0][20]\t Batch [22700][55000]\t Training Loss 0.9142\t Accuracy 0.8211\n",
      "Epoch [0][20]\t Batch [22750][55000]\t Training Loss 0.9139\t Accuracy 0.8211\n",
      "Epoch [0][20]\t Batch [22800][55000]\t Training Loss 0.9137\t Accuracy 0.8210\n",
      "Epoch [0][20]\t Batch [22850][55000]\t Training Loss 0.9136\t Accuracy 0.8211\n",
      "Epoch [0][20]\t Batch [22900][55000]\t Training Loss 0.9131\t Accuracy 0.8214\n",
      "Epoch [0][20]\t Batch [22950][55000]\t Training Loss 0.9128\t Accuracy 0.8216\n",
      "Epoch [0][20]\t Batch [23000][55000]\t Training Loss 0.9124\t Accuracy 0.8218\n",
      "Epoch [0][20]\t Batch [23050][55000]\t Training Loss 0.9121\t Accuracy 0.8218\n",
      "Epoch [0][20]\t Batch [23100][55000]\t Training Loss 0.9121\t Accuracy 0.8218\n",
      "Epoch [0][20]\t Batch [23150][55000]\t Training Loss 0.9118\t Accuracy 0.8220\n",
      "Epoch [0][20]\t Batch [23200][55000]\t Training Loss 0.9117\t Accuracy 0.8220\n",
      "Epoch [0][20]\t Batch [23250][55000]\t Training Loss 0.9113\t Accuracy 0.8221\n",
      "Epoch [0][20]\t Batch [23300][55000]\t Training Loss 0.9108\t Accuracy 0.8224\n",
      "Epoch [0][20]\t Batch [23350][55000]\t Training Loss 0.9106\t Accuracy 0.8225\n",
      "Epoch [0][20]\t Batch [23400][55000]\t Training Loss 0.9102\t Accuracy 0.8227\n",
      "Epoch [0][20]\t Batch [23450][55000]\t Training Loss 0.9100\t Accuracy 0.8229\n",
      "Epoch [0][20]\t Batch [23500][55000]\t Training Loss 0.9097\t Accuracy 0.8231\n",
      "Epoch [0][20]\t Batch [23550][55000]\t Training Loss 0.9094\t Accuracy 0.8232\n",
      "Epoch [0][20]\t Batch [23600][55000]\t Training Loss 0.9094\t Accuracy 0.8233\n",
      "Epoch [0][20]\t Batch [23650][55000]\t Training Loss 0.9093\t Accuracy 0.8234\n",
      "Epoch [0][20]\t Batch [23700][55000]\t Training Loss 0.9094\t Accuracy 0.8233\n",
      "Epoch [0][20]\t Batch [23750][55000]\t Training Loss 0.9098\t Accuracy 0.8231\n",
      "Epoch [0][20]\t Batch [23800][55000]\t Training Loss 0.9094\t Accuracy 0.8233\n",
      "Epoch [0][20]\t Batch [23850][55000]\t Training Loss 0.9092\t Accuracy 0.8234\n",
      "Epoch [0][20]\t Batch [23900][55000]\t Training Loss 0.9093\t Accuracy 0.8234\n",
      "Epoch [0][20]\t Batch [23950][55000]\t Training Loss 0.9092\t Accuracy 0.8236\n",
      "Epoch [0][20]\t Batch [24000][55000]\t Training Loss 0.9091\t Accuracy 0.8237\n",
      "Epoch [0][20]\t Batch [24050][55000]\t Training Loss 0.9089\t Accuracy 0.8239\n",
      "Epoch [0][20]\t Batch [24100][55000]\t Training Loss 0.9086\t Accuracy 0.8240\n",
      "Epoch [0][20]\t Batch [24150][55000]\t Training Loss 0.9084\t Accuracy 0.8242\n",
      "Epoch [0][20]\t Batch [24200][55000]\t Training Loss 0.9081\t Accuracy 0.8244\n",
      "Epoch [0][20]\t Batch [24250][55000]\t Training Loss 0.9081\t Accuracy 0.8244\n",
      "Epoch [0][20]\t Batch [24300][55000]\t Training Loss 0.9082\t Accuracy 0.8243\n",
      "Epoch [0][20]\t Batch [24350][55000]\t Training Loss 0.9080\t Accuracy 0.8245\n",
      "Epoch [0][20]\t Batch [24400][55000]\t Training Loss 0.9079\t Accuracy 0.8245\n",
      "Epoch [0][20]\t Batch [24450][55000]\t Training Loss 0.9078\t Accuracy 0.8245\n",
      "Epoch [0][20]\t Batch [24500][55000]\t Training Loss 0.9077\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [24550][55000]\t Training Loss 0.9077\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [24600][55000]\t Training Loss 0.9076\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [24650][55000]\t Training Loss 0.9076\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [24700][55000]\t Training Loss 0.9074\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [24750][55000]\t Training Loss 0.9076\t Accuracy 0.8245\n",
      "Epoch [0][20]\t Batch [24800][55000]\t Training Loss 0.9078\t Accuracy 0.8244\n",
      "Epoch [0][20]\t Batch [24850][55000]\t Training Loss 0.9075\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [24900][55000]\t Training Loss 0.9075\t Accuracy 0.8247\n",
      "Epoch [0][20]\t Batch [24950][55000]\t Training Loss 0.9077\t Accuracy 0.8246\n",
      "Epoch [0][20]\t Batch [25000][55000]\t Training Loss 0.9078\t Accuracy 0.8245\n",
      "Epoch [0][20]\t Batch [25050][55000]\t Training Loss 0.9075\t Accuracy 0.8247\n",
      "Epoch [0][20]\t Batch [25100][55000]\t Training Loss 0.9073\t Accuracy 0.8248\n",
      "Epoch [0][20]\t Batch [25150][55000]\t Training Loss 0.9070\t Accuracy 0.8249\n",
      "Epoch [0][20]\t Batch [25200][55000]\t Training Loss 0.9068\t Accuracy 0.8249\n",
      "Epoch [0][20]\t Batch [25250][55000]\t Training Loss 0.9066\t Accuracy 0.8249\n",
      "Epoch [0][20]\t Batch [25300][55000]\t Training Loss 0.9064\t Accuracy 0.8250\n",
      "Epoch [0][20]\t Batch [25350][55000]\t Training Loss 0.9065\t Accuracy 0.8250\n",
      "Epoch [0][20]\t Batch [25400][55000]\t Training Loss 0.9059\t Accuracy 0.8252\n",
      "Epoch [0][20]\t Batch [25450][55000]\t Training Loss 0.9053\t Accuracy 0.8255\n",
      "Epoch [0][20]\t Batch [25500][55000]\t Training Loss 0.9050\t Accuracy 0.8255\n",
      "Epoch [0][20]\t Batch [25550][55000]\t Training Loss 0.9046\t Accuracy 0.8256\n",
      "Epoch [0][20]\t Batch [25600][55000]\t Training Loss 0.9044\t Accuracy 0.8256\n",
      "Epoch [0][20]\t Batch [25650][55000]\t Training Loss 0.9041\t Accuracy 0.8257\n",
      "Epoch [0][20]\t Batch [25700][55000]\t Training Loss 0.9039\t Accuracy 0.8258\n",
      "Epoch [0][20]\t Batch [25750][55000]\t Training Loss 0.9035\t Accuracy 0.8260\n",
      "Epoch [0][20]\t Batch [25800][55000]\t Training Loss 0.9034\t Accuracy 0.8261\n",
      "Epoch [0][20]\t Batch [25850][55000]\t Training Loss 0.9034\t Accuracy 0.8262\n",
      "Epoch [0][20]\t Batch [25900][55000]\t Training Loss 0.9033\t Accuracy 0.8262\n",
      "Epoch [0][20]\t Batch [25950][55000]\t Training Loss 0.9032\t Accuracy 0.8263\n",
      "Epoch [0][20]\t Batch [26000][55000]\t Training Loss 0.9031\t Accuracy 0.8263\n",
      "Epoch [0][20]\t Batch [26050][55000]\t Training Loss 0.9028\t Accuracy 0.8264\n",
      "Epoch [0][20]\t Batch [26100][55000]\t Training Loss 0.9024\t Accuracy 0.8265\n",
      "Epoch [0][20]\t Batch [26150][55000]\t Training Loss 0.9022\t Accuracy 0.8265\n",
      "Epoch [0][20]\t Batch [26200][55000]\t Training Loss 0.9019\t Accuracy 0.8268\n",
      "Epoch [0][20]\t Batch [26250][55000]\t Training Loss 0.9017\t Accuracy 0.8269\n",
      "Epoch [0][20]\t Batch [26300][55000]\t Training Loss 0.9018\t Accuracy 0.8269\n",
      "Epoch [0][20]\t Batch [26350][55000]\t Training Loss 0.9016\t Accuracy 0.8272\n",
      "Epoch [0][20]\t Batch [26400][55000]\t Training Loss 0.9019\t Accuracy 0.8272\n",
      "Epoch [0][20]\t Batch [26450][55000]\t Training Loss 0.9019\t Accuracy 0.8273\n",
      "Epoch [0][20]\t Batch [26500][55000]\t Training Loss 0.9020\t Accuracy 0.8273\n",
      "Epoch [0][20]\t Batch [26550][55000]\t Training Loss 0.9022\t Accuracy 0.8273\n",
      "Epoch [0][20]\t Batch [26600][55000]\t Training Loss 0.9022\t Accuracy 0.8273\n",
      "Epoch [0][20]\t Batch [26650][55000]\t Training Loss 0.9025\t Accuracy 0.8272\n",
      "Epoch [0][20]\t Batch [26700][55000]\t Training Loss 0.9023\t Accuracy 0.8273\n",
      "Epoch [0][20]\t Batch [26750][55000]\t Training Loss 0.9025\t Accuracy 0.8271\n",
      "Epoch [0][20]\t Batch [26800][55000]\t Training Loss 0.9023\t Accuracy 0.8272\n",
      "Epoch [0][20]\t Batch [26850][55000]\t Training Loss 0.9021\t Accuracy 0.8273\n",
      "Epoch [0][20]\t Batch [26900][55000]\t Training Loss 0.9023\t Accuracy 0.8273\n",
      "Epoch [0][20]\t Batch [26950][55000]\t Training Loss 0.9021\t Accuracy 0.8274\n",
      "Epoch [0][20]\t Batch [27000][55000]\t Training Loss 0.9018\t Accuracy 0.8276\n",
      "Epoch [0][20]\t Batch [27050][55000]\t Training Loss 0.9015\t Accuracy 0.8278\n",
      "Epoch [0][20]\t Batch [27100][55000]\t Training Loss 0.9013\t Accuracy 0.8279\n",
      "Epoch [0][20]\t Batch [27150][55000]\t Training Loss 0.9011\t Accuracy 0.8280\n",
      "Epoch [0][20]\t Batch [27200][55000]\t Training Loss 0.9014\t Accuracy 0.8279\n",
      "Epoch [0][20]\t Batch [27250][55000]\t Training Loss 0.9012\t Accuracy 0.8280\n",
      "Epoch [0][20]\t Batch [27300][55000]\t Training Loss 0.9011\t Accuracy 0.8280\n",
      "Epoch [0][20]\t Batch [27350][55000]\t Training Loss 0.9010\t Accuracy 0.8281\n",
      "Epoch [0][20]\t Batch [27400][55000]\t Training Loss 0.9009\t Accuracy 0.8282\n",
      "Epoch [0][20]\t Batch [27450][55000]\t Training Loss 0.9007\t Accuracy 0.8283\n",
      "Epoch [0][20]\t Batch [27500][55000]\t Training Loss 0.9007\t Accuracy 0.8283\n",
      "Epoch [0][20]\t Batch [27550][55000]\t Training Loss 0.9006\t Accuracy 0.8284\n",
      "Epoch [0][20]\t Batch [27600][55000]\t Training Loss 0.9003\t Accuracy 0.8286\n",
      "Epoch [0][20]\t Batch [27650][55000]\t Training Loss 0.9003\t Accuracy 0.8286\n",
      "Epoch [0][20]\t Batch [27700][55000]\t Training Loss 0.9002\t Accuracy 0.8287\n",
      "Epoch [0][20]\t Batch [27750][55000]\t Training Loss 0.9003\t Accuracy 0.8288\n",
      "Epoch [0][20]\t Batch [27800][55000]\t Training Loss 0.9003\t Accuracy 0.8289\n",
      "Epoch [0][20]\t Batch [27850][55000]\t Training Loss 0.9002\t Accuracy 0.8289\n",
      "Epoch [0][20]\t Batch [27900][55000]\t Training Loss 0.9000\t Accuracy 0.8290\n",
      "Epoch [0][20]\t Batch [27950][55000]\t Training Loss 0.8997\t Accuracy 0.8292\n",
      "Epoch [0][20]\t Batch [28000][55000]\t Training Loss 0.8994\t Accuracy 0.8291\n",
      "Epoch [0][20]\t Batch [28050][55000]\t Training Loss 0.8990\t Accuracy 0.8294\n",
      "Epoch [0][20]\t Batch [28100][55000]\t Training Loss 0.8985\t Accuracy 0.8296\n",
      "Epoch [0][20]\t Batch [28150][55000]\t Training Loss 0.8982\t Accuracy 0.8297\n",
      "Epoch [0][20]\t Batch [28200][55000]\t Training Loss 0.8982\t Accuracy 0.8297\n",
      "Epoch [0][20]\t Batch [28250][55000]\t Training Loss 0.8977\t Accuracy 0.8298\n",
      "Epoch [0][20]\t Batch [28300][55000]\t Training Loss 0.8976\t Accuracy 0.8298\n",
      "Epoch [0][20]\t Batch [28350][55000]\t Training Loss 0.8972\t Accuracy 0.8301\n",
      "Epoch [0][20]\t Batch [28400][55000]\t Training Loss 0.8976\t Accuracy 0.8299\n",
      "Epoch [0][20]\t Batch [28450][55000]\t Training Loss 0.8972\t Accuracy 0.8301\n",
      "Epoch [0][20]\t Batch [28500][55000]\t Training Loss 0.8970\t Accuracy 0.8302\n",
      "Epoch [0][20]\t Batch [28550][55000]\t Training Loss 0.8968\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [28600][55000]\t Training Loss 0.8966\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [28650][55000]\t Training Loss 0.8968\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [28700][55000]\t Training Loss 0.8970\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [28750][55000]\t Training Loss 0.8970\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [28800][55000]\t Training Loss 0.8969\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [28850][55000]\t Training Loss 0.8967\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [28900][55000]\t Training Loss 0.8965\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [28950][55000]\t Training Loss 0.8964\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [29000][55000]\t Training Loss 0.8963\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [29050][55000]\t Training Loss 0.8962\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [29100][55000]\t Training Loss 0.8963\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [29150][55000]\t Training Loss 0.8966\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [29200][55000]\t Training Loss 0.8969\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [29250][55000]\t Training Loss 0.8970\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [29300][55000]\t Training Loss 0.8968\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [29350][55000]\t Training Loss 0.8968\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [29400][55000]\t Training Loss 0.8966\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [29450][55000]\t Training Loss 0.8961\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [29500][55000]\t Training Loss 0.8958\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [29550][55000]\t Training Loss 0.8957\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [29600][55000]\t Training Loss 0.8956\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [29650][55000]\t Training Loss 0.8955\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [29700][55000]\t Training Loss 0.8955\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [29750][55000]\t Training Loss 0.8958\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [29800][55000]\t Training Loss 0.8959\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [29850][55000]\t Training Loss 0.8961\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [29900][55000]\t Training Loss 0.8963\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [29950][55000]\t Training Loss 0.8965\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [30000][55000]\t Training Loss 0.8968\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [30050][55000]\t Training Loss 0.8969\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [30100][55000]\t Training Loss 0.8972\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [30150][55000]\t Training Loss 0.8976\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [30200][55000]\t Training Loss 0.8978\t Accuracy 0.8302\n",
      "Epoch [0][20]\t Batch [30250][55000]\t Training Loss 0.8978\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [30300][55000]\t Training Loss 0.8975\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [30350][55000]\t Training Loss 0.8974\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [30400][55000]\t Training Loss 0.8972\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [30450][55000]\t Training Loss 0.8972\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [30500][55000]\t Training Loss 0.8973\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [30550][55000]\t Training Loss 0.8976\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [30600][55000]\t Training Loss 0.8976\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [30650][55000]\t Training Loss 0.8979\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [30700][55000]\t Training Loss 0.8981\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [30750][55000]\t Training Loss 0.8982\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [30800][55000]\t Training Loss 0.8983\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [30850][55000]\t Training Loss 0.8982\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [30900][55000]\t Training Loss 0.8985\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [30950][55000]\t Training Loss 0.8983\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [31000][55000]\t Training Loss 0.8982\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [31050][55000]\t Training Loss 0.8983\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [31100][55000]\t Training Loss 0.8982\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [31150][55000]\t Training Loss 0.8982\t Accuracy 0.8308\n",
      "Epoch [0][20]\t Batch [31200][55000]\t Training Loss 0.8981\t Accuracy 0.8308\n",
      "Epoch [0][20]\t Batch [31250][55000]\t Training Loss 0.8980\t Accuracy 0.8309\n",
      "Epoch [0][20]\t Batch [31300][55000]\t Training Loss 0.8983\t Accuracy 0.8308\n",
      "Epoch [0][20]\t Batch [31350][55000]\t Training Loss 0.8989\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [31400][55000]\t Training Loss 0.8990\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [31450][55000]\t Training Loss 0.8992\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [31500][55000]\t Training Loss 0.8992\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [31550][55000]\t Training Loss 0.8992\t Accuracy 0.8308\n",
      "Epoch [0][20]\t Batch [31600][55000]\t Training Loss 0.8993\t Accuracy 0.8308\n",
      "Epoch [0][20]\t Batch [31650][55000]\t Training Loss 0.8994\t Accuracy 0.8309\n",
      "Epoch [0][20]\t Batch [31700][55000]\t Training Loss 0.8996\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [31750][55000]\t Training Loss 0.9000\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [31800][55000]\t Training Loss 0.9000\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [31850][55000]\t Training Loss 0.8999\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [31900][55000]\t Training Loss 0.8998\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [31950][55000]\t Training Loss 0.8997\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [32000][55000]\t Training Loss 0.8997\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [32050][55000]\t Training Loss 0.8996\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [32100][55000]\t Training Loss 0.8994\t Accuracy 0.8308\n",
      "Epoch [0][20]\t Batch [32150][55000]\t Training Loss 0.8996\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [32200][55000]\t Training Loss 0.8997\t Accuracy 0.8308\n",
      "Epoch [0][20]\t Batch [32250][55000]\t Training Loss 0.9000\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [32300][55000]\t Training Loss 0.9003\t Accuracy 0.8306\n",
      "Epoch [0][20]\t Batch [32350][55000]\t Training Loss 0.9005\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [32400][55000]\t Training Loss 0.9005\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [32450][55000]\t Training Loss 0.9006\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [32500][55000]\t Training Loss 0.9008\t Accuracy 0.8302\n",
      "Epoch [0][20]\t Batch [32550][55000]\t Training Loss 0.9010\t Accuracy 0.8301\n",
      "Epoch [0][20]\t Batch [32600][55000]\t Training Loss 0.9009\t Accuracy 0.8302\n",
      "Epoch [0][20]\t Batch [32650][55000]\t Training Loss 0.9007\t Accuracy 0.8303\n",
      "Epoch [0][20]\t Batch [32700][55000]\t Training Loss 0.9007\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [32750][55000]\t Training Loss 0.9007\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [32800][55000]\t Training Loss 0.9007\t Accuracy 0.8304\n",
      "Epoch [0][20]\t Batch [32850][55000]\t Training Loss 0.9007\t Accuracy 0.8305\n",
      "Epoch [0][20]\t Batch [32900][55000]\t Training Loss 0.9004\t Accuracy 0.8307\n",
      "Epoch [0][20]\t Batch [32950][55000]\t Training Loss 0.9003\t Accuracy 0.8308\n",
      "Epoch [0][20]\t Batch [33000][55000]\t Training Loss 0.9001\t Accuracy 0.8309\n",
      "Epoch [0][20]\t Batch [33050][55000]\t Training Loss 0.9002\t Accuracy 0.8309\n",
      "Epoch [0][20]\t Batch [33100][55000]\t Training Loss 0.9001\t Accuracy 0.8309\n",
      "Epoch [0][20]\t Batch [33150][55000]\t Training Loss 0.9001\t Accuracy 0.8309\n",
      "Epoch [0][20]\t Batch [33200][55000]\t Training Loss 0.9000\t Accuracy 0.8310\n",
      "Epoch [0][20]\t Batch [33250][55000]\t Training Loss 0.9001\t Accuracy 0.8309\n",
      "Epoch [0][20]\t Batch [33300][55000]\t Training Loss 0.9000\t Accuracy 0.8310\n",
      "Epoch [0][20]\t Batch [33350][55000]\t Training Loss 0.9000\t Accuracy 0.8310\n",
      "Epoch [0][20]\t Batch [33400][55000]\t Training Loss 0.9001\t Accuracy 0.8310\n",
      "Epoch [0][20]\t Batch [33450][55000]\t Training Loss 0.9001\t Accuracy 0.8309\n",
      "Epoch [0][20]\t Batch [33500][55000]\t Training Loss 0.9000\t Accuracy 0.8310\n",
      "Epoch [0][20]\t Batch [33550][55000]\t Training Loss 0.8999\t Accuracy 0.8310\n",
      "Epoch [0][20]\t Batch [33600][55000]\t Training Loss 0.8999\t Accuracy 0.8311\n",
      "Epoch [0][20]\t Batch [33650][55000]\t Training Loss 0.8998\t Accuracy 0.8312\n",
      "Epoch [0][20]\t Batch [33700][55000]\t Training Loss 0.8996\t Accuracy 0.8313\n",
      "Epoch [0][20]\t Batch [33750][55000]\t Training Loss 0.8994\t Accuracy 0.8314\n",
      "Epoch [0][20]\t Batch [33800][55000]\t Training Loss 0.8994\t Accuracy 0.8315\n",
      "Epoch [0][20]\t Batch [33850][55000]\t Training Loss 0.8990\t Accuracy 0.8316\n",
      "Epoch [0][20]\t Batch [33900][55000]\t Training Loss 0.8986\t Accuracy 0.8318\n",
      "Epoch [0][20]\t Batch [33950][55000]\t Training Loss 0.8982\t Accuracy 0.8320\n",
      "Epoch [0][20]\t Batch [34000][55000]\t Training Loss 0.8981\t Accuracy 0.8320\n",
      "Epoch [0][20]\t Batch [34050][55000]\t Training Loss 0.8983\t Accuracy 0.8320\n",
      "Epoch [0][20]\t Batch [34100][55000]\t Training Loss 0.8983\t Accuracy 0.8321\n",
      "Epoch [0][20]\t Batch [34150][55000]\t Training Loss 0.8983\t Accuracy 0.8322\n",
      "Epoch [0][20]\t Batch [34200][55000]\t Training Loss 0.8980\t Accuracy 0.8324\n",
      "Epoch [0][20]\t Batch [34250][55000]\t Training Loss 0.8978\t Accuracy 0.8325\n",
      "Epoch [0][20]\t Batch [34300][55000]\t Training Loss 0.8975\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [34350][55000]\t Training Loss 0.8974\t Accuracy 0.8328\n",
      "Epoch [0][20]\t Batch [34400][55000]\t Training Loss 0.8973\t Accuracy 0.8328\n",
      "Epoch [0][20]\t Batch [34450][55000]\t Training Loss 0.8974\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [34500][55000]\t Training Loss 0.8973\t Accuracy 0.8328\n",
      "Epoch [0][20]\t Batch [34550][55000]\t Training Loss 0.8973\t Accuracy 0.8328\n",
      "Epoch [0][20]\t Batch [34600][55000]\t Training Loss 0.8972\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [34650][55000]\t Training Loss 0.8971\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [34700][55000]\t Training Loss 0.8972\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [34750][55000]\t Training Loss 0.8973\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [34800][55000]\t Training Loss 0.8972\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [34850][55000]\t Training Loss 0.8976\t Accuracy 0.8326\n",
      "Epoch [0][20]\t Batch [34900][55000]\t Training Loss 0.8976\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [34950][55000]\t Training Loss 0.8976\t Accuracy 0.8327\n",
      "Epoch [0][20]\t Batch [35000][55000]\t Training Loss 0.8974\t Accuracy 0.8328\n",
      "Epoch [0][20]\t Batch [35050][55000]\t Training Loss 0.8972\t Accuracy 0.8330\n",
      "Epoch [0][20]\t Batch [35100][55000]\t Training Loss 0.8972\t Accuracy 0.8330\n",
      "Epoch [0][20]\t Batch [35150][55000]\t Training Loss 0.8972\t Accuracy 0.8330\n",
      "Epoch [0][20]\t Batch [35200][55000]\t Training Loss 0.8973\t Accuracy 0.8330\n",
      "Epoch [0][20]\t Batch [35250][55000]\t Training Loss 0.8973\t Accuracy 0.8331\n",
      "Epoch [0][20]\t Batch [35300][55000]\t Training Loss 0.8973\t Accuracy 0.8331\n",
      "Epoch [0][20]\t Batch [35350][55000]\t Training Loss 0.8971\t Accuracy 0.8332\n",
      "Epoch [0][20]\t Batch [35400][55000]\t Training Loss 0.8970\t Accuracy 0.8333\n",
      "Epoch [0][20]\t Batch [35450][55000]\t Training Loss 0.8969\t Accuracy 0.8333\n",
      "Epoch [0][20]\t Batch [35500][55000]\t Training Loss 0.8968\t Accuracy 0.8333\n",
      "Epoch [0][20]\t Batch [35550][55000]\t Training Loss 0.8966\t Accuracy 0.8334\n",
      "Epoch [0][20]\t Batch [35600][55000]\t Training Loss 0.8965\t Accuracy 0.8334\n",
      "Epoch [0][20]\t Batch [35650][55000]\t Training Loss 0.8968\t Accuracy 0.8333\n",
      "Epoch [0][20]\t Batch [35700][55000]\t Training Loss 0.8967\t Accuracy 0.8333\n",
      "Epoch [0][20]\t Batch [35750][55000]\t Training Loss 0.8966\t Accuracy 0.8333\n",
      "Epoch [0][20]\t Batch [35800][55000]\t Training Loss 0.8964\t Accuracy 0.8334\n",
      "Epoch [0][20]\t Batch [35850][55000]\t Training Loss 0.8962\t Accuracy 0.8335\n",
      "Epoch [0][20]\t Batch [35900][55000]\t Training Loss 0.8961\t Accuracy 0.8334\n",
      "Epoch [0][20]\t Batch [35950][55000]\t Training Loss 0.8962\t Accuracy 0.8335\n",
      "Epoch [0][20]\t Batch [36000][55000]\t Training Loss 0.8962\t Accuracy 0.8334\n",
      "Epoch [0][20]\t Batch [36050][55000]\t Training Loss 0.8964\t Accuracy 0.8334\n",
      "Epoch [0][20]\t Batch [36100][55000]\t Training Loss 0.8965\t Accuracy 0.8334\n",
      "Epoch [0][20]\t Batch [36150][55000]\t Training Loss 0.8964\t Accuracy 0.8334\n",
      "Epoch [0][20]\t Batch [36200][55000]\t Training Loss 0.8961\t Accuracy 0.8335\n",
      "Epoch [0][20]\t Batch [36250][55000]\t Training Loss 0.8960\t Accuracy 0.8335\n",
      "Epoch [0][20]\t Batch [36300][55000]\t Training Loss 0.8956\t Accuracy 0.8337\n",
      "Epoch [0][20]\t Batch [36350][55000]\t Training Loss 0.8954\t Accuracy 0.8337\n",
      "Epoch [0][20]\t Batch [36400][55000]\t Training Loss 0.8953\t Accuracy 0.8338\n",
      "Epoch [0][20]\t Batch [36450][55000]\t Training Loss 0.8954\t Accuracy 0.8337\n",
      "Epoch [0][20]\t Batch [36500][55000]\t Training Loss 0.8955\t Accuracy 0.8337\n",
      "Epoch [0][20]\t Batch [36550][55000]\t Training Loss 0.8953\t Accuracy 0.8338\n",
      "Epoch [0][20]\t Batch [36600][55000]\t Training Loss 0.8951\t Accuracy 0.8339\n",
      "Epoch [0][20]\t Batch [36650][55000]\t Training Loss 0.8948\t Accuracy 0.8340\n",
      "Epoch [0][20]\t Batch [36700][55000]\t Training Loss 0.8945\t Accuracy 0.8341\n",
      "Epoch [0][20]\t Batch [36750][55000]\t Training Loss 0.8943\t Accuracy 0.8342\n",
      "Epoch [0][20]\t Batch [36800][55000]\t Training Loss 0.8941\t Accuracy 0.8343\n",
      "Epoch [0][20]\t Batch [36850][55000]\t Training Loss 0.8940\t Accuracy 0.8343\n",
      "Epoch [0][20]\t Batch [36900][55000]\t Training Loss 0.8940\t Accuracy 0.8343\n",
      "Epoch [0][20]\t Batch [36950][55000]\t Training Loss 0.8939\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [37000][55000]\t Training Loss 0.8937\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [37050][55000]\t Training Loss 0.8935\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [37100][55000]\t Training Loss 0.8935\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [37150][55000]\t Training Loss 0.8935\t Accuracy 0.8347\n",
      "Epoch [0][20]\t Batch [37200][55000]\t Training Loss 0.8933\t Accuracy 0.8347\n",
      "Epoch [0][20]\t Batch [37250][55000]\t Training Loss 0.8932\t Accuracy 0.8347\n",
      "Epoch [0][20]\t Batch [37300][55000]\t Training Loss 0.8932\t Accuracy 0.8347\n",
      "Epoch [0][20]\t Batch [37350][55000]\t Training Loss 0.8932\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [37400][55000]\t Training Loss 0.8935\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [37450][55000]\t Training Loss 0.8937\t Accuracy 0.8344\n",
      "Epoch [0][20]\t Batch [37500][55000]\t Training Loss 0.8938\t Accuracy 0.8344\n",
      "Epoch [0][20]\t Batch [37550][55000]\t Training Loss 0.8938\t Accuracy 0.8343\n",
      "Epoch [0][20]\t Batch [37600][55000]\t Training Loss 0.8940\t Accuracy 0.8342\n",
      "Epoch [0][20]\t Batch [37650][55000]\t Training Loss 0.8940\t Accuracy 0.8342\n",
      "Epoch [0][20]\t Batch [37700][55000]\t Training Loss 0.8939\t Accuracy 0.8343\n",
      "Epoch [0][20]\t Batch [37750][55000]\t Training Loss 0.8938\t Accuracy 0.8343\n",
      "Epoch [0][20]\t Batch [37800][55000]\t Training Loss 0.8938\t Accuracy 0.8344\n",
      "Epoch [0][20]\t Batch [37850][55000]\t Training Loss 0.8938\t Accuracy 0.8343\n",
      "Epoch [0][20]\t Batch [37900][55000]\t Training Loss 0.8938\t Accuracy 0.8343\n",
      "Epoch [0][20]\t Batch [37950][55000]\t Training Loss 0.8938\t Accuracy 0.8344\n",
      "Epoch [0][20]\t Batch [38000][55000]\t Training Loss 0.8937\t Accuracy 0.8344\n",
      "Epoch [0][20]\t Batch [38050][55000]\t Training Loss 0.8937\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [38100][55000]\t Training Loss 0.8938\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [38150][55000]\t Training Loss 0.8936\t Accuracy 0.8347\n",
      "Epoch [0][20]\t Batch [38200][55000]\t Training Loss 0.8934\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [38250][55000]\t Training Loss 0.8934\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [38300][55000]\t Training Loss 0.8935\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [38350][55000]\t Training Loss 0.8936\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [38400][55000]\t Training Loss 0.8936\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [38450][55000]\t Training Loss 0.8935\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [38500][55000]\t Training Loss 0.8934\t Accuracy 0.8347\n",
      "Epoch [0][20]\t Batch [38550][55000]\t Training Loss 0.8934\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [38600][55000]\t Training Loss 0.8935\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [38650][55000]\t Training Loss 0.8936\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [38700][55000]\t Training Loss 0.8936\t Accuracy 0.8344\n",
      "Epoch [0][20]\t Batch [38750][55000]\t Training Loss 0.8934\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [38800][55000]\t Training Loss 0.8934\t Accuracy 0.8345\n",
      "Epoch [0][20]\t Batch [38850][55000]\t Training Loss 0.8932\t Accuracy 0.8346\n",
      "Epoch [0][20]\t Batch [38900][55000]\t Training Loss 0.8930\t Accuracy 0.8347\n",
      "Epoch [0][20]\t Batch [38950][55000]\t Training Loss 0.8927\t Accuracy 0.8348\n",
      "Epoch [0][20]\t Batch [39000][55000]\t Training Loss 0.8927\t Accuracy 0.8349\n",
      "Epoch [0][20]\t Batch [39050][55000]\t Training Loss 0.8925\t Accuracy 0.8349\n",
      "Epoch [0][20]\t Batch [39100][55000]\t Training Loss 0.8922\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [39150][55000]\t Training Loss 0.8920\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [39200][55000]\t Training Loss 0.8918\t Accuracy 0.8353\n",
      "Epoch [0][20]\t Batch [39250][55000]\t Training Loss 0.8916\t Accuracy 0.8354\n",
      "Epoch [0][20]\t Batch [39300][55000]\t Training Loss 0.8915\t Accuracy 0.8354\n",
      "Epoch [0][20]\t Batch [39350][55000]\t Training Loss 0.8918\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [39400][55000]\t Training Loss 0.8918\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [39450][55000]\t Training Loss 0.8920\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [39500][55000]\t Training Loss 0.8920\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [39550][55000]\t Training Loss 0.8920\t Accuracy 0.8350\n",
      "Epoch [0][20]\t Batch [39600][55000]\t Training Loss 0.8920\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [39650][55000]\t Training Loss 0.8919\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [39700][55000]\t Training Loss 0.8919\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [39750][55000]\t Training Loss 0.8920\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [39800][55000]\t Training Loss 0.8921\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [39850][55000]\t Training Loss 0.8922\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [39900][55000]\t Training Loss 0.8923\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [39950][55000]\t Training Loss 0.8925\t Accuracy 0.8350\n",
      "Epoch [0][20]\t Batch [40000][55000]\t Training Loss 0.8924\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [40050][55000]\t Training Loss 0.8924\t Accuracy 0.8350\n",
      "Epoch [0][20]\t Batch [40100][55000]\t Training Loss 0.8923\t Accuracy 0.8350\n",
      "Epoch [0][20]\t Batch [40150][55000]\t Training Loss 0.8922\t Accuracy 0.8350\n",
      "Epoch [0][20]\t Batch [40200][55000]\t Training Loss 0.8921\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [40250][55000]\t Training Loss 0.8920\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [40300][55000]\t Training Loss 0.8920\t Accuracy 0.8351\n",
      "Epoch [0][20]\t Batch [40350][55000]\t Training Loss 0.8918\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [40400][55000]\t Training Loss 0.8918\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [40450][55000]\t Training Loss 0.8917\t Accuracy 0.8353\n",
      "Epoch [0][20]\t Batch [40500][55000]\t Training Loss 0.8917\t Accuracy 0.8352\n",
      "Epoch [0][20]\t Batch [40550][55000]\t Training Loss 0.8917\t Accuracy 0.8353\n",
      "Epoch [0][20]\t Batch [40600][55000]\t Training Loss 0.8917\t Accuracy 0.8353\n",
      "Epoch [0][20]\t Batch [40650][55000]\t Training Loss 0.8916\t Accuracy 0.8353\n",
      "Epoch [0][20]\t Batch [40700][55000]\t Training Loss 0.8916\t Accuracy 0.8353\n",
      "Epoch [0][20]\t Batch [40750][55000]\t Training Loss 0.8915\t Accuracy 0.8354\n",
      "Epoch [0][20]\t Batch [40800][55000]\t Training Loss 0.8915\t Accuracy 0.8354\n",
      "Epoch [0][20]\t Batch [40850][55000]\t Training Loss 0.8913\t Accuracy 0.8355\n",
      "Epoch [0][20]\t Batch [40900][55000]\t Training Loss 0.8911\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [40950][55000]\t Training Loss 0.8909\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [41000][55000]\t Training Loss 0.8909\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [41050][55000]\t Training Loss 0.8910\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [41100][55000]\t Training Loss 0.8910\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [41150][55000]\t Training Loss 0.8910\t Accuracy 0.8358\n",
      "Epoch [0][20]\t Batch [41200][55000]\t Training Loss 0.8910\t Accuracy 0.8359\n",
      "Epoch [0][20]\t Batch [41250][55000]\t Training Loss 0.8909\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [41300][55000]\t Training Loss 0.8910\t Accuracy 0.8359\n",
      "Epoch [0][20]\t Batch [41350][55000]\t Training Loss 0.8913\t Accuracy 0.8358\n",
      "Epoch [0][20]\t Batch [41400][55000]\t Training Loss 0.8914\t Accuracy 0.8358\n",
      "Epoch [0][20]\t Batch [41450][55000]\t Training Loss 0.8914\t Accuracy 0.8358\n",
      "Epoch [0][20]\t Batch [41500][55000]\t Training Loss 0.8916\t Accuracy 0.8358\n",
      "Epoch [0][20]\t Batch [41550][55000]\t Training Loss 0.8918\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [41600][55000]\t Training Loss 0.8918\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [41650][55000]\t Training Loss 0.8917\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [41700][55000]\t Training Loss 0.8916\t Accuracy 0.8359\n",
      "Epoch [0][20]\t Batch [41750][55000]\t Training Loss 0.8918\t Accuracy 0.8359\n",
      "Epoch [0][20]\t Batch [41800][55000]\t Training Loss 0.8919\t Accuracy 0.8359\n",
      "Epoch [0][20]\t Batch [41850][55000]\t Training Loss 0.8919\t Accuracy 0.8359\n",
      "Epoch [0][20]\t Batch [41900][55000]\t Training Loss 0.8918\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [41950][55000]\t Training Loss 0.8918\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [42000][55000]\t Training Loss 0.8917\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [42050][55000]\t Training Loss 0.8916\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [42100][55000]\t Training Loss 0.8915\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [42150][55000]\t Training Loss 0.8916\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [42200][55000]\t Training Loss 0.8916\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [42250][55000]\t Training Loss 0.8916\t Accuracy 0.8359\n",
      "Epoch [0][20]\t Batch [42300][55000]\t Training Loss 0.8916\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [42350][55000]\t Training Loss 0.8918\t Accuracy 0.8358\n",
      "Epoch [0][20]\t Batch [42400][55000]\t Training Loss 0.8919\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [42450][55000]\t Training Loss 0.8920\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [42500][55000]\t Training Loss 0.8921\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [42550][55000]\t Training Loss 0.8924\t Accuracy 0.8354\n",
      "Epoch [0][20]\t Batch [42600][55000]\t Training Loss 0.8921\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [42650][55000]\t Training Loss 0.8919\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [42700][55000]\t Training Loss 0.8919\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [42750][55000]\t Training Loss 0.8918\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [42800][55000]\t Training Loss 0.8916\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [42850][55000]\t Training Loss 0.8915\t Accuracy 0.8358\n",
      "Epoch [0][20]\t Batch [42900][55000]\t Training Loss 0.8915\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [42950][55000]\t Training Loss 0.8915\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [43000][55000]\t Training Loss 0.8917\t Accuracy 0.8355\n",
      "Epoch [0][20]\t Batch [43050][55000]\t Training Loss 0.8919\t Accuracy 0.8355\n",
      "Epoch [0][20]\t Batch [43100][55000]\t Training Loss 0.8920\t Accuracy 0.8354\n",
      "Epoch [0][20]\t Batch [43150][55000]\t Training Loss 0.8920\t Accuracy 0.8355\n",
      "Epoch [0][20]\t Batch [43200][55000]\t Training Loss 0.8919\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [43250][55000]\t Training Loss 0.8919\t Accuracy 0.8356\n",
      "Epoch [0][20]\t Batch [43300][55000]\t Training Loss 0.8918\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [43350][55000]\t Training Loss 0.8916\t Accuracy 0.8357\n",
      "Epoch [0][20]\t Batch [43400][55000]\t Training Loss 0.8914\t Accuracy 0.8358\n",
      "Epoch [0][20]\t Batch [43450][55000]\t Training Loss 0.8912\t Accuracy 0.8359\n",
      "Epoch [0][20]\t Batch [43500][55000]\t Training Loss 0.8909\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [43550][55000]\t Training Loss 0.8909\t Accuracy 0.8360\n",
      "Epoch [0][20]\t Batch [43600][55000]\t Training Loss 0.8908\t Accuracy 0.8361\n",
      "Epoch [0][20]\t Batch [43650][55000]\t Training Loss 0.8906\t Accuracy 0.8362\n",
      "Epoch [0][20]\t Batch [43700][55000]\t Training Loss 0.8906\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [43750][55000]\t Training Loss 0.8905\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [43800][55000]\t Training Loss 0.8904\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [43850][55000]\t Training Loss 0.8905\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [43900][55000]\t Training Loss 0.8905\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [43950][55000]\t Training Loss 0.8906\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [44000][55000]\t Training Loss 0.8906\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [44050][55000]\t Training Loss 0.8906\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [44100][55000]\t Training Loss 0.8906\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [44150][55000]\t Training Loss 0.8907\t Accuracy 0.8362\n",
      "Epoch [0][20]\t Batch [44200][55000]\t Training Loss 0.8908\t Accuracy 0.8362\n",
      "Epoch [0][20]\t Batch [44250][55000]\t Training Loss 0.8908\t Accuracy 0.8362\n",
      "Epoch [0][20]\t Batch [44300][55000]\t Training Loss 0.8909\t Accuracy 0.8362\n",
      "Epoch [0][20]\t Batch [44350][55000]\t Training Loss 0.8909\t Accuracy 0.8361\n",
      "Epoch [0][20]\t Batch [44400][55000]\t Training Loss 0.8910\t Accuracy 0.8361\n",
      "Epoch [0][20]\t Batch [44450][55000]\t Training Loss 0.8910\t Accuracy 0.8362\n",
      "Epoch [0][20]\t Batch [44500][55000]\t Training Loss 0.8909\t Accuracy 0.8363\n",
      "Epoch [0][20]\t Batch [44550][55000]\t Training Loss 0.8907\t Accuracy 0.8364\n",
      "Epoch [0][20]\t Batch [44600][55000]\t Training Loss 0.8906\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [44650][55000]\t Training Loss 0.8904\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [44700][55000]\t Training Loss 0.8903\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [44750][55000]\t Training Loss 0.8903\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [44800][55000]\t Training Loss 0.8903\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [44850][55000]\t Training Loss 0.8903\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [44900][55000]\t Training Loss 0.8904\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [44950][55000]\t Training Loss 0.8904\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [45000][55000]\t Training Loss 0.8903\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [45050][55000]\t Training Loss 0.8905\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [45100][55000]\t Training Loss 0.8905\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [45150][55000]\t Training Loss 0.8905\t Accuracy 0.8364\n",
      "Epoch [0][20]\t Batch [45200][55000]\t Training Loss 0.8905\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [45250][55000]\t Training Loss 0.8905\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [45300][55000]\t Training Loss 0.8903\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [45350][55000]\t Training Loss 0.8902\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [45400][55000]\t Training Loss 0.8901\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [45450][55000]\t Training Loss 0.8902\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [45500][55000]\t Training Loss 0.8904\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [45550][55000]\t Training Loss 0.8904\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [45600][55000]\t Training Loss 0.8902\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [45650][55000]\t Training Loss 0.8903\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [45700][55000]\t Training Loss 0.8901\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [45750][55000]\t Training Loss 0.8901\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [45800][55000]\t Training Loss 0.8902\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [45850][55000]\t Training Loss 0.8902\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [45900][55000]\t Training Loss 0.8903\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [45950][55000]\t Training Loss 0.8903\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [46000][55000]\t Training Loss 0.8904\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [46050][55000]\t Training Loss 0.8905\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [46100][55000]\t Training Loss 0.8906\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [46150][55000]\t Training Loss 0.8906\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [46200][55000]\t Training Loss 0.8905\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [46250][55000]\t Training Loss 0.8905\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [46300][55000]\t Training Loss 0.8906\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [46350][55000]\t Training Loss 0.8906\t Accuracy 0.8366\n",
      "Epoch [0][20]\t Batch [46400][55000]\t Training Loss 0.8908\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [46450][55000]\t Training Loss 0.8908\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [46500][55000]\t Training Loss 0.8907\t Accuracy 0.8365\n",
      "Epoch [0][20]\t Batch [46550][55000]\t Training Loss 0.8906\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [46600][55000]\t Training Loss 0.8904\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [46650][55000]\t Training Loss 0.8904\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [46700][55000]\t Training Loss 0.8903\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [46750][55000]\t Training Loss 0.8903\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [46800][55000]\t Training Loss 0.8901\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [46850][55000]\t Training Loss 0.8900\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [46900][55000]\t Training Loss 0.8899\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [46950][55000]\t Training Loss 0.8897\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [47000][55000]\t Training Loss 0.8896\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [47050][55000]\t Training Loss 0.8895\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [47100][55000]\t Training Loss 0.8895\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [47150][55000]\t Training Loss 0.8893\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [47200][55000]\t Training Loss 0.8892\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [47250][55000]\t Training Loss 0.8893\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [47300][55000]\t Training Loss 0.8894\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [47350][55000]\t Training Loss 0.8895\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [47400][55000]\t Training Loss 0.8895\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [47450][55000]\t Training Loss 0.8895\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [47500][55000]\t Training Loss 0.8895\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [47550][55000]\t Training Loss 0.8895\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [47600][55000]\t Training Loss 0.8894\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [47650][55000]\t Training Loss 0.8895\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [47700][55000]\t Training Loss 0.8895\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [47750][55000]\t Training Loss 0.8894\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [47800][55000]\t Training Loss 0.8893\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [47850][55000]\t Training Loss 0.8890\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [47900][55000]\t Training Loss 0.8890\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [47950][55000]\t Training Loss 0.8891\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [48000][55000]\t Training Loss 0.8891\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [48050][55000]\t Training Loss 0.8889\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [48100][55000]\t Training Loss 0.8889\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [48150][55000]\t Training Loss 0.8886\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [48200][55000]\t Training Loss 0.8884\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [48250][55000]\t Training Loss 0.8882\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [48300][55000]\t Training Loss 0.8881\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [48350][55000]\t Training Loss 0.8879\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [48400][55000]\t Training Loss 0.8880\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [48450][55000]\t Training Loss 0.8878\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [48500][55000]\t Training Loss 0.8876\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [48550][55000]\t Training Loss 0.8876\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [48600][55000]\t Training Loss 0.8875\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [48650][55000]\t Training Loss 0.8874\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [48700][55000]\t Training Loss 0.8873\t Accuracy 0.8376\n",
      "Epoch [0][20]\t Batch [48750][55000]\t Training Loss 0.8871\t Accuracy 0.8377\n",
      "Epoch [0][20]\t Batch [48800][55000]\t Training Loss 0.8870\t Accuracy 0.8377\n",
      "Epoch [0][20]\t Batch [48850][55000]\t Training Loss 0.8868\t Accuracy 0.8377\n",
      "Epoch [0][20]\t Batch [48900][55000]\t Training Loss 0.8867\t Accuracy 0.8378\n",
      "Epoch [0][20]\t Batch [48950][55000]\t Training Loss 0.8869\t Accuracy 0.8377\n",
      "Epoch [0][20]\t Batch [49000][55000]\t Training Loss 0.8870\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [49050][55000]\t Training Loss 0.8873\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [49100][55000]\t Training Loss 0.8874\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [49150][55000]\t Training Loss 0.8875\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [49200][55000]\t Training Loss 0.8875\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [49250][55000]\t Training Loss 0.8876\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [49300][55000]\t Training Loss 0.8875\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [49350][55000]\t Training Loss 0.8873\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [49400][55000]\t Training Loss 0.8871\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [49450][55000]\t Training Loss 0.8871\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [49500][55000]\t Training Loss 0.8873\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [49550][55000]\t Training Loss 0.8876\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [49600][55000]\t Training Loss 0.8878\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [49650][55000]\t Training Loss 0.8878\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [49700][55000]\t Training Loss 0.8881\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [49750][55000]\t Training Loss 0.8880\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [49800][55000]\t Training Loss 0.8880\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [49850][55000]\t Training Loss 0.8881\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [49900][55000]\t Training Loss 0.8882\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [49950][55000]\t Training Loss 0.8882\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [50000][55000]\t Training Loss 0.8883\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [50050][55000]\t Training Loss 0.8883\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [50100][55000]\t Training Loss 0.8883\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [50150][55000]\t Training Loss 0.8882\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [50200][55000]\t Training Loss 0.8881\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [50250][55000]\t Training Loss 0.8882\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [50300][55000]\t Training Loss 0.8881\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [50350][55000]\t Training Loss 0.8882\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [50400][55000]\t Training Loss 0.8884\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [50450][55000]\t Training Loss 0.8886\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [50500][55000]\t Training Loss 0.8886\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [50550][55000]\t Training Loss 0.8886\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [50600][55000]\t Training Loss 0.8887\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [50650][55000]\t Training Loss 0.8887\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [50700][55000]\t Training Loss 0.8887\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [50750][55000]\t Training Loss 0.8887\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [50800][55000]\t Training Loss 0.8887\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [50850][55000]\t Training Loss 0.8886\t Accuracy 0.8367\n",
      "Epoch [0][20]\t Batch [50900][55000]\t Training Loss 0.8885\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [50950][55000]\t Training Loss 0.8884\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [51000][55000]\t Training Loss 0.8882\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [51050][55000]\t Training Loss 0.8881\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [51100][55000]\t Training Loss 0.8879\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [51150][55000]\t Training Loss 0.8879\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [51200][55000]\t Training Loss 0.8879\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [51250][55000]\t Training Loss 0.8879\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [51300][55000]\t Training Loss 0.8880\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [51350][55000]\t Training Loss 0.8880\t Accuracy 0.8368\n",
      "Epoch [0][20]\t Batch [51400][55000]\t Training Loss 0.8879\t Accuracy 0.8369\n",
      "Epoch [0][20]\t Batch [51450][55000]\t Training Loss 0.8878\t Accuracy 0.8370\n",
      "Epoch [0][20]\t Batch [51500][55000]\t Training Loss 0.8876\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [51550][55000]\t Training Loss 0.8875\t Accuracy 0.8371\n",
      "Epoch [0][20]\t Batch [51600][55000]\t Training Loss 0.8873\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [51650][55000]\t Training Loss 0.8872\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [51700][55000]\t Training Loss 0.8872\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [51750][55000]\t Training Loss 0.8872\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [51800][55000]\t Training Loss 0.8872\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [51850][55000]\t Training Loss 0.8871\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [51900][55000]\t Training Loss 0.8871\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [51950][55000]\t Training Loss 0.8869\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [52000][55000]\t Training Loss 0.8870\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [52050][55000]\t Training Loss 0.8870\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [52100][55000]\t Training Loss 0.8872\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [52150][55000]\t Training Loss 0.8874\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [52200][55000]\t Training Loss 0.8876\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [52250][55000]\t Training Loss 0.8877\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [52300][55000]\t Training Loss 0.8877\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [52350][55000]\t Training Loss 0.8876\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [52400][55000]\t Training Loss 0.8876\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [52450][55000]\t Training Loss 0.8874\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [52500][55000]\t Training Loss 0.8872\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [52550][55000]\t Training Loss 0.8871\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [52600][55000]\t Training Loss 0.8868\t Accuracy 0.8376\n",
      "Epoch [0][20]\t Batch [52650][55000]\t Training Loss 0.8866\t Accuracy 0.8377\n",
      "Epoch [0][20]\t Batch [52700][55000]\t Training Loss 0.8867\t Accuracy 0.8376\n",
      "Epoch [0][20]\t Batch [52750][55000]\t Training Loss 0.8868\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [52800][55000]\t Training Loss 0.8870\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [52850][55000]\t Training Loss 0.8870\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [52900][55000]\t Training Loss 0.8871\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [52950][55000]\t Training Loss 0.8872\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [53000][55000]\t Training Loss 0.8873\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53050][55000]\t Training Loss 0.8873\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [53100][55000]\t Training Loss 0.8872\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53150][55000]\t Training Loss 0.8871\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [53200][55000]\t Training Loss 0.8872\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53250][55000]\t Training Loss 0.8872\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53300][55000]\t Training Loss 0.8872\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53350][55000]\t Training Loss 0.8871\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53400][55000]\t Training Loss 0.8869\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [53450][55000]\t Training Loss 0.8868\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [53500][55000]\t Training Loss 0.8868\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [53550][55000]\t Training Loss 0.8868\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [53600][55000]\t Training Loss 0.8869\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [53650][55000]\t Training Loss 0.8869\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53700][55000]\t Training Loss 0.8869\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53750][55000]\t Training Loss 0.8868\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53800][55000]\t Training Loss 0.8867\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [53850][55000]\t Training Loss 0.8867\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [53900][55000]\t Training Loss 0.8866\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [53950][55000]\t Training Loss 0.8866\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [54000][55000]\t Training Loss 0.8867\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [54050][55000]\t Training Loss 0.8868\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [54100][55000]\t Training Loss 0.8869\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [54150][55000]\t Training Loss 0.8868\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [54200][55000]\t Training Loss 0.8869\t Accuracy 0.8372\n",
      "Epoch [0][20]\t Batch [54250][55000]\t Training Loss 0.8867\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [54300][55000]\t Training Loss 0.8867\t Accuracy 0.8373\n",
      "Epoch [0][20]\t Batch [54350][55000]\t Training Loss 0.8866\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [54400][55000]\t Training Loss 0.8865\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [54450][55000]\t Training Loss 0.8864\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [54500][55000]\t Training Loss 0.8863\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [54550][55000]\t Training Loss 0.8862\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [54600][55000]\t Training Loss 0.8862\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [54650][55000]\t Training Loss 0.8860\t Accuracy 0.8374\n",
      "Epoch [0][20]\t Batch [54700][55000]\t Training Loss 0.8859\t Accuracy 0.8375\n",
      "Epoch [0][20]\t Batch [54750][55000]\t Training Loss 0.8857\t Accuracy 0.8376\n",
      "Epoch [0][20]\t Batch [54800][55000]\t Training Loss 0.8857\t Accuracy 0.8376\n",
      "Epoch [0][20]\t Batch [54850][55000]\t Training Loss 0.8855\t Accuracy 0.8377\n",
      "Epoch [0][20]\t Batch [54900][55000]\t Training Loss 0.8856\t Accuracy 0.8377\n",
      "Epoch [0][20]\t Batch [54950][55000]\t Training Loss 0.8859\t Accuracy 0.8376\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8861\t Average training accuracy 0.8375\n",
      "Epoch [0]\t Average validation loss 0.8005\t Average validation accuracy 0.8734\n",
      "\n",
      "Epoch [1][20]\t Batch [0][55000]\t Training Loss 1.4548\t Accuracy 0.0000\n",
      "Epoch [1][20]\t Batch [50][55000]\t Training Loss 0.9489\t Accuracy 0.7451\n",
      "Epoch [1][20]\t Batch [100][55000]\t Training Loss 0.8566\t Accuracy 0.8218\n",
      "Epoch [1][20]\t Batch [150][55000]\t Training Loss 0.8578\t Accuracy 0.8278\n",
      "Epoch [1][20]\t Batch [200][55000]\t Training Loss 0.8679\t Accuracy 0.8259\n",
      "Epoch [1][20]\t Batch [250][55000]\t Training Loss 0.8522\t Accuracy 0.8327\n",
      "Epoch [1][20]\t Batch [300][55000]\t Training Loss 0.8519\t Accuracy 0.8306\n",
      "Epoch [1][20]\t Batch [350][55000]\t Training Loss 0.8322\t Accuracy 0.8376\n",
      "Epoch [1][20]\t Batch [400][55000]\t Training Loss 0.8184\t Accuracy 0.8404\n",
      "Epoch [1][20]\t Batch [450][55000]\t Training Loss 0.8239\t Accuracy 0.8404\n",
      "Epoch [1][20]\t Batch [500][55000]\t Training Loss 0.8301\t Accuracy 0.8423\n",
      "Epoch [1][20]\t Batch [550][55000]\t Training Loss 0.8467\t Accuracy 0.8385\n",
      "Epoch [1][20]\t Batch [600][55000]\t Training Loss 0.8511\t Accuracy 0.8436\n",
      "Epoch [1][20]\t Batch [650][55000]\t Training Loss 0.8709\t Accuracy 0.8326\n",
      "Epoch [1][20]\t Batch [700][55000]\t Training Loss 0.8685\t Accuracy 0.8388\n",
      "Epoch [1][20]\t Batch [750][55000]\t Training Loss 0.8650\t Accuracy 0.8402\n",
      "Epoch [1][20]\t Batch [800][55000]\t Training Loss 0.8589\t Accuracy 0.8414\n",
      "Epoch [1][20]\t Batch [850][55000]\t Training Loss 0.8573\t Accuracy 0.8425\n",
      "Epoch [1][20]\t Batch [900][55000]\t Training Loss 0.8639\t Accuracy 0.8413\n",
      "Epoch [1][20]\t Batch [950][55000]\t Training Loss 0.8707\t Accuracy 0.8370\n",
      "Epoch [1][20]\t Batch [1000][55000]\t Training Loss 0.8704\t Accuracy 0.8392\n",
      "Epoch [1][20]\t Batch [1050][55000]\t Training Loss 0.8781\t Accuracy 0.8363\n",
      "Epoch [1][20]\t Batch [1100][55000]\t Training Loss 0.8865\t Accuracy 0.8347\n",
      "Epoch [1][20]\t Batch [1150][55000]\t Training Loss 0.8957\t Accuracy 0.8315\n",
      "Epoch [1][20]\t Batch [1200][55000]\t Training Loss 0.8898\t Accuracy 0.8343\n",
      "Epoch [1][20]\t Batch [1250][55000]\t Training Loss 0.8911\t Accuracy 0.8337\n",
      "Epoch [1][20]\t Batch [1300][55000]\t Training Loss 0.8912\t Accuracy 0.8340\n",
      "Epoch [1][20]\t Batch [1350][55000]\t Training Loss 0.8881\t Accuracy 0.8349\n",
      "Epoch [1][20]\t Batch [1400][55000]\t Training Loss 0.8886\t Accuracy 0.8337\n",
      "Epoch [1][20]\t Batch [1450][55000]\t Training Loss 0.8897\t Accuracy 0.8325\n",
      "Epoch [1][20]\t Batch [1500][55000]\t Training Loss 0.8877\t Accuracy 0.8354\n",
      "Epoch [1][20]\t Batch [1550][55000]\t Training Loss 0.8874\t Accuracy 0.8362\n",
      "Epoch [1][20]\t Batch [1600][55000]\t Training Loss 0.8900\t Accuracy 0.8351\n",
      "Epoch [1][20]\t Batch [1650][55000]\t Training Loss 0.8871\t Accuracy 0.8365\n",
      "Epoch [1][20]\t Batch [1700][55000]\t Training Loss 0.8853\t Accuracy 0.8377\n",
      "Epoch [1][20]\t Batch [1750][55000]\t Training Loss 0.8797\t Accuracy 0.8389\n",
      "Epoch [1][20]\t Batch [1800][55000]\t Training Loss 0.8776\t Accuracy 0.8406\n",
      "Epoch [1][20]\t Batch [1850][55000]\t Training Loss 0.8760\t Accuracy 0.8412\n",
      "Epoch [1][20]\t Batch [1900][55000]\t Training Loss 0.8739\t Accuracy 0.8411\n",
      "Epoch [1][20]\t Batch [1950][55000]\t Training Loss 0.8723\t Accuracy 0.8426\n",
      "Epoch [1][20]\t Batch [2000][55000]\t Training Loss 0.8699\t Accuracy 0.8436\n",
      "Epoch [1][20]\t Batch [2050][55000]\t Training Loss 0.8691\t Accuracy 0.8440\n",
      "Epoch [1][20]\t Batch [2100][55000]\t Training Loss 0.8663\t Accuracy 0.8444\n",
      "Epoch [1][20]\t Batch [2150][55000]\t Training Loss 0.8631\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [2200][55000]\t Training Loss 0.8583\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [2250][55000]\t Training Loss 0.8574\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [2300][55000]\t Training Loss 0.8543\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [2350][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [2400][55000]\t Training Loss 0.8537\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [2450][55000]\t Training Loss 0.8564\t Accuracy 0.8446\n",
      "Epoch [1][20]\t Batch [2500][55000]\t Training Loss 0.8543\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [2550][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [2600][55000]\t Training Loss 0.8522\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [2650][55000]\t Training Loss 0.8511\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [2700][55000]\t Training Loss 0.8507\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [2750][55000]\t Training Loss 0.8507\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [2800][55000]\t Training Loss 0.8509\t Accuracy 0.8451\n",
      "Epoch [1][20]\t Batch [2850][55000]\t Training Loss 0.8498\t Accuracy 0.8450\n",
      "Epoch [1][20]\t Batch [2900][55000]\t Training Loss 0.8466\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [2950][55000]\t Training Loss 0.8466\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [3000][55000]\t Training Loss 0.8465\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [3050][55000]\t Training Loss 0.8475\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [3100][55000]\t Training Loss 0.8497\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [3150][55000]\t Training Loss 0.8496\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [3200][55000]\t Training Loss 0.8494\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [3250][55000]\t Training Loss 0.8489\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [3300][55000]\t Training Loss 0.8501\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [3350][55000]\t Training Loss 0.8483\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [3400][55000]\t Training Loss 0.8505\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [3450][55000]\t Training Loss 0.8502\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [3500][55000]\t Training Loss 0.8501\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [3550][55000]\t Training Loss 0.8516\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [3600][55000]\t Training Loss 0.8515\t Accuracy 0.8453\n",
      "Epoch [1][20]\t Batch [3650][55000]\t Training Loss 0.8501\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [3700][55000]\t Training Loss 0.8508\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [3750][55000]\t Training Loss 0.8507\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [3800][55000]\t Training Loss 0.8510\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [3850][55000]\t Training Loss 0.8506\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [3900][55000]\t Training Loss 0.8495\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [3950][55000]\t Training Loss 0.8484\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [4000][55000]\t Training Loss 0.8481\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [4050][55000]\t Training Loss 0.8466\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [4100][55000]\t Training Loss 0.8470\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [4150][55000]\t Training Loss 0.8474\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [4200][55000]\t Training Loss 0.8478\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [4250][55000]\t Training Loss 0.8465\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [4300][55000]\t Training Loss 0.8465\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [4350][55000]\t Training Loss 0.8471\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [4400][55000]\t Training Loss 0.8470\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [4450][55000]\t Training Loss 0.8472\t Accuracy 0.8510\n",
      "Epoch [1][20]\t Batch [4500][55000]\t Training Loss 0.8475\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [4550][55000]\t Training Loss 0.8458\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [4600][55000]\t Training Loss 0.8441\t Accuracy 0.8507\n",
      "Epoch [1][20]\t Batch [4650][55000]\t Training Loss 0.8440\t Accuracy 0.8504\n",
      "Epoch [1][20]\t Batch [4700][55000]\t Training Loss 0.8436\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [4750][55000]\t Training Loss 0.8425\t Accuracy 0.8510\n",
      "Epoch [1][20]\t Batch [4800][55000]\t Training Loss 0.8433\t Accuracy 0.8507\n",
      "Epoch [1][20]\t Batch [4850][55000]\t Training Loss 0.8444\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [4900][55000]\t Training Loss 0.8430\t Accuracy 0.8504\n",
      "Epoch [1][20]\t Batch [4950][55000]\t Training Loss 0.8433\t Accuracy 0.8509\n",
      "Epoch [1][20]\t Batch [5000][55000]\t Training Loss 0.8433\t Accuracy 0.8512\n",
      "Epoch [1][20]\t Batch [5050][55000]\t Training Loss 0.8429\t Accuracy 0.8513\n",
      "Epoch [1][20]\t Batch [5100][55000]\t Training Loss 0.8428\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [5150][55000]\t Training Loss 0.8431\t Accuracy 0.8513\n",
      "Epoch [1][20]\t Batch [5200][55000]\t Training Loss 0.8446\t Accuracy 0.8504\n",
      "Epoch [1][20]\t Batch [5250][55000]\t Training Loss 0.8436\t Accuracy 0.8511\n",
      "Epoch [1][20]\t Batch [5300][55000]\t Training Loss 0.8435\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [5350][55000]\t Training Loss 0.8441\t Accuracy 0.8509\n",
      "Epoch [1][20]\t Batch [5400][55000]\t Training Loss 0.8433\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [5450][55000]\t Training Loss 0.8426\t Accuracy 0.8512\n",
      "Epoch [1][20]\t Batch [5500][55000]\t Training Loss 0.8408\t Accuracy 0.8509\n",
      "Epoch [1][20]\t Batch [5550][55000]\t Training Loss 0.8407\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [5600][55000]\t Training Loss 0.8398\t Accuracy 0.8509\n",
      "Epoch [1][20]\t Batch [5650][55000]\t Training Loss 0.8403\t Accuracy 0.8503\n",
      "Epoch [1][20]\t Batch [5700][55000]\t Training Loss 0.8400\t Accuracy 0.8504\n",
      "Epoch [1][20]\t Batch [5750][55000]\t Training Loss 0.8404\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [5800][55000]\t Training Loss 0.8408\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [5850][55000]\t Training Loss 0.8410\t Accuracy 0.8503\n",
      "Epoch [1][20]\t Batch [5900][55000]\t Training Loss 0.8415\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [5950][55000]\t Training Loss 0.8414\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [6000][55000]\t Training Loss 0.8404\t Accuracy 0.8507\n",
      "Epoch [1][20]\t Batch [6050][55000]\t Training Loss 0.8390\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [6100][55000]\t Training Loss 0.8375\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [6150][55000]\t Training Loss 0.8354\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [6200][55000]\t Training Loss 0.8355\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [6250][55000]\t Training Loss 0.8344\t Accuracy 0.8523\n",
      "Epoch [1][20]\t Batch [6300][55000]\t Training Loss 0.8348\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [6350][55000]\t Training Loss 0.8344\t Accuracy 0.8526\n",
      "Epoch [1][20]\t Batch [6400][55000]\t Training Loss 0.8338\t Accuracy 0.8530\n",
      "Epoch [1][20]\t Batch [6450][55000]\t Training Loss 0.8333\t Accuracy 0.8529\n",
      "Epoch [1][20]\t Batch [6500][55000]\t Training Loss 0.8341\t Accuracy 0.8525\n",
      "Epoch [1][20]\t Batch [6550][55000]\t Training Loss 0.8332\t Accuracy 0.8527\n",
      "Epoch [1][20]\t Batch [6600][55000]\t Training Loss 0.8319\t Accuracy 0.8531\n",
      "Epoch [1][20]\t Batch [6650][55000]\t Training Loss 0.8307\t Accuracy 0.8533\n",
      "Epoch [1][20]\t Batch [6700][55000]\t Training Loss 0.8309\t Accuracy 0.8535\n",
      "Epoch [1][20]\t Batch [6750][55000]\t Training Loss 0.8311\t Accuracy 0.8542\n",
      "Epoch [1][20]\t Batch [6800][55000]\t Training Loss 0.8315\t Accuracy 0.8544\n",
      "Epoch [1][20]\t Batch [6850][55000]\t Training Loss 0.8337\t Accuracy 0.8542\n",
      "Epoch [1][20]\t Batch [6900][55000]\t Training Loss 0.8337\t Accuracy 0.8541\n",
      "Epoch [1][20]\t Batch [6950][55000]\t Training Loss 0.8343\t Accuracy 0.8528\n",
      "Epoch [1][20]\t Batch [7000][55000]\t Training Loss 0.8342\t Accuracy 0.8529\n",
      "Epoch [1][20]\t Batch [7050][55000]\t Training Loss 0.8348\t Accuracy 0.8522\n",
      "Epoch [1][20]\t Batch [7100][55000]\t Training Loss 0.8350\t Accuracy 0.8523\n",
      "Epoch [1][20]\t Batch [7150][55000]\t Training Loss 0.8352\t Accuracy 0.8527\n",
      "Epoch [1][20]\t Batch [7200][55000]\t Training Loss 0.8360\t Accuracy 0.8527\n",
      "Epoch [1][20]\t Batch [7250][55000]\t Training Loss 0.8383\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [7300][55000]\t Training Loss 0.8401\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [7350][55000]\t Training Loss 0.8417\t Accuracy 0.8517\n",
      "Epoch [1][20]\t Batch [7400][55000]\t Training Loss 0.8427\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [7450][55000]\t Training Loss 0.8426\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [7500][55000]\t Training Loss 0.8427\t Accuracy 0.8516\n",
      "Epoch [1][20]\t Batch [7550][55000]\t Training Loss 0.8432\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [7600][55000]\t Training Loss 0.8427\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [7650][55000]\t Training Loss 0.8432\t Accuracy 0.8522\n",
      "Epoch [1][20]\t Batch [7700][55000]\t Training Loss 0.8437\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [7750][55000]\t Training Loss 0.8438\t Accuracy 0.8519\n",
      "Epoch [1][20]\t Batch [7800][55000]\t Training Loss 0.8446\t Accuracy 0.8517\n",
      "Epoch [1][20]\t Batch [7850][55000]\t Training Loss 0.8447\t Accuracy 0.8519\n",
      "Epoch [1][20]\t Batch [7900][55000]\t Training Loss 0.8449\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [7950][55000]\t Training Loss 0.8450\t Accuracy 0.8513\n",
      "Epoch [1][20]\t Batch [8000][55000]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [8050][55000]\t Training Loss 0.8455\t Accuracy 0.8507\n",
      "Epoch [1][20]\t Batch [8100][55000]\t Training Loss 0.8440\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [8150][55000]\t Training Loss 0.8449\t Accuracy 0.8506\n",
      "Epoch [1][20]\t Batch [8200][55000]\t Training Loss 0.8449\t Accuracy 0.8503\n",
      "Epoch [1][20]\t Batch [8250][55000]\t Training Loss 0.8461\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [8300][55000]\t Training Loss 0.8464\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [8350][55000]\t Training Loss 0.8471\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [8400][55000]\t Training Loss 0.8465\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [8450][55000]\t Training Loss 0.8480\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [8500][55000]\t Training Loss 0.8472\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [8550][55000]\t Training Loss 0.8462\t Accuracy 0.8505\n",
      "Epoch [1][20]\t Batch [8600][55000]\t Training Loss 0.8454\t Accuracy 0.8507\n",
      "Epoch [1][20]\t Batch [8650][55000]\t Training Loss 0.8455\t Accuracy 0.8507\n",
      "Epoch [1][20]\t Batch [8700][55000]\t Training Loss 0.8462\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [8750][55000]\t Training Loss 0.8477\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [8800][55000]\t Training Loss 0.8485\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [8850][55000]\t Training Loss 0.8484\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [8900][55000]\t Training Loss 0.8501\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [8950][55000]\t Training Loss 0.8497\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [9000][55000]\t Training Loss 0.8490\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [9050][55000]\t Training Loss 0.8481\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [9100][55000]\t Training Loss 0.8479\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [9150][55000]\t Training Loss 0.8484\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [9200][55000]\t Training Loss 0.8480\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [9250][55000]\t Training Loss 0.8483\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [9300][55000]\t Training Loss 0.8485\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [9350][55000]\t Training Loss 0.8487\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [9400][55000]\t Training Loss 0.8489\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [9450][55000]\t Training Loss 0.8492\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [9500][55000]\t Training Loss 0.8483\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [9550][55000]\t Training Loss 0.8480\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [9600][55000]\t Training Loss 0.8485\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [9650][55000]\t Training Loss 0.8483\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [9700][55000]\t Training Loss 0.8478\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [9750][55000]\t Training Loss 0.8470\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [9800][55000]\t Training Loss 0.8475\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [9850][55000]\t Training Loss 0.8471\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [9900][55000]\t Training Loss 0.8466\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [9950][55000]\t Training Loss 0.8460\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [10000][55000]\t Training Loss 0.8457\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [10050][55000]\t Training Loss 0.8459\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [10100][55000]\t Training Loss 0.8457\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [10150][55000]\t Training Loss 0.8455\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [10200][55000]\t Training Loss 0.8456\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [10250][55000]\t Training Loss 0.8457\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [10300][55000]\t Training Loss 0.8456\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [10350][55000]\t Training Loss 0.8446\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [10400][55000]\t Training Loss 0.8439\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [10450][55000]\t Training Loss 0.8437\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [10500][55000]\t Training Loss 0.8427\t Accuracy 0.8505\n",
      "Epoch [1][20]\t Batch [10550][55000]\t Training Loss 0.8423\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [10600][55000]\t Training Loss 0.8419\t Accuracy 0.8510\n",
      "Epoch [1][20]\t Batch [10650][55000]\t Training Loss 0.8416\t Accuracy 0.8513\n",
      "Epoch [1][20]\t Batch [10700][55000]\t Training Loss 0.8414\t Accuracy 0.8516\n",
      "Epoch [1][20]\t Batch [10750][55000]\t Training Loss 0.8419\t Accuracy 0.8513\n",
      "Epoch [1][20]\t Batch [10800][55000]\t Training Loss 0.8424\t Accuracy 0.8510\n",
      "Epoch [1][20]\t Batch [10850][55000]\t Training Loss 0.8416\t Accuracy 0.8513\n",
      "Epoch [1][20]\t Batch [10900][55000]\t Training Loss 0.8412\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [10950][55000]\t Training Loss 0.8409\t Accuracy 0.8512\n",
      "Epoch [1][20]\t Batch [11000][55000]\t Training Loss 0.8407\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [11050][55000]\t Training Loss 0.8400\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [11100][55000]\t Training Loss 0.8393\t Accuracy 0.8519\n",
      "Epoch [1][20]\t Batch [11150][55000]\t Training Loss 0.8393\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [11200][55000]\t Training Loss 0.8390\t Accuracy 0.8522\n",
      "Epoch [1][20]\t Batch [11250][55000]\t Training Loss 0.8394\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [11300][55000]\t Training Loss 0.8390\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [11350][55000]\t Training Loss 0.8384\t Accuracy 0.8522\n",
      "Epoch [1][20]\t Batch [11400][55000]\t Training Loss 0.8386\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [11450][55000]\t Training Loss 0.8382\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [11500][55000]\t Training Loss 0.8380\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [11550][55000]\t Training Loss 0.8380\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [11600][55000]\t Training Loss 0.8392\t Accuracy 0.8511\n",
      "Epoch [1][20]\t Batch [11650][55000]\t Training Loss 0.8397\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [11700][55000]\t Training Loss 0.8396\t Accuracy 0.8510\n",
      "Epoch [1][20]\t Batch [11750][55000]\t Training Loss 0.8406\t Accuracy 0.8505\n",
      "Epoch [1][20]\t Batch [11800][55000]\t Training Loss 0.8408\t Accuracy 0.8504\n",
      "Epoch [1][20]\t Batch [11850][55000]\t Training Loss 0.8408\t Accuracy 0.8506\n",
      "Epoch [1][20]\t Batch [11900][55000]\t Training Loss 0.8411\t Accuracy 0.8505\n",
      "Epoch [1][20]\t Batch [11950][55000]\t Training Loss 0.8412\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [12000][55000]\t Training Loss 0.8412\t Accuracy 0.8511\n",
      "Epoch [1][20]\t Batch [12050][55000]\t Training Loss 0.8410\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [12100][55000]\t Training Loss 0.8409\t Accuracy 0.8513\n",
      "Epoch [1][20]\t Batch [12150][55000]\t Training Loss 0.8403\t Accuracy 0.8517\n",
      "Epoch [1][20]\t Batch [12200][55000]\t Training Loss 0.8406\t Accuracy 0.8517\n",
      "Epoch [1][20]\t Batch [12250][55000]\t Training Loss 0.8404\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [12300][55000]\t Training Loss 0.8405\t Accuracy 0.8517\n",
      "Epoch [1][20]\t Batch [12350][55000]\t Training Loss 0.8407\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [12400][55000]\t Training Loss 0.8410\t Accuracy 0.8519\n",
      "Epoch [1][20]\t Batch [12450][55000]\t Training Loss 0.8412\t Accuracy 0.8516\n",
      "Epoch [1][20]\t Batch [12500][55000]\t Training Loss 0.8414\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [12550][55000]\t Training Loss 0.8414\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [12600][55000]\t Training Loss 0.8421\t Accuracy 0.8512\n",
      "Epoch [1][20]\t Batch [12650][55000]\t Training Loss 0.8426\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [12700][55000]\t Training Loss 0.8433\t Accuracy 0.8506\n",
      "Epoch [1][20]\t Batch [12750][55000]\t Training Loss 0.8428\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [12800][55000]\t Training Loss 0.8434\t Accuracy 0.8506\n",
      "Epoch [1][20]\t Batch [12850][55000]\t Training Loss 0.8437\t Accuracy 0.8504\n",
      "Epoch [1][20]\t Batch [12900][55000]\t Training Loss 0.8436\t Accuracy 0.8506\n",
      "Epoch [1][20]\t Batch [12950][55000]\t Training Loss 0.8442\t Accuracy 0.8502\n",
      "Epoch [1][20]\t Batch [13000][55000]\t Training Loss 0.8442\t Accuracy 0.8502\n",
      "Epoch [1][20]\t Batch [13050][55000]\t Training Loss 0.8448\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [13100][55000]\t Training Loss 0.8455\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [13150][55000]\t Training Loss 0.8458\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [13200][55000]\t Training Loss 0.8460\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [13250][55000]\t Training Loss 0.8455\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [13300][55000]\t Training Loss 0.8454\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [13350][55000]\t Training Loss 0.8458\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [13400][55000]\t Training Loss 0.8461\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [13450][55000]\t Training Loss 0.8456\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [13500][55000]\t Training Loss 0.8452\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [13550][55000]\t Training Loss 0.8448\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [13600][55000]\t Training Loss 0.8439\t Accuracy 0.8502\n",
      "Epoch [1][20]\t Batch [13650][55000]\t Training Loss 0.8438\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [13700][55000]\t Training Loss 0.8446\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [13750][55000]\t Training Loss 0.8452\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [13800][55000]\t Training Loss 0.8453\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [13850][55000]\t Training Loss 0.8453\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [13900][55000]\t Training Loss 0.8455\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [13950][55000]\t Training Loss 0.8459\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [14000][55000]\t Training Loss 0.8467\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [14050][55000]\t Training Loss 0.8468\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [14100][55000]\t Training Loss 0.8469\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [14150][55000]\t Training Loss 0.8472\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [14200][55000]\t Training Loss 0.8471\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [14250][55000]\t Training Loss 0.8473\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [14300][55000]\t Training Loss 0.8477\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [14350][55000]\t Training Loss 0.8481\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [14400][55000]\t Training Loss 0.8489\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [14450][55000]\t Training Loss 0.8491\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [14500][55000]\t Training Loss 0.8492\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [14550][55000]\t Training Loss 0.8499\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [14600][55000]\t Training Loss 0.8499\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [14650][55000]\t Training Loss 0.8509\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [14700][55000]\t Training Loss 0.8517\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [14750][55000]\t Training Loss 0.8523\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [14800][55000]\t Training Loss 0.8532\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [14850][55000]\t Training Loss 0.8539\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [14900][55000]\t Training Loss 0.8538\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [14950][55000]\t Training Loss 0.8539\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [15000][55000]\t Training Loss 0.8538\t Accuracy 0.8454\n",
      "Epoch [1][20]\t Batch [15050][55000]\t Training Loss 0.8534\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [15100][55000]\t Training Loss 0.8532\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [15150][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [15200][55000]\t Training Loss 0.8538\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [15250][55000]\t Training Loss 0.8538\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [15300][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [15350][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [15400][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [15450][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [15500][55000]\t Training Loss 0.8537\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [15550][55000]\t Training Loss 0.8536\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [15600][55000]\t Training Loss 0.8535\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [15650][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [15700][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [15750][55000]\t Training Loss 0.8541\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [15800][55000]\t Training Loss 0.8546\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [15850][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [15900][55000]\t Training Loss 0.8554\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [15950][55000]\t Training Loss 0.8555\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [16000][55000]\t Training Loss 0.8556\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [16050][55000]\t Training Loss 0.8565\t Accuracy 0.8452\n",
      "Epoch [1][20]\t Batch [16100][55000]\t Training Loss 0.8565\t Accuracy 0.8452\n",
      "Epoch [1][20]\t Batch [16150][55000]\t Training Loss 0.8564\t Accuracy 0.8454\n",
      "Epoch [1][20]\t Batch [16200][55000]\t Training Loss 0.8565\t Accuracy 0.8451\n",
      "Epoch [1][20]\t Batch [16250][55000]\t Training Loss 0.8564\t Accuracy 0.8451\n",
      "Epoch [1][20]\t Batch [16300][55000]\t Training Loss 0.8561\t Accuracy 0.8452\n",
      "Epoch [1][20]\t Batch [16350][55000]\t Training Loss 0.8557\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [16400][55000]\t Training Loss 0.8558\t Accuracy 0.8454\n",
      "Epoch [1][20]\t Batch [16450][55000]\t Training Loss 0.8555\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [16500][55000]\t Training Loss 0.8553\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [16550][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [16600][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [16650][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [16700][55000]\t Training Loss 0.8552\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [16750][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [16800][55000]\t Training Loss 0.8557\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [16850][55000]\t Training Loss 0.8562\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [16900][55000]\t Training Loss 0.8565\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [16950][55000]\t Training Loss 0.8565\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [17000][55000]\t Training Loss 0.8572\t Accuracy 0.8452\n",
      "Epoch [1][20]\t Batch [17050][55000]\t Training Loss 0.8571\t Accuracy 0.8452\n",
      "Epoch [1][20]\t Batch [17100][55000]\t Training Loss 0.8576\t Accuracy 0.8450\n",
      "Epoch [1][20]\t Batch [17150][55000]\t Training Loss 0.8573\t Accuracy 0.8451\n",
      "Epoch [1][20]\t Batch [17200][55000]\t Training Loss 0.8574\t Accuracy 0.8450\n",
      "Epoch [1][20]\t Batch [17250][55000]\t Training Loss 0.8579\t Accuracy 0.8449\n",
      "Epoch [1][20]\t Batch [17300][55000]\t Training Loss 0.8577\t Accuracy 0.8449\n",
      "Epoch [1][20]\t Batch [17350][55000]\t Training Loss 0.8572\t Accuracy 0.8452\n",
      "Epoch [1][20]\t Batch [17400][55000]\t Training Loss 0.8573\t Accuracy 0.8452\n",
      "Epoch [1][20]\t Batch [17450][55000]\t Training Loss 0.8574\t Accuracy 0.8452\n",
      "Epoch [1][20]\t Batch [17500][55000]\t Training Loss 0.8575\t Accuracy 0.8453\n",
      "Epoch [1][20]\t Batch [17550][55000]\t Training Loss 0.8582\t Accuracy 0.8450\n",
      "Epoch [1][20]\t Batch [17600][55000]\t Training Loss 0.8588\t Accuracy 0.8448\n",
      "Epoch [1][20]\t Batch [17650][55000]\t Training Loss 0.8590\t Accuracy 0.8450\n",
      "Epoch [1][20]\t Batch [17700][55000]\t Training Loss 0.8597\t Accuracy 0.8448\n",
      "Epoch [1][20]\t Batch [17750][55000]\t Training Loss 0.8600\t Accuracy 0.8447\n",
      "Epoch [1][20]\t Batch [17800][55000]\t Training Loss 0.8604\t Accuracy 0.8446\n",
      "Epoch [1][20]\t Batch [17850][55000]\t Training Loss 0.8607\t Accuracy 0.8444\n",
      "Epoch [1][20]\t Batch [17900][55000]\t Training Loss 0.8611\t Accuracy 0.8443\n",
      "Epoch [1][20]\t Batch [17950][55000]\t Training Loss 0.8608\t Accuracy 0.8441\n",
      "Epoch [1][20]\t Batch [18000][55000]\t Training Loss 0.8605\t Accuracy 0.8443\n",
      "Epoch [1][20]\t Batch [18050][55000]\t Training Loss 0.8608\t Accuracy 0.8443\n",
      "Epoch [1][20]\t Batch [18100][55000]\t Training Loss 0.8606\t Accuracy 0.8444\n",
      "Epoch [1][20]\t Batch [18150][55000]\t Training Loss 0.8602\t Accuracy 0.8445\n",
      "Epoch [1][20]\t Batch [18200][55000]\t Training Loss 0.8597\t Accuracy 0.8447\n",
      "Epoch [1][20]\t Batch [18250][55000]\t Training Loss 0.8597\t Accuracy 0.8447\n",
      "Epoch [1][20]\t Batch [18300][55000]\t Training Loss 0.8593\t Accuracy 0.8448\n",
      "Epoch [1][20]\t Batch [18350][55000]\t Training Loss 0.8592\t Accuracy 0.8451\n",
      "Epoch [1][20]\t Batch [18400][55000]\t Training Loss 0.8592\t Accuracy 0.8450\n",
      "Epoch [1][20]\t Batch [18450][55000]\t Training Loss 0.8596\t Accuracy 0.8448\n",
      "Epoch [1][20]\t Batch [18500][55000]\t Training Loss 0.8596\t Accuracy 0.8448\n",
      "Epoch [1][20]\t Batch [18550][55000]\t Training Loss 0.8594\t Accuracy 0.8451\n",
      "Epoch [1][20]\t Batch [18600][55000]\t Training Loss 0.8594\t Accuracy 0.8453\n",
      "Epoch [1][20]\t Batch [18650][55000]\t Training Loss 0.8593\t Accuracy 0.8454\n",
      "Epoch [1][20]\t Batch [18700][55000]\t Training Loss 0.8595\t Accuracy 0.8453\n",
      "Epoch [1][20]\t Batch [18750][55000]\t Training Loss 0.8597\t Accuracy 0.8453\n",
      "Epoch [1][20]\t Batch [18800][55000]\t Training Loss 0.8593\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [18850][55000]\t Training Loss 0.8596\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [18900][55000]\t Training Loss 0.8590\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [18950][55000]\t Training Loss 0.8588\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [19000][55000]\t Training Loss 0.8588\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [19050][55000]\t Training Loss 0.8591\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [19100][55000]\t Training Loss 0.8594\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [19150][55000]\t Training Loss 0.8595\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [19200][55000]\t Training Loss 0.8596\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [19250][55000]\t Training Loss 0.8595\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [19300][55000]\t Training Loss 0.8593\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [19350][55000]\t Training Loss 0.8594\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [19400][55000]\t Training Loss 0.8591\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [19450][55000]\t Training Loss 0.8589\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [19500][55000]\t Training Loss 0.8585\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [19550][55000]\t Training Loss 0.8586\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [19600][55000]\t Training Loss 0.8585\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [19650][55000]\t Training Loss 0.8581\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [19700][55000]\t Training Loss 0.8576\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [19750][55000]\t Training Loss 0.8570\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [19800][55000]\t Training Loss 0.8564\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [19850][55000]\t Training Loss 0.8564\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [19900][55000]\t Training Loss 0.8562\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [19950][55000]\t Training Loss 0.8564\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [20000][55000]\t Training Loss 0.8563\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [20050][55000]\t Training Loss 0.8568\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [20100][55000]\t Training Loss 0.8569\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [20150][55000]\t Training Loss 0.8567\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [20200][55000]\t Training Loss 0.8571\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [20250][55000]\t Training Loss 0.8572\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [20300][55000]\t Training Loss 0.8573\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [20350][55000]\t Training Loss 0.8573\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [20400][55000]\t Training Loss 0.8570\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [20450][55000]\t Training Loss 0.8566\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [20500][55000]\t Training Loss 0.8563\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [20550][55000]\t Training Loss 0.8562\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [20600][55000]\t Training Loss 0.8562\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [20650][55000]\t Training Loss 0.8559\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [20700][55000]\t Training Loss 0.8557\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [20750][55000]\t Training Loss 0.8557\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [20800][55000]\t Training Loss 0.8557\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [20850][55000]\t Training Loss 0.8556\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [20900][55000]\t Training Loss 0.8559\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [20950][55000]\t Training Loss 0.8564\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [21000][55000]\t Training Loss 0.8566\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [21050][55000]\t Training Loss 0.8569\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [21100][55000]\t Training Loss 0.8566\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [21150][55000]\t Training Loss 0.8566\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [21200][55000]\t Training Loss 0.8563\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [21250][55000]\t Training Loss 0.8562\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [21300][55000]\t Training Loss 0.8557\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [21350][55000]\t Training Loss 0.8558\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [21400][55000]\t Training Loss 0.8559\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [21450][55000]\t Training Loss 0.8560\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [21500][55000]\t Training Loss 0.8556\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [21550][55000]\t Training Loss 0.8554\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [21600][55000]\t Training Loss 0.8556\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [21650][55000]\t Training Loss 0.8556\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [21700][55000]\t Training Loss 0.8555\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [21750][55000]\t Training Loss 0.8555\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [21800][55000]\t Training Loss 0.8549\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [21850][55000]\t Training Loss 0.8545\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [21900][55000]\t Training Loss 0.8541\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [21950][55000]\t Training Loss 0.8536\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [22000][55000]\t Training Loss 0.8532\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [22050][55000]\t Training Loss 0.8529\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [22100][55000]\t Training Loss 0.8530\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [22150][55000]\t Training Loss 0.8535\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [22200][55000]\t Training Loss 0.8537\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [22250][55000]\t Training Loss 0.8537\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [22300][55000]\t Training Loss 0.8539\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [22350][55000]\t Training Loss 0.8536\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [22400][55000]\t Training Loss 0.8535\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [22450][55000]\t Training Loss 0.8535\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [22500][55000]\t Training Loss 0.8540\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [22550][55000]\t Training Loss 0.8546\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [22600][55000]\t Training Loss 0.8550\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [22650][55000]\t Training Loss 0.8552\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [22700][55000]\t Training Loss 0.8550\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [22750][55000]\t Training Loss 0.8549\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [22800][55000]\t Training Loss 0.8548\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [22850][55000]\t Training Loss 0.8548\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [22900][55000]\t Training Loss 0.8545\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [22950][55000]\t Training Loss 0.8544\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [23000][55000]\t Training Loss 0.8541\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [23050][55000]\t Training Loss 0.8539\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [23100][55000]\t Training Loss 0.8541\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [23150][55000]\t Training Loss 0.8540\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [23200][55000]\t Training Loss 0.8540\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [23250][55000]\t Training Loss 0.8537\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [23300][55000]\t Training Loss 0.8533\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [23350][55000]\t Training Loss 0.8532\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [23400][55000]\t Training Loss 0.8529\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [23450][55000]\t Training Loss 0.8529\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [23500][55000]\t Training Loss 0.8527\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [23550][55000]\t Training Loss 0.8526\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [23600][55000]\t Training Loss 0.8526\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [23650][55000]\t Training Loss 0.8527\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [23700][55000]\t Training Loss 0.8529\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [23750][55000]\t Training Loss 0.8535\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [23800][55000]\t Training Loss 0.8531\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [23850][55000]\t Training Loss 0.8531\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [23900][55000]\t Training Loss 0.8532\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [23950][55000]\t Training Loss 0.8532\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [24000][55000]\t Training Loss 0.8533\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [24050][55000]\t Training Loss 0.8532\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [24100][55000]\t Training Loss 0.8531\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [24150][55000]\t Training Loss 0.8529\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [24200][55000]\t Training Loss 0.8528\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [24250][55000]\t Training Loss 0.8529\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [24300][55000]\t Training Loss 0.8531\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [24350][55000]\t Training Loss 0.8530\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [24400][55000]\t Training Loss 0.8530\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [24450][55000]\t Training Loss 0.8530\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [24500][55000]\t Training Loss 0.8530\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [24550][55000]\t Training Loss 0.8531\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [24600][55000]\t Training Loss 0.8532\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [24650][55000]\t Training Loss 0.8532\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [24700][55000]\t Training Loss 0.8532\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [24750][55000]\t Training Loss 0.8535\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [24800][55000]\t Training Loss 0.8538\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [24850][55000]\t Training Loss 0.8536\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [24900][55000]\t Training Loss 0.8537\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [24950][55000]\t Training Loss 0.8540\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [25000][55000]\t Training Loss 0.8543\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [25050][55000]\t Training Loss 0.8540\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [25100][55000]\t Training Loss 0.8539\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [25150][55000]\t Training Loss 0.8537\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [25200][55000]\t Training Loss 0.8536\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [25250][55000]\t Training Loss 0.8535\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [25300][55000]\t Training Loss 0.8535\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [25350][55000]\t Training Loss 0.8536\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [25400][55000]\t Training Loss 0.8531\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [25450][55000]\t Training Loss 0.8527\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [25500][55000]\t Training Loss 0.8525\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [25550][55000]\t Training Loss 0.8521\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [25600][55000]\t Training Loss 0.8521\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [25650][55000]\t Training Loss 0.8519\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [25700][55000]\t Training Loss 0.8517\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [25750][55000]\t Training Loss 0.8515\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [25800][55000]\t Training Loss 0.8514\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [25850][55000]\t Training Loss 0.8516\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [25900][55000]\t Training Loss 0.8516\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [25950][55000]\t Training Loss 0.8516\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [26000][55000]\t Training Loss 0.8516\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [26050][55000]\t Training Loss 0.8514\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [26100][55000]\t Training Loss 0.8511\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [26150][55000]\t Training Loss 0.8509\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [26200][55000]\t Training Loss 0.8507\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [26250][55000]\t Training Loss 0.8507\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [26300][55000]\t Training Loss 0.8509\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [26350][55000]\t Training Loss 0.8507\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [26400][55000]\t Training Loss 0.8511\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [26450][55000]\t Training Loss 0.8513\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [26500][55000]\t Training Loss 0.8515\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [26550][55000]\t Training Loss 0.8517\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [26600][55000]\t Training Loss 0.8519\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [26650][55000]\t Training Loss 0.8523\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [26700][55000]\t Training Loss 0.8522\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [26750][55000]\t Training Loss 0.8524\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [26800][55000]\t Training Loss 0.8523\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [26850][55000]\t Training Loss 0.8522\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [26900][55000]\t Training Loss 0.8525\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [26950][55000]\t Training Loss 0.8524\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [27000][55000]\t Training Loss 0.8522\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [27050][55000]\t Training Loss 0.8519\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [27100][55000]\t Training Loss 0.8518\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [27150][55000]\t Training Loss 0.8518\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [27200][55000]\t Training Loss 0.8521\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [27250][55000]\t Training Loss 0.8521\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [27300][55000]\t Training Loss 0.8521\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [27350][55000]\t Training Loss 0.8520\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [27400][55000]\t Training Loss 0.8520\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [27450][55000]\t Training Loss 0.8519\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [27500][55000]\t Training Loss 0.8519\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [27550][55000]\t Training Loss 0.8520\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [27600][55000]\t Training Loss 0.8518\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [27650][55000]\t Training Loss 0.8518\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [27700][55000]\t Training Loss 0.8519\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [27750][55000]\t Training Loss 0.8519\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [27800][55000]\t Training Loss 0.8521\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [27850][55000]\t Training Loss 0.8521\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [27900][55000]\t Training Loss 0.8520\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [27950][55000]\t Training Loss 0.8517\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [28000][55000]\t Training Loss 0.8515\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [28050][55000]\t Training Loss 0.8512\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [28100][55000]\t Training Loss 0.8508\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [28150][55000]\t Training Loss 0.8506\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [28200][55000]\t Training Loss 0.8507\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [28250][55000]\t Training Loss 0.8503\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [28300][55000]\t Training Loss 0.8502\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [28350][55000]\t Training Loss 0.8500\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [28400][55000]\t Training Loss 0.8504\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [28450][55000]\t Training Loss 0.8501\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [28500][55000]\t Training Loss 0.8500\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [28550][55000]\t Training Loss 0.8499\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [28600][55000]\t Training Loss 0.8498\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [28650][55000]\t Training Loss 0.8501\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [28700][55000]\t Training Loss 0.8503\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [28750][55000]\t Training Loss 0.8504\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [28800][55000]\t Training Loss 0.8504\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [28850][55000]\t Training Loss 0.8503\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [28900][55000]\t Training Loss 0.8502\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [28950][55000]\t Training Loss 0.8502\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [29000][55000]\t Training Loss 0.8501\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [29050][55000]\t Training Loss 0.8501\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [29100][55000]\t Training Loss 0.8502\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [29150][55000]\t Training Loss 0.8506\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [29200][55000]\t Training Loss 0.8510\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [29250][55000]\t Training Loss 0.8512\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [29300][55000]\t Training Loss 0.8510\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [29350][55000]\t Training Loss 0.8511\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [29400][55000]\t Training Loss 0.8510\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [29450][55000]\t Training Loss 0.8506\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [29500][55000]\t Training Loss 0.8504\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [29550][55000]\t Training Loss 0.8504\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [29600][55000]\t Training Loss 0.8503\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [29650][55000]\t Training Loss 0.8503\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [29700][55000]\t Training Loss 0.8504\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [29750][55000]\t Training Loss 0.8507\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [29800][55000]\t Training Loss 0.8509\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [29850][55000]\t Training Loss 0.8512\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [29900][55000]\t Training Loss 0.8515\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [29950][55000]\t Training Loss 0.8518\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [30000][55000]\t Training Loss 0.8521\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [30050][55000]\t Training Loss 0.8522\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [30100][55000]\t Training Loss 0.8526\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [30150][55000]\t Training Loss 0.8531\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [30200][55000]\t Training Loss 0.8533\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [30250][55000]\t Training Loss 0.8534\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [30300][55000]\t Training Loss 0.8532\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [30350][55000]\t Training Loss 0.8532\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [30400][55000]\t Training Loss 0.8530\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [30450][55000]\t Training Loss 0.8531\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [30500][55000]\t Training Loss 0.8533\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [30550][55000]\t Training Loss 0.8536\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [30600][55000]\t Training Loss 0.8537\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [30650][55000]\t Training Loss 0.8541\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [30700][55000]\t Training Loss 0.8544\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [30750][55000]\t Training Loss 0.8545\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [30800][55000]\t Training Loss 0.8547\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [30850][55000]\t Training Loss 0.8546\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [30900][55000]\t Training Loss 0.8550\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [30950][55000]\t Training Loss 0.8549\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [31000][55000]\t Training Loss 0.8549\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [31050][55000]\t Training Loss 0.8550\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [31100][55000]\t Training Loss 0.8550\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [31150][55000]\t Training Loss 0.8550\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [31200][55000]\t Training Loss 0.8551\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [31250][55000]\t Training Loss 0.8550\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [31300][55000]\t Training Loss 0.8553\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [31350][55000]\t Training Loss 0.8560\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [31400][55000]\t Training Loss 0.8561\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [31450][55000]\t Training Loss 0.8565\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [31500][55000]\t Training Loss 0.8565\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [31550][55000]\t Training Loss 0.8565\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [31600][55000]\t Training Loss 0.8567\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [31650][55000]\t Training Loss 0.8569\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [31700][55000]\t Training Loss 0.8571\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [31750][55000]\t Training Loss 0.8576\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [31800][55000]\t Training Loss 0.8577\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [31850][55000]\t Training Loss 0.8576\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [31900][55000]\t Training Loss 0.8576\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [31950][55000]\t Training Loss 0.8576\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [32000][55000]\t Training Loss 0.8576\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [32050][55000]\t Training Loss 0.8576\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [32100][55000]\t Training Loss 0.8574\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [32150][55000]\t Training Loss 0.8577\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [32200][55000]\t Training Loss 0.8579\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [32250][55000]\t Training Loss 0.8582\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [32300][55000]\t Training Loss 0.8585\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [32350][55000]\t Training Loss 0.8588\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [32400][55000]\t Training Loss 0.8589\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [32450][55000]\t Training Loss 0.8591\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [32500][55000]\t Training Loss 0.8593\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [32550][55000]\t Training Loss 0.8596\t Accuracy 0.8465\n",
      "Epoch [1][20]\t Batch [32600][55000]\t Training Loss 0.8596\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [32650][55000]\t Training Loss 0.8594\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [32700][55000]\t Training Loss 0.8594\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [32750][55000]\t Training Loss 0.8595\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [32800][55000]\t Training Loss 0.8596\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [32850][55000]\t Training Loss 0.8596\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [32900][55000]\t Training Loss 0.8594\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [32950][55000]\t Training Loss 0.8593\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33000][55000]\t Training Loss 0.8592\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33050][55000]\t Training Loss 0.8593\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33100][55000]\t Training Loss 0.8593\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33150][55000]\t Training Loss 0.8594\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33200][55000]\t Training Loss 0.8593\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [33250][55000]\t Training Loss 0.8595\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33300][55000]\t Training Loss 0.8594\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33350][55000]\t Training Loss 0.8595\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33400][55000]\t Training Loss 0.8596\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33450][55000]\t Training Loss 0.8597\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [33500][55000]\t Training Loss 0.8596\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [33550][55000]\t Training Loss 0.8596\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [33600][55000]\t Training Loss 0.8596\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [33650][55000]\t Training Loss 0.8596\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33700][55000]\t Training Loss 0.8595\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [33750][55000]\t Training Loss 0.8593\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [33800][55000]\t Training Loss 0.8593\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [33850][55000]\t Training Loss 0.8590\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [33900][55000]\t Training Loss 0.8587\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [33950][55000]\t Training Loss 0.8583\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [34000][55000]\t Training Loss 0.8583\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [34050][55000]\t Training Loss 0.8585\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [34100][55000]\t Training Loss 0.8586\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [34150][55000]\t Training Loss 0.8587\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [34200][55000]\t Training Loss 0.8584\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [34250][55000]\t Training Loss 0.8582\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [34300][55000]\t Training Loss 0.8581\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [34350][55000]\t Training Loss 0.8580\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [34400][55000]\t Training Loss 0.8579\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [34450][55000]\t Training Loss 0.8580\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [34500][55000]\t Training Loss 0.8581\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [34550][55000]\t Training Loss 0.8580\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [34600][55000]\t Training Loss 0.8580\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [34650][55000]\t Training Loss 0.8580\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [34700][55000]\t Training Loss 0.8581\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [34750][55000]\t Training Loss 0.8583\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [34800][55000]\t Training Loss 0.8583\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [34850][55000]\t Training Loss 0.8587\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [34900][55000]\t Training Loss 0.8587\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [34950][55000]\t Training Loss 0.8588\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [35000][55000]\t Training Loss 0.8587\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [35050][55000]\t Training Loss 0.8585\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [35100][55000]\t Training Loss 0.8586\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [35150][55000]\t Training Loss 0.8586\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [35200][55000]\t Training Loss 0.8587\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [35250][55000]\t Training Loss 0.8588\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [35300][55000]\t Training Loss 0.8589\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [35350][55000]\t Training Loss 0.8587\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [35400][55000]\t Training Loss 0.8587\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [35450][55000]\t Training Loss 0.8586\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [35500][55000]\t Training Loss 0.8586\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [35550][55000]\t Training Loss 0.8584\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [35600][55000]\t Training Loss 0.8583\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [35650][55000]\t Training Loss 0.8587\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [35700][55000]\t Training Loss 0.8586\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [35750][55000]\t Training Loss 0.8585\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [35800][55000]\t Training Loss 0.8585\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [35850][55000]\t Training Loss 0.8583\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [35900][55000]\t Training Loss 0.8583\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [35950][55000]\t Training Loss 0.8583\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [36000][55000]\t Training Loss 0.8584\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [36050][55000]\t Training Loss 0.8586\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [36100][55000]\t Training Loss 0.8588\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [36150][55000]\t Training Loss 0.8587\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [36200][55000]\t Training Loss 0.8585\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [36250][55000]\t Training Loss 0.8584\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [36300][55000]\t Training Loss 0.8581\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [36350][55000]\t Training Loss 0.8580\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [36400][55000]\t Training Loss 0.8579\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [36450][55000]\t Training Loss 0.8580\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [36500][55000]\t Training Loss 0.8582\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [36550][55000]\t Training Loss 0.8581\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [36600][55000]\t Training Loss 0.8578\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [36650][55000]\t Training Loss 0.8577\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [36700][55000]\t Training Loss 0.8574\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [36750][55000]\t Training Loss 0.8572\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [36800][55000]\t Training Loss 0.8571\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [36850][55000]\t Training Loss 0.8571\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [36900][55000]\t Training Loss 0.8571\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [36950][55000]\t Training Loss 0.8570\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [37000][55000]\t Training Loss 0.8568\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [37050][55000]\t Training Loss 0.8567\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [37100][55000]\t Training Loss 0.8568\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [37150][55000]\t Training Loss 0.8568\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [37200][55000]\t Training Loss 0.8567\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [37250][55000]\t Training Loss 0.8566\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [37300][55000]\t Training Loss 0.8566\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [37350][55000]\t Training Loss 0.8567\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [37400][55000]\t Training Loss 0.8570\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [37450][55000]\t Training Loss 0.8573\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [37500][55000]\t Training Loss 0.8574\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [37550][55000]\t Training Loss 0.8575\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [37600][55000]\t Training Loss 0.8577\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [37650][55000]\t Training Loss 0.8577\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [37700][55000]\t Training Loss 0.8577\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [37750][55000]\t Training Loss 0.8577\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [37800][55000]\t Training Loss 0.8577\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [37850][55000]\t Training Loss 0.8577\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [37900][55000]\t Training Loss 0.8578\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [37950][55000]\t Training Loss 0.8578\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [38000][55000]\t Training Loss 0.8578\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [38050][55000]\t Training Loss 0.8578\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [38100][55000]\t Training Loss 0.8579\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [38150][55000]\t Training Loss 0.8578\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [38200][55000]\t Training Loss 0.8577\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [38250][55000]\t Training Loss 0.8577\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [38300][55000]\t Training Loss 0.8578\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [38350][55000]\t Training Loss 0.8579\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [38400][55000]\t Training Loss 0.8580\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [38450][55000]\t Training Loss 0.8580\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [38500][55000]\t Training Loss 0.8578\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [38550][55000]\t Training Loss 0.8579\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [38600][55000]\t Training Loss 0.8581\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [38650][55000]\t Training Loss 0.8582\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [38700][55000]\t Training Loss 0.8582\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [38750][55000]\t Training Loss 0.8581\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [38800][55000]\t Training Loss 0.8581\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [38850][55000]\t Training Loss 0.8580\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [38900][55000]\t Training Loss 0.8578\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [38950][55000]\t Training Loss 0.8576\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [39000][55000]\t Training Loss 0.8576\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [39050][55000]\t Training Loss 0.8575\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [39100][55000]\t Training Loss 0.8572\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [39150][55000]\t Training Loss 0.8570\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [39200][55000]\t Training Loss 0.8569\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [39250][55000]\t Training Loss 0.8567\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [39300][55000]\t Training Loss 0.8567\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [39350][55000]\t Training Loss 0.8570\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [39400][55000]\t Training Loss 0.8570\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [39450][55000]\t Training Loss 0.8573\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [39500][55000]\t Training Loss 0.8573\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [39550][55000]\t Training Loss 0.8574\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [39600][55000]\t Training Loss 0.8573\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [39650][55000]\t Training Loss 0.8573\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [39700][55000]\t Training Loss 0.8573\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [39750][55000]\t Training Loss 0.8575\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [39800][55000]\t Training Loss 0.8577\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [39850][55000]\t Training Loss 0.8578\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [39900][55000]\t Training Loss 0.8579\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [39950][55000]\t Training Loss 0.8581\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40000][55000]\t Training Loss 0.8581\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40050][55000]\t Training Loss 0.8581\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [40100][55000]\t Training Loss 0.8580\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [40150][55000]\t Training Loss 0.8580\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [40200][55000]\t Training Loss 0.8579\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40250][55000]\t Training Loss 0.8579\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [40300][55000]\t Training Loss 0.8579\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [40350][55000]\t Training Loss 0.8578\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40400][55000]\t Training Loss 0.8578\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40450][55000]\t Training Loss 0.8577\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40500][55000]\t Training Loss 0.8578\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [40550][55000]\t Training Loss 0.8578\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40600][55000]\t Training Loss 0.8578\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40650][55000]\t Training Loss 0.8577\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [40700][55000]\t Training Loss 0.8578\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [40750][55000]\t Training Loss 0.8577\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40800][55000]\t Training Loss 0.8578\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40850][55000]\t Training Loss 0.8576\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [40900][55000]\t Training Loss 0.8574\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [40950][55000]\t Training Loss 0.8573\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [41000][55000]\t Training Loss 0.8573\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [41050][55000]\t Training Loss 0.8574\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [41100][55000]\t Training Loss 0.8575\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [41150][55000]\t Training Loss 0.8576\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [41200][55000]\t Training Loss 0.8575\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [41250][55000]\t Training Loss 0.8574\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [41300][55000]\t Training Loss 0.8577\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [41350][55000]\t Training Loss 0.8579\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [41400][55000]\t Training Loss 0.8581\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [41450][55000]\t Training Loss 0.8582\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [41500][55000]\t Training Loss 0.8584\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [41550][55000]\t Training Loss 0.8586\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [41600][55000]\t Training Loss 0.8586\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [41650][55000]\t Training Loss 0.8586\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [41700][55000]\t Training Loss 0.8585\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [41750][55000]\t Training Loss 0.8587\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [41800][55000]\t Training Loss 0.8589\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [41850][55000]\t Training Loss 0.8589\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [41900][55000]\t Training Loss 0.8589\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [41950][55000]\t Training Loss 0.8589\t Accuracy 0.8481\n",
      "Epoch [1][20]\t Batch [42000][55000]\t Training Loss 0.8588\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [42050][55000]\t Training Loss 0.8588\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [42100][55000]\t Training Loss 0.8587\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [42150][55000]\t Training Loss 0.8588\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [42200][55000]\t Training Loss 0.8588\t Accuracy 0.8479\n",
      "Epoch [1][20]\t Batch [42250][55000]\t Training Loss 0.8589\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [42300][55000]\t Training Loss 0.8589\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [42350][55000]\t Training Loss 0.8591\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [42400][55000]\t Training Loss 0.8593\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [42450][55000]\t Training Loss 0.8594\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [42500][55000]\t Training Loss 0.8596\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [42550][55000]\t Training Loss 0.8598\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [42600][55000]\t Training Loss 0.8596\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [42650][55000]\t Training Loss 0.8594\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [42700][55000]\t Training Loss 0.8594\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [42750][55000]\t Training Loss 0.8594\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [42800][55000]\t Training Loss 0.8593\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [42850][55000]\t Training Loss 0.8592\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [42900][55000]\t Training Loss 0.8593\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [42950][55000]\t Training Loss 0.8593\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [43000][55000]\t Training Loss 0.8595\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [43050][55000]\t Training Loss 0.8597\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [43100][55000]\t Training Loss 0.8599\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [43150][55000]\t Training Loss 0.8599\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [43200][55000]\t Training Loss 0.8598\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [43250][55000]\t Training Loss 0.8599\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [43300][55000]\t Training Loss 0.8598\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [43350][55000]\t Training Loss 0.8596\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [43400][55000]\t Training Loss 0.8594\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [43450][55000]\t Training Loss 0.8593\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [43500][55000]\t Training Loss 0.8590\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [43550][55000]\t Training Loss 0.8591\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [43600][55000]\t Training Loss 0.8590\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [43650][55000]\t Training Loss 0.8588\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [43700][55000]\t Training Loss 0.8588\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [43750][55000]\t Training Loss 0.8588\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [43800][55000]\t Training Loss 0.8587\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [43850][55000]\t Training Loss 0.8588\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [43900][55000]\t Training Loss 0.8589\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [43950][55000]\t Training Loss 0.8590\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [44000][55000]\t Training Loss 0.8590\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [44050][55000]\t Training Loss 0.8591\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [44100][55000]\t Training Loss 0.8591\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [44150][55000]\t Training Loss 0.8593\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [44200][55000]\t Training Loss 0.8594\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [44250][55000]\t Training Loss 0.8594\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [44300][55000]\t Training Loss 0.8595\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [44350][55000]\t Training Loss 0.8596\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [44400][55000]\t Training Loss 0.8597\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [44450][55000]\t Training Loss 0.8598\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [44500][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [44550][55000]\t Training Loss 0.8595\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [44600][55000]\t Training Loss 0.8594\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [44650][55000]\t Training Loss 0.8593\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [44700][55000]\t Training Loss 0.8592\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [44750][55000]\t Training Loss 0.8592\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [44800][55000]\t Training Loss 0.8592\t Accuracy 0.8477\n",
      "Epoch [1][20]\t Batch [44850][55000]\t Training Loss 0.8593\t Accuracy 0.8476\n",
      "Epoch [1][20]\t Batch [44900][55000]\t Training Loss 0.8594\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [44950][55000]\t Training Loss 0.8594\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [45000][55000]\t Training Loss 0.8594\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [45050][55000]\t Training Loss 0.8595\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [45100][55000]\t Training Loss 0.8596\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [45150][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45200][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45250][55000]\t Training Loss 0.8597\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [45300][55000]\t Training Loss 0.8596\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [45350][55000]\t Training Loss 0.8595\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [45400][55000]\t Training Loss 0.8594\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [45450][55000]\t Training Loss 0.8596\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [45500][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45550][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45600][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45650][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45700][55000]\t Training Loss 0.8596\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45750][55000]\t Training Loss 0.8596\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45800][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45850][55000]\t Training Loss 0.8598\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45900][55000]\t Training Loss 0.8599\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [45950][55000]\t Training Loss 0.8599\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [46000][55000]\t Training Loss 0.8600\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [46050][55000]\t Training Loss 0.8601\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46100][55000]\t Training Loss 0.8603\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46150][55000]\t Training Loss 0.8603\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46200][55000]\t Training Loss 0.8603\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46250][55000]\t Training Loss 0.8603\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46300][55000]\t Training Loss 0.8604\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [46350][55000]\t Training Loss 0.8605\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [46400][55000]\t Training Loss 0.8606\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [46450][55000]\t Training Loss 0.8607\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [46500][55000]\t Training Loss 0.8606\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [46550][55000]\t Training Loss 0.8605\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46600][55000]\t Training Loss 0.8604\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46650][55000]\t Training Loss 0.8604\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46700][55000]\t Training Loss 0.8603\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46750][55000]\t Training Loss 0.8604\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46800][55000]\t Training Loss 0.8602\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [46850][55000]\t Training Loss 0.8601\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [46900][55000]\t Training Loss 0.8600\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [46950][55000]\t Training Loss 0.8599\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47000][55000]\t Training Loss 0.8598\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47050][55000]\t Training Loss 0.8598\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47100][55000]\t Training Loss 0.8597\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [47150][55000]\t Training Loss 0.8596\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [47200][55000]\t Training Loss 0.8596\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47250][55000]\t Training Loss 0.8597\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [47300][55000]\t Training Loss 0.8598\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [47350][55000]\t Training Loss 0.8599\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [47400][55000]\t Training Loss 0.8599\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [47450][55000]\t Training Loss 0.8599\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [47500][55000]\t Training Loss 0.8600\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47550][55000]\t Training Loss 0.8600\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47600][55000]\t Training Loss 0.8599\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [47650][55000]\t Training Loss 0.8601\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47700][55000]\t Training Loss 0.8600\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [47750][55000]\t Training Loss 0.8600\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [47800][55000]\t Training Loss 0.8599\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [47850][55000]\t Training Loss 0.8597\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47900][55000]\t Training Loss 0.8597\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [47950][55000]\t Training Loss 0.8598\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [48000][55000]\t Training Loss 0.8598\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [48050][55000]\t Training Loss 0.8597\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [48100][55000]\t Training Loss 0.8597\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [48150][55000]\t Training Loss 0.8594\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [48200][55000]\t Training Loss 0.8592\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [48250][55000]\t Training Loss 0.8591\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [48300][55000]\t Training Loss 0.8590\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [48350][55000]\t Training Loss 0.8589\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [48400][55000]\t Training Loss 0.8590\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [48450][55000]\t Training Loss 0.8588\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [48500][55000]\t Training Loss 0.8586\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [48550][55000]\t Training Loss 0.8586\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [48600][55000]\t Training Loss 0.8586\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [48650][55000]\t Training Loss 0.8585\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [48700][55000]\t Training Loss 0.8584\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [48750][55000]\t Training Loss 0.8582\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [48800][55000]\t Training Loss 0.8581\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [48850][55000]\t Training Loss 0.8580\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [48900][55000]\t Training Loss 0.8579\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [48950][55000]\t Training Loss 0.8581\t Accuracy 0.8474\n",
      "Epoch [1][20]\t Batch [49000][55000]\t Training Loss 0.8583\t Accuracy 0.8472\n",
      "Epoch [1][20]\t Batch [49050][55000]\t Training Loss 0.8586\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [49100][55000]\t Training Loss 0.8587\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [49150][55000]\t Training Loss 0.8588\t Accuracy 0.8470\n",
      "Epoch [1][20]\t Batch [49200][55000]\t Training Loss 0.8588\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [49250][55000]\t Training Loss 0.8590\t Accuracy 0.8467\n",
      "Epoch [1][20]\t Batch [49300][55000]\t Training Loss 0.8589\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [49350][55000]\t Training Loss 0.8587\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [49400][55000]\t Training Loss 0.8586\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [49450][55000]\t Training Loss 0.8585\t Accuracy 0.8468\n",
      "Epoch [1][20]\t Batch [49500][55000]\t Training Loss 0.8588\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [49550][55000]\t Training Loss 0.8591\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [49600][55000]\t Training Loss 0.8594\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [49650][55000]\t Training Loss 0.8594\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [49700][55000]\t Training Loss 0.8597\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [49750][55000]\t Training Loss 0.8597\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [49800][55000]\t Training Loss 0.8596\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [49850][55000]\t Training Loss 0.8597\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [49900][55000]\t Training Loss 0.8599\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [49950][55000]\t Training Loss 0.8599\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [50000][55000]\t Training Loss 0.8600\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [50050][55000]\t Training Loss 0.8600\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [50100][55000]\t Training Loss 0.8601\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [50150][55000]\t Training Loss 0.8600\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [50200][55000]\t Training Loss 0.8599\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [50250][55000]\t Training Loss 0.8601\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [50300][55000]\t Training Loss 0.8600\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [50350][55000]\t Training Loss 0.8601\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [50400][55000]\t Training Loss 0.8603\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [50450][55000]\t Training Loss 0.8606\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [50500][55000]\t Training Loss 0.8606\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [50550][55000]\t Training Loss 0.8606\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [50600][55000]\t Training Loss 0.8607\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [50650][55000]\t Training Loss 0.8608\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [50700][55000]\t Training Loss 0.8607\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [50750][55000]\t Training Loss 0.8608\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [50800][55000]\t Training Loss 0.8608\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [50850][55000]\t Training Loss 0.8608\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [50900][55000]\t Training Loss 0.8607\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [50950][55000]\t Training Loss 0.8606\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [51000][55000]\t Training Loss 0.8604\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [51050][55000]\t Training Loss 0.8603\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [51100][55000]\t Training Loss 0.8602\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [51150][55000]\t Training Loss 0.8602\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [51200][55000]\t Training Loss 0.8603\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [51250][55000]\t Training Loss 0.8603\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [51300][55000]\t Training Loss 0.8604\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [51350][55000]\t Training Loss 0.8604\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [51400][55000]\t Training Loss 0.8603\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [51450][55000]\t Training Loss 0.8602\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [51500][55000]\t Training Loss 0.8601\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [51550][55000]\t Training Loss 0.8600\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [51600][55000]\t Training Loss 0.8598\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [51650][55000]\t Training Loss 0.8598\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [51700][55000]\t Training Loss 0.8598\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [51750][55000]\t Training Loss 0.8598\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [51800][55000]\t Training Loss 0.8599\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [51850][55000]\t Training Loss 0.8598\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [51900][55000]\t Training Loss 0.8597\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [51950][55000]\t Training Loss 0.8596\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [52000][55000]\t Training Loss 0.8597\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [52050][55000]\t Training Loss 0.8597\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [52100][55000]\t Training Loss 0.8599\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [52150][55000]\t Training Loss 0.8602\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [52200][55000]\t Training Loss 0.8603\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [52250][55000]\t Training Loss 0.8605\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [52300][55000]\t Training Loss 0.8605\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [52350][55000]\t Training Loss 0.8605\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [52400][55000]\t Training Loss 0.8605\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [52450][55000]\t Training Loss 0.8603\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [52500][55000]\t Training Loss 0.8601\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [52550][55000]\t Training Loss 0.8600\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [52600][55000]\t Training Loss 0.8598\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [52650][55000]\t Training Loss 0.8596\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [52700][55000]\t Training Loss 0.8597\t Accuracy 0.8463\n",
      "Epoch [1][20]\t Batch [52750][55000]\t Training Loss 0.8598\t Accuracy 0.8462\n",
      "Epoch [1][20]\t Batch [52800][55000]\t Training Loss 0.8600\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [52850][55000]\t Training Loss 0.8600\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [52900][55000]\t Training Loss 0.8602\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [52950][55000]\t Training Loss 0.8604\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [53000][55000]\t Training Loss 0.8604\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [53050][55000]\t Training Loss 0.8605\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53100][55000]\t Training Loss 0.8604\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53150][55000]\t Training Loss 0.8604\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [53200][55000]\t Training Loss 0.8604\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53250][55000]\t Training Loss 0.8605\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53300][55000]\t Training Loss 0.8605\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53350][55000]\t Training Loss 0.8604\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53400][55000]\t Training Loss 0.8603\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [53450][55000]\t Training Loss 0.8602\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [53500][55000]\t Training Loss 0.8602\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [53550][55000]\t Training Loss 0.8602\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53600][55000]\t Training Loss 0.8604\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [53650][55000]\t Training Loss 0.8603\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53700][55000]\t Training Loss 0.8603\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53750][55000]\t Training Loss 0.8603\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53800][55000]\t Training Loss 0.8602\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [53850][55000]\t Training Loss 0.8602\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [53900][55000]\t Training Loss 0.8601\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [53950][55000]\t Training Loss 0.8602\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [54000][55000]\t Training Loss 0.8603\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [54050][55000]\t Training Loss 0.8604\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [54100][55000]\t Training Loss 0.8606\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [54150][55000]\t Training Loss 0.8605\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [54200][55000]\t Training Loss 0.8605\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [54250][55000]\t Training Loss 0.8604\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [54300][55000]\t Training Loss 0.8604\t Accuracy 0.8457\n",
      "Epoch [1][20]\t Batch [54350][55000]\t Training Loss 0.8603\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [54400][55000]\t Training Loss 0.8603\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [54450][55000]\t Training Loss 0.8602\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [54500][55000]\t Training Loss 0.8601\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [54550][55000]\t Training Loss 0.8600\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [54600][55000]\t Training Loss 0.8600\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [54650][55000]\t Training Loss 0.8599\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [54700][55000]\t Training Loss 0.8597\t Accuracy 0.8458\n",
      "Epoch [1][20]\t Batch [54750][55000]\t Training Loss 0.8596\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [54800][55000]\t Training Loss 0.8596\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [54850][55000]\t Training Loss 0.8594\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [54900][55000]\t Training Loss 0.8596\t Accuracy 0.8460\n",
      "Epoch [1][20]\t Batch [54950][55000]\t Training Loss 0.8599\t Accuracy 0.8459\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8601\t Average training accuracy 0.8458\n",
      "Epoch [1]\t Average validation loss 0.7965\t Average validation accuracy 0.8718\n",
      "\n",
      "Epoch [2][20]\t Batch [0][55000]\t Training Loss 1.4606\t Accuracy 0.0000\n",
      "Epoch [2][20]\t Batch [50][55000]\t Training Loss 0.9448\t Accuracy 0.7255\n",
      "Epoch [2][20]\t Batch [100][55000]\t Training Loss 0.8532\t Accuracy 0.8020\n",
      "Epoch [2][20]\t Batch [150][55000]\t Training Loss 0.8548\t Accuracy 0.8146\n",
      "Epoch [2][20]\t Batch [200][55000]\t Training Loss 0.8650\t Accuracy 0.8159\n",
      "Epoch [2][20]\t Batch [250][55000]\t Training Loss 0.8481\t Accuracy 0.8247\n",
      "Epoch [2][20]\t Batch [300][55000]\t Training Loss 0.8484\t Accuracy 0.8239\n",
      "Epoch [2][20]\t Batch [350][55000]\t Training Loss 0.8290\t Accuracy 0.8262\n",
      "Epoch [2][20]\t Batch [400][55000]\t Training Loss 0.8152\t Accuracy 0.8304\n",
      "Epoch [2][20]\t Batch [450][55000]\t Training Loss 0.8208\t Accuracy 0.8337\n",
      "Epoch [2][20]\t Batch [500][55000]\t Training Loss 0.8270\t Accuracy 0.8363\n",
      "Epoch [2][20]\t Batch [550][55000]\t Training Loss 0.8439\t Accuracy 0.8330\n",
      "Epoch [2][20]\t Batch [600][55000]\t Training Loss 0.8484\t Accuracy 0.8369\n",
      "Epoch [2][20]\t Batch [650][55000]\t Training Loss 0.8682\t Accuracy 0.8264\n",
      "Epoch [2][20]\t Batch [700][55000]\t Training Loss 0.8658\t Accuracy 0.8317\n",
      "Epoch [2][20]\t Batch [750][55000]\t Training Loss 0.8621\t Accuracy 0.8336\n",
      "Epoch [2][20]\t Batch [800][55000]\t Training Loss 0.8562\t Accuracy 0.8352\n",
      "Epoch [2][20]\t Batch [850][55000]\t Training Loss 0.8544\t Accuracy 0.8367\n",
      "Epoch [2][20]\t Batch [900][55000]\t Training Loss 0.8610\t Accuracy 0.8357\n",
      "Epoch [2][20]\t Batch [950][55000]\t Training Loss 0.8679\t Accuracy 0.8307\n",
      "Epoch [2][20]\t Batch [1000][55000]\t Training Loss 0.8674\t Accuracy 0.8332\n",
      "Epoch [2][20]\t Batch [1050][55000]\t Training Loss 0.8751\t Accuracy 0.8316\n",
      "Epoch [2][20]\t Batch [1100][55000]\t Training Loss 0.8835\t Accuracy 0.8292\n",
      "Epoch [2][20]\t Batch [1150][55000]\t Training Loss 0.8928\t Accuracy 0.8262\n",
      "Epoch [2][20]\t Batch [1200][55000]\t Training Loss 0.8869\t Accuracy 0.8293\n",
      "Epoch [2][20]\t Batch [1250][55000]\t Training Loss 0.8882\t Accuracy 0.8289\n",
      "Epoch [2][20]\t Batch [1300][55000]\t Training Loss 0.8885\t Accuracy 0.8286\n",
      "Epoch [2][20]\t Batch [1350][55000]\t Training Loss 0.8853\t Accuracy 0.8298\n",
      "Epoch [2][20]\t Batch [1400][55000]\t Training Loss 0.8858\t Accuracy 0.8287\n",
      "Epoch [2][20]\t Batch [1450][55000]\t Training Loss 0.8869\t Accuracy 0.8277\n",
      "Epoch [2][20]\t Batch [1500][55000]\t Training Loss 0.8849\t Accuracy 0.8308\n",
      "Epoch [2][20]\t Batch [1550][55000]\t Training Loss 0.8845\t Accuracy 0.8311\n",
      "Epoch [2][20]\t Batch [1600][55000]\t Training Loss 0.8871\t Accuracy 0.8301\n",
      "Epoch [2][20]\t Batch [1650][55000]\t Training Loss 0.8841\t Accuracy 0.8316\n",
      "Epoch [2][20]\t Batch [1700][55000]\t Training Loss 0.8824\t Accuracy 0.8330\n",
      "Epoch [2][20]\t Batch [1750][55000]\t Training Loss 0.8768\t Accuracy 0.8338\n",
      "Epoch [2][20]\t Batch [1800][55000]\t Training Loss 0.8746\t Accuracy 0.8356\n",
      "Epoch [2][20]\t Batch [1850][55000]\t Training Loss 0.8730\t Accuracy 0.8363\n",
      "Epoch [2][20]\t Batch [1900][55000]\t Training Loss 0.8709\t Accuracy 0.8364\n",
      "Epoch [2][20]\t Batch [1950][55000]\t Training Loss 0.8693\t Accuracy 0.8375\n",
      "Epoch [2][20]\t Batch [2000][55000]\t Training Loss 0.8669\t Accuracy 0.8391\n",
      "Epoch [2][20]\t Batch [2050][55000]\t Training Loss 0.8660\t Accuracy 0.8396\n",
      "Epoch [2][20]\t Batch [2100][55000]\t Training Loss 0.8632\t Accuracy 0.8401\n",
      "Epoch [2][20]\t Batch [2150][55000]\t Training Loss 0.8600\t Accuracy 0.8419\n",
      "Epoch [2][20]\t Batch [2200][55000]\t Training Loss 0.8552\t Accuracy 0.8437\n",
      "Epoch [2][20]\t Batch [2250][55000]\t Training Loss 0.8543\t Accuracy 0.8441\n",
      "Epoch [2][20]\t Batch [2300][55000]\t Training Loss 0.8513\t Accuracy 0.8435\n",
      "Epoch [2][20]\t Batch [2350][55000]\t Training Loss 0.8498\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [2400][55000]\t Training Loss 0.8507\t Accuracy 0.8421\n",
      "Epoch [2][20]\t Batch [2450][55000]\t Training Loss 0.8535\t Accuracy 0.8405\n",
      "Epoch [2][20]\t Batch [2500][55000]\t Training Loss 0.8513\t Accuracy 0.8425\n",
      "Epoch [2][20]\t Batch [2550][55000]\t Training Loss 0.8500\t Accuracy 0.8424\n",
      "Epoch [2][20]\t Batch [2600][55000]\t Training Loss 0.8493\t Accuracy 0.8420\n",
      "Epoch [2][20]\t Batch [2650][55000]\t Training Loss 0.8482\t Accuracy 0.8423\n",
      "Epoch [2][20]\t Batch [2700][55000]\t Training Loss 0.8479\t Accuracy 0.8427\n",
      "Epoch [2][20]\t Batch [2750][55000]\t Training Loss 0.8478\t Accuracy 0.8426\n",
      "Epoch [2][20]\t Batch [2800][55000]\t Training Loss 0.8480\t Accuracy 0.8411\n",
      "Epoch [2][20]\t Batch [2850][55000]\t Training Loss 0.8469\t Accuracy 0.8408\n",
      "Epoch [2][20]\t Batch [2900][55000]\t Training Loss 0.8437\t Accuracy 0.8418\n",
      "Epoch [2][20]\t Batch [2950][55000]\t Training Loss 0.8438\t Accuracy 0.8411\n",
      "Epoch [2][20]\t Batch [3000][55000]\t Training Loss 0.8436\t Accuracy 0.8417\n",
      "Epoch [2][20]\t Batch [3050][55000]\t Training Loss 0.8447\t Accuracy 0.8414\n",
      "Epoch [2][20]\t Batch [3100][55000]\t Training Loss 0.8469\t Accuracy 0.8410\n",
      "Epoch [2][20]\t Batch [3150][55000]\t Training Loss 0.8468\t Accuracy 0.8413\n",
      "Epoch [2][20]\t Batch [3200][55000]\t Training Loss 0.8466\t Accuracy 0.8425\n",
      "Epoch [2][20]\t Batch [3250][55000]\t Training Loss 0.8460\t Accuracy 0.8419\n",
      "Epoch [2][20]\t Batch [3300][55000]\t Training Loss 0.8473\t Accuracy 0.8416\n",
      "Epoch [2][20]\t Batch [3350][55000]\t Training Loss 0.8455\t Accuracy 0.8430\n",
      "Epoch [2][20]\t Batch [3400][55000]\t Training Loss 0.8478\t Accuracy 0.8415\n",
      "Epoch [2][20]\t Batch [3450][55000]\t Training Loss 0.8474\t Accuracy 0.8424\n",
      "Epoch [2][20]\t Batch [3500][55000]\t Training Loss 0.8474\t Accuracy 0.8420\n",
      "Epoch [2][20]\t Batch [3550][55000]\t Training Loss 0.8489\t Accuracy 0.8415\n",
      "Epoch [2][20]\t Batch [3600][55000]\t Training Loss 0.8488\t Accuracy 0.8409\n",
      "Epoch [2][20]\t Batch [3650][55000]\t Training Loss 0.8474\t Accuracy 0.8417\n",
      "Epoch [2][20]\t Batch [3700][55000]\t Training Loss 0.8481\t Accuracy 0.8414\n",
      "Epoch [2][20]\t Batch [3750][55000]\t Training Loss 0.8480\t Accuracy 0.8416\n",
      "Epoch [2][20]\t Batch [3800][55000]\t Training Loss 0.8483\t Accuracy 0.8416\n",
      "Epoch [2][20]\t Batch [3850][55000]\t Training Loss 0.8479\t Accuracy 0.8419\n",
      "Epoch [2][20]\t Batch [3900][55000]\t Training Loss 0.8468\t Accuracy 0.8431\n",
      "Epoch [2][20]\t Batch [3950][55000]\t Training Loss 0.8457\t Accuracy 0.8443\n",
      "Epoch [2][20]\t Batch [4000][55000]\t Training Loss 0.8454\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [4050][55000]\t Training Loss 0.8439\t Accuracy 0.8457\n",
      "Epoch [2][20]\t Batch [4100][55000]\t Training Loss 0.8443\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [4150][55000]\t Training Loss 0.8447\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [4200][55000]\t Training Loss 0.8452\t Accuracy 0.8441\n",
      "Epoch [2][20]\t Batch [4250][55000]\t Training Loss 0.8438\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [4300][55000]\t Training Loss 0.8438\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [4350][55000]\t Training Loss 0.8443\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [4400][55000]\t Training Loss 0.8443\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [4450][55000]\t Training Loss 0.8444\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [4500][55000]\t Training Loss 0.8447\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [4550][55000]\t Training Loss 0.8431\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [4600][55000]\t Training Loss 0.8414\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [4650][55000]\t Training Loss 0.8412\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [4700][55000]\t Training Loss 0.8408\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [4750][55000]\t Training Loss 0.8397\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [4800][55000]\t Training Loss 0.8404\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [4850][55000]\t Training Loss 0.8416\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [4900][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [4950][55000]\t Training Loss 0.8404\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [5000][55000]\t Training Loss 0.8405\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [5050][55000]\t Training Loss 0.8400\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [5100][55000]\t Training Loss 0.8400\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [5150][55000]\t Training Loss 0.8403\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [5200][55000]\t Training Loss 0.8418\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [5250][55000]\t Training Loss 0.8408\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [5300][55000]\t Training Loss 0.8407\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [5350][55000]\t Training Loss 0.8413\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [5400][55000]\t Training Loss 0.8405\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [5450][55000]\t Training Loss 0.8399\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [5500][55000]\t Training Loss 0.8380\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [5550][55000]\t Training Loss 0.8380\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [5600][55000]\t Training Loss 0.8370\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [5650][55000]\t Training Loss 0.8376\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [5700][55000]\t Training Loss 0.8373\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [5750][55000]\t Training Loss 0.8377\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [5800][55000]\t Training Loss 0.8381\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [5850][55000]\t Training Loss 0.8383\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [5900][55000]\t Training Loss 0.8388\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [5950][55000]\t Training Loss 0.8387\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [6000][55000]\t Training Loss 0.8377\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [6050][55000]\t Training Loss 0.8363\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [6100][55000]\t Training Loss 0.8348\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [6150][55000]\t Training Loss 0.8327\t Accuracy 0.8483\n",
      "Epoch [2][20]\t Batch [6200][55000]\t Training Loss 0.8328\t Accuracy 0.8483\n",
      "Epoch [2][20]\t Batch [6250][55000]\t Training Loss 0.8317\t Accuracy 0.8487\n",
      "Epoch [2][20]\t Batch [6300][55000]\t Training Loss 0.8321\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [6350][55000]\t Training Loss 0.8317\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [6400][55000]\t Training Loss 0.8312\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [6450][55000]\t Training Loss 0.8306\t Accuracy 0.8496\n",
      "Epoch [2][20]\t Batch [6500][55000]\t Training Loss 0.8314\t Accuracy 0.8493\n",
      "Epoch [2][20]\t Batch [6550][55000]\t Training Loss 0.8305\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [6600][55000]\t Training Loss 0.8292\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [6650][55000]\t Training Loss 0.8280\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [6700][55000]\t Training Loss 0.8281\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [6750][55000]\t Training Loss 0.8283\t Accuracy 0.8510\n",
      "Epoch [2][20]\t Batch [6800][55000]\t Training Loss 0.8288\t Accuracy 0.8511\n",
      "Epoch [2][20]\t Batch [6850][55000]\t Training Loss 0.8309\t Accuracy 0.8507\n",
      "Epoch [2][20]\t Batch [6900][55000]\t Training Loss 0.8309\t Accuracy 0.8506\n",
      "Epoch [2][20]\t Batch [6950][55000]\t Training Loss 0.8315\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [7000][55000]\t Training Loss 0.8314\t Accuracy 0.8496\n",
      "Epoch [2][20]\t Batch [7050][55000]\t Training Loss 0.8320\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [7100][55000]\t Training Loss 0.8322\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [7150][55000]\t Training Loss 0.8324\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [7200][55000]\t Training Loss 0.8332\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [7250][55000]\t Training Loss 0.8355\t Accuracy 0.8488\n",
      "Epoch [2][20]\t Batch [7300][55000]\t Training Loss 0.8374\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [7350][55000]\t Training Loss 0.8389\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [7400][55000]\t Training Loss 0.8399\t Accuracy 0.8487\n",
      "Epoch [2][20]\t Batch [7450][55000]\t Training Loss 0.8399\t Accuracy 0.8489\n",
      "Epoch [2][20]\t Batch [7500][55000]\t Training Loss 0.8399\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [7550][55000]\t Training Loss 0.8404\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [7600][55000]\t Training Loss 0.8399\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [7650][55000]\t Training Loss 0.8404\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [7700][55000]\t Training Loss 0.8409\t Accuracy 0.8489\n",
      "Epoch [2][20]\t Batch [7750][55000]\t Training Loss 0.8410\t Accuracy 0.8488\n",
      "Epoch [2][20]\t Batch [7800][55000]\t Training Loss 0.8418\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [7850][55000]\t Training Loss 0.8419\t Accuracy 0.8488\n",
      "Epoch [2][20]\t Batch [7900][55000]\t Training Loss 0.8421\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [7950][55000]\t Training Loss 0.8421\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [8000][55000]\t Training Loss 0.8421\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [8050][55000]\t Training Loss 0.8426\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [8100][55000]\t Training Loss 0.8412\t Accuracy 0.8483\n",
      "Epoch [2][20]\t Batch [8150][55000]\t Training Loss 0.8420\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [8200][55000]\t Training Loss 0.8420\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [8250][55000]\t Training Loss 0.8433\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [8300][55000]\t Training Loss 0.8436\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [8350][55000]\t Training Loss 0.8442\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [8400][55000]\t Training Loss 0.8436\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [8450][55000]\t Training Loss 0.8451\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [8500][55000]\t Training Loss 0.8443\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [8550][55000]\t Training Loss 0.8433\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [8600][55000]\t Training Loss 0.8425\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [8650][55000]\t Training Loss 0.8426\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [8700][55000]\t Training Loss 0.8433\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [8750][55000]\t Training Loss 0.8448\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [8800][55000]\t Training Loss 0.8456\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [8850][55000]\t Training Loss 0.8455\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [8900][55000]\t Training Loss 0.8473\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [8950][55000]\t Training Loss 0.8468\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [9000][55000]\t Training Loss 0.8461\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [9050][55000]\t Training Loss 0.8453\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [9100][55000]\t Training Loss 0.8450\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [9150][55000]\t Training Loss 0.8455\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [9200][55000]\t Training Loss 0.8451\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [9250][55000]\t Training Loss 0.8454\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [9300][55000]\t Training Loss 0.8456\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [9350][55000]\t Training Loss 0.8458\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [9400][55000]\t Training Loss 0.8461\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [9450][55000]\t Training Loss 0.8463\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [9500][55000]\t Training Loss 0.8454\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [9550][55000]\t Training Loss 0.8452\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [9600][55000]\t Training Loss 0.8456\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [9650][55000]\t Training Loss 0.8454\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [9700][55000]\t Training Loss 0.8449\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [9750][55000]\t Training Loss 0.8441\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [9800][55000]\t Training Loss 0.8447\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [9850][55000]\t Training Loss 0.8442\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [9900][55000]\t Training Loss 0.8437\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [9950][55000]\t Training Loss 0.8431\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [10000][55000]\t Training Loss 0.8429\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [10050][55000]\t Training Loss 0.8430\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [10100][55000]\t Training Loss 0.8428\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [10150][55000]\t Training Loss 0.8426\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [10200][55000]\t Training Loss 0.8428\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [10250][55000]\t Training Loss 0.8429\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [10300][55000]\t Training Loss 0.8428\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [10350][55000]\t Training Loss 0.8418\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [10400][55000]\t Training Loss 0.8410\t Accuracy 0.8483\n",
      "Epoch [2][20]\t Batch [10450][55000]\t Training Loss 0.8409\t Accuracy 0.8481\n",
      "Epoch [2][20]\t Batch [10500][55000]\t Training Loss 0.8399\t Accuracy 0.8487\n",
      "Epoch [2][20]\t Batch [10550][55000]\t Training Loss 0.8395\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [10600][55000]\t Training Loss 0.8390\t Accuracy 0.8492\n",
      "Epoch [2][20]\t Batch [10650][55000]\t Training Loss 0.8388\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [10700][55000]\t Training Loss 0.8385\t Accuracy 0.8498\n",
      "Epoch [2][20]\t Batch [10750][55000]\t Training Loss 0.8391\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [10800][55000]\t Training Loss 0.8395\t Accuracy 0.8492\n",
      "Epoch [2][20]\t Batch [10850][55000]\t Training Loss 0.8388\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [10900][55000]\t Training Loss 0.8384\t Accuracy 0.8496\n",
      "Epoch [2][20]\t Batch [10950][55000]\t Training Loss 0.8380\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [11000][55000]\t Training Loss 0.8379\t Accuracy 0.8497\n",
      "Epoch [2][20]\t Batch [11050][55000]\t Training Loss 0.8372\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [11100][55000]\t Training Loss 0.8365\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [11150][55000]\t Training Loss 0.8365\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [11200][55000]\t Training Loss 0.8362\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [11250][55000]\t Training Loss 0.8365\t Accuracy 0.8501\n",
      "Epoch [2][20]\t Batch [11300][55000]\t Training Loss 0.8361\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [11350][55000]\t Training Loss 0.8356\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [11400][55000]\t Training Loss 0.8357\t Accuracy 0.8501\n",
      "Epoch [2][20]\t Batch [11450][55000]\t Training Loss 0.8354\t Accuracy 0.8501\n",
      "Epoch [2][20]\t Batch [11500][55000]\t Training Loss 0.8352\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [11550][55000]\t Training Loss 0.8352\t Accuracy 0.8498\n",
      "Epoch [2][20]\t Batch [11600][55000]\t Training Loss 0.8364\t Accuracy 0.8492\n",
      "Epoch [2][20]\t Batch [11650][55000]\t Training Loss 0.8369\t Accuracy 0.8489\n",
      "Epoch [2][20]\t Batch [11700][55000]\t Training Loss 0.8369\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [11750][55000]\t Training Loss 0.8378\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [11800][55000]\t Training Loss 0.8380\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [11850][55000]\t Training Loss 0.8381\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [11900][55000]\t Training Loss 0.8383\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [11950][55000]\t Training Loss 0.8384\t Accuracy 0.8488\n",
      "Epoch [2][20]\t Batch [12000][55000]\t Training Loss 0.8384\t Accuracy 0.8491\n",
      "Epoch [2][20]\t Batch [12050][55000]\t Training Loss 0.8382\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [12100][55000]\t Training Loss 0.8382\t Accuracy 0.8493\n",
      "Epoch [2][20]\t Batch [12150][55000]\t Training Loss 0.8375\t Accuracy 0.8497\n",
      "Epoch [2][20]\t Batch [12200][55000]\t Training Loss 0.8378\t Accuracy 0.8497\n",
      "Epoch [2][20]\t Batch [12250][55000]\t Training Loss 0.8377\t Accuracy 0.8497\n",
      "Epoch [2][20]\t Batch [12300][55000]\t Training Loss 0.8377\t Accuracy 0.8497\n",
      "Epoch [2][20]\t Batch [12350][55000]\t Training Loss 0.8379\t Accuracy 0.8498\n",
      "Epoch [2][20]\t Batch [12400][55000]\t Training Loss 0.8382\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [12450][55000]\t Training Loss 0.8384\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [12500][55000]\t Training Loss 0.8386\t Accuracy 0.8493\n",
      "Epoch [2][20]\t Batch [12550][55000]\t Training Loss 0.8386\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [12600][55000]\t Training Loss 0.8394\t Accuracy 0.8491\n",
      "Epoch [2][20]\t Batch [12650][55000]\t Training Loss 0.8399\t Accuracy 0.8488\n",
      "Epoch [2][20]\t Batch [12700][55000]\t Training Loss 0.8405\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [12750][55000]\t Training Loss 0.8401\t Accuracy 0.8487\n",
      "Epoch [2][20]\t Batch [12800][55000]\t Training Loss 0.8407\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [12850][55000]\t Training Loss 0.8409\t Accuracy 0.8483\n",
      "Epoch [2][20]\t Batch [12900][55000]\t Training Loss 0.8409\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [12950][55000]\t Training Loss 0.8414\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [13000][55000]\t Training Loss 0.8414\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [13050][55000]\t Training Loss 0.8421\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [13100][55000]\t Training Loss 0.8427\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [13150][55000]\t Training Loss 0.8430\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [13200][55000]\t Training Loss 0.8432\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [13250][55000]\t Training Loss 0.8427\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [13300][55000]\t Training Loss 0.8427\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [13350][55000]\t Training Loss 0.8430\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [13400][55000]\t Training Loss 0.8433\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [13450][55000]\t Training Loss 0.8428\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [13500][55000]\t Training Loss 0.8424\t Accuracy 0.8478\n",
      "Epoch [2][20]\t Batch [13550][55000]\t Training Loss 0.8420\t Accuracy 0.8478\n",
      "Epoch [2][20]\t Batch [13600][55000]\t Training Loss 0.8412\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [13650][55000]\t Training Loss 0.8410\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [13700][55000]\t Training Loss 0.8418\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [13750][55000]\t Training Loss 0.8425\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [13800][55000]\t Training Loss 0.8426\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [13850][55000]\t Training Loss 0.8425\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [13900][55000]\t Training Loss 0.8428\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [13950][55000]\t Training Loss 0.8431\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [14000][55000]\t Training Loss 0.8439\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [14050][55000]\t Training Loss 0.8440\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [14100][55000]\t Training Loss 0.8441\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [14150][55000]\t Training Loss 0.8444\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [14200][55000]\t Training Loss 0.8443\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [14250][55000]\t Training Loss 0.8445\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [14300][55000]\t Training Loss 0.8449\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [14350][55000]\t Training Loss 0.8452\t Accuracy 0.8457\n",
      "Epoch [2][20]\t Batch [14400][55000]\t Training Loss 0.8461\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [14450][55000]\t Training Loss 0.8463\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [14500][55000]\t Training Loss 0.8464\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [14550][55000]\t Training Loss 0.8471\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [14600][55000]\t Training Loss 0.8471\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [14650][55000]\t Training Loss 0.8481\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [14700][55000]\t Training Loss 0.8489\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [14750][55000]\t Training Loss 0.8495\t Accuracy 0.8445\n",
      "Epoch [2][20]\t Batch [14800][55000]\t Training Loss 0.8504\t Accuracy 0.8443\n",
      "Epoch [2][20]\t Batch [14850][55000]\t Training Loss 0.8510\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [14900][55000]\t Training Loss 0.8509\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [14950][55000]\t Training Loss 0.8510\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [15000][55000]\t Training Loss 0.8510\t Accuracy 0.8438\n",
      "Epoch [2][20]\t Batch [15050][55000]\t Training Loss 0.8506\t Accuracy 0.8441\n",
      "Epoch [2][20]\t Batch [15100][55000]\t Training Loss 0.8504\t Accuracy 0.8442\n",
      "Epoch [2][20]\t Batch [15150][55000]\t Training Loss 0.8508\t Accuracy 0.8442\n",
      "Epoch [2][20]\t Batch [15200][55000]\t Training Loss 0.8510\t Accuracy 0.8442\n",
      "Epoch [2][20]\t Batch [15250][55000]\t Training Loss 0.8509\t Accuracy 0.8443\n",
      "Epoch [2][20]\t Batch [15300][55000]\t Training Loss 0.8509\t Accuracy 0.8443\n",
      "Epoch [2][20]\t Batch [15350][55000]\t Training Loss 0.8507\t Accuracy 0.8445\n",
      "Epoch [2][20]\t Batch [15400][55000]\t Training Loss 0.8510\t Accuracy 0.8447\n",
      "Epoch [2][20]\t Batch [15450][55000]\t Training Loss 0.8511\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [15500][55000]\t Training Loss 0.8509\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [15550][55000]\t Training Loss 0.8508\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [15600][55000]\t Training Loss 0.8506\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [15650][55000]\t Training Loss 0.8505\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [15700][55000]\t Training Loss 0.8504\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [15750][55000]\t Training Loss 0.8513\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [15800][55000]\t Training Loss 0.8517\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [15850][55000]\t Training Loss 0.8521\t Accuracy 0.8447\n",
      "Epoch [2][20]\t Batch [15900][55000]\t Training Loss 0.8526\t Accuracy 0.8443\n",
      "Epoch [2][20]\t Batch [15950][55000]\t Training Loss 0.8527\t Accuracy 0.8444\n",
      "Epoch [2][20]\t Batch [16000][55000]\t Training Loss 0.8527\t Accuracy 0.8442\n",
      "Epoch [2][20]\t Batch [16050][55000]\t Training Loss 0.8537\t Accuracy 0.8438\n",
      "Epoch [2][20]\t Batch [16100][55000]\t Training Loss 0.8536\t Accuracy 0.8438\n",
      "Epoch [2][20]\t Batch [16150][55000]\t Training Loss 0.8536\t Accuracy 0.8440\n",
      "Epoch [2][20]\t Batch [16200][55000]\t Training Loss 0.8537\t Accuracy 0.8437\n",
      "Epoch [2][20]\t Batch [16250][55000]\t Training Loss 0.8535\t Accuracy 0.8436\n",
      "Epoch [2][20]\t Batch [16300][55000]\t Training Loss 0.8532\t Accuracy 0.8437\n",
      "Epoch [2][20]\t Batch [16350][55000]\t Training Loss 0.8529\t Accuracy 0.8440\n",
      "Epoch [2][20]\t Batch [16400][55000]\t Training Loss 0.8529\t Accuracy 0.8440\n",
      "Epoch [2][20]\t Batch [16450][55000]\t Training Loss 0.8527\t Accuracy 0.8442\n",
      "Epoch [2][20]\t Batch [16500][55000]\t Training Loss 0.8525\t Accuracy 0.8444\n",
      "Epoch [2][20]\t Batch [16550][55000]\t Training Loss 0.8521\t Accuracy 0.8445\n",
      "Epoch [2][20]\t Batch [16600][55000]\t Training Loss 0.8522\t Accuracy 0.8445\n",
      "Epoch [2][20]\t Batch [16650][55000]\t Training Loss 0.8522\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [16700][55000]\t Training Loss 0.8523\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [16750][55000]\t Training Loss 0.8522\t Accuracy 0.8447\n",
      "Epoch [2][20]\t Batch [16800][55000]\t Training Loss 0.8528\t Accuracy 0.8445\n",
      "Epoch [2][20]\t Batch [16850][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [2][20]\t Batch [16900][55000]\t Training Loss 0.8536\t Accuracy 0.8443\n",
      "Epoch [2][20]\t Batch [16950][55000]\t Training Loss 0.8536\t Accuracy 0.8442\n",
      "Epoch [2][20]\t Batch [17000][55000]\t Training Loss 0.8543\t Accuracy 0.8438\n",
      "Epoch [2][20]\t Batch [17050][55000]\t Training Loss 0.8542\t Accuracy 0.8438\n",
      "Epoch [2][20]\t Batch [17100][55000]\t Training Loss 0.8547\t Accuracy 0.8436\n",
      "Epoch [2][20]\t Batch [17150][55000]\t Training Loss 0.8545\t Accuracy 0.8437\n",
      "Epoch [2][20]\t Batch [17200][55000]\t Training Loss 0.8545\t Accuracy 0.8437\n",
      "Epoch [2][20]\t Batch [17250][55000]\t Training Loss 0.8550\t Accuracy 0.8435\n",
      "Epoch [2][20]\t Batch [17300][55000]\t Training Loss 0.8549\t Accuracy 0.8436\n",
      "Epoch [2][20]\t Batch [17350][55000]\t Training Loss 0.8543\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [17400][55000]\t Training Loss 0.8544\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [17450][55000]\t Training Loss 0.8545\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [17500][55000]\t Training Loss 0.8546\t Accuracy 0.8440\n",
      "Epoch [2][20]\t Batch [17550][55000]\t Training Loss 0.8553\t Accuracy 0.8437\n",
      "Epoch [2][20]\t Batch [17600][55000]\t Training Loss 0.8559\t Accuracy 0.8435\n",
      "Epoch [2][20]\t Batch [17650][55000]\t Training Loss 0.8561\t Accuracy 0.8437\n",
      "Epoch [2][20]\t Batch [17700][55000]\t Training Loss 0.8568\t Accuracy 0.8435\n",
      "Epoch [2][20]\t Batch [17750][55000]\t Training Loss 0.8572\t Accuracy 0.8434\n",
      "Epoch [2][20]\t Batch [17800][55000]\t Training Loss 0.8575\t Accuracy 0.8432\n",
      "Epoch [2][20]\t Batch [17850][55000]\t Training Loss 0.8578\t Accuracy 0.8430\n",
      "Epoch [2][20]\t Batch [17900][55000]\t Training Loss 0.8582\t Accuracy 0.8429\n",
      "Epoch [2][20]\t Batch [17950][55000]\t Training Loss 0.8580\t Accuracy 0.8427\n",
      "Epoch [2][20]\t Batch [18000][55000]\t Training Loss 0.8577\t Accuracy 0.8430\n",
      "Epoch [2][20]\t Batch [18050][55000]\t Training Loss 0.8579\t Accuracy 0.8429\n",
      "Epoch [2][20]\t Batch [18100][55000]\t Training Loss 0.8577\t Accuracy 0.8431\n",
      "Epoch [2][20]\t Batch [18150][55000]\t Training Loss 0.8573\t Accuracy 0.8432\n",
      "Epoch [2][20]\t Batch [18200][55000]\t Training Loss 0.8569\t Accuracy 0.8435\n",
      "Epoch [2][20]\t Batch [18250][55000]\t Training Loss 0.8568\t Accuracy 0.8435\n",
      "Epoch [2][20]\t Batch [18300][55000]\t Training Loss 0.8564\t Accuracy 0.8436\n",
      "Epoch [2][20]\t Batch [18350][55000]\t Training Loss 0.8563\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [18400][55000]\t Training Loss 0.8563\t Accuracy 0.8438\n",
      "Epoch [2][20]\t Batch [18450][55000]\t Training Loss 0.8567\t Accuracy 0.8436\n",
      "Epoch [2][20]\t Batch [18500][55000]\t Training Loss 0.8568\t Accuracy 0.8436\n",
      "Epoch [2][20]\t Batch [18550][55000]\t Training Loss 0.8565\t Accuracy 0.8439\n",
      "Epoch [2][20]\t Batch [18600][55000]\t Training Loss 0.8565\t Accuracy 0.8441\n",
      "Epoch [2][20]\t Batch [18650][55000]\t Training Loss 0.8564\t Accuracy 0.8442\n",
      "Epoch [2][20]\t Batch [18700][55000]\t Training Loss 0.8566\t Accuracy 0.8441\n",
      "Epoch [2][20]\t Batch [18750][55000]\t Training Loss 0.8568\t Accuracy 0.8441\n",
      "Epoch [2][20]\t Batch [18800][55000]\t Training Loss 0.8564\t Accuracy 0.8445\n",
      "Epoch [2][20]\t Batch [18850][55000]\t Training Loss 0.8567\t Accuracy 0.8445\n",
      "Epoch [2][20]\t Batch [18900][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [18950][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [19000][55000]\t Training Loss 0.8559\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [19050][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [19100][55000]\t Training Loss 0.8566\t Accuracy 0.8447\n",
      "Epoch [2][20]\t Batch [19150][55000]\t Training Loss 0.8567\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [19200][55000]\t Training Loss 0.8568\t Accuracy 0.8445\n",
      "Epoch [2][20]\t Batch [19250][55000]\t Training Loss 0.8566\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [19300][55000]\t Training Loss 0.8564\t Accuracy 0.8447\n",
      "Epoch [2][20]\t Batch [19350][55000]\t Training Loss 0.8565\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [19400][55000]\t Training Loss 0.8563\t Accuracy 0.8447\n",
      "Epoch [2][20]\t Batch [19450][55000]\t Training Loss 0.8560\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [19500][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [19550][55000]\t Training Loss 0.8557\t Accuracy 0.8447\n",
      "Epoch [2][20]\t Batch [19600][55000]\t Training Loss 0.8557\t Accuracy 0.8444\n",
      "Epoch [2][20]\t Batch [19650][55000]\t Training Loss 0.8553\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [19700][55000]\t Training Loss 0.8547\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [19750][55000]\t Training Loss 0.8542\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [19800][55000]\t Training Loss 0.8535\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [19850][55000]\t Training Loss 0.8536\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [19900][55000]\t Training Loss 0.8533\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [19950][55000]\t Training Loss 0.8535\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [20000][55000]\t Training Loss 0.8534\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [20050][55000]\t Training Loss 0.8540\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [20100][55000]\t Training Loss 0.8541\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [20150][55000]\t Training Loss 0.8539\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [20200][55000]\t Training Loss 0.8543\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [20250][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [20300][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [20350][55000]\t Training Loss 0.8545\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [20400][55000]\t Training Loss 0.8541\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [20450][55000]\t Training Loss 0.8538\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [20500][55000]\t Training Loss 0.8535\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [20550][55000]\t Training Loss 0.8534\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [20600][55000]\t Training Loss 0.8534\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [20650][55000]\t Training Loss 0.8531\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [20700][55000]\t Training Loss 0.8529\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [20750][55000]\t Training Loss 0.8529\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [20800][55000]\t Training Loss 0.8529\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [20850][55000]\t Training Loss 0.8528\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [20900][55000]\t Training Loss 0.8531\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [20950][55000]\t Training Loss 0.8536\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [21000][55000]\t Training Loss 0.8538\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [21050][55000]\t Training Loss 0.8541\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [21100][55000]\t Training Loss 0.8538\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [21150][55000]\t Training Loss 0.8538\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [21200][55000]\t Training Loss 0.8535\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [21250][55000]\t Training Loss 0.8534\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [21300][55000]\t Training Loss 0.8529\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [21350][55000]\t Training Loss 0.8530\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [21400][55000]\t Training Loss 0.8531\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [21450][55000]\t Training Loss 0.8532\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [21500][55000]\t Training Loss 0.8527\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [21550][55000]\t Training Loss 0.8525\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [21600][55000]\t Training Loss 0.8527\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [21650][55000]\t Training Loss 0.8527\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [21700][55000]\t Training Loss 0.8527\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [21750][55000]\t Training Loss 0.8527\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [21800][55000]\t Training Loss 0.8521\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [21850][55000]\t Training Loss 0.8517\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [21900][55000]\t Training Loss 0.8513\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [21950][55000]\t Training Loss 0.8508\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [22000][55000]\t Training Loss 0.8504\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [22050][55000]\t Training Loss 0.8500\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [22100][55000]\t Training Loss 0.8502\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [22150][55000]\t Training Loss 0.8506\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [22200][55000]\t Training Loss 0.8508\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [22250][55000]\t Training Loss 0.8509\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [22300][55000]\t Training Loss 0.8510\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [22350][55000]\t Training Loss 0.8508\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [22400][55000]\t Training Loss 0.8507\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [22450][55000]\t Training Loss 0.8507\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [22500][55000]\t Training Loss 0.8511\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [22550][55000]\t Training Loss 0.8517\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [22600][55000]\t Training Loss 0.8522\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [22650][55000]\t Training Loss 0.8523\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [22700][55000]\t Training Loss 0.8522\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [22750][55000]\t Training Loss 0.8521\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [22800][55000]\t Training Loss 0.8520\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [22850][55000]\t Training Loss 0.8520\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [22900][55000]\t Training Loss 0.8517\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [22950][55000]\t Training Loss 0.8516\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [23000][55000]\t Training Loss 0.8513\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [23050][55000]\t Training Loss 0.8511\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [23100][55000]\t Training Loss 0.8512\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [23150][55000]\t Training Loss 0.8512\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [23200][55000]\t Training Loss 0.8512\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [23250][55000]\t Training Loss 0.8509\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [23300][55000]\t Training Loss 0.8506\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [23350][55000]\t Training Loss 0.8504\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [23400][55000]\t Training Loss 0.8501\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [23450][55000]\t Training Loss 0.8501\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [23500][55000]\t Training Loss 0.8499\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [23550][55000]\t Training Loss 0.8498\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [23600][55000]\t Training Loss 0.8498\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [23650][55000]\t Training Loss 0.8499\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [23700][55000]\t Training Loss 0.8501\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [23750][55000]\t Training Loss 0.8507\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [23800][55000]\t Training Loss 0.8503\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [23850][55000]\t Training Loss 0.8503\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [23900][55000]\t Training Loss 0.8504\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [23950][55000]\t Training Loss 0.8505\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [24000][55000]\t Training Loss 0.8505\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [24050][55000]\t Training Loss 0.8504\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [24100][55000]\t Training Loss 0.8503\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [24150][55000]\t Training Loss 0.8502\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [24200][55000]\t Training Loss 0.8500\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [24250][55000]\t Training Loss 0.8501\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [24300][55000]\t Training Loss 0.8503\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [24350][55000]\t Training Loss 0.8503\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [24400][55000]\t Training Loss 0.8502\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [24450][55000]\t Training Loss 0.8502\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [24500][55000]\t Training Loss 0.8503\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [24550][55000]\t Training Loss 0.8503\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [24600][55000]\t Training Loss 0.8504\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [24650][55000]\t Training Loss 0.8504\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [24700][55000]\t Training Loss 0.8504\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [24750][55000]\t Training Loss 0.8507\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [24800][55000]\t Training Loss 0.8510\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [24850][55000]\t Training Loss 0.8509\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [24900][55000]\t Training Loss 0.8510\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [24950][55000]\t Training Loss 0.8513\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [25000][55000]\t Training Loss 0.8515\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [25050][55000]\t Training Loss 0.8512\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [25100][55000]\t Training Loss 0.8512\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [25150][55000]\t Training Loss 0.8510\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [25200][55000]\t Training Loss 0.8508\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [25250][55000]\t Training Loss 0.8508\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [25300][55000]\t Training Loss 0.8507\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [25350][55000]\t Training Loss 0.8508\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [25400][55000]\t Training Loss 0.8504\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [25450][55000]\t Training Loss 0.8499\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [25500][55000]\t Training Loss 0.8497\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [25550][55000]\t Training Loss 0.8494\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [25600][55000]\t Training Loss 0.8493\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [25650][55000]\t Training Loss 0.8492\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [25700][55000]\t Training Loss 0.8490\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [25750][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [25800][55000]\t Training Loss 0.8487\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [25850][55000]\t Training Loss 0.8488\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [25900][55000]\t Training Loss 0.8488\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [25950][55000]\t Training Loss 0.8489\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [26000][55000]\t Training Loss 0.8488\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [26050][55000]\t Training Loss 0.8486\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [26100][55000]\t Training Loss 0.8484\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [26150][55000]\t Training Loss 0.8482\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [26200][55000]\t Training Loss 0.8480\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [26250][55000]\t Training Loss 0.8479\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [26300][55000]\t Training Loss 0.8481\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [26350][55000]\t Training Loss 0.8480\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [26400][55000]\t Training Loss 0.8484\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [26450][55000]\t Training Loss 0.8486\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [26500][55000]\t Training Loss 0.8487\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [26550][55000]\t Training Loss 0.8490\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [26600][55000]\t Training Loss 0.8492\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [26650][55000]\t Training Loss 0.8495\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [26700][55000]\t Training Loss 0.8494\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [26750][55000]\t Training Loss 0.8496\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [26800][55000]\t Training Loss 0.8495\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [26850][55000]\t Training Loss 0.8494\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [26900][55000]\t Training Loss 0.8497\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [26950][55000]\t Training Loss 0.8496\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [27000][55000]\t Training Loss 0.8494\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [27050][55000]\t Training Loss 0.8492\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [27100][55000]\t Training Loss 0.8491\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [27150][55000]\t Training Loss 0.8490\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [27200][55000]\t Training Loss 0.8494\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [27250][55000]\t Training Loss 0.8493\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [27300][55000]\t Training Loss 0.8493\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [27350][55000]\t Training Loss 0.8493\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [27400][55000]\t Training Loss 0.8492\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [27450][55000]\t Training Loss 0.8492\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [27500][55000]\t Training Loss 0.8492\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [27550][55000]\t Training Loss 0.8492\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [27600][55000]\t Training Loss 0.8490\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [27650][55000]\t Training Loss 0.8491\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [27700][55000]\t Training Loss 0.8491\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [27750][55000]\t Training Loss 0.8492\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [27800][55000]\t Training Loss 0.8493\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [27850][55000]\t Training Loss 0.8493\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [27900][55000]\t Training Loss 0.8492\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [27950][55000]\t Training Loss 0.8490\t Accuracy 0.8478\n",
      "Epoch [2][20]\t Batch [28000][55000]\t Training Loss 0.8487\t Accuracy 0.8478\n",
      "Epoch [2][20]\t Batch [28050][55000]\t Training Loss 0.8484\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [28100][55000]\t Training Loss 0.8481\t Accuracy 0.8481\n",
      "Epoch [2][20]\t Batch [28150][55000]\t Training Loss 0.8479\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [28200][55000]\t Training Loss 0.8480\t Accuracy 0.8481\n",
      "Epoch [2][20]\t Batch [28250][55000]\t Training Loss 0.8476\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [28300][55000]\t Training Loss 0.8475\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [28350][55000]\t Training Loss 0.8472\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [28400][55000]\t Training Loss 0.8477\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [28450][55000]\t Training Loss 0.8474\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [28500][55000]\t Training Loss 0.8473\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [28550][55000]\t Training Loss 0.8472\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [28600][55000]\t Training Loss 0.8470\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [28650][55000]\t Training Loss 0.8473\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [28700][55000]\t Training Loss 0.8476\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [28750][55000]\t Training Loss 0.8477\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [28800][55000]\t Training Loss 0.8476\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [28850][55000]\t Training Loss 0.8476\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [28900][55000]\t Training Loss 0.8474\t Accuracy 0.8486\n",
      "Epoch [2][20]\t Batch [28950][55000]\t Training Loss 0.8474\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [29000][55000]\t Training Loss 0.8474\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [29050][55000]\t Training Loss 0.8474\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [29100][55000]\t Training Loss 0.8475\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [29150][55000]\t Training Loss 0.8479\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [29200][55000]\t Training Loss 0.8482\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [29250][55000]\t Training Loss 0.8484\t Accuracy 0.8481\n",
      "Epoch [2][20]\t Batch [29300][55000]\t Training Loss 0.8483\t Accuracy 0.8481\n",
      "Epoch [2][20]\t Batch [29350][55000]\t Training Loss 0.8484\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [29400][55000]\t Training Loss 0.8483\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [29450][55000]\t Training Loss 0.8479\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [29500][55000]\t Training Loss 0.8477\t Accuracy 0.8481\n",
      "Epoch [2][20]\t Batch [29550][55000]\t Training Loss 0.8476\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [29600][55000]\t Training Loss 0.8476\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [29650][55000]\t Training Loss 0.8476\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [29700][55000]\t Training Loss 0.8476\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [29750][55000]\t Training Loss 0.8480\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [29800][55000]\t Training Loss 0.8482\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [29850][55000]\t Training Loss 0.8485\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [29900][55000]\t Training Loss 0.8488\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [29950][55000]\t Training Loss 0.8491\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [30000][55000]\t Training Loss 0.8494\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [30050][55000]\t Training Loss 0.8495\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [30100][55000]\t Training Loss 0.8499\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [30150][55000]\t Training Loss 0.8504\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [30200][55000]\t Training Loss 0.8506\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [30250][55000]\t Training Loss 0.8507\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [30300][55000]\t Training Loss 0.8505\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [30350][55000]\t Training Loss 0.8505\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [30400][55000]\t Training Loss 0.8503\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [30450][55000]\t Training Loss 0.8504\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [30500][55000]\t Training Loss 0.8506\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [30550][55000]\t Training Loss 0.8509\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [30600][55000]\t Training Loss 0.8510\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [30650][55000]\t Training Loss 0.8514\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [30700][55000]\t Training Loss 0.8517\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [30750][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [30800][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [30850][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [30900][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [30950][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [31000][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [31050][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [31100][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [31150][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [31200][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [31250][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [31300][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [31350][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [31400][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [31450][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [31500][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [31550][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [31600][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [31650][55000]\t Training Loss 0.8542\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [31700][55000]\t Training Loss 0.8544\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [31750][55000]\t Training Loss 0.8548\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [31800][55000]\t Training Loss 0.8550\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [31850][55000]\t Training Loss 0.8549\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [31900][55000]\t Training Loss 0.8549\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [31950][55000]\t Training Loss 0.8548\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [32000][55000]\t Training Loss 0.8549\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [32050][55000]\t Training Loss 0.8548\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [32100][55000]\t Training Loss 0.8547\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [32150][55000]\t Training Loss 0.8550\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [32200][55000]\t Training Loss 0.8552\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [32250][55000]\t Training Loss 0.8555\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [32300][55000]\t Training Loss 0.8558\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [32350][55000]\t Training Loss 0.8561\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [32400][55000]\t Training Loss 0.8562\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [32450][55000]\t Training Loss 0.8564\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [32500][55000]\t Training Loss 0.8566\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [32550][55000]\t Training Loss 0.8569\t Accuracy 0.8457\n",
      "Epoch [2][20]\t Batch [32600][55000]\t Training Loss 0.8569\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [32650][55000]\t Training Loss 0.8567\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [32700][55000]\t Training Loss 0.8567\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [32750][55000]\t Training Loss 0.8568\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [32800][55000]\t Training Loss 0.8569\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [32850][55000]\t Training Loss 0.8568\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [32900][55000]\t Training Loss 0.8566\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [32950][55000]\t Training Loss 0.8566\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [33000][55000]\t Training Loss 0.8565\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [33050][55000]\t Training Loss 0.8566\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [33100][55000]\t Training Loss 0.8566\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [33150][55000]\t Training Loss 0.8567\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [33200][55000]\t Training Loss 0.8566\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [33250][55000]\t Training Loss 0.8568\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [33300][55000]\t Training Loss 0.8567\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [33350][55000]\t Training Loss 0.8568\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [33400][55000]\t Training Loss 0.8569\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [33450][55000]\t Training Loss 0.8570\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [33500][55000]\t Training Loss 0.8569\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [33550][55000]\t Training Loss 0.8569\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [33600][55000]\t Training Loss 0.8569\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [33650][55000]\t Training Loss 0.8569\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [33700][55000]\t Training Loss 0.8568\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [33750][55000]\t Training Loss 0.8566\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [33800][55000]\t Training Loss 0.8566\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [33850][55000]\t Training Loss 0.8563\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [33900][55000]\t Training Loss 0.8560\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [33950][55000]\t Training Loss 0.8556\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [34000][55000]\t Training Loss 0.8556\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [34050][55000]\t Training Loss 0.8558\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [34100][55000]\t Training Loss 0.8559\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [34150][55000]\t Training Loss 0.8560\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [34200][55000]\t Training Loss 0.8557\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [34250][55000]\t Training Loss 0.8555\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [34300][55000]\t Training Loss 0.8553\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [34350][55000]\t Training Loss 0.8553\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [34400][55000]\t Training Loss 0.8552\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [34450][55000]\t Training Loss 0.8553\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [34500][55000]\t Training Loss 0.8554\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [34550][55000]\t Training Loss 0.8554\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [34600][55000]\t Training Loss 0.8554\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [34650][55000]\t Training Loss 0.8553\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [34700][55000]\t Training Loss 0.8554\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [34750][55000]\t Training Loss 0.8556\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [34800][55000]\t Training Loss 0.8556\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [34850][55000]\t Training Loss 0.8560\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [34900][55000]\t Training Loss 0.8561\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [34950][55000]\t Training Loss 0.8561\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [35000][55000]\t Training Loss 0.8560\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [35050][55000]\t Training Loss 0.8558\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [35100][55000]\t Training Loss 0.8559\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [35150][55000]\t Training Loss 0.8560\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [35200][55000]\t Training Loss 0.8561\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [35250][55000]\t Training Loss 0.8562\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [35300][55000]\t Training Loss 0.8562\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [35350][55000]\t Training Loss 0.8560\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [35400][55000]\t Training Loss 0.8560\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [35450][55000]\t Training Loss 0.8559\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [35500][55000]\t Training Loss 0.8559\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [35550][55000]\t Training Loss 0.8558\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [35600][55000]\t Training Loss 0.8557\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [35650][55000]\t Training Loss 0.8560\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [35700][55000]\t Training Loss 0.8560\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [35750][55000]\t Training Loss 0.8559\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [35800][55000]\t Training Loss 0.8558\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [35850][55000]\t Training Loss 0.8556\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [35900][55000]\t Training Loss 0.8556\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [35950][55000]\t Training Loss 0.8557\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [36000][55000]\t Training Loss 0.8558\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [36050][55000]\t Training Loss 0.8559\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [36100][55000]\t Training Loss 0.8561\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [36150][55000]\t Training Loss 0.8561\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [36200][55000]\t Training Loss 0.8558\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [36250][55000]\t Training Loss 0.8557\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [36300][55000]\t Training Loss 0.8554\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [36350][55000]\t Training Loss 0.8553\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [36400][55000]\t Training Loss 0.8552\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [36450][55000]\t Training Loss 0.8554\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [36500][55000]\t Training Loss 0.8555\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [36550][55000]\t Training Loss 0.8554\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [36600][55000]\t Training Loss 0.8552\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [36650][55000]\t Training Loss 0.8550\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [36700][55000]\t Training Loss 0.8548\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [36750][55000]\t Training Loss 0.8546\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [36800][55000]\t Training Loss 0.8544\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [36850][55000]\t Training Loss 0.8544\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [36900][55000]\t Training Loss 0.8545\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [36950][55000]\t Training Loss 0.8543\t Accuracy 0.8478\n",
      "Epoch [2][20]\t Batch [37000][55000]\t Training Loss 0.8542\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [37050][55000]\t Training Loss 0.8541\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [37100][55000]\t Training Loss 0.8542\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [37150][55000]\t Training Loss 0.8542\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [37200][55000]\t Training Loss 0.8541\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [37250][55000]\t Training Loss 0.8540\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [37300][55000]\t Training Loss 0.8540\t Accuracy 0.8480\n",
      "Epoch [2][20]\t Batch [37350][55000]\t Training Loss 0.8541\t Accuracy 0.8478\n",
      "Epoch [2][20]\t Batch [37400][55000]\t Training Loss 0.8544\t Accuracy 0.8478\n",
      "Epoch [2][20]\t Batch [37450][55000]\t Training Loss 0.8547\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [37500][55000]\t Training Loss 0.8548\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [37550][55000]\t Training Loss 0.8549\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [37600][55000]\t Training Loss 0.8551\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [37650][55000]\t Training Loss 0.8551\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [37700][55000]\t Training Loss 0.8550\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [37750][55000]\t Training Loss 0.8550\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [37800][55000]\t Training Loss 0.8550\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [37850][55000]\t Training Loss 0.8551\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [37900][55000]\t Training Loss 0.8551\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [37950][55000]\t Training Loss 0.8551\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [38000][55000]\t Training Loss 0.8551\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [38050][55000]\t Training Loss 0.8551\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [38100][55000]\t Training Loss 0.8553\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [38150][55000]\t Training Loss 0.8551\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [38200][55000]\t Training Loss 0.8550\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [38250][55000]\t Training Loss 0.8550\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [38300][55000]\t Training Loss 0.8551\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [38350][55000]\t Training Loss 0.8553\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [38400][55000]\t Training Loss 0.8554\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [38450][55000]\t Training Loss 0.8553\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [38500][55000]\t Training Loss 0.8552\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [38550][55000]\t Training Loss 0.8552\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [38600][55000]\t Training Loss 0.8554\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [38650][55000]\t Training Loss 0.8556\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [38700][55000]\t Training Loss 0.8556\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [38750][55000]\t Training Loss 0.8554\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [38800][55000]\t Training Loss 0.8554\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [38850][55000]\t Training Loss 0.8553\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [38900][55000]\t Training Loss 0.8551\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [38950][55000]\t Training Loss 0.8549\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [39000][55000]\t Training Loss 0.8549\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [39050][55000]\t Training Loss 0.8548\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [39100][55000]\t Training Loss 0.8545\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [39150][55000]\t Training Loss 0.8544\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [39200][55000]\t Training Loss 0.8542\t Accuracy 0.8478\n",
      "Epoch [2][20]\t Batch [39250][55000]\t Training Loss 0.8540\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [39300][55000]\t Training Loss 0.8540\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [39350][55000]\t Training Loss 0.8543\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [39400][55000]\t Training Loss 0.8543\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [39450][55000]\t Training Loss 0.8546\t Accuracy 0.8476\n",
      "Epoch [2][20]\t Batch [39500][55000]\t Training Loss 0.8547\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [39550][55000]\t Training Loss 0.8547\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [39600][55000]\t Training Loss 0.8547\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [39650][55000]\t Training Loss 0.8547\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [39700][55000]\t Training Loss 0.8547\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [39750][55000]\t Training Loss 0.8548\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [39800][55000]\t Training Loss 0.8550\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [39850][55000]\t Training Loss 0.8552\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [39900][55000]\t Training Loss 0.8552\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [39950][55000]\t Training Loss 0.8555\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40000][55000]\t Training Loss 0.8555\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40050][55000]\t Training Loss 0.8554\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [40100][55000]\t Training Loss 0.8554\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [40150][55000]\t Training Loss 0.8553\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [40200][55000]\t Training Loss 0.8552\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40250][55000]\t Training Loss 0.8552\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [40300][55000]\t Training Loss 0.8552\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40350][55000]\t Training Loss 0.8551\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40400][55000]\t Training Loss 0.8551\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40450][55000]\t Training Loss 0.8550\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40500][55000]\t Training Loss 0.8551\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [40550][55000]\t Training Loss 0.8551\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40600][55000]\t Training Loss 0.8552\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [40650][55000]\t Training Loss 0.8551\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [40700][55000]\t Training Loss 0.8552\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [40750][55000]\t Training Loss 0.8551\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40800][55000]\t Training Loss 0.8551\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40850][55000]\t Training Loss 0.8550\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [40900][55000]\t Training Loss 0.8548\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [40950][55000]\t Training Loss 0.8547\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41000][55000]\t Training Loss 0.8546\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41050][55000]\t Training Loss 0.8548\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41100][55000]\t Training Loss 0.8548\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41150][55000]\t Training Loss 0.8549\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41200][55000]\t Training Loss 0.8549\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [41250][55000]\t Training Loss 0.8548\t Accuracy 0.8475\n",
      "Epoch [2][20]\t Batch [41300][55000]\t Training Loss 0.8550\t Accuracy 0.8474\n",
      "Epoch [2][20]\t Batch [41350][55000]\t Training Loss 0.8553\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41400][55000]\t Training Loss 0.8554\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41450][55000]\t Training Loss 0.8555\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41500][55000]\t Training Loss 0.8558\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [41550][55000]\t Training Loss 0.8560\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [41600][55000]\t Training Loss 0.8560\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [41650][55000]\t Training Loss 0.8560\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [41700][55000]\t Training Loss 0.8559\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41750][55000]\t Training Loss 0.8560\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41800][55000]\t Training Loss 0.8562\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [41850][55000]\t Training Loss 0.8563\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [41900][55000]\t Training Loss 0.8562\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [41950][55000]\t Training Loss 0.8563\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [42000][55000]\t Training Loss 0.8562\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [42050][55000]\t Training Loss 0.8562\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [42100][55000]\t Training Loss 0.8560\t Accuracy 0.8472\n",
      "Epoch [2][20]\t Batch [42150][55000]\t Training Loss 0.8562\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [42200][55000]\t Training Loss 0.8562\t Accuracy 0.8471\n",
      "Epoch [2][20]\t Batch [42250][55000]\t Training Loss 0.8563\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [42300][55000]\t Training Loss 0.8563\t Accuracy 0.8470\n",
      "Epoch [2][20]\t Batch [42350][55000]\t Training Loss 0.8565\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [42400][55000]\t Training Loss 0.8567\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [42450][55000]\t Training Loss 0.8568\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [42500][55000]\t Training Loss 0.8569\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [42550][55000]\t Training Loss 0.8572\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [42600][55000]\t Training Loss 0.8570\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [42650][55000]\t Training Loss 0.8568\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [42700][55000]\t Training Loss 0.8568\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [42750][55000]\t Training Loss 0.8568\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [42800][55000]\t Training Loss 0.8567\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [42850][55000]\t Training Loss 0.8566\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [42900][55000]\t Training Loss 0.8566\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [42950][55000]\t Training Loss 0.8567\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [43000][55000]\t Training Loss 0.8569\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [43050][55000]\t Training Loss 0.8571\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [43100][55000]\t Training Loss 0.8573\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [43150][55000]\t Training Loss 0.8573\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [43200][55000]\t Training Loss 0.8572\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [43250][55000]\t Training Loss 0.8573\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [43300][55000]\t Training Loss 0.8572\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [43350][55000]\t Training Loss 0.8570\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [43400][55000]\t Training Loss 0.8568\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [43450][55000]\t Training Loss 0.8567\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [43500][55000]\t Training Loss 0.8564\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [43550][55000]\t Training Loss 0.8565\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [43600][55000]\t Training Loss 0.8564\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [43650][55000]\t Training Loss 0.8562\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [43700][55000]\t Training Loss 0.8562\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [43750][55000]\t Training Loss 0.8562\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [43800][55000]\t Training Loss 0.8561\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [43850][55000]\t Training Loss 0.8562\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [43900][55000]\t Training Loss 0.8563\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [43950][55000]\t Training Loss 0.8564\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [44000][55000]\t Training Loss 0.8564\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [44050][55000]\t Training Loss 0.8565\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [44100][55000]\t Training Loss 0.8565\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [44150][55000]\t Training Loss 0.8567\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [44200][55000]\t Training Loss 0.8568\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [44250][55000]\t Training Loss 0.8568\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [44300][55000]\t Training Loss 0.8569\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [44350][55000]\t Training Loss 0.8570\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [44400][55000]\t Training Loss 0.8571\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [44450][55000]\t Training Loss 0.8572\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [44500][55000]\t Training Loss 0.8571\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [44550][55000]\t Training Loss 0.8569\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [44600][55000]\t Training Loss 0.8568\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [44650][55000]\t Training Loss 0.8567\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [44700][55000]\t Training Loss 0.8566\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [44750][55000]\t Training Loss 0.8566\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [44800][55000]\t Training Loss 0.8566\t Accuracy 0.8469\n",
      "Epoch [2][20]\t Batch [44850][55000]\t Training Loss 0.8567\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [44900][55000]\t Training Loss 0.8568\t Accuracy 0.8468\n",
      "Epoch [2][20]\t Batch [44950][55000]\t Training Loss 0.8569\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [45000][55000]\t Training Loss 0.8568\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45050][55000]\t Training Loss 0.8570\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45100][55000]\t Training Loss 0.8570\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45150][55000]\t Training Loss 0.8571\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [45200][55000]\t Training Loss 0.8571\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45250][55000]\t Training Loss 0.8571\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45300][55000]\t Training Loss 0.8570\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45350][55000]\t Training Loss 0.8569\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [45400][55000]\t Training Loss 0.8568\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [45450][55000]\t Training Loss 0.8570\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [45500][55000]\t Training Loss 0.8572\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [45550][55000]\t Training Loss 0.8572\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [45600][55000]\t Training Loss 0.8571\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45650][55000]\t Training Loss 0.8571\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [45700][55000]\t Training Loss 0.8571\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [45750][55000]\t Training Loss 0.8570\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45800][55000]\t Training Loss 0.8571\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45850][55000]\t Training Loss 0.8572\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [45900][55000]\t Training Loss 0.8573\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [45950][55000]\t Training Loss 0.8574\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [46000][55000]\t Training Loss 0.8574\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [46050][55000]\t Training Loss 0.8576\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [46100][55000]\t Training Loss 0.8577\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [46150][55000]\t Training Loss 0.8578\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46200][55000]\t Training Loss 0.8577\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46250][55000]\t Training Loss 0.8577\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46300][55000]\t Training Loss 0.8578\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46350][55000]\t Training Loss 0.8579\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46400][55000]\t Training Loss 0.8581\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [46450][55000]\t Training Loss 0.8582\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [46500][55000]\t Training Loss 0.8581\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [46550][55000]\t Training Loss 0.8580\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46600][55000]\t Training Loss 0.8579\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46650][55000]\t Training Loss 0.8579\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46700][55000]\t Training Loss 0.8578\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [46750][55000]\t Training Loss 0.8578\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [46800][55000]\t Training Loss 0.8576\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [46850][55000]\t Training Loss 0.8575\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [46900][55000]\t Training Loss 0.8575\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [46950][55000]\t Training Loss 0.8574\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47000][55000]\t Training Loss 0.8573\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [47050][55000]\t Training Loss 0.8572\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47100][55000]\t Training Loss 0.8572\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47150][55000]\t Training Loss 0.8571\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47200][55000]\t Training Loss 0.8570\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [47250][55000]\t Training Loss 0.8571\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [47300][55000]\t Training Loss 0.8572\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [47350][55000]\t Training Loss 0.8574\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [47400][55000]\t Training Loss 0.8574\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47450][55000]\t Training Loss 0.8574\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47500][55000]\t Training Loss 0.8575\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47550][55000]\t Training Loss 0.8575\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47600][55000]\t Training Loss 0.8574\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [47650][55000]\t Training Loss 0.8575\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47700][55000]\t Training Loss 0.8575\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [47750][55000]\t Training Loss 0.8574\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [47800][55000]\t Training Loss 0.8574\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [47850][55000]\t Training Loss 0.8571\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47900][55000]\t Training Loss 0.8571\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [47950][55000]\t Training Loss 0.8573\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [48000][55000]\t Training Loss 0.8573\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [48050][55000]\t Training Loss 0.8571\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [48100][55000]\t Training Loss 0.8571\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [48150][55000]\t Training Loss 0.8569\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [48200][55000]\t Training Loss 0.8567\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [48250][55000]\t Training Loss 0.8566\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [48300][55000]\t Training Loss 0.8564\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [48350][55000]\t Training Loss 0.8563\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [48400][55000]\t Training Loss 0.8565\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [48450][55000]\t Training Loss 0.8562\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [48500][55000]\t Training Loss 0.8561\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [48550][55000]\t Training Loss 0.8561\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [48600][55000]\t Training Loss 0.8561\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [48650][55000]\t Training Loss 0.8560\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [48700][55000]\t Training Loss 0.8559\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [48750][55000]\t Training Loss 0.8557\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [48800][55000]\t Training Loss 0.8556\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [48850][55000]\t Training Loss 0.8555\t Accuracy 0.8466\n",
      "Epoch [2][20]\t Batch [48900][55000]\t Training Loss 0.8554\t Accuracy 0.8467\n",
      "Epoch [2][20]\t Batch [48950][55000]\t Training Loss 0.8556\t Accuracy 0.8465\n",
      "Epoch [2][20]\t Batch [49000][55000]\t Training Loss 0.8558\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [49050][55000]\t Training Loss 0.8561\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [49100][55000]\t Training Loss 0.8562\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [49150][55000]\t Training Loss 0.8563\t Accuracy 0.8462\n",
      "Epoch [2][20]\t Batch [49200][55000]\t Training Loss 0.8563\t Accuracy 0.8461\n",
      "Epoch [2][20]\t Batch [49250][55000]\t Training Loss 0.8565\t Accuracy 0.8459\n",
      "Epoch [2][20]\t Batch [49300][55000]\t Training Loss 0.8564\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [49350][55000]\t Training Loss 0.8562\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [49400][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [49450][55000]\t Training Loss 0.8560\t Accuracy 0.8460\n",
      "Epoch [2][20]\t Batch [49500][55000]\t Training Loss 0.8562\t Accuracy 0.8458\n",
      "Epoch [2][20]\t Batch [49550][55000]\t Training Loss 0.8566\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [49600][55000]\t Training Loss 0.8569\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [49650][55000]\t Training Loss 0.8569\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [49700][55000]\t Training Loss 0.8571\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [49750][55000]\t Training Loss 0.8572\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [49800][55000]\t Training Loss 0.8571\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [49850][55000]\t Training Loss 0.8572\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [49900][55000]\t Training Loss 0.8573\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [49950][55000]\t Training Loss 0.8574\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [50000][55000]\t Training Loss 0.8575\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [50050][55000]\t Training Loss 0.8575\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [50100][55000]\t Training Loss 0.8576\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [50150][55000]\t Training Loss 0.8575\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [50200][55000]\t Training Loss 0.8574\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [50250][55000]\t Training Loss 0.8576\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [50300][55000]\t Training Loss 0.8575\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [50350][55000]\t Training Loss 0.8576\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [50400][55000]\t Training Loss 0.8578\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [50450][55000]\t Training Loss 0.8581\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [50500][55000]\t Training Loss 0.8581\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [50550][55000]\t Training Loss 0.8581\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [50600][55000]\t Training Loss 0.8582\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [50650][55000]\t Training Loss 0.8583\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [50700][55000]\t Training Loss 0.8582\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [50750][55000]\t Training Loss 0.8583\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [50800][55000]\t Training Loss 0.8583\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [50850][55000]\t Training Loss 0.8583\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [50900][55000]\t Training Loss 0.8582\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [50950][55000]\t Training Loss 0.8581\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [51000][55000]\t Training Loss 0.8579\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [51050][55000]\t Training Loss 0.8578\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [51100][55000]\t Training Loss 0.8577\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [51150][55000]\t Training Loss 0.8577\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [51200][55000]\t Training Loss 0.8578\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [51250][55000]\t Training Loss 0.8578\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [51300][55000]\t Training Loss 0.8579\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [51350][55000]\t Training Loss 0.8579\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [51400][55000]\t Training Loss 0.8579\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [51450][55000]\t Training Loss 0.8577\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [51500][55000]\t Training Loss 0.8576\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [51550][55000]\t Training Loss 0.8575\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [51600][55000]\t Training Loss 0.8574\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [51650][55000]\t Training Loss 0.8573\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [51700][55000]\t Training Loss 0.8573\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [51750][55000]\t Training Loss 0.8573\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [51800][55000]\t Training Loss 0.8574\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [51850][55000]\t Training Loss 0.8573\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [51900][55000]\t Training Loss 0.8572\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [51950][55000]\t Training Loss 0.8571\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [52000][55000]\t Training Loss 0.8572\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [52050][55000]\t Training Loss 0.8572\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [52100][55000]\t Training Loss 0.8574\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [52150][55000]\t Training Loss 0.8577\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [52200][55000]\t Training Loss 0.8578\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [52250][55000]\t Training Loss 0.8580\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [52300][55000]\t Training Loss 0.8580\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [52350][55000]\t Training Loss 0.8580\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [52400][55000]\t Training Loss 0.8580\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [52450][55000]\t Training Loss 0.8578\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [52500][55000]\t Training Loss 0.8577\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [52550][55000]\t Training Loss 0.8575\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [52600][55000]\t Training Loss 0.8573\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [52650][55000]\t Training Loss 0.8571\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [52700][55000]\t Training Loss 0.8572\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [52750][55000]\t Training Loss 0.8574\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [52800][55000]\t Training Loss 0.8575\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [52850][55000]\t Training Loss 0.8575\t Accuracy 0.8454\n",
      "Epoch [2][20]\t Batch [52900][55000]\t Training Loss 0.8577\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [52950][55000]\t Training Loss 0.8579\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [53000][55000]\t Training Loss 0.8580\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53050][55000]\t Training Loss 0.8580\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [53100][55000]\t Training Loss 0.8579\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53150][55000]\t Training Loss 0.8579\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53200][55000]\t Training Loss 0.8580\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53250][55000]\t Training Loss 0.8580\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [53300][55000]\t Training Loss 0.8580\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [53350][55000]\t Training Loss 0.8580\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [53400][55000]\t Training Loss 0.8578\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53450][55000]\t Training Loss 0.8577\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53500][55000]\t Training Loss 0.8577\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53550][55000]\t Training Loss 0.8577\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53600][55000]\t Training Loss 0.8579\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [53650][55000]\t Training Loss 0.8579\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [53700][55000]\t Training Loss 0.8579\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [53750][55000]\t Training Loss 0.8578\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [53800][55000]\t Training Loss 0.8578\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53850][55000]\t Training Loss 0.8577\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [53900][55000]\t Training Loss 0.8577\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [53950][55000]\t Training Loss 0.8577\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54000][55000]\t Training Loss 0.8579\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54050][55000]\t Training Loss 0.8580\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54100][55000]\t Training Loss 0.8581\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [54150][55000]\t Training Loss 0.8580\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [54200][55000]\t Training Loss 0.8581\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [54250][55000]\t Training Loss 0.8580\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [54300][55000]\t Training Loss 0.8579\t Accuracy 0.8449\n",
      "Epoch [2][20]\t Batch [54350][55000]\t Training Loss 0.8579\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54400][55000]\t Training Loss 0.8578\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54450][55000]\t Training Loss 0.8578\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54500][55000]\t Training Loss 0.8577\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [54550][55000]\t Training Loss 0.8576\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54600][55000]\t Training Loss 0.8576\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54650][55000]\t Training Loss 0.8575\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54700][55000]\t Training Loss 0.8573\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [54750][55000]\t Training Loss 0.8572\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [54800][55000]\t Training Loss 0.8572\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [54850][55000]\t Training Loss 0.8570\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [54900][55000]\t Training Loss 0.8572\t Accuracy 0.8452\n",
      "Epoch [2][20]\t Batch [54950][55000]\t Training Loss 0.8575\t Accuracy 0.8451\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8576\t Average training accuracy 0.8450\n",
      "Epoch [2]\t Average validation loss 0.7948\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [3][20]\t Batch [0][55000]\t Training Loss 1.4609\t Accuracy 0.0000\n",
      "Epoch [3][20]\t Batch [50][55000]\t Training Loss 0.9427\t Accuracy 0.7255\n",
      "Epoch [3][20]\t Batch [100][55000]\t Training Loss 0.8519\t Accuracy 0.8020\n",
      "Epoch [3][20]\t Batch [150][55000]\t Training Loss 0.8533\t Accuracy 0.8146\n",
      "Epoch [3][20]\t Batch [200][55000]\t Training Loss 0.8637\t Accuracy 0.8159\n",
      "Epoch [3][20]\t Batch [250][55000]\t Training Loss 0.8463\t Accuracy 0.8247\n",
      "Epoch [3][20]\t Batch [300][55000]\t Training Loss 0.8471\t Accuracy 0.8239\n",
      "Epoch [3][20]\t Batch [350][55000]\t Training Loss 0.8276\t Accuracy 0.8262\n",
      "Epoch [3][20]\t Batch [400][55000]\t Training Loss 0.8139\t Accuracy 0.8304\n",
      "Epoch [3][20]\t Batch [450][55000]\t Training Loss 0.8195\t Accuracy 0.8337\n",
      "Epoch [3][20]\t Batch [500][55000]\t Training Loss 0.8258\t Accuracy 0.8363\n",
      "Epoch [3][20]\t Batch [550][55000]\t Training Loss 0.8427\t Accuracy 0.8330\n",
      "Epoch [3][20]\t Batch [600][55000]\t Training Loss 0.8472\t Accuracy 0.8369\n",
      "Epoch [3][20]\t Batch [650][55000]\t Training Loss 0.8671\t Accuracy 0.8264\n",
      "Epoch [3][20]\t Batch [700][55000]\t Training Loss 0.8647\t Accuracy 0.8317\n",
      "Epoch [3][20]\t Batch [750][55000]\t Training Loss 0.8610\t Accuracy 0.8336\n",
      "Epoch [3][20]\t Batch [800][55000]\t Training Loss 0.8550\t Accuracy 0.8352\n",
      "Epoch [3][20]\t Batch [850][55000]\t Training Loss 0.8533\t Accuracy 0.8367\n",
      "Epoch [3][20]\t Batch [900][55000]\t Training Loss 0.8598\t Accuracy 0.8357\n",
      "Epoch [3][20]\t Batch [950][55000]\t Training Loss 0.8667\t Accuracy 0.8307\n",
      "Epoch [3][20]\t Batch [1000][55000]\t Training Loss 0.8662\t Accuracy 0.8332\n",
      "Epoch [3][20]\t Batch [1050][55000]\t Training Loss 0.8739\t Accuracy 0.8316\n",
      "Epoch [3][20]\t Batch [1100][55000]\t Training Loss 0.8822\t Accuracy 0.8302\n",
      "Epoch [3][20]\t Batch [1150][55000]\t Training Loss 0.8916\t Accuracy 0.8271\n",
      "Epoch [3][20]\t Batch [1200][55000]\t Training Loss 0.8857\t Accuracy 0.8301\n",
      "Epoch [3][20]\t Batch [1250][55000]\t Training Loss 0.8870\t Accuracy 0.8297\n",
      "Epoch [3][20]\t Batch [1300][55000]\t Training Loss 0.8873\t Accuracy 0.8294\n",
      "Epoch [3][20]\t Batch [1350][55000]\t Training Loss 0.8841\t Accuracy 0.8305\n",
      "Epoch [3][20]\t Batch [1400][55000]\t Training Loss 0.8846\t Accuracy 0.8287\n",
      "Epoch [3][20]\t Batch [1450][55000]\t Training Loss 0.8856\t Accuracy 0.8277\n",
      "Epoch [3][20]\t Batch [1500][55000]\t Training Loss 0.8837\t Accuracy 0.8308\n",
      "Epoch [3][20]\t Batch [1550][55000]\t Training Loss 0.8833\t Accuracy 0.8311\n",
      "Epoch [3][20]\t Batch [1600][55000]\t Training Loss 0.8859\t Accuracy 0.8301\n",
      "Epoch [3][20]\t Batch [1650][55000]\t Training Loss 0.8828\t Accuracy 0.8316\n",
      "Epoch [3][20]\t Batch [1700][55000]\t Training Loss 0.8812\t Accuracy 0.8325\n",
      "Epoch [3][20]\t Batch [1750][55000]\t Training Loss 0.8755\t Accuracy 0.8332\n",
      "Epoch [3][20]\t Batch [1800][55000]\t Training Loss 0.8734\t Accuracy 0.8351\n",
      "Epoch [3][20]\t Batch [1850][55000]\t Training Loss 0.8717\t Accuracy 0.8358\n",
      "Epoch [3][20]\t Batch [1900][55000]\t Training Loss 0.8696\t Accuracy 0.8359\n",
      "Epoch [3][20]\t Batch [1950][55000]\t Training Loss 0.8681\t Accuracy 0.8370\n",
      "Epoch [3][20]\t Batch [2000][55000]\t Training Loss 0.8656\t Accuracy 0.8381\n",
      "Epoch [3][20]\t Batch [2050][55000]\t Training Loss 0.8647\t Accuracy 0.8386\n",
      "Epoch [3][20]\t Batch [2100][55000]\t Training Loss 0.8619\t Accuracy 0.8391\n",
      "Epoch [3][20]\t Batch [2150][55000]\t Training Loss 0.8586\t Accuracy 0.8410\n",
      "Epoch [3][20]\t Batch [2200][55000]\t Training Loss 0.8538\t Accuracy 0.8428\n",
      "Epoch [3][20]\t Batch [2250][55000]\t Training Loss 0.8530\t Accuracy 0.8432\n",
      "Epoch [3][20]\t Batch [2300][55000]\t Training Loss 0.8500\t Accuracy 0.8427\n",
      "Epoch [3][20]\t Batch [2350][55000]\t Training Loss 0.8485\t Accuracy 0.8430\n",
      "Epoch [3][20]\t Batch [2400][55000]\t Training Loss 0.8494\t Accuracy 0.8413\n",
      "Epoch [3][20]\t Batch [2450][55000]\t Training Loss 0.8522\t Accuracy 0.8397\n",
      "Epoch [3][20]\t Batch [2500][55000]\t Training Loss 0.8500\t Accuracy 0.8417\n",
      "Epoch [3][20]\t Batch [2550][55000]\t Training Loss 0.8487\t Accuracy 0.8416\n",
      "Epoch [3][20]\t Batch [2600][55000]\t Training Loss 0.8481\t Accuracy 0.8416\n",
      "Epoch [3][20]\t Batch [2650][55000]\t Training Loss 0.8469\t Accuracy 0.8419\n",
      "Epoch [3][20]\t Batch [2700][55000]\t Training Loss 0.8466\t Accuracy 0.8423\n",
      "Epoch [3][20]\t Batch [2750][55000]\t Training Loss 0.8466\t Accuracy 0.8422\n",
      "Epoch [3][20]\t Batch [2800][55000]\t Training Loss 0.8468\t Accuracy 0.8404\n",
      "Epoch [3][20]\t Batch [2850][55000]\t Training Loss 0.8457\t Accuracy 0.8401\n",
      "Epoch [3][20]\t Batch [2900][55000]\t Training Loss 0.8425\t Accuracy 0.8411\n",
      "Epoch [3][20]\t Batch [2950][55000]\t Training Loss 0.8425\t Accuracy 0.8404\n",
      "Epoch [3][20]\t Batch [3000][55000]\t Training Loss 0.8424\t Accuracy 0.8411\n",
      "Epoch [3][20]\t Batch [3050][55000]\t Training Loss 0.8434\t Accuracy 0.8407\n",
      "Epoch [3][20]\t Batch [3100][55000]\t Training Loss 0.8457\t Accuracy 0.8404\n",
      "Epoch [3][20]\t Batch [3150][55000]\t Training Loss 0.8456\t Accuracy 0.8407\n",
      "Epoch [3][20]\t Batch [3200][55000]\t Training Loss 0.8454\t Accuracy 0.8419\n",
      "Epoch [3][20]\t Batch [3250][55000]\t Training Loss 0.8448\t Accuracy 0.8413\n",
      "Epoch [3][20]\t Batch [3300][55000]\t Training Loss 0.8461\t Accuracy 0.8410\n",
      "Epoch [3][20]\t Batch [3350][55000]\t Training Loss 0.8443\t Accuracy 0.8424\n",
      "Epoch [3][20]\t Batch [3400][55000]\t Training Loss 0.8466\t Accuracy 0.8409\n",
      "Epoch [3][20]\t Batch [3450][55000]\t Training Loss 0.8463\t Accuracy 0.8418\n",
      "Epoch [3][20]\t Batch [3500][55000]\t Training Loss 0.8463\t Accuracy 0.8415\n",
      "Epoch [3][20]\t Batch [3550][55000]\t Training Loss 0.8478\t Accuracy 0.8409\n",
      "Epoch [3][20]\t Batch [3600][55000]\t Training Loss 0.8477\t Accuracy 0.8406\n",
      "Epoch [3][20]\t Batch [3650][55000]\t Training Loss 0.8462\t Accuracy 0.8414\n",
      "Epoch [3][20]\t Batch [3700][55000]\t Training Loss 0.8470\t Accuracy 0.8409\n",
      "Epoch [3][20]\t Batch [3750][55000]\t Training Loss 0.8469\t Accuracy 0.8411\n",
      "Epoch [3][20]\t Batch [3800][55000]\t Training Loss 0.8472\t Accuracy 0.8411\n",
      "Epoch [3][20]\t Batch [3850][55000]\t Training Loss 0.8468\t Accuracy 0.8413\n",
      "Epoch [3][20]\t Batch [3900][55000]\t Training Loss 0.8457\t Accuracy 0.8426\n",
      "Epoch [3][20]\t Batch [3950][55000]\t Training Loss 0.8446\t Accuracy 0.8438\n",
      "Epoch [3][20]\t Batch [4000][55000]\t Training Loss 0.8442\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [4050][55000]\t Training Loss 0.8427\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [4100][55000]\t Training Loss 0.8431\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [4150][55000]\t Training Loss 0.8436\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [4200][55000]\t Training Loss 0.8440\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [4250][55000]\t Training Loss 0.8426\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [4300][55000]\t Training Loss 0.8427\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [4350][55000]\t Training Loss 0.8432\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [4400][55000]\t Training Loss 0.8431\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [4450][55000]\t Training Loss 0.8433\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [4500][55000]\t Training Loss 0.8435\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [4550][55000]\t Training Loss 0.8419\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [4600][55000]\t Training Loss 0.8402\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [4650][55000]\t Training Loss 0.8400\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [4700][55000]\t Training Loss 0.8396\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [4750][55000]\t Training Loss 0.8385\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [4800][55000]\t Training Loss 0.8392\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [4850][55000]\t Training Loss 0.8404\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [4900][55000]\t Training Loss 0.8389\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [4950][55000]\t Training Loss 0.8392\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [5000][55000]\t Training Loss 0.8393\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [5050][55000]\t Training Loss 0.8388\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [5100][55000]\t Training Loss 0.8388\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [5150][55000]\t Training Loss 0.8391\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [5200][55000]\t Training Loss 0.8406\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [5250][55000]\t Training Loss 0.8397\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [5300][55000]\t Training Loss 0.8396\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [5350][55000]\t Training Loss 0.8402\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [5400][55000]\t Training Loss 0.8394\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [5450][55000]\t Training Loss 0.8387\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [5500][55000]\t Training Loss 0.8369\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [5550][55000]\t Training Loss 0.8368\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [5600][55000]\t Training Loss 0.8359\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [5650][55000]\t Training Loss 0.8365\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [5700][55000]\t Training Loss 0.8362\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [5750][55000]\t Training Loss 0.8366\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [5800][55000]\t Training Loss 0.8370\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [5850][55000]\t Training Loss 0.8371\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [5900][55000]\t Training Loss 0.8377\t Accuracy 0.8453\n",
      "Epoch [3][20]\t Batch [5950][55000]\t Training Loss 0.8375\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [6000][55000]\t Training Loss 0.8365\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [6050][55000]\t Training Loss 0.8352\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [6100][55000]\t Training Loss 0.8337\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [6150][55000]\t Training Loss 0.8316\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [6200][55000]\t Training Loss 0.8317\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [6250][55000]\t Training Loss 0.8305\t Accuracy 0.8479\n",
      "Epoch [3][20]\t Batch [6300][55000]\t Training Loss 0.8309\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [6350][55000]\t Training Loss 0.8306\t Accuracy 0.8482\n",
      "Epoch [3][20]\t Batch [6400][55000]\t Training Loss 0.8300\t Accuracy 0.8486\n",
      "Epoch [3][20]\t Batch [6450][55000]\t Training Loss 0.8295\t Accuracy 0.8489\n",
      "Epoch [3][20]\t Batch [6500][55000]\t Training Loss 0.8303\t Accuracy 0.8485\n",
      "Epoch [3][20]\t Batch [6550][55000]\t Training Loss 0.8293\t Accuracy 0.8487\n",
      "Epoch [3][20]\t Batch [6600][55000]\t Training Loss 0.8280\t Accuracy 0.8493\n",
      "Epoch [3][20]\t Batch [6650][55000]\t Training Loss 0.8268\t Accuracy 0.8495\n",
      "Epoch [3][20]\t Batch [6700][55000]\t Training Loss 0.8270\t Accuracy 0.8494\n",
      "Epoch [3][20]\t Batch [6750][55000]\t Training Loss 0.8272\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [6800][55000]\t Training Loss 0.8276\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [6850][55000]\t Training Loss 0.8298\t Accuracy 0.8498\n",
      "Epoch [3][20]\t Batch [6900][55000]\t Training Loss 0.8297\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [6950][55000]\t Training Loss 0.8304\t Accuracy 0.8487\n",
      "Epoch [3][20]\t Batch [7000][55000]\t Training Loss 0.8303\t Accuracy 0.8487\n",
      "Epoch [3][20]\t Batch [7050][55000]\t Training Loss 0.8308\t Accuracy 0.8482\n",
      "Epoch [3][20]\t Batch [7100][55000]\t Training Loss 0.8310\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [7150][55000]\t Training Loss 0.8312\t Accuracy 0.8488\n",
      "Epoch [3][20]\t Batch [7200][55000]\t Training Loss 0.8320\t Accuracy 0.8488\n",
      "Epoch [3][20]\t Batch [7250][55000]\t Training Loss 0.8343\t Accuracy 0.8482\n",
      "Epoch [3][20]\t Batch [7300][55000]\t Training Loss 0.8362\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [7350][55000]\t Training Loss 0.8377\t Accuracy 0.8479\n",
      "Epoch [3][20]\t Batch [7400][55000]\t Training Loss 0.8387\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [7450][55000]\t Training Loss 0.8387\t Accuracy 0.8482\n",
      "Epoch [3][20]\t Batch [7500][55000]\t Training Loss 0.8387\t Accuracy 0.8479\n",
      "Epoch [3][20]\t Batch [7550][55000]\t Training Loss 0.8392\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [7600][55000]\t Training Loss 0.8387\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [7650][55000]\t Training Loss 0.8392\t Accuracy 0.8484\n",
      "Epoch [3][20]\t Batch [7700][55000]\t Training Loss 0.8397\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [7750][55000]\t Training Loss 0.8398\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [7800][55000]\t Training Loss 0.8406\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [7850][55000]\t Training Loss 0.8407\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [7900][55000]\t Training Loss 0.8409\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [7950][55000]\t Training Loss 0.8409\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [8000][55000]\t Training Loss 0.8409\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [8050][55000]\t Training Loss 0.8415\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [8100][55000]\t Training Loss 0.8400\t Accuracy 0.8478\n",
      "Epoch [3][20]\t Batch [8150][55000]\t Training Loss 0.8408\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [8200][55000]\t Training Loss 0.8408\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [8250][55000]\t Training Loss 0.8421\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [8300][55000]\t Training Loss 0.8424\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [8350][55000]\t Training Loss 0.8430\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [8400][55000]\t Training Loss 0.8424\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [8450][55000]\t Training Loss 0.8439\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [8500][55000]\t Training Loss 0.8431\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [8550][55000]\t Training Loss 0.8421\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [8600][55000]\t Training Loss 0.8413\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [8650][55000]\t Training Loss 0.8414\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [8700][55000]\t Training Loss 0.8420\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [8750][55000]\t Training Loss 0.8436\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [8800][55000]\t Training Loss 0.8444\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [8850][55000]\t Training Loss 0.8443\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [8900][55000]\t Training Loss 0.8461\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [8950][55000]\t Training Loss 0.8456\t Accuracy 0.8453\n",
      "Epoch [3][20]\t Batch [9000][55000]\t Training Loss 0.8449\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [9050][55000]\t Training Loss 0.8441\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [9100][55000]\t Training Loss 0.8438\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [9150][55000]\t Training Loss 0.8443\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [9200][55000]\t Training Loss 0.8439\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [9250][55000]\t Training Loss 0.8442\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [9300][55000]\t Training Loss 0.8444\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [9350][55000]\t Training Loss 0.8446\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [9400][55000]\t Training Loss 0.8449\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [9450][55000]\t Training Loss 0.8451\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [9500][55000]\t Training Loss 0.8442\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [9550][55000]\t Training Loss 0.8440\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [9600][55000]\t Training Loss 0.8444\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [9650][55000]\t Training Loss 0.8442\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [9700][55000]\t Training Loss 0.8437\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [9750][55000]\t Training Loss 0.8429\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [9800][55000]\t Training Loss 0.8435\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [9850][55000]\t Training Loss 0.8430\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [9900][55000]\t Training Loss 0.8425\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [9950][55000]\t Training Loss 0.8419\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [10000][55000]\t Training Loss 0.8417\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [10050][55000]\t Training Loss 0.8418\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [10100][55000]\t Training Loss 0.8416\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [10150][55000]\t Training Loss 0.8414\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [10200][55000]\t Training Loss 0.8416\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [10250][55000]\t Training Loss 0.8417\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [10300][55000]\t Training Loss 0.8416\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [10350][55000]\t Training Loss 0.8407\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [10400][55000]\t Training Loss 0.8399\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [10450][55000]\t Training Loss 0.8397\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [10500][55000]\t Training Loss 0.8387\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [10550][55000]\t Training Loss 0.8383\t Accuracy 0.8485\n",
      "Epoch [3][20]\t Batch [10600][55000]\t Training Loss 0.8378\t Accuracy 0.8486\n",
      "Epoch [3][20]\t Batch [10650][55000]\t Training Loss 0.8376\t Accuracy 0.8489\n",
      "Epoch [3][20]\t Batch [10700][55000]\t Training Loss 0.8373\t Accuracy 0.8493\n",
      "Epoch [3][20]\t Batch [10750][55000]\t Training Loss 0.8378\t Accuracy 0.8489\n",
      "Epoch [3][20]\t Batch [10800][55000]\t Training Loss 0.8383\t Accuracy 0.8486\n",
      "Epoch [3][20]\t Batch [10850][55000]\t Training Loss 0.8376\t Accuracy 0.8489\n",
      "Epoch [3][20]\t Batch [10900][55000]\t Training Loss 0.8371\t Accuracy 0.8491\n",
      "Epoch [3][20]\t Batch [10950][55000]\t Training Loss 0.8368\t Accuracy 0.8489\n",
      "Epoch [3][20]\t Batch [11000][55000]\t Training Loss 0.8367\t Accuracy 0.8491\n",
      "Epoch [3][20]\t Batch [11050][55000]\t Training Loss 0.8360\t Accuracy 0.8493\n",
      "Epoch [3][20]\t Batch [11100][55000]\t Training Loss 0.8353\t Accuracy 0.8495\n",
      "Epoch [3][20]\t Batch [11150][55000]\t Training Loss 0.8353\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [11200][55000]\t Training Loss 0.8349\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [11250][55000]\t Training Loss 0.8353\t Accuracy 0.8495\n",
      "Epoch [3][20]\t Batch [11300][55000]\t Training Loss 0.8349\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [11350][55000]\t Training Loss 0.8344\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [11400][55000]\t Training Loss 0.8345\t Accuracy 0.8496\n",
      "Epoch [3][20]\t Batch [11450][55000]\t Training Loss 0.8342\t Accuracy 0.8495\n",
      "Epoch [3][20]\t Batch [11500][55000]\t Training Loss 0.8339\t Accuracy 0.8494\n",
      "Epoch [3][20]\t Batch [11550][55000]\t Training Loss 0.8340\t Accuracy 0.8493\n",
      "Epoch [3][20]\t Batch [11600][55000]\t Training Loss 0.8352\t Accuracy 0.8486\n",
      "Epoch [3][20]\t Batch [11650][55000]\t Training Loss 0.8357\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [11700][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [3][20]\t Batch [11750][55000]\t Training Loss 0.8366\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [11800][55000]\t Training Loss 0.8368\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [11850][55000]\t Training Loss 0.8369\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [11900][55000]\t Training Loss 0.8372\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [11950][55000]\t Training Loss 0.8373\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [12000][55000]\t Training Loss 0.8373\t Accuracy 0.8486\n",
      "Epoch [3][20]\t Batch [12050][55000]\t Training Loss 0.8371\t Accuracy 0.8489\n",
      "Epoch [3][20]\t Batch [12100][55000]\t Training Loss 0.8370\t Accuracy 0.8488\n",
      "Epoch [3][20]\t Batch [12150][55000]\t Training Loss 0.8363\t Accuracy 0.8492\n",
      "Epoch [3][20]\t Batch [12200][55000]\t Training Loss 0.8367\t Accuracy 0.8492\n",
      "Epoch [3][20]\t Batch [12250][55000]\t Training Loss 0.8365\t Accuracy 0.8492\n",
      "Epoch [3][20]\t Batch [12300][55000]\t Training Loss 0.8366\t Accuracy 0.8492\n",
      "Epoch [3][20]\t Batch [12350][55000]\t Training Loss 0.8368\t Accuracy 0.8493\n",
      "Epoch [3][20]\t Batch [12400][55000]\t Training Loss 0.8371\t Accuracy 0.8494\n",
      "Epoch [3][20]\t Batch [12450][55000]\t Training Loss 0.8373\t Accuracy 0.8490\n",
      "Epoch [3][20]\t Batch [12500][55000]\t Training Loss 0.8375\t Accuracy 0.8488\n",
      "Epoch [3][20]\t Batch [12550][55000]\t Training Loss 0.8375\t Accuracy 0.8489\n",
      "Epoch [3][20]\t Batch [12600][55000]\t Training Loss 0.8382\t Accuracy 0.8487\n",
      "Epoch [3][20]\t Batch [12650][55000]\t Training Loss 0.8387\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [12700][55000]\t Training Loss 0.8394\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [12750][55000]\t Training Loss 0.8390\t Accuracy 0.8482\n",
      "Epoch [3][20]\t Batch [12800][55000]\t Training Loss 0.8395\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [12850][55000]\t Training Loss 0.8398\t Accuracy 0.8479\n",
      "Epoch [3][20]\t Batch [12900][55000]\t Training Loss 0.8397\t Accuracy 0.8482\n",
      "Epoch [3][20]\t Batch [12950][55000]\t Training Loss 0.8402\t Accuracy 0.8478\n",
      "Epoch [3][20]\t Batch [13000][55000]\t Training Loss 0.8402\t Accuracy 0.8478\n",
      "Epoch [3][20]\t Batch [13050][55000]\t Training Loss 0.8409\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [13100][55000]\t Training Loss 0.8415\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [13150][55000]\t Training Loss 0.8419\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [13200][55000]\t Training Loss 0.8421\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [13250][55000]\t Training Loss 0.8416\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [13300][55000]\t Training Loss 0.8415\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [13350][55000]\t Training Loss 0.8419\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [13400][55000]\t Training Loss 0.8421\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [13450][55000]\t Training Loss 0.8416\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [13500][55000]\t Training Loss 0.8413\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [13550][55000]\t Training Loss 0.8409\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [13600][55000]\t Training Loss 0.8400\t Accuracy 0.8478\n",
      "Epoch [3][20]\t Batch [13650][55000]\t Training Loss 0.8399\t Accuracy 0.8478\n",
      "Epoch [3][20]\t Batch [13700][55000]\t Training Loss 0.8407\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [13750][55000]\t Training Loss 0.8413\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [13800][55000]\t Training Loss 0.8414\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [13850][55000]\t Training Loss 0.8414\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [13900][55000]\t Training Loss 0.8416\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [13950][55000]\t Training Loss 0.8419\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [14000][55000]\t Training Loss 0.8427\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [14050][55000]\t Training Loss 0.8429\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [14100][55000]\t Training Loss 0.8429\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [14150][55000]\t Training Loss 0.8432\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [14200][55000]\t Training Loss 0.8431\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [14250][55000]\t Training Loss 0.8434\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [14300][55000]\t Training Loss 0.8437\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [14350][55000]\t Training Loss 0.8441\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [14400][55000]\t Training Loss 0.8449\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [14450][55000]\t Training Loss 0.8451\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [14500][55000]\t Training Loss 0.8452\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [14550][55000]\t Training Loss 0.8459\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [14600][55000]\t Training Loss 0.8459\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [14650][55000]\t Training Loss 0.8469\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [14700][55000]\t Training Loss 0.8477\t Accuracy 0.8444\n",
      "Epoch [3][20]\t Batch [14750][55000]\t Training Loss 0.8483\t Accuracy 0.8441\n",
      "Epoch [3][20]\t Batch [14800][55000]\t Training Loss 0.8492\t Accuracy 0.8439\n",
      "Epoch [3][20]\t Batch [14850][55000]\t Training Loss 0.8498\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [14900][55000]\t Training Loss 0.8498\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [14950][55000]\t Training Loss 0.8499\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [15000][55000]\t Training Loss 0.8498\t Accuracy 0.8435\n",
      "Epoch [3][20]\t Batch [15050][55000]\t Training Loss 0.8494\t Accuracy 0.8438\n",
      "Epoch [3][20]\t Batch [15100][55000]\t Training Loss 0.8492\t Accuracy 0.8439\n",
      "Epoch [3][20]\t Batch [15150][55000]\t Training Loss 0.8496\t Accuracy 0.8438\n",
      "Epoch [3][20]\t Batch [15200][55000]\t Training Loss 0.8498\t Accuracy 0.8438\n",
      "Epoch [3][20]\t Batch [15250][55000]\t Training Loss 0.8497\t Accuracy 0.8440\n",
      "Epoch [3][20]\t Batch [15300][55000]\t Training Loss 0.8498\t Accuracy 0.8440\n",
      "Epoch [3][20]\t Batch [15350][55000]\t Training Loss 0.8495\t Accuracy 0.8442\n",
      "Epoch [3][20]\t Batch [15400][55000]\t Training Loss 0.8498\t Accuracy 0.8444\n",
      "Epoch [3][20]\t Batch [15450][55000]\t Training Loss 0.8499\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [15500][55000]\t Training Loss 0.8497\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [15550][55000]\t Training Loss 0.8496\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [15600][55000]\t Training Loss 0.8494\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [15650][55000]\t Training Loss 0.8494\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [15700][55000]\t Training Loss 0.8492\t Accuracy 0.8453\n",
      "Epoch [3][20]\t Batch [15750][55000]\t Training Loss 0.8501\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [15800][55000]\t Training Loss 0.8505\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [15850][55000]\t Training Loss 0.8509\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [15900][55000]\t Training Loss 0.8514\t Accuracy 0.8442\n",
      "Epoch [3][20]\t Batch [15950][55000]\t Training Loss 0.8515\t Accuracy 0.8442\n",
      "Epoch [3][20]\t Batch [16000][55000]\t Training Loss 0.8515\t Accuracy 0.8440\n",
      "Epoch [3][20]\t Batch [16050][55000]\t Training Loss 0.8525\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [16100][55000]\t Training Loss 0.8525\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [16150][55000]\t Training Loss 0.8524\t Accuracy 0.8438\n",
      "Epoch [3][20]\t Batch [16200][55000]\t Training Loss 0.8525\t Accuracy 0.8435\n",
      "Epoch [3][20]\t Batch [16250][55000]\t Training Loss 0.8523\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [16300][55000]\t Training Loss 0.8520\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [16350][55000]\t Training Loss 0.8517\t Accuracy 0.8437\n",
      "Epoch [3][20]\t Batch [16400][55000]\t Training Loss 0.8518\t Accuracy 0.8437\n",
      "Epoch [3][20]\t Batch [16450][55000]\t Training Loss 0.8515\t Accuracy 0.8440\n",
      "Epoch [3][20]\t Batch [16500][55000]\t Training Loss 0.8513\t Accuracy 0.8441\n",
      "Epoch [3][20]\t Batch [16550][55000]\t Training Loss 0.8509\t Accuracy 0.8442\n",
      "Epoch [3][20]\t Batch [16600][55000]\t Training Loss 0.8510\t Accuracy 0.8442\n",
      "Epoch [3][20]\t Batch [16650][55000]\t Training Loss 0.8510\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [16700][55000]\t Training Loss 0.8511\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [16750][55000]\t Training Loss 0.8510\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [16800][55000]\t Training Loss 0.8516\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [16850][55000]\t Training Loss 0.8521\t Accuracy 0.8440\n",
      "Epoch [3][20]\t Batch [16900][55000]\t Training Loss 0.8524\t Accuracy 0.8440\n",
      "Epoch [3][20]\t Batch [16950][55000]\t Training Loss 0.8524\t Accuracy 0.8439\n",
      "Epoch [3][20]\t Batch [17000][55000]\t Training Loss 0.8531\t Accuracy 0.8435\n",
      "Epoch [3][20]\t Batch [17050][55000]\t Training Loss 0.8530\t Accuracy 0.8435\n",
      "Epoch [3][20]\t Batch [17100][55000]\t Training Loss 0.8535\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [17150][55000]\t Training Loss 0.8533\t Accuracy 0.8435\n",
      "Epoch [3][20]\t Batch [17200][55000]\t Training Loss 0.8533\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [17250][55000]\t Training Loss 0.8538\t Accuracy 0.8433\n",
      "Epoch [3][20]\t Batch [17300][55000]\t Training Loss 0.8537\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [17350][55000]\t Training Loss 0.8531\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [17400][55000]\t Training Loss 0.8532\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [17450][55000]\t Training Loss 0.8534\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [17500][55000]\t Training Loss 0.8534\t Accuracy 0.8437\n",
      "Epoch [3][20]\t Batch [17550][55000]\t Training Loss 0.8541\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [17600][55000]\t Training Loss 0.8547\t Accuracy 0.8432\n",
      "Epoch [3][20]\t Batch [17650][55000]\t Training Loss 0.8549\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [17700][55000]\t Training Loss 0.8556\t Accuracy 0.8432\n",
      "Epoch [3][20]\t Batch [17750][55000]\t Training Loss 0.8560\t Accuracy 0.8432\n",
      "Epoch [3][20]\t Batch [17800][55000]\t Training Loss 0.8564\t Accuracy 0.8429\n",
      "Epoch [3][20]\t Batch [17850][55000]\t Training Loss 0.8566\t Accuracy 0.8428\n",
      "Epoch [3][20]\t Batch [17900][55000]\t Training Loss 0.8570\t Accuracy 0.8426\n",
      "Epoch [3][20]\t Batch [17950][55000]\t Training Loss 0.8568\t Accuracy 0.8425\n",
      "Epoch [3][20]\t Batch [18000][55000]\t Training Loss 0.8564\t Accuracy 0.8427\n",
      "Epoch [3][20]\t Batch [18050][55000]\t Training Loss 0.8567\t Accuracy 0.8427\n",
      "Epoch [3][20]\t Batch [18100][55000]\t Training Loss 0.8565\t Accuracy 0.8428\n",
      "Epoch [3][20]\t Batch [18150][55000]\t Training Loss 0.8561\t Accuracy 0.8429\n",
      "Epoch [3][20]\t Batch [18200][55000]\t Training Loss 0.8557\t Accuracy 0.8432\n",
      "Epoch [3][20]\t Batch [18250][55000]\t Training Loss 0.8556\t Accuracy 0.8432\n",
      "Epoch [3][20]\t Batch [18300][55000]\t Training Loss 0.8552\t Accuracy 0.8433\n",
      "Epoch [3][20]\t Batch [18350][55000]\t Training Loss 0.8551\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [18400][55000]\t Training Loss 0.8551\t Accuracy 0.8435\n",
      "Epoch [3][20]\t Batch [18450][55000]\t Training Loss 0.8555\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [18500][55000]\t Training Loss 0.8556\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [18550][55000]\t Training Loss 0.8553\t Accuracy 0.8436\n",
      "Epoch [3][20]\t Batch [18600][55000]\t Training Loss 0.8553\t Accuracy 0.8439\n",
      "Epoch [3][20]\t Batch [18650][55000]\t Training Loss 0.8552\t Accuracy 0.8440\n",
      "Epoch [3][20]\t Batch [18700][55000]\t Training Loss 0.8554\t Accuracy 0.8439\n",
      "Epoch [3][20]\t Batch [18750][55000]\t Training Loss 0.8556\t Accuracy 0.8439\n",
      "Epoch [3][20]\t Batch [18800][55000]\t Training Loss 0.8552\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [18850][55000]\t Training Loss 0.8555\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [18900][55000]\t Training Loss 0.8550\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [18950][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [19000][55000]\t Training Loss 0.8547\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [19050][55000]\t Training Loss 0.8551\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [19100][55000]\t Training Loss 0.8554\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [19150][55000]\t Training Loss 0.8555\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [19200][55000]\t Training Loss 0.8556\t Accuracy 0.8442\n",
      "Epoch [3][20]\t Batch [19250][55000]\t Training Loss 0.8554\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [19300][55000]\t Training Loss 0.8552\t Accuracy 0.8444\n",
      "Epoch [3][20]\t Batch [19350][55000]\t Training Loss 0.8553\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [19400][55000]\t Training Loss 0.8551\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [19450][55000]\t Training Loss 0.8548\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [19500][55000]\t Training Loss 0.8545\t Accuracy 0.8444\n",
      "Epoch [3][20]\t Batch [19550][55000]\t Training Loss 0.8545\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [19600][55000]\t Training Loss 0.8545\t Accuracy 0.8441\n",
      "Epoch [3][20]\t Batch [19650][55000]\t Training Loss 0.8541\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [19700][55000]\t Training Loss 0.8535\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [19750][55000]\t Training Loss 0.8530\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [19800][55000]\t Training Loss 0.8524\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [19850][55000]\t Training Loss 0.8524\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [19900][55000]\t Training Loss 0.8522\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [19950][55000]\t Training Loss 0.8523\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [20000][55000]\t Training Loss 0.8523\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [20050][55000]\t Training Loss 0.8528\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [20100][55000]\t Training Loss 0.8529\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [20150][55000]\t Training Loss 0.8527\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [20200][55000]\t Training Loss 0.8531\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [20250][55000]\t Training Loss 0.8532\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [20300][55000]\t Training Loss 0.8533\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [20350][55000]\t Training Loss 0.8533\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [20400][55000]\t Training Loss 0.8530\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [20450][55000]\t Training Loss 0.8526\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [20500][55000]\t Training Loss 0.8523\t Accuracy 0.8453\n",
      "Epoch [3][20]\t Batch [20550][55000]\t Training Loss 0.8523\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [20600][55000]\t Training Loss 0.8522\t Accuracy 0.8453\n",
      "Epoch [3][20]\t Batch [20650][55000]\t Training Loss 0.8519\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [20700][55000]\t Training Loss 0.8517\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [20750][55000]\t Training Loss 0.8517\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [20800][55000]\t Training Loss 0.8517\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [20850][55000]\t Training Loss 0.8517\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [20900][55000]\t Training Loss 0.8520\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [20950][55000]\t Training Loss 0.8525\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [21000][55000]\t Training Loss 0.8527\t Accuracy 0.8444\n",
      "Epoch [3][20]\t Batch [21050][55000]\t Training Loss 0.8529\t Accuracy 0.8444\n",
      "Epoch [3][20]\t Batch [21100][55000]\t Training Loss 0.8527\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [21150][55000]\t Training Loss 0.8526\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [21200][55000]\t Training Loss 0.8524\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [21250][55000]\t Training Loss 0.8522\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [21300][55000]\t Training Loss 0.8518\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [21350][55000]\t Training Loss 0.8519\t Accuracy 0.8453\n",
      "Epoch [3][20]\t Batch [21400][55000]\t Training Loss 0.8519\t Accuracy 0.8453\n",
      "Epoch [3][20]\t Batch [21450][55000]\t Training Loss 0.8520\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [21500][55000]\t Training Loss 0.8516\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [21550][55000]\t Training Loss 0.8514\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [21600][55000]\t Training Loss 0.8515\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [21650][55000]\t Training Loss 0.8515\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [21700][55000]\t Training Loss 0.8515\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [21750][55000]\t Training Loss 0.8515\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [21800][55000]\t Training Loss 0.8509\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [21850][55000]\t Training Loss 0.8505\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [21900][55000]\t Training Loss 0.8501\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [21950][55000]\t Training Loss 0.8496\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [22000][55000]\t Training Loss 0.8492\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [22050][55000]\t Training Loss 0.8489\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [22100][55000]\t Training Loss 0.8490\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [22150][55000]\t Training Loss 0.8494\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [22200][55000]\t Training Loss 0.8497\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [22250][55000]\t Training Loss 0.8497\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [22300][55000]\t Training Loss 0.8499\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [22350][55000]\t Training Loss 0.8496\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [22400][55000]\t Training Loss 0.8495\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [22450][55000]\t Training Loss 0.8495\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [22500][55000]\t Training Loss 0.8500\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [22550][55000]\t Training Loss 0.8506\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [22600][55000]\t Training Loss 0.8510\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [22650][55000]\t Training Loss 0.8512\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [22700][55000]\t Training Loss 0.8510\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [22750][55000]\t Training Loss 0.8509\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [22800][55000]\t Training Loss 0.8508\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [22850][55000]\t Training Loss 0.8508\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [22900][55000]\t Training Loss 0.8506\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [22950][55000]\t Training Loss 0.8504\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [23000][55000]\t Training Loss 0.8501\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [23050][55000]\t Training Loss 0.8500\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [23100][55000]\t Training Loss 0.8501\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [23150][55000]\t Training Loss 0.8500\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [23200][55000]\t Training Loss 0.8500\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [23250][55000]\t Training Loss 0.8498\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [23300][55000]\t Training Loss 0.8494\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [23350][55000]\t Training Loss 0.8493\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [23400][55000]\t Training Loss 0.8490\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [23450][55000]\t Training Loss 0.8490\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [23500][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [23550][55000]\t Training Loss 0.8486\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [23600][55000]\t Training Loss 0.8487\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [23650][55000]\t Training Loss 0.8488\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [23700][55000]\t Training Loss 0.8490\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [23750][55000]\t Training Loss 0.8495\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [23800][55000]\t Training Loss 0.8492\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [23850][55000]\t Training Loss 0.8491\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [23900][55000]\t Training Loss 0.8493\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [23950][55000]\t Training Loss 0.8493\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [24000][55000]\t Training Loss 0.8493\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [24050][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [24100][55000]\t Training Loss 0.8491\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [24150][55000]\t Training Loss 0.8490\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [24200][55000]\t Training Loss 0.8488\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [24250][55000]\t Training Loss 0.8490\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [24300][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [24350][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [24400][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [24450][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [24500][55000]\t Training Loss 0.8491\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [24550][55000]\t Training Loss 0.8492\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [24600][55000]\t Training Loss 0.8493\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [24650][55000]\t Training Loss 0.8493\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [24700][55000]\t Training Loss 0.8493\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [24750][55000]\t Training Loss 0.8495\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [24800][55000]\t Training Loss 0.8499\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [24850][55000]\t Training Loss 0.8497\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [24900][55000]\t Training Loss 0.8498\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [24950][55000]\t Training Loss 0.8501\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [25000][55000]\t Training Loss 0.8504\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [25050][55000]\t Training Loss 0.8501\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [25100][55000]\t Training Loss 0.8500\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [25150][55000]\t Training Loss 0.8498\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [25200][55000]\t Training Loss 0.8497\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [25250][55000]\t Training Loss 0.8496\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [25300][55000]\t Training Loss 0.8496\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [25350][55000]\t Training Loss 0.8497\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [25400][55000]\t Training Loss 0.8493\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [25450][55000]\t Training Loss 0.8488\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [25500][55000]\t Training Loss 0.8486\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [25550][55000]\t Training Loss 0.8482\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [25600][55000]\t Training Loss 0.8482\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [25650][55000]\t Training Loss 0.8480\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [25700][55000]\t Training Loss 0.8478\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [25750][55000]\t Training Loss 0.8476\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [25800][55000]\t Training Loss 0.8475\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [25850][55000]\t Training Loss 0.8477\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [25900][55000]\t Training Loss 0.8477\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [25950][55000]\t Training Loss 0.8477\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [26000][55000]\t Training Loss 0.8477\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [26050][55000]\t Training Loss 0.8475\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [26100][55000]\t Training Loss 0.8472\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [26150][55000]\t Training Loss 0.8470\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [26200][55000]\t Training Loss 0.8468\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [26250][55000]\t Training Loss 0.8468\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [26300][55000]\t Training Loss 0.8470\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [26350][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [26400][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [26450][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [26500][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [26550][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [26600][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [26650][55000]\t Training Loss 0.8484\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [26700][55000]\t Training Loss 0.8483\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [26750][55000]\t Training Loss 0.8485\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [26800][55000]\t Training Loss 0.8484\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [26850][55000]\t Training Loss 0.8483\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [26900][55000]\t Training Loss 0.8486\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [26950][55000]\t Training Loss 0.8485\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [27000][55000]\t Training Loss 0.8483\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [27050][55000]\t Training Loss 0.8481\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [27100][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [27150][55000]\t Training Loss 0.8479\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [27200][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [27250][55000]\t Training Loss 0.8482\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [27300][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [27350][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [27400][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [27450][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [27500][55000]\t Training Loss 0.8480\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [27550][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [27600][55000]\t Training Loss 0.8479\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [27650][55000]\t Training Loss 0.8480\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [27700][55000]\t Training Loss 0.8480\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [27750][55000]\t Training Loss 0.8481\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [27800][55000]\t Training Loss 0.8482\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [27850][55000]\t Training Loss 0.8482\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [27900][55000]\t Training Loss 0.8481\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [27950][55000]\t Training Loss 0.8479\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [28000][55000]\t Training Loss 0.8476\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [28050][55000]\t Training Loss 0.8473\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [28100][55000]\t Training Loss 0.8470\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [28150][55000]\t Training Loss 0.8468\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [28200][55000]\t Training Loss 0.8469\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [28250][55000]\t Training Loss 0.8465\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [28300][55000]\t Training Loss 0.8464\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [28350][55000]\t Training Loss 0.8461\t Accuracy 0.8479\n",
      "Epoch [3][20]\t Batch [28400][55000]\t Training Loss 0.8466\t Accuracy 0.8478\n",
      "Epoch [3][20]\t Batch [28450][55000]\t Training Loss 0.8463\t Accuracy 0.8479\n",
      "Epoch [3][20]\t Batch [28500][55000]\t Training Loss 0.8462\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [28550][55000]\t Training Loss 0.8461\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [28600][55000]\t Training Loss 0.8459\t Accuracy 0.8482\n",
      "Epoch [3][20]\t Batch [28650][55000]\t Training Loss 0.8462\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [28700][55000]\t Training Loss 0.8465\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [28750][55000]\t Training Loss 0.8466\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [28800][55000]\t Training Loss 0.8465\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [28850][55000]\t Training Loss 0.8465\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [28900][55000]\t Training Loss 0.8463\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [28950][55000]\t Training Loss 0.8463\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [29000][55000]\t Training Loss 0.8462\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [29050][55000]\t Training Loss 0.8463\t Accuracy 0.8480\n",
      "Epoch [3][20]\t Batch [29100][55000]\t Training Loss 0.8464\t Accuracy 0.8479\n",
      "Epoch [3][20]\t Batch [29150][55000]\t Training Loss 0.8468\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [29200][55000]\t Training Loss 0.8471\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [29250][55000]\t Training Loss 0.8473\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [29300][55000]\t Training Loss 0.8472\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [29350][55000]\t Training Loss 0.8473\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [29400][55000]\t Training Loss 0.8472\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [29450][55000]\t Training Loss 0.8468\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [29500][55000]\t Training Loss 0.8466\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [29550][55000]\t Training Loss 0.8465\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [29600][55000]\t Training Loss 0.8465\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [29650][55000]\t Training Loss 0.8465\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [29700][55000]\t Training Loss 0.8466\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [29750][55000]\t Training Loss 0.8469\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [29800][55000]\t Training Loss 0.8471\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [29850][55000]\t Training Loss 0.8474\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [29900][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [29950][55000]\t Training Loss 0.8480\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [30000][55000]\t Training Loss 0.8483\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [30050][55000]\t Training Loss 0.8484\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [30100][55000]\t Training Loss 0.8488\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [30150][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [30200][55000]\t Training Loss 0.8495\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [30250][55000]\t Training Loss 0.8496\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [30300][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [30350][55000]\t Training Loss 0.8494\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [30400][55000]\t Training Loss 0.8492\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [30450][55000]\t Training Loss 0.8493\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [30500][55000]\t Training Loss 0.8495\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [30550][55000]\t Training Loss 0.8499\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [30600][55000]\t Training Loss 0.8499\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [30650][55000]\t Training Loss 0.8503\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [30700][55000]\t Training Loss 0.8506\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [30750][55000]\t Training Loss 0.8507\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [30800][55000]\t Training Loss 0.8510\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [30850][55000]\t Training Loss 0.8508\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [30900][55000]\t Training Loss 0.8512\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [30950][55000]\t Training Loss 0.8511\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [31000][55000]\t Training Loss 0.8511\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [31050][55000]\t Training Loss 0.8512\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [31100][55000]\t Training Loss 0.8512\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [31150][55000]\t Training Loss 0.8513\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [31200][55000]\t Training Loss 0.8513\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [31250][55000]\t Training Loss 0.8512\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [31300][55000]\t Training Loss 0.8516\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [31350][55000]\t Training Loss 0.8522\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [31400][55000]\t Training Loss 0.8523\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [31450][55000]\t Training Loss 0.8527\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [31500][55000]\t Training Loss 0.8527\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [31550][55000]\t Training Loss 0.8527\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [31600][55000]\t Training Loss 0.8529\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [31650][55000]\t Training Loss 0.8531\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [31700][55000]\t Training Loss 0.8533\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [31750][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [31800][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [31850][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [31900][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [31950][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [32000][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [32050][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [32100][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [32150][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [32200][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [32250][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [32300][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [32350][55000]\t Training Loss 0.8551\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [32400][55000]\t Training Loss 0.8551\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [32450][55000]\t Training Loss 0.8553\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [32500][55000]\t Training Loss 0.8555\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [32550][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [32600][55000]\t Training Loss 0.8558\t Accuracy 0.8453\n",
      "Epoch [3][20]\t Batch [32650][55000]\t Training Loss 0.8556\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [32700][55000]\t Training Loss 0.8556\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [32750][55000]\t Training Loss 0.8557\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [32800][55000]\t Training Loss 0.8558\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [32850][55000]\t Training Loss 0.8558\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [32900][55000]\t Training Loss 0.8556\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [32950][55000]\t Training Loss 0.8555\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [33000][55000]\t Training Loss 0.8554\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [33050][55000]\t Training Loss 0.8555\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [33100][55000]\t Training Loss 0.8555\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [33150][55000]\t Training Loss 0.8556\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [33200][55000]\t Training Loss 0.8555\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [33250][55000]\t Training Loss 0.8557\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [33300][55000]\t Training Loss 0.8556\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [33350][55000]\t Training Loss 0.8557\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [33400][55000]\t Training Loss 0.8558\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [33450][55000]\t Training Loss 0.8559\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [33500][55000]\t Training Loss 0.8558\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [33550][55000]\t Training Loss 0.8558\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [33600][55000]\t Training Loss 0.8558\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [33650][55000]\t Training Loss 0.8558\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [33700][55000]\t Training Loss 0.8557\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [33750][55000]\t Training Loss 0.8555\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [33800][55000]\t Training Loss 0.8555\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [33850][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [33900][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [33950][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [34000][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [34050][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [34100][55000]\t Training Loss 0.8548\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [34150][55000]\t Training Loss 0.8549\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [34200][55000]\t Training Loss 0.8547\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [34250][55000]\t Training Loss 0.8544\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [34300][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [34350][55000]\t Training Loss 0.8542\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [34400][55000]\t Training Loss 0.8542\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [34450][55000]\t Training Loss 0.8543\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [34500][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [34550][55000]\t Training Loss 0.8543\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [34600][55000]\t Training Loss 0.8543\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [34650][55000]\t Training Loss 0.8543\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [34700][55000]\t Training Loss 0.8544\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [34750][55000]\t Training Loss 0.8545\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [34800][55000]\t Training Loss 0.8545\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [34850][55000]\t Training Loss 0.8549\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [34900][55000]\t Training Loss 0.8550\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [34950][55000]\t Training Loss 0.8550\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [35000][55000]\t Training Loss 0.8549\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [35050][55000]\t Training Loss 0.8547\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [35100][55000]\t Training Loss 0.8548\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [35150][55000]\t Training Loss 0.8549\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [35200][55000]\t Training Loss 0.8550\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [35250][55000]\t Training Loss 0.8551\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [35300][55000]\t Training Loss 0.8551\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [35350][55000]\t Training Loss 0.8550\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [35400][55000]\t Training Loss 0.8549\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [35450][55000]\t Training Loss 0.8548\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [35500][55000]\t Training Loss 0.8548\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [35550][55000]\t Training Loss 0.8547\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [35600][55000]\t Training Loss 0.8546\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [35650][55000]\t Training Loss 0.8549\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [35700][55000]\t Training Loss 0.8549\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [35750][55000]\t Training Loss 0.8548\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [35800][55000]\t Training Loss 0.8547\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [35850][55000]\t Training Loss 0.8546\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [35900][55000]\t Training Loss 0.8545\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [35950][55000]\t Training Loss 0.8546\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [36000][55000]\t Training Loss 0.8547\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [36050][55000]\t Training Loss 0.8549\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [36100][55000]\t Training Loss 0.8550\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [36150][55000]\t Training Loss 0.8550\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [36200][55000]\t Training Loss 0.8548\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [36250][55000]\t Training Loss 0.8547\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [36300][55000]\t Training Loss 0.8543\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [36350][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [36400][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [36450][55000]\t Training Loss 0.8543\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [36500][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [36550][55000]\t Training Loss 0.8543\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [36600][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [36650][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [36700][55000]\t Training Loss 0.8537\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [36750][55000]\t Training Loss 0.8535\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [36800][55000]\t Training Loss 0.8534\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [36850][55000]\t Training Loss 0.8534\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [36900][55000]\t Training Loss 0.8534\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [36950][55000]\t Training Loss 0.8533\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [37000][55000]\t Training Loss 0.8531\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [37050][55000]\t Training Loss 0.8530\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [37100][55000]\t Training Loss 0.8531\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [37150][55000]\t Training Loss 0.8531\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [37200][55000]\t Training Loss 0.8530\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [37250][55000]\t Training Loss 0.8529\t Accuracy 0.8477\n",
      "Epoch [3][20]\t Batch [37300][55000]\t Training Loss 0.8529\t Accuracy 0.8476\n",
      "Epoch [3][20]\t Batch [37350][55000]\t Training Loss 0.8530\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [37400][55000]\t Training Loss 0.8533\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [37450][55000]\t Training Loss 0.8536\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [37500][55000]\t Training Loss 0.8537\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [37550][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [37600][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [37650][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [37700][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [37750][55000]\t Training Loss 0.8539\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [37800][55000]\t Training Loss 0.8539\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [37850][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [37900][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [37950][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [38000][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [38050][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [38100][55000]\t Training Loss 0.8542\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [38150][55000]\t Training Loss 0.8540\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [38200][55000]\t Training Loss 0.8540\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [38250][55000]\t Training Loss 0.8539\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [38300][55000]\t Training Loss 0.8541\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [38350][55000]\t Training Loss 0.8542\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [38400][55000]\t Training Loss 0.8543\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [38450][55000]\t Training Loss 0.8543\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [38500][55000]\t Training Loss 0.8541\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [38550][55000]\t Training Loss 0.8542\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [38600][55000]\t Training Loss 0.8544\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [38650][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [38700][55000]\t Training Loss 0.8545\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [38750][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [38800][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [38850][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [38900][55000]\t Training Loss 0.8541\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [38950][55000]\t Training Loss 0.8539\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [39000][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [39050][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [39100][55000]\t Training Loss 0.8534\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [39150][55000]\t Training Loss 0.8533\t Accuracy 0.8474\n",
      "Epoch [3][20]\t Batch [39200][55000]\t Training Loss 0.8531\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [39250][55000]\t Training Loss 0.8530\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [39300][55000]\t Training Loss 0.8530\t Accuracy 0.8475\n",
      "Epoch [3][20]\t Batch [39350][55000]\t Training Loss 0.8533\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [39400][55000]\t Training Loss 0.8533\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [39450][55000]\t Training Loss 0.8536\t Accuracy 0.8472\n",
      "Epoch [3][20]\t Batch [39500][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [39550][55000]\t Training Loss 0.8537\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [39600][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [39650][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [39700][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [39750][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [39800][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [39850][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [39900][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [39950][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40000][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40050][55000]\t Training Loss 0.8544\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [40100][55000]\t Training Loss 0.8544\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [40150][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40200][55000]\t Training Loss 0.8542\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40250][55000]\t Training Loss 0.8542\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40300][55000]\t Training Loss 0.8542\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40350][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40400][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40450][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [40500][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40550][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40600][55000]\t Training Loss 0.8542\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40650][55000]\t Training Loss 0.8540\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [40700][55000]\t Training Loss 0.8541\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [40750][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40800][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [40850][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [40900][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [40950][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [41000][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [41050][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [41100][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [41150][55000]\t Training Loss 0.8539\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [41200][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [41250][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [3][20]\t Batch [41300][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [3][20]\t Batch [41350][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [41400][55000]\t Training Loss 0.8544\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [41450][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [41500][55000]\t Training Loss 0.8547\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [41550][55000]\t Training Loss 0.8549\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [41600][55000]\t Training Loss 0.8549\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [41650][55000]\t Training Loss 0.8549\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [41700][55000]\t Training Loss 0.8548\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [41750][55000]\t Training Loss 0.8550\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [41800][55000]\t Training Loss 0.8552\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [41850][55000]\t Training Loss 0.8553\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [41900][55000]\t Training Loss 0.8552\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [41950][55000]\t Training Loss 0.8552\t Accuracy 0.8468\n",
      "Epoch [3][20]\t Batch [42000][55000]\t Training Loss 0.8552\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [42050][55000]\t Training Loss 0.8551\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [42100][55000]\t Training Loss 0.8550\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [42150][55000]\t Training Loss 0.8551\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [42200][55000]\t Training Loss 0.8551\t Accuracy 0.8467\n",
      "Epoch [3][20]\t Batch [42250][55000]\t Training Loss 0.8552\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [42300][55000]\t Training Loss 0.8552\t Accuracy 0.8466\n",
      "Epoch [3][20]\t Batch [42350][55000]\t Training Loss 0.8555\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [42400][55000]\t Training Loss 0.8556\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [42450][55000]\t Training Loss 0.8558\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [42500][55000]\t Training Loss 0.8559\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [42550][55000]\t Training Loss 0.8562\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [42600][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [42650][55000]\t Training Loss 0.8558\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [42700][55000]\t Training Loss 0.8558\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [42750][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [42800][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [42850][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [42900][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [42950][55000]\t Training Loss 0.8556\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [43000][55000]\t Training Loss 0.8559\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [43050][55000]\t Training Loss 0.8560\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [43100][55000]\t Training Loss 0.8563\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [43150][55000]\t Training Loss 0.8563\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [43200][55000]\t Training Loss 0.8562\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [43250][55000]\t Training Loss 0.8562\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [43300][55000]\t Training Loss 0.8561\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [43350][55000]\t Training Loss 0.8559\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [43400][55000]\t Training Loss 0.8558\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [43450][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [43500][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [43550][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [43600][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [43650][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [43700][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [43750][55000]\t Training Loss 0.8551\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [43800][55000]\t Training Loss 0.8551\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [43850][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [43900][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [43950][55000]\t Training Loss 0.8553\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [44000][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [44050][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [44100][55000]\t Training Loss 0.8555\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [44150][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [44200][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [44250][55000]\t Training Loss 0.8558\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [44300][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [44350][55000]\t Training Loss 0.8560\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [44400][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [44450][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [44500][55000]\t Training Loss 0.8560\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [44550][55000]\t Training Loss 0.8559\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [44600][55000]\t Training Loss 0.8558\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [44650][55000]\t Training Loss 0.8557\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [44700][55000]\t Training Loss 0.8556\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [44750][55000]\t Training Loss 0.8556\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [44800][55000]\t Training Loss 0.8556\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [44850][55000]\t Training Loss 0.8557\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [44900][55000]\t Training Loss 0.8558\t Accuracy 0.8464\n",
      "Epoch [3][20]\t Batch [44950][55000]\t Training Loss 0.8558\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [45000][55000]\t Training Loss 0.8558\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45050][55000]\t Training Loss 0.8560\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45100][55000]\t Training Loss 0.8560\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45150][55000]\t Training Loss 0.8561\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [45200][55000]\t Training Loss 0.8561\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45250][55000]\t Training Loss 0.8561\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45300][55000]\t Training Loss 0.8560\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45350][55000]\t Training Loss 0.8559\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [45400][55000]\t Training Loss 0.8558\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [45450][55000]\t Training Loss 0.8560\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [45500][55000]\t Training Loss 0.8562\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [45550][55000]\t Training Loss 0.8562\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45600][55000]\t Training Loss 0.8561\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45650][55000]\t Training Loss 0.8561\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [45700][55000]\t Training Loss 0.8560\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [45750][55000]\t Training Loss 0.8560\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45800][55000]\t Training Loss 0.8561\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45850][55000]\t Training Loss 0.8562\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [45900][55000]\t Training Loss 0.8563\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [45950][55000]\t Training Loss 0.8564\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [46000][55000]\t Training Loss 0.8564\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [46050][55000]\t Training Loss 0.8566\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [46100][55000]\t Training Loss 0.8567\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [46150][55000]\t Training Loss 0.8567\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [46200][55000]\t Training Loss 0.8567\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [46250][55000]\t Training Loss 0.8567\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [46300][55000]\t Training Loss 0.8568\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [46350][55000]\t Training Loss 0.8569\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [46400][55000]\t Training Loss 0.8571\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [46450][55000]\t Training Loss 0.8572\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [46500][55000]\t Training Loss 0.8571\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [46550][55000]\t Training Loss 0.8570\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [46600][55000]\t Training Loss 0.8569\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [46650][55000]\t Training Loss 0.8569\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [46700][55000]\t Training Loss 0.8568\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [46750][55000]\t Training Loss 0.8568\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [46800][55000]\t Training Loss 0.8566\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [46850][55000]\t Training Loss 0.8565\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [46900][55000]\t Training Loss 0.8565\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [46950][55000]\t Training Loss 0.8564\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [47000][55000]\t Training Loss 0.8563\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47050][55000]\t Training Loss 0.8562\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47100][55000]\t Training Loss 0.8562\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [47150][55000]\t Training Loss 0.8561\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [47200][55000]\t Training Loss 0.8560\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47250][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47300][55000]\t Training Loss 0.8562\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47350][55000]\t Training Loss 0.8564\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47400][55000]\t Training Loss 0.8564\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47450][55000]\t Training Loss 0.8564\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [47500][55000]\t Training Loss 0.8564\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [47550][55000]\t Training Loss 0.8565\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [47600][55000]\t Training Loss 0.8564\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47650][55000]\t Training Loss 0.8565\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [47700][55000]\t Training Loss 0.8565\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [47750][55000]\t Training Loss 0.8564\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [47800][55000]\t Training Loss 0.8564\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [47850][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47900][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [47950][55000]\t Training Loss 0.8563\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [48000][55000]\t Training Loss 0.8563\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [48050][55000]\t Training Loss 0.8561\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [48100][55000]\t Training Loss 0.8561\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [48150][55000]\t Training Loss 0.8559\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [48200][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [48250][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [48300][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [48350][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [48400][55000]\t Training Loss 0.8555\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [48450][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [48500][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [48550][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [48600][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [48650][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [48700][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [48750][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [48800][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [48850][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [48900][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [3][20]\t Batch [48950][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [3][20]\t Batch [49000][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [3][20]\t Batch [49050][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [49100][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [49150][55000]\t Training Loss 0.8553\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [49200][55000]\t Training Loss 0.8553\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [49250][55000]\t Training Loss 0.8555\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [49300][55000]\t Training Loss 0.8554\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [49350][55000]\t Training Loss 0.8552\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [49400][55000]\t Training Loss 0.8551\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [49450][55000]\t Training Loss 0.8550\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [49500][55000]\t Training Loss 0.8552\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [49550][55000]\t Training Loss 0.8556\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [49600][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [49650][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [49700][55000]\t Training Loss 0.8562\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [49750][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [49800][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [49850][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [49900][55000]\t Training Loss 0.8563\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [49950][55000]\t Training Loss 0.8564\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [50000][55000]\t Training Loss 0.8565\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [50050][55000]\t Training Loss 0.8565\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [50100][55000]\t Training Loss 0.8566\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [50150][55000]\t Training Loss 0.8565\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [50200][55000]\t Training Loss 0.8564\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [50250][55000]\t Training Loss 0.8566\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [50300][55000]\t Training Loss 0.8565\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [50350][55000]\t Training Loss 0.8566\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [50400][55000]\t Training Loss 0.8568\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [50450][55000]\t Training Loss 0.8571\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [50500][55000]\t Training Loss 0.8571\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [50550][55000]\t Training Loss 0.8571\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [50600][55000]\t Training Loss 0.8572\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [50650][55000]\t Training Loss 0.8573\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [50700][55000]\t Training Loss 0.8572\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [50750][55000]\t Training Loss 0.8573\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [50800][55000]\t Training Loss 0.8573\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [50850][55000]\t Training Loss 0.8573\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [50900][55000]\t Training Loss 0.8572\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [50950][55000]\t Training Loss 0.8571\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [51000][55000]\t Training Loss 0.8569\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [51050][55000]\t Training Loss 0.8568\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [51100][55000]\t Training Loss 0.8567\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [51150][55000]\t Training Loss 0.8567\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [51200][55000]\t Training Loss 0.8568\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [51250][55000]\t Training Loss 0.8568\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [51300][55000]\t Training Loss 0.8569\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [51350][55000]\t Training Loss 0.8569\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [51400][55000]\t Training Loss 0.8569\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [51450][55000]\t Training Loss 0.8567\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [51500][55000]\t Training Loss 0.8566\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [51550][55000]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [51600][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [51650][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [51700][55000]\t Training Loss 0.8563\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [51750][55000]\t Training Loss 0.8563\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [51800][55000]\t Training Loss 0.8564\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [51850][55000]\t Training Loss 0.8563\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [51900][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [51950][55000]\t Training Loss 0.8561\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [52000][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [52050][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [52100][55000]\t Training Loss 0.8564\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [52150][55000]\t Training Loss 0.8567\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [52200][55000]\t Training Loss 0.8568\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [52250][55000]\t Training Loss 0.8570\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [52300][55000]\t Training Loss 0.8571\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [52350][55000]\t Training Loss 0.8570\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [52400][55000]\t Training Loss 0.8570\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [52450][55000]\t Training Loss 0.8568\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [52500][55000]\t Training Loss 0.8567\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [52550][55000]\t Training Loss 0.8565\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [52600][55000]\t Training Loss 0.8563\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [52650][55000]\t Training Loss 0.8561\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [52700][55000]\t Training Loss 0.8563\t Accuracy 0.8452\n",
      "Epoch [3][20]\t Batch [52750][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [52800][55000]\t Training Loss 0.8565\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [52850][55000]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [3][20]\t Batch [52900][55000]\t Training Loss 0.8567\t Accuracy 0.8449\n",
      "Epoch [3][20]\t Batch [52950][55000]\t Training Loss 0.8569\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [53000][55000]\t Training Loss 0.8570\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [53050][55000]\t Training Loss 0.8570\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [53100][55000]\t Training Loss 0.8569\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53150][55000]\t Training Loss 0.8569\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [53200][55000]\t Training Loss 0.8570\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53250][55000]\t Training Loss 0.8571\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53300][55000]\t Training Loss 0.8571\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53350][55000]\t Training Loss 0.8570\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53400][55000]\t Training Loss 0.8568\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53450][55000]\t Training Loss 0.8567\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [53500][55000]\t Training Loss 0.8568\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53550][55000]\t Training Loss 0.8567\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53600][55000]\t Training Loss 0.8569\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [53650][55000]\t Training Loss 0.8569\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [53700][55000]\t Training Loss 0.8569\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [53750][55000]\t Training Loss 0.8568\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53800][55000]\t Training Loss 0.8568\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53850][55000]\t Training Loss 0.8568\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53900][55000]\t Training Loss 0.8567\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [53950][55000]\t Training Loss 0.8567\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [54000][55000]\t Training Loss 0.8569\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [54050][55000]\t Training Loss 0.8570\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [54100][55000]\t Training Loss 0.8572\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [54150][55000]\t Training Loss 0.8571\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [54200][55000]\t Training Loss 0.8571\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [54250][55000]\t Training Loss 0.8570\t Accuracy 0.8445\n",
      "Epoch [3][20]\t Batch [54300][55000]\t Training Loss 0.8570\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [54350][55000]\t Training Loss 0.8569\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [54400][55000]\t Training Loss 0.8569\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [54450][55000]\t Training Loss 0.8568\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [54500][55000]\t Training Loss 0.8567\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [54550][55000]\t Training Loss 0.8566\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [54600][55000]\t Training Loss 0.8566\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [54650][55000]\t Training Loss 0.8565\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [54700][55000]\t Training Loss 0.8563\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [54750][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [54800][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [54850][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [54900][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [54950][55000]\t Training Loss 0.8565\t Accuracy 0.8447\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8567\t Average training accuracy 0.8446\n",
      "Epoch [3]\t Average validation loss 0.7941\t Average validation accuracy 0.8698\n",
      "\n",
      "Epoch [4][20]\t Batch [0][55000]\t Training Loss 1.4638\t Accuracy 0.0000\n",
      "Epoch [4][20]\t Batch [50][55000]\t Training Loss 0.9417\t Accuracy 0.7255\n",
      "Epoch [4][20]\t Batch [100][55000]\t Training Loss 0.8514\t Accuracy 0.8020\n",
      "Epoch [4][20]\t Batch [150][55000]\t Training Loss 0.8526\t Accuracy 0.8146\n",
      "Epoch [4][20]\t Batch [200][55000]\t Training Loss 0.8630\t Accuracy 0.8159\n",
      "Epoch [4][20]\t Batch [250][55000]\t Training Loss 0.8454\t Accuracy 0.8247\n",
      "Epoch [4][20]\t Batch [300][55000]\t Training Loss 0.8466\t Accuracy 0.8239\n",
      "Epoch [4][20]\t Batch [350][55000]\t Training Loss 0.8270\t Accuracy 0.8262\n",
      "Epoch [4][20]\t Batch [400][55000]\t Training Loss 0.8133\t Accuracy 0.8304\n",
      "Epoch [4][20]\t Batch [450][55000]\t Training Loss 0.8189\t Accuracy 0.8315\n",
      "Epoch [4][20]\t Batch [500][55000]\t Training Loss 0.8252\t Accuracy 0.8343\n",
      "Epoch [4][20]\t Batch [550][55000]\t Training Loss 0.8422\t Accuracy 0.8312\n",
      "Epoch [4][20]\t Batch [600][55000]\t Training Loss 0.8467\t Accuracy 0.8353\n",
      "Epoch [4][20]\t Batch [650][55000]\t Training Loss 0.8666\t Accuracy 0.8249\n",
      "Epoch [4][20]\t Batch [700][55000]\t Training Loss 0.8642\t Accuracy 0.8302\n",
      "Epoch [4][20]\t Batch [750][55000]\t Training Loss 0.8605\t Accuracy 0.8309\n",
      "Epoch [4][20]\t Batch [800][55000]\t Training Loss 0.8544\t Accuracy 0.8327\n",
      "Epoch [4][20]\t Batch [850][55000]\t Training Loss 0.8528\t Accuracy 0.8343\n",
      "Epoch [4][20]\t Batch [900][55000]\t Training Loss 0.8593\t Accuracy 0.8335\n",
      "Epoch [4][20]\t Batch [950][55000]\t Training Loss 0.8662\t Accuracy 0.8286\n",
      "Epoch [4][20]\t Batch [1000][55000]\t Training Loss 0.8656\t Accuracy 0.8312\n",
      "Epoch [4][20]\t Batch [1050][55000]\t Training Loss 0.8733\t Accuracy 0.8297\n",
      "Epoch [4][20]\t Batch [1100][55000]\t Training Loss 0.8817\t Accuracy 0.8283\n",
      "Epoch [4][20]\t Batch [1150][55000]\t Training Loss 0.8910\t Accuracy 0.8254\n",
      "Epoch [4][20]\t Batch [1200][55000]\t Training Loss 0.8852\t Accuracy 0.8285\n",
      "Epoch [4][20]\t Batch [1250][55000]\t Training Loss 0.8865\t Accuracy 0.8281\n",
      "Epoch [4][20]\t Batch [1300][55000]\t Training Loss 0.8868\t Accuracy 0.8278\n",
      "Epoch [4][20]\t Batch [1350][55000]\t Training Loss 0.8836\t Accuracy 0.8290\n",
      "Epoch [4][20]\t Batch [1400][55000]\t Training Loss 0.8841\t Accuracy 0.8280\n",
      "Epoch [4][20]\t Batch [1450][55000]\t Training Loss 0.8851\t Accuracy 0.8270\n",
      "Epoch [4][20]\t Batch [1500][55000]\t Training Loss 0.8832\t Accuracy 0.8301\n",
      "Epoch [4][20]\t Batch [1550][55000]\t Training Loss 0.8828\t Accuracy 0.8304\n",
      "Epoch [4][20]\t Batch [1600][55000]\t Training Loss 0.8854\t Accuracy 0.8295\n",
      "Epoch [4][20]\t Batch [1650][55000]\t Training Loss 0.8823\t Accuracy 0.8310\n",
      "Epoch [4][20]\t Batch [1700][55000]\t Training Loss 0.8806\t Accuracy 0.8319\n",
      "Epoch [4][20]\t Batch [1750][55000]\t Training Loss 0.8750\t Accuracy 0.8327\n",
      "Epoch [4][20]\t Batch [1800][55000]\t Training Loss 0.8728\t Accuracy 0.8345\n",
      "Epoch [4][20]\t Batch [1850][55000]\t Training Loss 0.8712\t Accuracy 0.8352\n",
      "Epoch [4][20]\t Batch [1900][55000]\t Training Loss 0.8691\t Accuracy 0.8353\n",
      "Epoch [4][20]\t Batch [1950][55000]\t Training Loss 0.8676\t Accuracy 0.8365\n",
      "Epoch [4][20]\t Batch [2000][55000]\t Training Loss 0.8651\t Accuracy 0.8376\n",
      "Epoch [4][20]\t Batch [2050][55000]\t Training Loss 0.8642\t Accuracy 0.8381\n",
      "Epoch [4][20]\t Batch [2100][55000]\t Training Loss 0.8613\t Accuracy 0.8386\n",
      "Epoch [4][20]\t Batch [2150][55000]\t Training Loss 0.8581\t Accuracy 0.8405\n",
      "Epoch [4][20]\t Batch [2200][55000]\t Training Loss 0.8533\t Accuracy 0.8423\n",
      "Epoch [4][20]\t Batch [2250][55000]\t Training Loss 0.8525\t Accuracy 0.8427\n",
      "Epoch [4][20]\t Batch [2300][55000]\t Training Loss 0.8495\t Accuracy 0.8422\n",
      "Epoch [4][20]\t Batch [2350][55000]\t Training Loss 0.8480\t Accuracy 0.8426\n",
      "Epoch [4][20]\t Batch [2400][55000]\t Training Loss 0.8489\t Accuracy 0.8409\n",
      "Epoch [4][20]\t Batch [2450][55000]\t Training Loss 0.8517\t Accuracy 0.8397\n",
      "Epoch [4][20]\t Batch [2500][55000]\t Training Loss 0.8495\t Accuracy 0.8417\n",
      "Epoch [4][20]\t Batch [2550][55000]\t Training Loss 0.8482\t Accuracy 0.8416\n",
      "Epoch [4][20]\t Batch [2600][55000]\t Training Loss 0.8476\t Accuracy 0.8416\n",
      "Epoch [4][20]\t Batch [2650][55000]\t Training Loss 0.8464\t Accuracy 0.8419\n",
      "Epoch [4][20]\t Batch [2700][55000]\t Training Loss 0.8462\t Accuracy 0.8423\n",
      "Epoch [4][20]\t Batch [2750][55000]\t Training Loss 0.8461\t Accuracy 0.8422\n",
      "Epoch [4][20]\t Batch [2800][55000]\t Training Loss 0.8463\t Accuracy 0.8404\n",
      "Epoch [4][20]\t Batch [2850][55000]\t Training Loss 0.8452\t Accuracy 0.8401\n",
      "Epoch [4][20]\t Batch [2900][55000]\t Training Loss 0.8420\t Accuracy 0.8411\n",
      "Epoch [4][20]\t Batch [2950][55000]\t Training Loss 0.8421\t Accuracy 0.8404\n",
      "Epoch [4][20]\t Batch [3000][55000]\t Training Loss 0.8420\t Accuracy 0.8411\n",
      "Epoch [4][20]\t Batch [3050][55000]\t Training Loss 0.8430\t Accuracy 0.8407\n",
      "Epoch [4][20]\t Batch [3100][55000]\t Training Loss 0.8453\t Accuracy 0.8404\n",
      "Epoch [4][20]\t Batch [3150][55000]\t Training Loss 0.8452\t Accuracy 0.8407\n",
      "Epoch [4][20]\t Batch [3200][55000]\t Training Loss 0.8450\t Accuracy 0.8419\n",
      "Epoch [4][20]\t Batch [3250][55000]\t Training Loss 0.8443\t Accuracy 0.8413\n",
      "Epoch [4][20]\t Batch [3300][55000]\t Training Loss 0.8456\t Accuracy 0.8410\n",
      "Epoch [4][20]\t Batch [3350][55000]\t Training Loss 0.8439\t Accuracy 0.8424\n",
      "Epoch [4][20]\t Batch [3400][55000]\t Training Loss 0.8462\t Accuracy 0.8409\n",
      "Epoch [4][20]\t Batch [3450][55000]\t Training Loss 0.8458\t Accuracy 0.8418\n",
      "Epoch [4][20]\t Batch [3500][55000]\t Training Loss 0.8458\t Accuracy 0.8415\n",
      "Epoch [4][20]\t Batch [3550][55000]\t Training Loss 0.8474\t Accuracy 0.8409\n",
      "Epoch [4][20]\t Batch [3600][55000]\t Training Loss 0.8472\t Accuracy 0.8406\n",
      "Epoch [4][20]\t Batch [3650][55000]\t Training Loss 0.8458\t Accuracy 0.8414\n",
      "Epoch [4][20]\t Batch [3700][55000]\t Training Loss 0.8466\t Accuracy 0.8409\n",
      "Epoch [4][20]\t Batch [3750][55000]\t Training Loss 0.8465\t Accuracy 0.8411\n",
      "Epoch [4][20]\t Batch [3800][55000]\t Training Loss 0.8468\t Accuracy 0.8411\n",
      "Epoch [4][20]\t Batch [3850][55000]\t Training Loss 0.8464\t Accuracy 0.8413\n",
      "Epoch [4][20]\t Batch [3900][55000]\t Training Loss 0.8453\t Accuracy 0.8426\n",
      "Epoch [4][20]\t Batch [3950][55000]\t Training Loss 0.8441\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [4000][55000]\t Training Loss 0.8438\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [4050][55000]\t Training Loss 0.8423\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [4100][55000]\t Training Loss 0.8427\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [4150][55000]\t Training Loss 0.8431\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [4200][55000]\t Training Loss 0.8436\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [4250][55000]\t Training Loss 0.8422\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [4300][55000]\t Training Loss 0.8422\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [4350][55000]\t Training Loss 0.8427\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [4400][55000]\t Training Loss 0.8426\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [4450][55000]\t Training Loss 0.8428\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [4500][55000]\t Training Loss 0.8431\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [4550][55000]\t Training Loss 0.8414\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [4600][55000]\t Training Loss 0.8397\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [4650][55000]\t Training Loss 0.8396\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [4700][55000]\t Training Loss 0.8392\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [4750][55000]\t Training Loss 0.8380\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [4800][55000]\t Training Loss 0.8387\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [4850][55000]\t Training Loss 0.8399\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [4900][55000]\t Training Loss 0.8384\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [4950][55000]\t Training Loss 0.8387\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [5000][55000]\t Training Loss 0.8388\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [5050][55000]\t Training Loss 0.8383\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [5100][55000]\t Training Loss 0.8383\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [5150][55000]\t Training Loss 0.8386\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [5200][55000]\t Training Loss 0.8401\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [5250][55000]\t Training Loss 0.8392\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [5300][55000]\t Training Loss 0.8391\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [5350][55000]\t Training Loss 0.8397\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [5400][55000]\t Training Loss 0.8389\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [5450][55000]\t Training Loss 0.8382\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [5500][55000]\t Training Loss 0.8364\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [5550][55000]\t Training Loss 0.8364\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [5600][55000]\t Training Loss 0.8355\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [5650][55000]\t Training Loss 0.8360\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [5700][55000]\t Training Loss 0.8357\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [5750][55000]\t Training Loss 0.8362\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [5800][55000]\t Training Loss 0.8365\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [5850][55000]\t Training Loss 0.8367\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [5900][55000]\t Training Loss 0.8372\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [5950][55000]\t Training Loss 0.8371\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [6000][55000]\t Training Loss 0.8361\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [6050][55000]\t Training Loss 0.8347\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [6100][55000]\t Training Loss 0.8333\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [6150][55000]\t Training Loss 0.8312\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [6200][55000]\t Training Loss 0.8313\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [6250][55000]\t Training Loss 0.8301\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [6300][55000]\t Training Loss 0.8305\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [6350][55000]\t Training Loss 0.8301\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [6400][55000]\t Training Loss 0.8296\t Accuracy 0.8485\n",
      "Epoch [4][20]\t Batch [6450][55000]\t Training Loss 0.8290\t Accuracy 0.8487\n",
      "Epoch [4][20]\t Batch [6500][55000]\t Training Loss 0.8298\t Accuracy 0.8483\n",
      "Epoch [4][20]\t Batch [6550][55000]\t Training Loss 0.8288\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [6600][55000]\t Training Loss 0.8275\t Accuracy 0.8491\n",
      "Epoch [4][20]\t Batch [6650][55000]\t Training Loss 0.8263\t Accuracy 0.8493\n",
      "Epoch [4][20]\t Batch [6700][55000]\t Training Loss 0.8265\t Accuracy 0.8493\n",
      "Epoch [4][20]\t Batch [6750][55000]\t Training Loss 0.8267\t Accuracy 0.8501\n",
      "Epoch [4][20]\t Batch [6800][55000]\t Training Loss 0.8271\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [6850][55000]\t Training Loss 0.8293\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [6900][55000]\t Training Loss 0.8293\t Accuracy 0.8496\n",
      "Epoch [4][20]\t Batch [6950][55000]\t Training Loss 0.8299\t Accuracy 0.8485\n",
      "Epoch [4][20]\t Batch [7000][55000]\t Training Loss 0.8298\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [7050][55000]\t Training Loss 0.8303\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [7100][55000]\t Training Loss 0.8306\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [7150][55000]\t Training Loss 0.8308\t Accuracy 0.8487\n",
      "Epoch [4][20]\t Batch [7200][55000]\t Training Loss 0.8316\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [7250][55000]\t Training Loss 0.8339\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [7300][55000]\t Training Loss 0.8357\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [7350][55000]\t Training Loss 0.8372\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [7400][55000]\t Training Loss 0.8383\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [7450][55000]\t Training Loss 0.8382\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [7500][55000]\t Training Loss 0.8382\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [7550][55000]\t Training Loss 0.8387\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [7600][55000]\t Training Loss 0.8383\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [7650][55000]\t Training Loss 0.8387\t Accuracy 0.8483\n",
      "Epoch [4][20]\t Batch [7700][55000]\t Training Loss 0.8392\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [7750][55000]\t Training Loss 0.8393\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [7800][55000]\t Training Loss 0.8401\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [7850][55000]\t Training Loss 0.8402\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [7900][55000]\t Training Loss 0.8404\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [7950][55000]\t Training Loss 0.8405\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [8000][55000]\t Training Loss 0.8405\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [8050][55000]\t Training Loss 0.8410\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [8100][55000]\t Training Loss 0.8396\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [8150][55000]\t Training Loss 0.8404\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [8200][55000]\t Training Loss 0.8404\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [8250][55000]\t Training Loss 0.8416\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [8300][55000]\t Training Loss 0.8419\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [8350][55000]\t Training Loss 0.8425\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [8400][55000]\t Training Loss 0.8419\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [8450][55000]\t Training Loss 0.8434\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [8500][55000]\t Training Loss 0.8426\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [8550][55000]\t Training Loss 0.8416\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [8600][55000]\t Training Loss 0.8408\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [8650][55000]\t Training Loss 0.8409\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [8700][55000]\t Training Loss 0.8415\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [8750][55000]\t Training Loss 0.8431\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [8800][55000]\t Training Loss 0.8439\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [8850][55000]\t Training Loss 0.8438\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [8900][55000]\t Training Loss 0.8456\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [8950][55000]\t Training Loss 0.8452\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [9000][55000]\t Training Loss 0.8444\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [9050][55000]\t Training Loss 0.8436\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [9100][55000]\t Training Loss 0.8434\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [9150][55000]\t Training Loss 0.8439\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [9200][55000]\t Training Loss 0.8434\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [9250][55000]\t Training Loss 0.8437\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [9300][55000]\t Training Loss 0.8440\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [9350][55000]\t Training Loss 0.8442\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [9400][55000]\t Training Loss 0.8444\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [9450][55000]\t Training Loss 0.8446\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [9500][55000]\t Training Loss 0.8438\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [9550][55000]\t Training Loss 0.8435\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [9600][55000]\t Training Loss 0.8440\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [9650][55000]\t Training Loss 0.8438\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [9700][55000]\t Training Loss 0.8432\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [9750][55000]\t Training Loss 0.8424\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [9800][55000]\t Training Loss 0.8430\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [9850][55000]\t Training Loss 0.8426\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [9900][55000]\t Training Loss 0.8421\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [9950][55000]\t Training Loss 0.8415\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [10000][55000]\t Training Loss 0.8412\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [10050][55000]\t Training Loss 0.8414\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [10100][55000]\t Training Loss 0.8411\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [10150][55000]\t Training Loss 0.8410\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [10200][55000]\t Training Loss 0.8411\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [10250][55000]\t Training Loss 0.8413\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [10300][55000]\t Training Loss 0.8412\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [10350][55000]\t Training Loss 0.8402\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [10400][55000]\t Training Loss 0.8394\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [10450][55000]\t Training Loss 0.8392\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [10500][55000]\t Training Loss 0.8382\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [10550][55000]\t Training Loss 0.8378\t Accuracy 0.8484\n",
      "Epoch [4][20]\t Batch [10600][55000]\t Training Loss 0.8374\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [10650][55000]\t Training Loss 0.8371\t Accuracy 0.8489\n",
      "Epoch [4][20]\t Batch [10700][55000]\t Training Loss 0.8368\t Accuracy 0.8493\n",
      "Epoch [4][20]\t Batch [10750][55000]\t Training Loss 0.8374\t Accuracy 0.8489\n",
      "Epoch [4][20]\t Batch [10800][55000]\t Training Loss 0.8378\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [10850][55000]\t Training Loss 0.8371\t Accuracy 0.8489\n",
      "Epoch [4][20]\t Batch [10900][55000]\t Training Loss 0.8367\t Accuracy 0.8491\n",
      "Epoch [4][20]\t Batch [10950][55000]\t Training Loss 0.8364\t Accuracy 0.8489\n",
      "Epoch [4][20]\t Batch [11000][55000]\t Training Loss 0.8362\t Accuracy 0.8491\n",
      "Epoch [4][20]\t Batch [11050][55000]\t Training Loss 0.8355\t Accuracy 0.8493\n",
      "Epoch [4][20]\t Batch [11100][55000]\t Training Loss 0.8348\t Accuracy 0.8495\n",
      "Epoch [4][20]\t Batch [11150][55000]\t Training Loss 0.8348\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [11200][55000]\t Training Loss 0.8345\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [11250][55000]\t Training Loss 0.8348\t Accuracy 0.8495\n",
      "Epoch [4][20]\t Batch [11300][55000]\t Training Loss 0.8344\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [11350][55000]\t Training Loss 0.8339\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [11400][55000]\t Training Loss 0.8341\t Accuracy 0.8496\n",
      "Epoch [4][20]\t Batch [11450][55000]\t Training Loss 0.8337\t Accuracy 0.8495\n",
      "Epoch [4][20]\t Batch [11500][55000]\t Training Loss 0.8335\t Accuracy 0.8494\n",
      "Epoch [4][20]\t Batch [11550][55000]\t Training Loss 0.8335\t Accuracy 0.8493\n",
      "Epoch [4][20]\t Batch [11600][55000]\t Training Loss 0.8347\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [11650][55000]\t Training Loss 0.8352\t Accuracy 0.8483\n",
      "Epoch [4][20]\t Batch [11700][55000]\t Training Loss 0.8352\t Accuracy 0.8485\n",
      "Epoch [4][20]\t Batch [11750][55000]\t Training Loss 0.8361\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [11800][55000]\t Training Loss 0.8364\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [11850][55000]\t Training Loss 0.8364\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [11900][55000]\t Training Loss 0.8367\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [11950][55000]\t Training Loss 0.8368\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [12000][55000]\t Training Loss 0.8368\t Accuracy 0.8485\n",
      "Epoch [4][20]\t Batch [12050][55000]\t Training Loss 0.8366\t Accuracy 0.8488\n",
      "Epoch [4][20]\t Batch [12100][55000]\t Training Loss 0.8365\t Accuracy 0.8487\n",
      "Epoch [4][20]\t Batch [12150][55000]\t Training Loss 0.8359\t Accuracy 0.8491\n",
      "Epoch [4][20]\t Batch [12200][55000]\t Training Loss 0.8362\t Accuracy 0.8491\n",
      "Epoch [4][20]\t Batch [12250][55000]\t Training Loss 0.8361\t Accuracy 0.8492\n",
      "Epoch [4][20]\t Batch [12300][55000]\t Training Loss 0.8361\t Accuracy 0.8491\n",
      "Epoch [4][20]\t Batch [12350][55000]\t Training Loss 0.8363\t Accuracy 0.8492\n",
      "Epoch [4][20]\t Batch [12400][55000]\t Training Loss 0.8366\t Accuracy 0.8493\n",
      "Epoch [4][20]\t Batch [12450][55000]\t Training Loss 0.8368\t Accuracy 0.8489\n",
      "Epoch [4][20]\t Batch [12500][55000]\t Training Loss 0.8370\t Accuracy 0.8487\n",
      "Epoch [4][20]\t Batch [12550][55000]\t Training Loss 0.8370\t Accuracy 0.8489\n",
      "Epoch [4][20]\t Batch [12600][55000]\t Training Loss 0.8378\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [12650][55000]\t Training Loss 0.8383\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [12700][55000]\t Training Loss 0.8390\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [12750][55000]\t Training Loss 0.8385\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [12800][55000]\t Training Loss 0.8391\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [12850][55000]\t Training Loss 0.8393\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [12900][55000]\t Training Loss 0.8393\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [12950][55000]\t Training Loss 0.8398\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [13000][55000]\t Training Loss 0.8398\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [13050][55000]\t Training Loss 0.8405\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [13100][55000]\t Training Loss 0.8411\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [13150][55000]\t Training Loss 0.8414\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [13200][55000]\t Training Loss 0.8416\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [13250][55000]\t Training Loss 0.8411\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [13300][55000]\t Training Loss 0.8411\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [13350][55000]\t Training Loss 0.8414\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [13400][55000]\t Training Loss 0.8417\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [13450][55000]\t Training Loss 0.8412\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [13500][55000]\t Training Loss 0.8409\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [13550][55000]\t Training Loss 0.8404\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [13600][55000]\t Training Loss 0.8396\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [13650][55000]\t Training Loss 0.8394\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [13700][55000]\t Training Loss 0.8403\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [13750][55000]\t Training Loss 0.8409\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [13800][55000]\t Training Loss 0.8410\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [13850][55000]\t Training Loss 0.8409\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [13900][55000]\t Training Loss 0.8412\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [13950][55000]\t Training Loss 0.8415\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [14000][55000]\t Training Loss 0.8423\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [14050][55000]\t Training Loss 0.8424\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [14100][55000]\t Training Loss 0.8425\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [14150][55000]\t Training Loss 0.8428\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [14200][55000]\t Training Loss 0.8426\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [14250][55000]\t Training Loss 0.8429\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [14300][55000]\t Training Loss 0.8433\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [14350][55000]\t Training Loss 0.8436\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [14400][55000]\t Training Loss 0.8445\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [14450][55000]\t Training Loss 0.8447\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [14500][55000]\t Training Loss 0.8448\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [14550][55000]\t Training Loss 0.8455\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [14600][55000]\t Training Loss 0.8455\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [14650][55000]\t Training Loss 0.8465\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [14700][55000]\t Training Loss 0.8472\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [14750][55000]\t Training Loss 0.8479\t Accuracy 0.8441\n",
      "Epoch [4][20]\t Batch [14800][55000]\t Training Loss 0.8488\t Accuracy 0.8439\n",
      "Epoch [4][20]\t Batch [14850][55000]\t Training Loss 0.8494\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [14900][55000]\t Training Loss 0.8493\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [14950][55000]\t Training Loss 0.8494\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [15000][55000]\t Training Loss 0.8493\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [15050][55000]\t Training Loss 0.8490\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [15100][55000]\t Training Loss 0.8488\t Accuracy 0.8439\n",
      "Epoch [4][20]\t Batch [15150][55000]\t Training Loss 0.8491\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [15200][55000]\t Training Loss 0.8494\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [15250][55000]\t Training Loss 0.8493\t Accuracy 0.8440\n",
      "Epoch [4][20]\t Batch [15300][55000]\t Training Loss 0.8493\t Accuracy 0.8440\n",
      "Epoch [4][20]\t Batch [15350][55000]\t Training Loss 0.8490\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [15400][55000]\t Training Loss 0.8494\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [15450][55000]\t Training Loss 0.8495\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [15500][55000]\t Training Loss 0.8492\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [15550][55000]\t Training Loss 0.8491\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [15600][55000]\t Training Loss 0.8490\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [15650][55000]\t Training Loss 0.8489\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [15700][55000]\t Training Loss 0.8488\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [15750][55000]\t Training Loss 0.8496\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [15800][55000]\t Training Loss 0.8501\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [15850][55000]\t Training Loss 0.8504\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [15900][55000]\t Training Loss 0.8509\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [15950][55000]\t Training Loss 0.8510\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [16000][55000]\t Training Loss 0.8510\t Accuracy 0.8441\n",
      "Epoch [4][20]\t Batch [16050][55000]\t Training Loss 0.8520\t Accuracy 0.8437\n",
      "Epoch [4][20]\t Batch [16100][55000]\t Training Loss 0.8520\t Accuracy 0.8437\n",
      "Epoch [4][20]\t Batch [16150][55000]\t Training Loss 0.8519\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [16200][55000]\t Training Loss 0.8520\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [16250][55000]\t Training Loss 0.8519\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [16300][55000]\t Training Loss 0.8516\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [16350][55000]\t Training Loss 0.8512\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [16400][55000]\t Training Loss 0.8513\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [16450][55000]\t Training Loss 0.8510\t Accuracy 0.8440\n",
      "Epoch [4][20]\t Batch [16500][55000]\t Training Loss 0.8508\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [16550][55000]\t Training Loss 0.8504\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [16600][55000]\t Training Loss 0.8505\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [16650][55000]\t Training Loss 0.8505\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [16700][55000]\t Training Loss 0.8506\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [16750][55000]\t Training Loss 0.8505\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [16800][55000]\t Training Loss 0.8511\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [16850][55000]\t Training Loss 0.8517\t Accuracy 0.8441\n",
      "Epoch [4][20]\t Batch [16900][55000]\t Training Loss 0.8519\t Accuracy 0.8441\n",
      "Epoch [4][20]\t Batch [16950][55000]\t Training Loss 0.8519\t Accuracy 0.8440\n",
      "Epoch [4][20]\t Batch [17000][55000]\t Training Loss 0.8527\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [17050][55000]\t Training Loss 0.8526\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [17100][55000]\t Training Loss 0.8530\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [17150][55000]\t Training Loss 0.8528\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [17200][55000]\t Training Loss 0.8529\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [17250][55000]\t Training Loss 0.8534\t Accuracy 0.8434\n",
      "Epoch [4][20]\t Batch [17300][55000]\t Training Loss 0.8532\t Accuracy 0.8434\n",
      "Epoch [4][20]\t Batch [17350][55000]\t Training Loss 0.8526\t Accuracy 0.8437\n",
      "Epoch [4][20]\t Batch [17400][55000]\t Training Loss 0.8528\t Accuracy 0.8437\n",
      "Epoch [4][20]\t Batch [17450][55000]\t Training Loss 0.8529\t Accuracy 0.8436\n",
      "Epoch [4][20]\t Batch [17500][55000]\t Training Loss 0.8529\t Accuracy 0.8437\n",
      "Epoch [4][20]\t Batch [17550][55000]\t Training Loss 0.8536\t Accuracy 0.8434\n",
      "Epoch [4][20]\t Batch [17600][55000]\t Training Loss 0.8542\t Accuracy 0.8432\n",
      "Epoch [4][20]\t Batch [17650][55000]\t Training Loss 0.8544\t Accuracy 0.8434\n",
      "Epoch [4][20]\t Batch [17700][55000]\t Training Loss 0.8551\t Accuracy 0.8432\n",
      "Epoch [4][20]\t Batch [17750][55000]\t Training Loss 0.8555\t Accuracy 0.8432\n",
      "Epoch [4][20]\t Batch [17800][55000]\t Training Loss 0.8559\t Accuracy 0.8429\n",
      "Epoch [4][20]\t Batch [17850][55000]\t Training Loss 0.8561\t Accuracy 0.8427\n",
      "Epoch [4][20]\t Batch [17900][55000]\t Training Loss 0.8565\t Accuracy 0.8426\n",
      "Epoch [4][20]\t Batch [17950][55000]\t Training Loss 0.8563\t Accuracy 0.8424\n",
      "Epoch [4][20]\t Batch [18000][55000]\t Training Loss 0.8560\t Accuracy 0.8426\n",
      "Epoch [4][20]\t Batch [18050][55000]\t Training Loss 0.8562\t Accuracy 0.8426\n",
      "Epoch [4][20]\t Batch [18100][55000]\t Training Loss 0.8560\t Accuracy 0.8428\n",
      "Epoch [4][20]\t Batch [18150][55000]\t Training Loss 0.8557\t Accuracy 0.8428\n",
      "Epoch [4][20]\t Batch [18200][55000]\t Training Loss 0.8552\t Accuracy 0.8431\n",
      "Epoch [4][20]\t Batch [18250][55000]\t Training Loss 0.8552\t Accuracy 0.8431\n",
      "Epoch [4][20]\t Batch [18300][55000]\t Training Loss 0.8547\t Accuracy 0.8432\n",
      "Epoch [4][20]\t Batch [18350][55000]\t Training Loss 0.8546\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [18400][55000]\t Training Loss 0.8546\t Accuracy 0.8434\n",
      "Epoch [4][20]\t Batch [18450][55000]\t Training Loss 0.8550\t Accuracy 0.8433\n",
      "Epoch [4][20]\t Batch [18500][55000]\t Training Loss 0.8551\t Accuracy 0.8433\n",
      "Epoch [4][20]\t Batch [18550][55000]\t Training Loss 0.8548\t Accuracy 0.8435\n",
      "Epoch [4][20]\t Batch [18600][55000]\t Training Loss 0.8548\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [18650][55000]\t Training Loss 0.8547\t Accuracy 0.8439\n",
      "Epoch [4][20]\t Batch [18700][55000]\t Training Loss 0.8549\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [18750][55000]\t Training Loss 0.8551\t Accuracy 0.8438\n",
      "Epoch [4][20]\t Batch [18800][55000]\t Training Loss 0.8547\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [18850][55000]\t Training Loss 0.8550\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [18900][55000]\t Training Loss 0.8545\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [18950][55000]\t Training Loss 0.8543\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [19000][55000]\t Training Loss 0.8542\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [19050][55000]\t Training Loss 0.8546\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [19100][55000]\t Training Loss 0.8549\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [19150][55000]\t Training Loss 0.8550\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [19200][55000]\t Training Loss 0.8551\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [19250][55000]\t Training Loss 0.8549\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [19300][55000]\t Training Loss 0.8548\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [19350][55000]\t Training Loss 0.8548\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [19400][55000]\t Training Loss 0.8546\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [19450][55000]\t Training Loss 0.8543\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [19500][55000]\t Training Loss 0.8540\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [19550][55000]\t Training Loss 0.8541\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [19600][55000]\t Training Loss 0.8540\t Accuracy 0.8440\n",
      "Epoch [4][20]\t Batch [19650][55000]\t Training Loss 0.8536\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [19700][55000]\t Training Loss 0.8530\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [19750][55000]\t Training Loss 0.8525\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [19800][55000]\t Training Loss 0.8519\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [19850][55000]\t Training Loss 0.8519\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [19900][55000]\t Training Loss 0.8517\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [19950][55000]\t Training Loss 0.8519\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [20000][55000]\t Training Loss 0.8518\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [20050][55000]\t Training Loss 0.8523\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [20100][55000]\t Training Loss 0.8524\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [20150][55000]\t Training Loss 0.8522\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [20200][55000]\t Training Loss 0.8526\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [20250][55000]\t Training Loss 0.8527\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [20300][55000]\t Training Loss 0.8528\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [20350][55000]\t Training Loss 0.8528\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [20400][55000]\t Training Loss 0.8525\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [20450][55000]\t Training Loss 0.8521\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [20500][55000]\t Training Loss 0.8518\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [20550][55000]\t Training Loss 0.8518\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [20600][55000]\t Training Loss 0.8518\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [20650][55000]\t Training Loss 0.8514\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [20700][55000]\t Training Loss 0.8512\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [20750][55000]\t Training Loss 0.8512\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [20800][55000]\t Training Loss 0.8512\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [20850][55000]\t Training Loss 0.8512\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [20900][55000]\t Training Loss 0.8515\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [20950][55000]\t Training Loss 0.8520\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [21000][55000]\t Training Loss 0.8522\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [21050][55000]\t Training Loss 0.8524\t Accuracy 0.8444\n",
      "Epoch [4][20]\t Batch [21100][55000]\t Training Loss 0.8522\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [21150][55000]\t Training Loss 0.8522\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [21200][55000]\t Training Loss 0.8519\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [21250][55000]\t Training Loss 0.8517\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [21300][55000]\t Training Loss 0.8513\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [21350][55000]\t Training Loss 0.8514\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [21400][55000]\t Training Loss 0.8514\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [21450][55000]\t Training Loss 0.8515\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [21500][55000]\t Training Loss 0.8511\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [21550][55000]\t Training Loss 0.8509\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [21600][55000]\t Training Loss 0.8511\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [21650][55000]\t Training Loss 0.8511\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [21700][55000]\t Training Loss 0.8510\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [21750][55000]\t Training Loss 0.8510\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [21800][55000]\t Training Loss 0.8504\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [21850][55000]\t Training Loss 0.8500\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [21900][55000]\t Training Loss 0.8496\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [21950][55000]\t Training Loss 0.8492\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [22000][55000]\t Training Loss 0.8487\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [22050][55000]\t Training Loss 0.8484\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [22100][55000]\t Training Loss 0.8485\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [22150][55000]\t Training Loss 0.8490\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [22200][55000]\t Training Loss 0.8492\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [22250][55000]\t Training Loss 0.8492\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [22300][55000]\t Training Loss 0.8494\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [22350][55000]\t Training Loss 0.8491\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [22400][55000]\t Training Loss 0.8491\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [22450][55000]\t Training Loss 0.8491\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [22500][55000]\t Training Loss 0.8495\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [22550][55000]\t Training Loss 0.8501\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [22600][55000]\t Training Loss 0.8505\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [22650][55000]\t Training Loss 0.8507\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [22700][55000]\t Training Loss 0.8506\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [22750][55000]\t Training Loss 0.8504\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [22800][55000]\t Training Loss 0.8503\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [22850][55000]\t Training Loss 0.8504\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [22900][55000]\t Training Loss 0.8501\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [22950][55000]\t Training Loss 0.8499\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [23000][55000]\t Training Loss 0.8497\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [23050][55000]\t Training Loss 0.8495\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [23100][55000]\t Training Loss 0.8496\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [23150][55000]\t Training Loss 0.8495\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [23200][55000]\t Training Loss 0.8496\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [23250][55000]\t Training Loss 0.8493\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [23300][55000]\t Training Loss 0.8489\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [23350][55000]\t Training Loss 0.8488\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [23400][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [23450][55000]\t Training Loss 0.8485\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [23500][55000]\t Training Loss 0.8483\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [23550][55000]\t Training Loss 0.8482\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [23600][55000]\t Training Loss 0.8482\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [23650][55000]\t Training Loss 0.8483\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [23700][55000]\t Training Loss 0.8485\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [23750][55000]\t Training Loss 0.8491\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [23800][55000]\t Training Loss 0.8487\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [23850][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [23900][55000]\t Training Loss 0.8488\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [23950][55000]\t Training Loss 0.8488\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [24000][55000]\t Training Loss 0.8489\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [24050][55000]\t Training Loss 0.8488\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [24100][55000]\t Training Loss 0.8487\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [24150][55000]\t Training Loss 0.8485\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [24200][55000]\t Training Loss 0.8484\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [24250][55000]\t Training Loss 0.8485\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [24300][55000]\t Training Loss 0.8487\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [24350][55000]\t Training Loss 0.8486\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [24400][55000]\t Training Loss 0.8486\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [24450][55000]\t Training Loss 0.8486\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [24500][55000]\t Training Loss 0.8486\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [24550][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [24600][55000]\t Training Loss 0.8488\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [24650][55000]\t Training Loss 0.8488\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [24700][55000]\t Training Loss 0.8488\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [24750][55000]\t Training Loss 0.8491\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [24800][55000]\t Training Loss 0.8494\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [24850][55000]\t Training Loss 0.8493\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [24900][55000]\t Training Loss 0.8494\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [24950][55000]\t Training Loss 0.8497\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [25000][55000]\t Training Loss 0.8499\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [25050][55000]\t Training Loss 0.8496\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [25100][55000]\t Training Loss 0.8495\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [25150][55000]\t Training Loss 0.8493\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [25200][55000]\t Training Loss 0.8492\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [25250][55000]\t Training Loss 0.8491\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [25300][55000]\t Training Loss 0.8491\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [25350][55000]\t Training Loss 0.8492\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [25400][55000]\t Training Loss 0.8488\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [25450][55000]\t Training Loss 0.8483\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [25500][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [25550][55000]\t Training Loss 0.8478\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [25600][55000]\t Training Loss 0.8477\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [25650][55000]\t Training Loss 0.8476\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [25700][55000]\t Training Loss 0.8474\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [25750][55000]\t Training Loss 0.8471\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [25800][55000]\t Training Loss 0.8471\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [25850][55000]\t Training Loss 0.8472\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [25900][55000]\t Training Loss 0.8472\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [25950][55000]\t Training Loss 0.8473\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [26000][55000]\t Training Loss 0.8472\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [26050][55000]\t Training Loss 0.8470\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [26100][55000]\t Training Loss 0.8468\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [26150][55000]\t Training Loss 0.8466\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [26200][55000]\t Training Loss 0.8464\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [26250][55000]\t Training Loss 0.8463\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [26300][55000]\t Training Loss 0.8465\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [26350][55000]\t Training Loss 0.8464\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [26400][55000]\t Training Loss 0.8468\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [26450][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [26500][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [26550][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [26600][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [26650][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [26700][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [26750][55000]\t Training Loss 0.8480\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [26800][55000]\t Training Loss 0.8479\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [26850][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [26900][55000]\t Training Loss 0.8481\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [26950][55000]\t Training Loss 0.8480\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [27000][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [27050][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [27100][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [27150][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [27200][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [27250][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [27300][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [27350][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [27400][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [27450][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [27500][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [27550][55000]\t Training Loss 0.8476\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [27600][55000]\t Training Loss 0.8474\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [27650][55000]\t Training Loss 0.8475\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [27700][55000]\t Training Loss 0.8475\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [27750][55000]\t Training Loss 0.8476\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [27800][55000]\t Training Loss 0.8477\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [27850][55000]\t Training Loss 0.8478\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [27900][55000]\t Training Loss 0.8476\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [27950][55000]\t Training Loss 0.8474\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [28000][55000]\t Training Loss 0.8472\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [28050][55000]\t Training Loss 0.8468\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [28100][55000]\t Training Loss 0.8465\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [28150][55000]\t Training Loss 0.8463\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [28200][55000]\t Training Loss 0.8464\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [28250][55000]\t Training Loss 0.8460\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [28300][55000]\t Training Loss 0.8459\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [28350][55000]\t Training Loss 0.8457\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [28400][55000]\t Training Loss 0.8461\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [28450][55000]\t Training Loss 0.8458\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [28500][55000]\t Training Loss 0.8457\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [28550][55000]\t Training Loss 0.8456\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [28600][55000]\t Training Loss 0.8455\t Accuracy 0.8483\n",
      "Epoch [4][20]\t Batch [28650][55000]\t Training Loss 0.8457\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [28700][55000]\t Training Loss 0.8460\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [28750][55000]\t Training Loss 0.8461\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [28800][55000]\t Training Loss 0.8461\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [28850][55000]\t Training Loss 0.8460\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [28900][55000]\t Training Loss 0.8459\t Accuracy 0.8482\n",
      "Epoch [4][20]\t Batch [28950][55000]\t Training Loss 0.8458\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [29000][55000]\t Training Loss 0.8458\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [29050][55000]\t Training Loss 0.8458\t Accuracy 0.8481\n",
      "Epoch [4][20]\t Batch [29100][55000]\t Training Loss 0.8459\t Accuracy 0.8480\n",
      "Epoch [4][20]\t Batch [29150][55000]\t Training Loss 0.8463\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [29200][55000]\t Training Loss 0.8466\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [29250][55000]\t Training Loss 0.8469\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [29300][55000]\t Training Loss 0.8467\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [29350][55000]\t Training Loss 0.8468\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [29400][55000]\t Training Loss 0.8467\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [29450][55000]\t Training Loss 0.8463\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [29500][55000]\t Training Loss 0.8461\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [29550][55000]\t Training Loss 0.8461\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [29600][55000]\t Training Loss 0.8461\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [29650][55000]\t Training Loss 0.8460\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [29700][55000]\t Training Loss 0.8461\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [29750][55000]\t Training Loss 0.8465\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [29800][55000]\t Training Loss 0.8466\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [29850][55000]\t Training Loss 0.8469\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [29900][55000]\t Training Loss 0.8472\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [29950][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [30000][55000]\t Training Loss 0.8479\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [30050][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [30100][55000]\t Training Loss 0.8484\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [30150][55000]\t Training Loss 0.8488\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [30200][55000]\t Training Loss 0.8491\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [30250][55000]\t Training Loss 0.8491\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [30300][55000]\t Training Loss 0.8489\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [30350][55000]\t Training Loss 0.8489\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [30400][55000]\t Training Loss 0.8488\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [30450][55000]\t Training Loss 0.8488\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [30500][55000]\t Training Loss 0.8490\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [30550][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [30600][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [30650][55000]\t Training Loss 0.8499\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [30700][55000]\t Training Loss 0.8501\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [30750][55000]\t Training Loss 0.8503\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [30800][55000]\t Training Loss 0.8505\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [30850][55000]\t Training Loss 0.8504\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [30900][55000]\t Training Loss 0.8508\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [30950][55000]\t Training Loss 0.8506\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [31000][55000]\t Training Loss 0.8506\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [31050][55000]\t Training Loss 0.8507\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [31100][55000]\t Training Loss 0.8507\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [31150][55000]\t Training Loss 0.8508\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [31200][55000]\t Training Loss 0.8508\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [31250][55000]\t Training Loss 0.8508\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [31300][55000]\t Training Loss 0.8511\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [31350][55000]\t Training Loss 0.8518\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [31400][55000]\t Training Loss 0.8519\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [31450][55000]\t Training Loss 0.8522\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [31500][55000]\t Training Loss 0.8522\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [31550][55000]\t Training Loss 0.8522\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [31600][55000]\t Training Loss 0.8524\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [31650][55000]\t Training Loss 0.8526\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [31700][55000]\t Training Loss 0.8529\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [31750][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [31800][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [31850][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [31900][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [31950][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [32000][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [32050][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [32100][55000]\t Training Loss 0.8532\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [32150][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [32200][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [32250][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [32300][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [32350][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [32400][55000]\t Training Loss 0.8546\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [32450][55000]\t Training Loss 0.8548\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [32500][55000]\t Training Loss 0.8550\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [32550][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [32600][55000]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [4][20]\t Batch [32650][55000]\t Training Loss 0.8551\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [32700][55000]\t Training Loss 0.8551\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [32750][55000]\t Training Loss 0.8552\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [32800][55000]\t Training Loss 0.8553\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [32850][55000]\t Training Loss 0.8553\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [32900][55000]\t Training Loss 0.8551\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [32950][55000]\t Training Loss 0.8550\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33000][55000]\t Training Loss 0.8549\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33050][55000]\t Training Loss 0.8550\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33100][55000]\t Training Loss 0.8550\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33150][55000]\t Training Loss 0.8551\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [33200][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [33250][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33300][55000]\t Training Loss 0.8551\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33350][55000]\t Training Loss 0.8552\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [33400][55000]\t Training Loss 0.8554\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33450][55000]\t Training Loss 0.8554\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [33500][55000]\t Training Loss 0.8553\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [33550][55000]\t Training Loss 0.8553\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [33600][55000]\t Training Loss 0.8554\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33650][55000]\t Training Loss 0.8553\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [33700][55000]\t Training Loss 0.8552\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [33750][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [33800][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [33850][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [33900][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [33950][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [34000][55000]\t Training Loss 0.8541\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [34050][55000]\t Training Loss 0.8542\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [34100][55000]\t Training Loss 0.8543\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [34150][55000]\t Training Loss 0.8544\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [34200][55000]\t Training Loss 0.8542\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [34250][55000]\t Training Loss 0.8540\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [34300][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [34350][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [34400][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [34450][55000]\t Training Loss 0.8538\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [34500][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [34550][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [34600][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [34650][55000]\t Training Loss 0.8538\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [34700][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [34750][55000]\t Training Loss 0.8541\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [34800][55000]\t Training Loss 0.8540\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [34850][55000]\t Training Loss 0.8545\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [34900][55000]\t Training Loss 0.8545\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [34950][55000]\t Training Loss 0.8546\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [35000][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [35050][55000]\t Training Loss 0.8543\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [35100][55000]\t Training Loss 0.8544\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [35150][55000]\t Training Loss 0.8544\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [35200][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [35250][55000]\t Training Loss 0.8546\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [35300][55000]\t Training Loss 0.8546\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [35350][55000]\t Training Loss 0.8545\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [35400][55000]\t Training Loss 0.8545\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [35450][55000]\t Training Loss 0.8544\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [35500][55000]\t Training Loss 0.8544\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [35550][55000]\t Training Loss 0.8542\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [35600][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [35650][55000]\t Training Loss 0.8545\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [35700][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [35750][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [35800][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [35850][55000]\t Training Loss 0.8541\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [35900][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [35950][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [36000][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [36050][55000]\t Training Loss 0.8544\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [36100][55000]\t Training Loss 0.8546\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [36150][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [36200][55000]\t Training Loss 0.8543\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [36250][55000]\t Training Loss 0.8542\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [36300][55000]\t Training Loss 0.8539\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [36350][55000]\t Training Loss 0.8537\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [36400][55000]\t Training Loss 0.8537\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [36450][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [36500][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [36550][55000]\t Training Loss 0.8539\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [36600][55000]\t Training Loss 0.8536\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [36650][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [36700][55000]\t Training Loss 0.8532\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [36750][55000]\t Training Loss 0.8530\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [36800][55000]\t Training Loss 0.8529\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [36850][55000]\t Training Loss 0.8529\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [36900][55000]\t Training Loss 0.8529\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [36950][55000]\t Training Loss 0.8528\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [37000][55000]\t Training Loss 0.8527\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [37050][55000]\t Training Loss 0.8525\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [37100][55000]\t Training Loss 0.8526\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [37150][55000]\t Training Loss 0.8526\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [37200][55000]\t Training Loss 0.8525\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [37250][55000]\t Training Loss 0.8524\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [37300][55000]\t Training Loss 0.8525\t Accuracy 0.8477\n",
      "Epoch [4][20]\t Batch [37350][55000]\t Training Loss 0.8526\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [37400][55000]\t Training Loss 0.8528\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [37450][55000]\t Training Loss 0.8531\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [37500][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [37550][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [37600][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [37650][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [37700][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [37750][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [37800][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [37850][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [37900][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [37950][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [38000][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [38050][55000]\t Training Loss 0.8536\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [38100][55000]\t Training Loss 0.8537\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [38150][55000]\t Training Loss 0.8536\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [38200][55000]\t Training Loss 0.8535\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [38250][55000]\t Training Loss 0.8535\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [38300][55000]\t Training Loss 0.8536\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [38350][55000]\t Training Loss 0.8538\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [38400][55000]\t Training Loss 0.8539\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [38450][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [38500][55000]\t Training Loss 0.8537\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [38550][55000]\t Training Loss 0.8537\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [38600][55000]\t Training Loss 0.8539\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [38650][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [38700][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [38750][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [38800][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [38850][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [38900][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [38950][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [39000][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [39050][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [39100][55000]\t Training Loss 0.8530\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [39150][55000]\t Training Loss 0.8529\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [39200][55000]\t Training Loss 0.8527\t Accuracy 0.8475\n",
      "Epoch [4][20]\t Batch [39250][55000]\t Training Loss 0.8525\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [39300][55000]\t Training Loss 0.8525\t Accuracy 0.8476\n",
      "Epoch [4][20]\t Batch [39350][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [39400][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [4][20]\t Batch [39450][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [4][20]\t Batch [39500][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [39550][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [39600][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [39650][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [39700][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [39750][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [39800][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [39850][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [39900][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [39950][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40000][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40050][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [40100][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [40150][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40200][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40250][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40300][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40350][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40400][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40450][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [40500][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40550][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40600][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40650][55000]\t Training Loss 0.8536\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [40700][55000]\t Training Loss 0.8536\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [40750][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40800][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [40850][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [40900][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [40950][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [41000][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [41050][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [41100][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [41150][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [41200][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [41250][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [41300][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [4][20]\t Batch [41350][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [41400][55000]\t Training Loss 0.8539\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [41450][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [41500][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [41550][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [41600][55000]\t Training Loss 0.8545\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [41650][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [41700][55000]\t Training Loss 0.8544\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [41750][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [41800][55000]\t Training Loss 0.8547\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [41850][55000]\t Training Loss 0.8548\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [41900][55000]\t Training Loss 0.8547\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [41950][55000]\t Training Loss 0.8548\t Accuracy 0.8469\n",
      "Epoch [4][20]\t Batch [42000][55000]\t Training Loss 0.8547\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [42050][55000]\t Training Loss 0.8546\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [42100][55000]\t Training Loss 0.8545\t Accuracy 0.8468\n",
      "Epoch [4][20]\t Batch [42150][55000]\t Training Loss 0.8547\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [42200][55000]\t Training Loss 0.8547\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [42250][55000]\t Training Loss 0.8548\t Accuracy 0.8467\n",
      "Epoch [4][20]\t Batch [42300][55000]\t Training Loss 0.8548\t Accuracy 0.8466\n",
      "Epoch [4][20]\t Batch [42350][55000]\t Training Loss 0.8550\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [42400][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [42450][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [42500][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [42550][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [42600][55000]\t Training Loss 0.8555\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [42650][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [42700][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [42750][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [42800][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [42850][55000]\t Training Loss 0.8551\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [42900][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [42950][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [43000][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [43050][55000]\t Training Loss 0.8556\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [43100][55000]\t Training Loss 0.8558\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [43150][55000]\t Training Loss 0.8558\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [43200][55000]\t Training Loss 0.8557\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [43250][55000]\t Training Loss 0.8558\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [43300][55000]\t Training Loss 0.8557\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [43350][55000]\t Training Loss 0.8555\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [43400][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [43450][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [43500][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [43550][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [43600][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [43650][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [43700][55000]\t Training Loss 0.8547\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [43750][55000]\t Training Loss 0.8547\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [43800][55000]\t Training Loss 0.8546\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [43850][55000]\t Training Loss 0.8547\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [43900][55000]\t Training Loss 0.8548\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [43950][55000]\t Training Loss 0.8549\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [44000][55000]\t Training Loss 0.8549\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [44050][55000]\t Training Loss 0.8550\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [44100][55000]\t Training Loss 0.8550\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [44150][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [44200][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [44250][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [44300][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [44350][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [44400][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [44450][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [44500][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [44550][55000]\t Training Loss 0.8554\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [44600][55000]\t Training Loss 0.8553\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [44650][55000]\t Training Loss 0.8552\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [44700][55000]\t Training Loss 0.8551\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [44750][55000]\t Training Loss 0.8551\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [44800][55000]\t Training Loss 0.8551\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [44850][55000]\t Training Loss 0.8552\t Accuracy 0.8465\n",
      "Epoch [4][20]\t Batch [44900][55000]\t Training Loss 0.8553\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [44950][55000]\t Training Loss 0.8554\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [45000][55000]\t Training Loss 0.8553\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [45050][55000]\t Training Loss 0.8555\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45100][55000]\t Training Loss 0.8555\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [45150][55000]\t Training Loss 0.8557\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45200][55000]\t Training Loss 0.8557\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45250][55000]\t Training Loss 0.8556\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [45300][55000]\t Training Loss 0.8555\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [45350][55000]\t Training Loss 0.8554\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [45400][55000]\t Training Loss 0.8554\t Accuracy 0.8464\n",
      "Epoch [4][20]\t Batch [45450][55000]\t Training Loss 0.8555\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [45500][55000]\t Training Loss 0.8557\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45550][55000]\t Training Loss 0.8557\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45600][55000]\t Training Loss 0.8556\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45650][55000]\t Training Loss 0.8557\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45700][55000]\t Training Loss 0.8556\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45750][55000]\t Training Loss 0.8556\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [45800][55000]\t Training Loss 0.8557\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45850][55000]\t Training Loss 0.8557\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45900][55000]\t Training Loss 0.8558\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [45950][55000]\t Training Loss 0.8559\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [46000][55000]\t Training Loss 0.8560\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [46050][55000]\t Training Loss 0.8561\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [46100][55000]\t Training Loss 0.8562\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [46150][55000]\t Training Loss 0.8563\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46200][55000]\t Training Loss 0.8563\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46250][55000]\t Training Loss 0.8562\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46300][55000]\t Training Loss 0.8563\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46350][55000]\t Training Loss 0.8564\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [46400][55000]\t Training Loss 0.8566\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [46450][55000]\t Training Loss 0.8567\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [46500][55000]\t Training Loss 0.8566\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [46550][55000]\t Training Loss 0.8565\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46600][55000]\t Training Loss 0.8564\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46650][55000]\t Training Loss 0.8564\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [46700][55000]\t Training Loss 0.8563\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46750][55000]\t Training Loss 0.8563\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46800][55000]\t Training Loss 0.8562\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46850][55000]\t Training Loss 0.8561\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46900][55000]\t Training Loss 0.8560\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [46950][55000]\t Training Loss 0.8559\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [47000][55000]\t Training Loss 0.8558\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47050][55000]\t Training Loss 0.8558\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47100][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [47150][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [47200][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47250][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47300][55000]\t Training Loss 0.8558\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47350][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47400][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47450][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47500][55000]\t Training Loss 0.8560\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [47550][55000]\t Training Loss 0.8560\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [47600][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47650][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [47700][55000]\t Training Loss 0.8560\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [47750][55000]\t Training Loss 0.8560\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [47800][55000]\t Training Loss 0.8559\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [47850][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47900][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [47950][55000]\t Training Loss 0.8558\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [48000][55000]\t Training Loss 0.8558\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [48050][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [48100][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [48150][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [48200][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [48250][55000]\t Training Loss 0.8551\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [48300][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [48350][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [48400][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [48450][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [48500][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [48550][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [48600][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [48650][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [48700][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [48750][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [48800][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [48850][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [48900][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [4][20]\t Batch [48950][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [49000][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [4][20]\t Batch [49050][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [4][20]\t Batch [49100][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [4][20]\t Batch [49150][55000]\t Training Loss 0.8548\t Accuracy 0.8458\n",
      "Epoch [4][20]\t Batch [49200][55000]\t Training Loss 0.8548\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [49250][55000]\t Training Loss 0.8550\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [49300][55000]\t Training Loss 0.8549\t Accuracy 0.8456\n",
      "Epoch [4][20]\t Batch [49350][55000]\t Training Loss 0.8548\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [49400][55000]\t Training Loss 0.8546\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [49450][55000]\t Training Loss 0.8546\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [49500][55000]\t Training Loss 0.8548\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [49550][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [49600][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [49650][55000]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [49700][55000]\t Training Loss 0.8557\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [49750][55000]\t Training Loss 0.8557\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [49800][55000]\t Training Loss 0.8557\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [49850][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [49900][55000]\t Training Loss 0.8559\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [49950][55000]\t Training Loss 0.8559\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [50000][55000]\t Training Loss 0.8560\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [50050][55000]\t Training Loss 0.8561\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [50100][55000]\t Training Loss 0.8561\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [50150][55000]\t Training Loss 0.8560\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [50200][55000]\t Training Loss 0.8560\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [50250][55000]\t Training Loss 0.8562\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [50300][55000]\t Training Loss 0.8560\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [50350][55000]\t Training Loss 0.8561\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [50400][55000]\t Training Loss 0.8564\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [50450][55000]\t Training Loss 0.8567\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [50500][55000]\t Training Loss 0.8567\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [50550][55000]\t Training Loss 0.8567\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [50600][55000]\t Training Loss 0.8568\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [50650][55000]\t Training Loss 0.8568\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [50700][55000]\t Training Loss 0.8568\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [50750][55000]\t Training Loss 0.8568\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [50800][55000]\t Training Loss 0.8568\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [50850][55000]\t Training Loss 0.8569\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [50900][55000]\t Training Loss 0.8568\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [50950][55000]\t Training Loss 0.8567\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [51000][55000]\t Training Loss 0.8565\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [51050][55000]\t Training Loss 0.8564\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [51100][55000]\t Training Loss 0.8563\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [51150][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [51200][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [51250][55000]\t Training Loss 0.8563\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [51300][55000]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [51350][55000]\t Training Loss 0.8565\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [51400][55000]\t Training Loss 0.8564\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [51450][55000]\t Training Loss 0.8563\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [51500][55000]\t Training Loss 0.8561\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [51550][55000]\t Training Loss 0.8561\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [51600][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [51650][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [51700][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [51750][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [51800][55000]\t Training Loss 0.8559\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [51850][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [51900][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [51950][55000]\t Training Loss 0.8557\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [52000][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [52050][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [52100][55000]\t Training Loss 0.8560\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [52150][55000]\t Training Loss 0.8563\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [52200][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [52250][55000]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [52300][55000]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [52350][55000]\t Training Loss 0.8565\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [52400][55000]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [52450][55000]\t Training Loss 0.8563\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [52500][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [52550][55000]\t Training Loss 0.8561\t Accuracy 0.8452\n",
      "Epoch [4][20]\t Batch [52600][55000]\t Training Loss 0.8559\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [52650][55000]\t Training Loss 0.8557\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [52700][55000]\t Training Loss 0.8558\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [52750][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [52800][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [52850][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [52900][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [52950][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [53000][55000]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [53050][55000]\t Training Loss 0.8566\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53100][55000]\t Training Loss 0.8565\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53150][55000]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [53200][55000]\t Training Loss 0.8565\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53250][55000]\t Training Loss 0.8566\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53300][55000]\t Training Loss 0.8566\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53350][55000]\t Training Loss 0.8565\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53400][55000]\t Training Loss 0.8564\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [53450][55000]\t Training Loss 0.8563\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [53500][55000]\t Training Loss 0.8563\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [53550][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53600][55000]\t Training Loss 0.8565\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [53650][55000]\t Training Loss 0.8565\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53700][55000]\t Training Loss 0.8565\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53750][55000]\t Training Loss 0.8564\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53800][55000]\t Training Loss 0.8563\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [53850][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53900][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [53950][55000]\t Training Loss 0.8563\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [54000][55000]\t Training Loss 0.8564\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [54050][55000]\t Training Loss 0.8565\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [54100][55000]\t Training Loss 0.8567\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [54150][55000]\t Training Loss 0.8566\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [54200][55000]\t Training Loss 0.8567\t Accuracy 0.8446\n",
      "Epoch [4][20]\t Batch [54250][55000]\t Training Loss 0.8565\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [54300][55000]\t Training Loss 0.8565\t Accuracy 0.8447\n",
      "Epoch [4][20]\t Batch [54350][55000]\t Training Loss 0.8564\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [54400][55000]\t Training Loss 0.8564\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [54450][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [54500][55000]\t Training Loss 0.8562\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [54550][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [54600][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [54650][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [54700][55000]\t Training Loss 0.8559\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [54750][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [54800][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [4][20]\t Batch [54850][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [54900][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [54950][55000]\t Training Loss 0.8561\t Accuracy 0.8448\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8562\t Average training accuracy 0.8448\n",
      "Epoch [4]\t Average validation loss 0.7937\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [5][20]\t Batch [0][55000]\t Training Loss 1.4683\t Accuracy 0.0000\n",
      "Epoch [5][20]\t Batch [50][55000]\t Training Loss 0.9413\t Accuracy 0.7255\n",
      "Epoch [5][20]\t Batch [100][55000]\t Training Loss 0.8511\t Accuracy 0.8020\n",
      "Epoch [5][20]\t Batch [150][55000]\t Training Loss 0.8522\t Accuracy 0.8146\n",
      "Epoch [5][20]\t Batch [200][55000]\t Training Loss 0.8627\t Accuracy 0.8159\n",
      "Epoch [5][20]\t Batch [250][55000]\t Training Loss 0.8450\t Accuracy 0.8247\n",
      "Epoch [5][20]\t Batch [300][55000]\t Training Loss 0.8463\t Accuracy 0.8239\n",
      "Epoch [5][20]\t Batch [350][55000]\t Training Loss 0.8267\t Accuracy 0.8262\n",
      "Epoch [5][20]\t Batch [400][55000]\t Training Loss 0.8129\t Accuracy 0.8304\n",
      "Epoch [5][20]\t Batch [450][55000]\t Training Loss 0.8186\t Accuracy 0.8315\n",
      "Epoch [5][20]\t Batch [500][55000]\t Training Loss 0.8249\t Accuracy 0.8343\n",
      "Epoch [5][20]\t Batch [550][55000]\t Training Loss 0.8420\t Accuracy 0.8312\n",
      "Epoch [5][20]\t Batch [600][55000]\t Training Loss 0.8465\t Accuracy 0.8353\n",
      "Epoch [5][20]\t Batch [650][55000]\t Training Loss 0.8663\t Accuracy 0.8249\n",
      "Epoch [5][20]\t Batch [700][55000]\t Training Loss 0.8639\t Accuracy 0.8302\n",
      "Epoch [5][20]\t Batch [750][55000]\t Training Loss 0.8602\t Accuracy 0.8309\n",
      "Epoch [5][20]\t Batch [800][55000]\t Training Loss 0.8542\t Accuracy 0.8327\n",
      "Epoch [5][20]\t Batch [850][55000]\t Training Loss 0.8525\t Accuracy 0.8343\n",
      "Epoch [5][20]\t Batch [900][55000]\t Training Loss 0.8591\t Accuracy 0.8335\n",
      "Epoch [5][20]\t Batch [950][55000]\t Training Loss 0.8659\t Accuracy 0.8286\n",
      "Epoch [5][20]\t Batch [1000][55000]\t Training Loss 0.8654\t Accuracy 0.8312\n",
      "Epoch [5][20]\t Batch [1050][55000]\t Training Loss 0.8730\t Accuracy 0.8297\n",
      "Epoch [5][20]\t Batch [1100][55000]\t Training Loss 0.8815\t Accuracy 0.8283\n",
      "Epoch [5][20]\t Batch [1150][55000]\t Training Loss 0.8907\t Accuracy 0.8254\n",
      "Epoch [5][20]\t Batch [1200][55000]\t Training Loss 0.8849\t Accuracy 0.8285\n",
      "Epoch [5][20]\t Batch [1250][55000]\t Training Loss 0.8862\t Accuracy 0.8281\n",
      "Epoch [5][20]\t Batch [1300][55000]\t Training Loss 0.8865\t Accuracy 0.8278\n",
      "Epoch [5][20]\t Batch [1350][55000]\t Training Loss 0.8833\t Accuracy 0.8290\n",
      "Epoch [5][20]\t Batch [1400][55000]\t Training Loss 0.8839\t Accuracy 0.8280\n",
      "Epoch [5][20]\t Batch [1450][55000]\t Training Loss 0.8849\t Accuracy 0.8270\n",
      "Epoch [5][20]\t Batch [1500][55000]\t Training Loss 0.8829\t Accuracy 0.8301\n",
      "Epoch [5][20]\t Batch [1550][55000]\t Training Loss 0.8825\t Accuracy 0.8304\n",
      "Epoch [5][20]\t Batch [1600][55000]\t Training Loss 0.8851\t Accuracy 0.8295\n",
      "Epoch [5][20]\t Batch [1650][55000]\t Training Loss 0.8820\t Accuracy 0.8310\n",
      "Epoch [5][20]\t Batch [1700][55000]\t Training Loss 0.8804\t Accuracy 0.8319\n",
      "Epoch [5][20]\t Batch [1750][55000]\t Training Loss 0.8748\t Accuracy 0.8327\n",
      "Epoch [5][20]\t Batch [1800][55000]\t Training Loss 0.8726\t Accuracy 0.8345\n",
      "Epoch [5][20]\t Batch [1850][55000]\t Training Loss 0.8710\t Accuracy 0.8352\n",
      "Epoch [5][20]\t Batch [1900][55000]\t Training Loss 0.8688\t Accuracy 0.8353\n",
      "Epoch [5][20]\t Batch [1950][55000]\t Training Loss 0.8673\t Accuracy 0.8365\n",
      "Epoch [5][20]\t Batch [2000][55000]\t Training Loss 0.8648\t Accuracy 0.8376\n",
      "Epoch [5][20]\t Batch [2050][55000]\t Training Loss 0.8639\t Accuracy 0.8381\n",
      "Epoch [5][20]\t Batch [2100][55000]\t Training Loss 0.8611\t Accuracy 0.8386\n",
      "Epoch [5][20]\t Batch [2150][55000]\t Training Loss 0.8578\t Accuracy 0.8405\n",
      "Epoch [5][20]\t Batch [2200][55000]\t Training Loss 0.8530\t Accuracy 0.8423\n",
      "Epoch [5][20]\t Batch [2250][55000]\t Training Loss 0.8522\t Accuracy 0.8427\n",
      "Epoch [5][20]\t Batch [2300][55000]\t Training Loss 0.8492\t Accuracy 0.8422\n",
      "Epoch [5][20]\t Batch [2350][55000]\t Training Loss 0.8477\t Accuracy 0.8426\n",
      "Epoch [5][20]\t Batch [2400][55000]\t Training Loss 0.8486\t Accuracy 0.8409\n",
      "Epoch [5][20]\t Batch [2450][55000]\t Training Loss 0.8515\t Accuracy 0.8397\n",
      "Epoch [5][20]\t Batch [2500][55000]\t Training Loss 0.8493\t Accuracy 0.8417\n",
      "Epoch [5][20]\t Batch [2550][55000]\t Training Loss 0.8480\t Accuracy 0.8416\n",
      "Epoch [5][20]\t Batch [2600][55000]\t Training Loss 0.8474\t Accuracy 0.8416\n",
      "Epoch [5][20]\t Batch [2650][55000]\t Training Loss 0.8462\t Accuracy 0.8419\n",
      "Epoch [5][20]\t Batch [2700][55000]\t Training Loss 0.8460\t Accuracy 0.8423\n",
      "Epoch [5][20]\t Batch [2750][55000]\t Training Loss 0.8459\t Accuracy 0.8422\n",
      "Epoch [5][20]\t Batch [2800][55000]\t Training Loss 0.8461\t Accuracy 0.8404\n",
      "Epoch [5][20]\t Batch [2850][55000]\t Training Loss 0.8450\t Accuracy 0.8401\n",
      "Epoch [5][20]\t Batch [2900][55000]\t Training Loss 0.8418\t Accuracy 0.8411\n",
      "Epoch [5][20]\t Batch [2950][55000]\t Training Loss 0.8419\t Accuracy 0.8404\n",
      "Epoch [5][20]\t Batch [3000][55000]\t Training Loss 0.8417\t Accuracy 0.8411\n",
      "Epoch [5][20]\t Batch [3050][55000]\t Training Loss 0.8428\t Accuracy 0.8407\n",
      "Epoch [5][20]\t Batch [3100][55000]\t Training Loss 0.8451\t Accuracy 0.8404\n",
      "Epoch [5][20]\t Batch [3150][55000]\t Training Loss 0.8450\t Accuracy 0.8407\n",
      "Epoch [5][20]\t Batch [3200][55000]\t Training Loss 0.8447\t Accuracy 0.8419\n",
      "Epoch [5][20]\t Batch [3250][55000]\t Training Loss 0.8441\t Accuracy 0.8413\n",
      "Epoch [5][20]\t Batch [3300][55000]\t Training Loss 0.8454\t Accuracy 0.8410\n",
      "Epoch [5][20]\t Batch [3350][55000]\t Training Loss 0.8436\t Accuracy 0.8424\n",
      "Epoch [5][20]\t Batch [3400][55000]\t Training Loss 0.8460\t Accuracy 0.8409\n",
      "Epoch [5][20]\t Batch [3450][55000]\t Training Loss 0.8456\t Accuracy 0.8418\n",
      "Epoch [5][20]\t Batch [3500][55000]\t Training Loss 0.8456\t Accuracy 0.8415\n",
      "Epoch [5][20]\t Batch [3550][55000]\t Training Loss 0.8472\t Accuracy 0.8409\n",
      "Epoch [5][20]\t Batch [3600][55000]\t Training Loss 0.8470\t Accuracy 0.8406\n",
      "Epoch [5][20]\t Batch [3650][55000]\t Training Loss 0.8456\t Accuracy 0.8414\n",
      "Epoch [5][20]\t Batch [3700][55000]\t Training Loss 0.8464\t Accuracy 0.8409\n",
      "Epoch [5][20]\t Batch [3750][55000]\t Training Loss 0.8463\t Accuracy 0.8411\n",
      "Epoch [5][20]\t Batch [3800][55000]\t Training Loss 0.8466\t Accuracy 0.8411\n",
      "Epoch [5][20]\t Batch [3850][55000]\t Training Loss 0.8462\t Accuracy 0.8416\n",
      "Epoch [5][20]\t Batch [3900][55000]\t Training Loss 0.8450\t Accuracy 0.8429\n",
      "Epoch [5][20]\t Batch [3950][55000]\t Training Loss 0.8439\t Accuracy 0.8441\n",
      "Epoch [5][20]\t Batch [4000][55000]\t Training Loss 0.8436\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [4050][55000]\t Training Loss 0.8420\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [4100][55000]\t Training Loss 0.8425\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [4150][55000]\t Training Loss 0.8429\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [4200][55000]\t Training Loss 0.8434\t Accuracy 0.8438\n",
      "Epoch [5][20]\t Batch [4250][55000]\t Training Loss 0.8420\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [4300][55000]\t Training Loss 0.8420\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [4350][55000]\t Training Loss 0.8425\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [4400][55000]\t Training Loss 0.8424\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [4450][55000]\t Training Loss 0.8426\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [4500][55000]\t Training Loss 0.8429\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [4550][55000]\t Training Loss 0.8412\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [4600][55000]\t Training Loss 0.8395\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [4650][55000]\t Training Loss 0.8393\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [4700][55000]\t Training Loss 0.8389\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [4750][55000]\t Training Loss 0.8378\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [4800][55000]\t Training Loss 0.8385\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [4850][55000]\t Training Loss 0.8396\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [4900][55000]\t Training Loss 0.8382\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [4950][55000]\t Training Loss 0.8384\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [5000][55000]\t Training Loss 0.8386\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [5050][55000]\t Training Loss 0.8381\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [5100][55000]\t Training Loss 0.8381\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [5150][55000]\t Training Loss 0.8384\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [5200][55000]\t Training Loss 0.8399\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [5250][55000]\t Training Loss 0.8390\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [5300][55000]\t Training Loss 0.8389\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [5350][55000]\t Training Loss 0.8395\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [5400][55000]\t Training Loss 0.8387\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [5450][55000]\t Training Loss 0.8380\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [5500][55000]\t Training Loss 0.8362\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [5550][55000]\t Training Loss 0.8362\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [5600][55000]\t Training Loss 0.8353\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [5650][55000]\t Training Loss 0.8358\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [5700][55000]\t Training Loss 0.8355\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [5750][55000]\t Training Loss 0.8360\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [5800][55000]\t Training Loss 0.8363\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [5850][55000]\t Training Loss 0.8365\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [5900][55000]\t Training Loss 0.8370\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [5950][55000]\t Training Loss 0.8369\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [6000][55000]\t Training Loss 0.8359\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [6050][55000]\t Training Loss 0.8345\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [6100][55000]\t Training Loss 0.8330\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [6150][55000]\t Training Loss 0.8310\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [6200][55000]\t Training Loss 0.8310\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [6250][55000]\t Training Loss 0.8299\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [6300][55000]\t Training Loss 0.8303\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [6350][55000]\t Training Loss 0.8299\t Accuracy 0.8488\n",
      "Epoch [5][20]\t Batch [6400][55000]\t Training Loss 0.8293\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [6450][55000]\t Training Loss 0.8288\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [6500][55000]\t Training Loss 0.8296\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [6550][55000]\t Training Loss 0.8286\t Accuracy 0.8493\n",
      "Epoch [5][20]\t Batch [6600][55000]\t Training Loss 0.8273\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [6650][55000]\t Training Loss 0.8261\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [6700][55000]\t Training Loss 0.8263\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [6750][55000]\t Training Loss 0.8265\t Accuracy 0.8507\n",
      "Epoch [5][20]\t Batch [6800][55000]\t Training Loss 0.8269\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [6850][55000]\t Training Loss 0.8291\t Accuracy 0.8502\n",
      "Epoch [5][20]\t Batch [6900][55000]\t Training Loss 0.8290\t Accuracy 0.8502\n",
      "Epoch [5][20]\t Batch [6950][55000]\t Training Loss 0.8297\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [7000][55000]\t Training Loss 0.8295\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [7050][55000]\t Training Loss 0.8301\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [7100][55000]\t Training Loss 0.8303\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [7150][55000]\t Training Loss 0.8305\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [7200][55000]\t Training Loss 0.8313\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [7250][55000]\t Training Loss 0.8336\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [7300][55000]\t Training Loss 0.8355\t Accuracy 0.8480\n",
      "Epoch [5][20]\t Batch [7350][55000]\t Training Loss 0.8370\t Accuracy 0.8480\n",
      "Epoch [5][20]\t Batch [7400][55000]\t Training Loss 0.8380\t Accuracy 0.8481\n",
      "Epoch [5][20]\t Batch [7450][55000]\t Training Loss 0.8380\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [7500][55000]\t Training Loss 0.8380\t Accuracy 0.8480\n",
      "Epoch [5][20]\t Batch [7550][55000]\t Training Loss 0.8385\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [7600][55000]\t Training Loss 0.8380\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [7650][55000]\t Training Loss 0.8385\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [7700][55000]\t Training Loss 0.8390\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [7750][55000]\t Training Loss 0.8391\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [7800][55000]\t Training Loss 0.8398\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [7850][55000]\t Training Loss 0.8400\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [7900][55000]\t Training Loss 0.8402\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [7950][55000]\t Training Loss 0.8402\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [8000][55000]\t Training Loss 0.8402\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [8050][55000]\t Training Loss 0.8407\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [8100][55000]\t Training Loss 0.8393\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [8150][55000]\t Training Loss 0.8401\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [8200][55000]\t Training Loss 0.8401\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [8250][55000]\t Training Loss 0.8414\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [8300][55000]\t Training Loss 0.8417\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [8350][55000]\t Training Loss 0.8423\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [8400][55000]\t Training Loss 0.8416\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [8450][55000]\t Training Loss 0.8432\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [8500][55000]\t Training Loss 0.8424\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [8550][55000]\t Training Loss 0.8414\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [8600][55000]\t Training Loss 0.8406\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [8650][55000]\t Training Loss 0.8407\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [8700][55000]\t Training Loss 0.8413\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [8750][55000]\t Training Loss 0.8428\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [8800][55000]\t Training Loss 0.8436\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [8850][55000]\t Training Loss 0.8436\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [8900][55000]\t Training Loss 0.8453\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [8950][55000]\t Training Loss 0.8449\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [9000][55000]\t Training Loss 0.8442\t Accuracy 0.8454\n",
      "Epoch [5][20]\t Batch [9050][55000]\t Training Loss 0.8433\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [9100][55000]\t Training Loss 0.8431\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [9150][55000]\t Training Loss 0.8436\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [9200][55000]\t Training Loss 0.8432\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [9250][55000]\t Training Loss 0.8435\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [9300][55000]\t Training Loss 0.8437\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [9350][55000]\t Training Loss 0.8439\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [9400][55000]\t Training Loss 0.8442\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [9450][55000]\t Training Loss 0.8444\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [9500][55000]\t Training Loss 0.8435\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [9550][55000]\t Training Loss 0.8433\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [9600][55000]\t Training Loss 0.8437\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [9650][55000]\t Training Loss 0.8435\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [9700][55000]\t Training Loss 0.8430\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [9750][55000]\t Training Loss 0.8422\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [9800][55000]\t Training Loss 0.8427\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [9850][55000]\t Training Loss 0.8423\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [9900][55000]\t Training Loss 0.8418\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [9950][55000]\t Training Loss 0.8412\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [10000][55000]\t Training Loss 0.8409\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [10050][55000]\t Training Loss 0.8411\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [10100][55000]\t Training Loss 0.8409\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [10150][55000]\t Training Loss 0.8407\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [10200][55000]\t Training Loss 0.8409\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [10250][55000]\t Training Loss 0.8410\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [10300][55000]\t Training Loss 0.8409\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [10350][55000]\t Training Loss 0.8400\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [10400][55000]\t Training Loss 0.8392\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [10450][55000]\t Training Loss 0.8390\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [10500][55000]\t Training Loss 0.8380\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [10550][55000]\t Training Loss 0.8375\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [10600][55000]\t Training Loss 0.8371\t Accuracy 0.8489\n",
      "Epoch [5][20]\t Batch [10650][55000]\t Training Loss 0.8368\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [10700][55000]\t Training Loss 0.8366\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [10750][55000]\t Training Loss 0.8371\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [10800][55000]\t Training Loss 0.8376\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [10850][55000]\t Training Loss 0.8369\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [10900][55000]\t Training Loss 0.8364\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [10950][55000]\t Training Loss 0.8361\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [11000][55000]\t Training Loss 0.8360\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [11050][55000]\t Training Loss 0.8352\t Accuracy 0.8497\n",
      "Epoch [5][20]\t Batch [11100][55000]\t Training Loss 0.8346\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [11150][55000]\t Training Loss 0.8346\t Accuracy 0.8501\n",
      "Epoch [5][20]\t Batch [11200][55000]\t Training Loss 0.8342\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [11250][55000]\t Training Loss 0.8346\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [11300][55000]\t Training Loss 0.8342\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [11350][55000]\t Training Loss 0.8337\t Accuracy 0.8501\n",
      "Epoch [5][20]\t Batch [11400][55000]\t Training Loss 0.8338\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [11450][55000]\t Training Loss 0.8335\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [11500][55000]\t Training Loss 0.8332\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [11550][55000]\t Training Loss 0.8333\t Accuracy 0.8497\n",
      "Epoch [5][20]\t Batch [11600][55000]\t Training Loss 0.8345\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [11650][55000]\t Training Loss 0.8350\t Accuracy 0.8488\n",
      "Epoch [5][20]\t Batch [11700][55000]\t Training Loss 0.8350\t Accuracy 0.8489\n",
      "Epoch [5][20]\t Batch [11750][55000]\t Training Loss 0.8359\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [11800][55000]\t Training Loss 0.8361\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [11850][55000]\t Training Loss 0.8362\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [11900][55000]\t Training Loss 0.8365\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [11950][55000]\t Training Loss 0.8366\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [12000][55000]\t Training Loss 0.8366\t Accuracy 0.8489\n",
      "Epoch [5][20]\t Batch [12050][55000]\t Training Loss 0.8364\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [12100][55000]\t Training Loss 0.8363\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [12150][55000]\t Training Loss 0.8356\t Accuracy 0.8496\n",
      "Epoch [5][20]\t Batch [12200][55000]\t Training Loss 0.8360\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [12250][55000]\t Training Loss 0.8358\t Accuracy 0.8496\n",
      "Epoch [5][20]\t Batch [12300][55000]\t Training Loss 0.8359\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [12350][55000]\t Training Loss 0.8361\t Accuracy 0.8496\n",
      "Epoch [5][20]\t Batch [12400][55000]\t Training Loss 0.8364\t Accuracy 0.8497\n",
      "Epoch [5][20]\t Batch [12450][55000]\t Training Loss 0.8366\t Accuracy 0.8493\n",
      "Epoch [5][20]\t Batch [12500][55000]\t Training Loss 0.8368\t Accuracy 0.8491\n",
      "Epoch [5][20]\t Batch [12550][55000]\t Training Loss 0.8368\t Accuracy 0.8493\n",
      "Epoch [5][20]\t Batch [12600][55000]\t Training Loss 0.8375\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [12650][55000]\t Training Loss 0.8380\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [12700][55000]\t Training Loss 0.8387\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [12750][55000]\t Training Loss 0.8383\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [12800][55000]\t Training Loss 0.8389\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [12850][55000]\t Training Loss 0.8391\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [12900][55000]\t Training Loss 0.8391\t Accuracy 0.8485\n",
      "Epoch [5][20]\t Batch [12950][55000]\t Training Loss 0.8396\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [13000][55000]\t Training Loss 0.8395\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [13050][55000]\t Training Loss 0.8402\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [13100][55000]\t Training Loss 0.8408\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [13150][55000]\t Training Loss 0.8412\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [13200][55000]\t Training Loss 0.8414\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [13250][55000]\t Training Loss 0.8409\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [13300][55000]\t Training Loss 0.8408\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [13350][55000]\t Training Loss 0.8412\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [13400][55000]\t Training Loss 0.8415\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [13450][55000]\t Training Loss 0.8410\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [13500][55000]\t Training Loss 0.8406\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [13550][55000]\t Training Loss 0.8402\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [13600][55000]\t Training Loss 0.8393\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [13650][55000]\t Training Loss 0.8392\t Accuracy 0.8481\n",
      "Epoch [5][20]\t Batch [13700][55000]\t Training Loss 0.8400\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [13750][55000]\t Training Loss 0.8406\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [13800][55000]\t Training Loss 0.8407\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [13850][55000]\t Training Loss 0.8407\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [13900][55000]\t Training Loss 0.8409\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [13950][55000]\t Training Loss 0.8413\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [14000][55000]\t Training Loss 0.8420\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [14050][55000]\t Training Loss 0.8422\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [14100][55000]\t Training Loss 0.8423\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [14150][55000]\t Training Loss 0.8425\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [14200][55000]\t Training Loss 0.8424\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [14250][55000]\t Training Loss 0.8427\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [14300][55000]\t Training Loss 0.8430\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [14350][55000]\t Training Loss 0.8434\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [14400][55000]\t Training Loss 0.8442\t Accuracy 0.8454\n",
      "Epoch [5][20]\t Batch [14450][55000]\t Training Loss 0.8444\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [14500][55000]\t Training Loss 0.8445\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [14550][55000]\t Training Loss 0.8452\t Accuracy 0.8454\n",
      "Epoch [5][20]\t Batch [14600][55000]\t Training Loss 0.8452\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [14650][55000]\t Training Loss 0.8462\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [14700][55000]\t Training Loss 0.8470\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [14750][55000]\t Training Loss 0.8476\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [14800][55000]\t Training Loss 0.8485\t Accuracy 0.8443\n",
      "Epoch [5][20]\t Batch [14850][55000]\t Training Loss 0.8491\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [14900][55000]\t Training Loss 0.8490\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [14950][55000]\t Training Loss 0.8492\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [15000][55000]\t Training Loss 0.8491\t Accuracy 0.8439\n",
      "Epoch [5][20]\t Batch [15050][55000]\t Training Loss 0.8487\t Accuracy 0.8442\n",
      "Epoch [5][20]\t Batch [15100][55000]\t Training Loss 0.8485\t Accuracy 0.8443\n",
      "Epoch [5][20]\t Batch [15150][55000]\t Training Loss 0.8489\t Accuracy 0.8442\n",
      "Epoch [5][20]\t Batch [15200][55000]\t Training Loss 0.8491\t Accuracy 0.8442\n",
      "Epoch [5][20]\t Batch [15250][55000]\t Training Loss 0.8490\t Accuracy 0.8443\n",
      "Epoch [5][20]\t Batch [15300][55000]\t Training Loss 0.8490\t Accuracy 0.8443\n",
      "Epoch [5][20]\t Batch [15350][55000]\t Training Loss 0.8488\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [15400][55000]\t Training Loss 0.8491\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [15450][55000]\t Training Loss 0.8492\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [15500][55000]\t Training Loss 0.8490\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [15550][55000]\t Training Loss 0.8489\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [15600][55000]\t Training Loss 0.8487\t Accuracy 0.8454\n",
      "Epoch [5][20]\t Batch [15650][55000]\t Training Loss 0.8486\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [15700][55000]\t Training Loss 0.8485\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [15750][55000]\t Training Loss 0.8494\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [15800][55000]\t Training Loss 0.8498\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [15850][55000]\t Training Loss 0.8502\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [15900][55000]\t Training Loss 0.8507\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [15950][55000]\t Training Loss 0.8508\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [16000][55000]\t Training Loss 0.8508\t Accuracy 0.8444\n",
      "Epoch [5][20]\t Batch [16050][55000]\t Training Loss 0.8518\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [16100][55000]\t Training Loss 0.8517\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [16150][55000]\t Training Loss 0.8516\t Accuracy 0.8441\n",
      "Epoch [5][20]\t Batch [16200][55000]\t Training Loss 0.8518\t Accuracy 0.8438\n",
      "Epoch [5][20]\t Batch [16250][55000]\t Training Loss 0.8516\t Accuracy 0.8438\n",
      "Epoch [5][20]\t Batch [16300][55000]\t Training Loss 0.8513\t Accuracy 0.8438\n",
      "Epoch [5][20]\t Batch [16350][55000]\t Training Loss 0.8510\t Accuracy 0.8441\n",
      "Epoch [5][20]\t Batch [16400][55000]\t Training Loss 0.8510\t Accuracy 0.8441\n",
      "Epoch [5][20]\t Batch [16450][55000]\t Training Loss 0.8507\t Accuracy 0.8443\n",
      "Epoch [5][20]\t Batch [16500][55000]\t Training Loss 0.8506\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [16550][55000]\t Training Loss 0.8501\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [16600][55000]\t Training Loss 0.8502\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [16650][55000]\t Training Loss 0.8502\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [16700][55000]\t Training Loss 0.8504\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [16750][55000]\t Training Loss 0.8502\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [16800][55000]\t Training Loss 0.8508\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [16850][55000]\t Training Loss 0.8514\t Accuracy 0.8444\n",
      "Epoch [5][20]\t Batch [16900][55000]\t Training Loss 0.8516\t Accuracy 0.8444\n",
      "Epoch [5][20]\t Batch [16950][55000]\t Training Loss 0.8516\t Accuracy 0.8443\n",
      "Epoch [5][20]\t Batch [17000][55000]\t Training Loss 0.8524\t Accuracy 0.8439\n",
      "Epoch [5][20]\t Batch [17050][55000]\t Training Loss 0.8523\t Accuracy 0.8439\n",
      "Epoch [5][20]\t Batch [17100][55000]\t Training Loss 0.8527\t Accuracy 0.8438\n",
      "Epoch [5][20]\t Batch [17150][55000]\t Training Loss 0.8525\t Accuracy 0.8439\n",
      "Epoch [5][20]\t Batch [17200][55000]\t Training Loss 0.8526\t Accuracy 0.8438\n",
      "Epoch [5][20]\t Batch [17250][55000]\t Training Loss 0.8531\t Accuracy 0.8437\n",
      "Epoch [5][20]\t Batch [17300][55000]\t Training Loss 0.8529\t Accuracy 0.8437\n",
      "Epoch [5][20]\t Batch [17350][55000]\t Training Loss 0.8523\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [17400][55000]\t Training Loss 0.8525\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [17450][55000]\t Training Loss 0.8526\t Accuracy 0.8439\n",
      "Epoch [5][20]\t Batch [17500][55000]\t Training Loss 0.8526\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [17550][55000]\t Training Loss 0.8533\t Accuracy 0.8437\n",
      "Epoch [5][20]\t Batch [17600][55000]\t Training Loss 0.8539\t Accuracy 0.8435\n",
      "Epoch [5][20]\t Batch [17650][55000]\t Training Loss 0.8541\t Accuracy 0.8437\n",
      "Epoch [5][20]\t Batch [17700][55000]\t Training Loss 0.8548\t Accuracy 0.8435\n",
      "Epoch [5][20]\t Batch [17750][55000]\t Training Loss 0.8552\t Accuracy 0.8434\n",
      "Epoch [5][20]\t Batch [17800][55000]\t Training Loss 0.8556\t Accuracy 0.8432\n",
      "Epoch [5][20]\t Batch [17850][55000]\t Training Loss 0.8559\t Accuracy 0.8430\n",
      "Epoch [5][20]\t Batch [17900][55000]\t Training Loss 0.8562\t Accuracy 0.8429\n",
      "Epoch [5][20]\t Batch [17950][55000]\t Training Loss 0.8560\t Accuracy 0.8427\n",
      "Epoch [5][20]\t Batch [18000][55000]\t Training Loss 0.8557\t Accuracy 0.8429\n",
      "Epoch [5][20]\t Batch [18050][55000]\t Training Loss 0.8559\t Accuracy 0.8429\n",
      "Epoch [5][20]\t Batch [18100][55000]\t Training Loss 0.8558\t Accuracy 0.8430\n",
      "Epoch [5][20]\t Batch [18150][55000]\t Training Loss 0.8554\t Accuracy 0.8431\n",
      "Epoch [5][20]\t Batch [18200][55000]\t Training Loss 0.8549\t Accuracy 0.8434\n",
      "Epoch [5][20]\t Batch [18250][55000]\t Training Loss 0.8549\t Accuracy 0.8434\n",
      "Epoch [5][20]\t Batch [18300][55000]\t Training Loss 0.8544\t Accuracy 0.8435\n",
      "Epoch [5][20]\t Batch [18350][55000]\t Training Loss 0.8543\t Accuracy 0.8438\n",
      "Epoch [5][20]\t Batch [18400][55000]\t Training Loss 0.8543\t Accuracy 0.8437\n",
      "Epoch [5][20]\t Batch [18450][55000]\t Training Loss 0.8547\t Accuracy 0.8435\n",
      "Epoch [5][20]\t Batch [18500][55000]\t Training Loss 0.8548\t Accuracy 0.8435\n",
      "Epoch [5][20]\t Batch [18550][55000]\t Training Loss 0.8545\t Accuracy 0.8438\n",
      "Epoch [5][20]\t Batch [18600][55000]\t Training Loss 0.8546\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [18650][55000]\t Training Loss 0.8544\t Accuracy 0.8442\n",
      "Epoch [5][20]\t Batch [18700][55000]\t Training Loss 0.8546\t Accuracy 0.8440\n",
      "Epoch [5][20]\t Batch [18750][55000]\t Training Loss 0.8549\t Accuracy 0.8441\n",
      "Epoch [5][20]\t Batch [18800][55000]\t Training Loss 0.8545\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [18850][55000]\t Training Loss 0.8547\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [18900][55000]\t Training Loss 0.8542\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [18950][55000]\t Training Loss 0.8540\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [19000][55000]\t Training Loss 0.8539\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [19050][55000]\t Training Loss 0.8543\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [19100][55000]\t Training Loss 0.8546\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [19150][55000]\t Training Loss 0.8547\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [19200][55000]\t Training Loss 0.8548\t Accuracy 0.8444\n",
      "Epoch [5][20]\t Batch [19250][55000]\t Training Loss 0.8547\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [19300][55000]\t Training Loss 0.8545\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [19350][55000]\t Training Loss 0.8545\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [19400][55000]\t Training Loss 0.8543\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [19450][55000]\t Training Loss 0.8541\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [19500][55000]\t Training Loss 0.8537\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [19550][55000]\t Training Loss 0.8538\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [19600][55000]\t Training Loss 0.8537\t Accuracy 0.8443\n",
      "Epoch [5][20]\t Batch [19650][55000]\t Training Loss 0.8533\t Accuracy 0.8445\n",
      "Epoch [5][20]\t Batch [19700][55000]\t Training Loss 0.8528\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [19750][55000]\t Training Loss 0.8522\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [19800][55000]\t Training Loss 0.8516\t Accuracy 0.8454\n",
      "Epoch [5][20]\t Batch [19850][55000]\t Training Loss 0.8517\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [19900][55000]\t Training Loss 0.8514\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [19950][55000]\t Training Loss 0.8516\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [20000][55000]\t Training Loss 0.8515\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [20050][55000]\t Training Loss 0.8520\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [20100][55000]\t Training Loss 0.8522\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [20150][55000]\t Training Loss 0.8520\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [20200][55000]\t Training Loss 0.8523\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [20250][55000]\t Training Loss 0.8524\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [20300][55000]\t Training Loss 0.8525\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [20350][55000]\t Training Loss 0.8525\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [20400][55000]\t Training Loss 0.8522\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [20450][55000]\t Training Loss 0.8518\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [20500][55000]\t Training Loss 0.8516\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [20550][55000]\t Training Loss 0.8515\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [20600][55000]\t Training Loss 0.8515\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [20650][55000]\t Training Loss 0.8511\t Accuracy 0.8454\n",
      "Epoch [5][20]\t Batch [20700][55000]\t Training Loss 0.8509\t Accuracy 0.8454\n",
      "Epoch [5][20]\t Batch [20750][55000]\t Training Loss 0.8509\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [20800][55000]\t Training Loss 0.8510\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [20850][55000]\t Training Loss 0.8509\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [20900][55000]\t Training Loss 0.8512\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [20950][55000]\t Training Loss 0.8517\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [21000][55000]\t Training Loss 0.8519\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [21050][55000]\t Training Loss 0.8522\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [21100][55000]\t Training Loss 0.8519\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [21150][55000]\t Training Loss 0.8519\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [21200][55000]\t Training Loss 0.8516\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [21250][55000]\t Training Loss 0.8514\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [21300][55000]\t Training Loss 0.8510\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [21350][55000]\t Training Loss 0.8511\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [21400][55000]\t Training Loss 0.8511\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [21450][55000]\t Training Loss 0.8512\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [21500][55000]\t Training Loss 0.8508\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [21550][55000]\t Training Loss 0.8506\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [21600][55000]\t Training Loss 0.8508\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [21650][55000]\t Training Loss 0.8508\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [21700][55000]\t Training Loss 0.8507\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [21750][55000]\t Training Loss 0.8507\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [21800][55000]\t Training Loss 0.8502\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [21850][55000]\t Training Loss 0.8498\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [21900][55000]\t Training Loss 0.8494\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [21950][55000]\t Training Loss 0.8489\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [22000][55000]\t Training Loss 0.8485\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [22050][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [22100][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [22150][55000]\t Training Loss 0.8487\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [22200][55000]\t Training Loss 0.8489\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [22250][55000]\t Training Loss 0.8489\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [22300][55000]\t Training Loss 0.8491\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [22350][55000]\t Training Loss 0.8489\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [22400][55000]\t Training Loss 0.8488\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [22450][55000]\t Training Loss 0.8488\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [22500][55000]\t Training Loss 0.8492\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [22550][55000]\t Training Loss 0.8498\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [22600][55000]\t Training Loss 0.8502\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [22650][55000]\t Training Loss 0.8504\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [22700][55000]\t Training Loss 0.8503\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [22750][55000]\t Training Loss 0.8502\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [22800][55000]\t Training Loss 0.8501\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [22850][55000]\t Training Loss 0.8501\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [22900][55000]\t Training Loss 0.8498\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [22950][55000]\t Training Loss 0.8497\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [23000][55000]\t Training Loss 0.8494\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [23050][55000]\t Training Loss 0.8492\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [23100][55000]\t Training Loss 0.8493\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [23150][55000]\t Training Loss 0.8492\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [23200][55000]\t Training Loss 0.8493\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [23250][55000]\t Training Loss 0.8490\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [23300][55000]\t Training Loss 0.8487\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [23350][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [23400][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [23450][55000]\t Training Loss 0.8482\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [23500][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [23550][55000]\t Training Loss 0.8479\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [23600][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [23650][55000]\t Training Loss 0.8481\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [23700][55000]\t Training Loss 0.8482\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [23750][55000]\t Training Loss 0.8488\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [23800][55000]\t Training Loss 0.8485\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [23850][55000]\t Training Loss 0.8484\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [23900][55000]\t Training Loss 0.8486\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [23950][55000]\t Training Loss 0.8486\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [24000][55000]\t Training Loss 0.8486\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [24050][55000]\t Training Loss 0.8485\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [24100][55000]\t Training Loss 0.8484\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [24150][55000]\t Training Loss 0.8483\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [24200][55000]\t Training Loss 0.8481\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [24250][55000]\t Training Loss 0.8482\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [24300][55000]\t Training Loss 0.8484\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [24350][55000]\t Training Loss 0.8484\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [24400][55000]\t Training Loss 0.8484\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [24450][55000]\t Training Loss 0.8484\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [24500][55000]\t Training Loss 0.8484\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [24550][55000]\t Training Loss 0.8484\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [24600][55000]\t Training Loss 0.8485\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [24650][55000]\t Training Loss 0.8486\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [24700][55000]\t Training Loss 0.8485\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [24750][55000]\t Training Loss 0.8488\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [24800][55000]\t Training Loss 0.8491\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [24850][55000]\t Training Loss 0.8490\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [24900][55000]\t Training Loss 0.8491\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [24950][55000]\t Training Loss 0.8494\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [25000][55000]\t Training Loss 0.8496\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [25050][55000]\t Training Loss 0.8493\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [25100][55000]\t Training Loss 0.8493\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [25150][55000]\t Training Loss 0.8491\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [25200][55000]\t Training Loss 0.8490\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [25250][55000]\t Training Loss 0.8489\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [25300][55000]\t Training Loss 0.8488\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [25350][55000]\t Training Loss 0.8490\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [25400][55000]\t Training Loss 0.8485\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [25450][55000]\t Training Loss 0.8480\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [25500][55000]\t Training Loss 0.8479\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [25550][55000]\t Training Loss 0.8475\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [25600][55000]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [25650][55000]\t Training Loss 0.8473\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [25700][55000]\t Training Loss 0.8471\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [25750][55000]\t Training Loss 0.8469\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [25800][55000]\t Training Loss 0.8468\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [25850][55000]\t Training Loss 0.8469\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [25900][55000]\t Training Loss 0.8470\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [25950][55000]\t Training Loss 0.8470\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [26000][55000]\t Training Loss 0.8470\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [26050][55000]\t Training Loss 0.8468\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [26100][55000]\t Training Loss 0.8465\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [26150][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [26200][55000]\t Training Loss 0.8461\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [26250][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [26300][55000]\t Training Loss 0.8463\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [26350][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [26400][55000]\t Training Loss 0.8465\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [26450][55000]\t Training Loss 0.8467\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [26500][55000]\t Training Loss 0.8469\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [26550][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [26600][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [26650][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [26700][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [26750][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [26800][55000]\t Training Loss 0.8477\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [26850][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [26900][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [26950][55000]\t Training Loss 0.8477\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [27000][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [27050][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [27100][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [27150][55000]\t Training Loss 0.8472\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [27200][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [27250][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [27300][55000]\t Training Loss 0.8475\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [27350][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [27400][55000]\t Training Loss 0.8474\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [27450][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [27500][55000]\t Training Loss 0.8473\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [27550][55000]\t Training Loss 0.8474\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [27600][55000]\t Training Loss 0.8472\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [27650][55000]\t Training Loss 0.8472\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [27700][55000]\t Training Loss 0.8472\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [27750][55000]\t Training Loss 0.8473\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [27800][55000]\t Training Loss 0.8474\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [27850][55000]\t Training Loss 0.8475\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [27900][55000]\t Training Loss 0.8473\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [27950][55000]\t Training Loss 0.8471\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [28000][55000]\t Training Loss 0.8469\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [28050][55000]\t Training Loss 0.8466\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [28100][55000]\t Training Loss 0.8462\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [28150][55000]\t Training Loss 0.8460\t Accuracy 0.8480\n",
      "Epoch [5][20]\t Batch [28200][55000]\t Training Loss 0.8461\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [28250][55000]\t Training Loss 0.8457\t Accuracy 0.8480\n",
      "Epoch [5][20]\t Batch [28300][55000]\t Training Loss 0.8456\t Accuracy 0.8480\n",
      "Epoch [5][20]\t Batch [28350][55000]\t Training Loss 0.8454\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [28400][55000]\t Training Loss 0.8459\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [28450][55000]\t Training Loss 0.8456\t Accuracy 0.8481\n",
      "Epoch [5][20]\t Batch [28500][55000]\t Training Loss 0.8454\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [28550][55000]\t Training Loss 0.8453\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [28600][55000]\t Training Loss 0.8452\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [28650][55000]\t Training Loss 0.8455\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [28700][55000]\t Training Loss 0.8457\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [28750][55000]\t Training Loss 0.8458\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [28800][55000]\t Training Loss 0.8458\t Accuracy 0.8481\n",
      "Epoch [5][20]\t Batch [28850][55000]\t Training Loss 0.8457\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [28900][55000]\t Training Loss 0.8456\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [28950][55000]\t Training Loss 0.8456\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [29000][55000]\t Training Loss 0.8455\t Accuracy 0.8481\n",
      "Epoch [5][20]\t Batch [29050][55000]\t Training Loss 0.8455\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [29100][55000]\t Training Loss 0.8456\t Accuracy 0.8481\n",
      "Epoch [5][20]\t Batch [29150][55000]\t Training Loss 0.8460\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [29200][55000]\t Training Loss 0.8464\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [29250][55000]\t Training Loss 0.8466\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [29300][55000]\t Training Loss 0.8464\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [29350][55000]\t Training Loss 0.8466\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [29400][55000]\t Training Loss 0.8465\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [29450][55000]\t Training Loss 0.8461\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [29500][55000]\t Training Loss 0.8459\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [29550][55000]\t Training Loss 0.8458\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [29600][55000]\t Training Loss 0.8458\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [29650][55000]\t Training Loss 0.8457\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [29700][55000]\t Training Loss 0.8458\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [29750][55000]\t Training Loss 0.8462\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [29800][55000]\t Training Loss 0.8463\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [29850][55000]\t Training Loss 0.8466\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [29900][55000]\t Training Loss 0.8470\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [29950][55000]\t Training Loss 0.8472\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [30000][55000]\t Training Loss 0.8476\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [30050][55000]\t Training Loss 0.8477\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [30100][55000]\t Training Loss 0.8481\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [30150][55000]\t Training Loss 0.8485\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [30200][55000]\t Training Loss 0.8488\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [30250][55000]\t Training Loss 0.8489\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [30300][55000]\t Training Loss 0.8487\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [30350][55000]\t Training Loss 0.8486\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [30400][55000]\t Training Loss 0.8485\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [30450][55000]\t Training Loss 0.8485\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [30500][55000]\t Training Loss 0.8487\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [30550][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [30600][55000]\t Training Loss 0.8492\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [30650][55000]\t Training Loss 0.8496\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [30700][55000]\t Training Loss 0.8499\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [30750][55000]\t Training Loss 0.8500\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [30800][55000]\t Training Loss 0.8502\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [30850][55000]\t Training Loss 0.8501\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [30900][55000]\t Training Loss 0.8505\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [30950][55000]\t Training Loss 0.8504\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [31000][55000]\t Training Loss 0.8503\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [31050][55000]\t Training Loss 0.8505\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [31100][55000]\t Training Loss 0.8505\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [31150][55000]\t Training Loss 0.8505\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [31200][55000]\t Training Loss 0.8506\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [31250][55000]\t Training Loss 0.8505\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [31300][55000]\t Training Loss 0.8508\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [31350][55000]\t Training Loss 0.8515\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [31400][55000]\t Training Loss 0.8516\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [31450][55000]\t Training Loss 0.8519\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [31500][55000]\t Training Loss 0.8520\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [31550][55000]\t Training Loss 0.8520\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [31600][55000]\t Training Loss 0.8521\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [31650][55000]\t Training Loss 0.8523\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [31700][55000]\t Training Loss 0.8526\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [31750][55000]\t Training Loss 0.8530\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [31800][55000]\t Training Loss 0.8531\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [31850][55000]\t Training Loss 0.8531\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [31900][55000]\t Training Loss 0.8530\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [31950][55000]\t Training Loss 0.8530\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [32000][55000]\t Training Loss 0.8530\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [32050][55000]\t Training Loss 0.8530\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [32100][55000]\t Training Loss 0.8529\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [32150][55000]\t Training Loss 0.8531\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [32200][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [32250][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [32300][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [32350][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [32400][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [32450][55000]\t Training Loss 0.8545\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [32500][55000]\t Training Loss 0.8548\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [32550][55000]\t Training Loss 0.8551\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [32600][55000]\t Training Loss 0.8550\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [32650][55000]\t Training Loss 0.8549\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [32700][55000]\t Training Loss 0.8548\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [32750][55000]\t Training Loss 0.8549\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [32800][55000]\t Training Loss 0.8550\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [32850][55000]\t Training Loss 0.8550\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [32900][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [32950][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33000][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33050][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33100][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33150][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [33200][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [33250][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33300][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33350][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33400][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [33450][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [33500][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [33550][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [33600][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [33650][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33700][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [33750][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [33800][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [33850][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [33900][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [33950][55000]\t Training Loss 0.8538\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [34000][55000]\t Training Loss 0.8538\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [34050][55000]\t Training Loss 0.8540\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [34100][55000]\t Training Loss 0.8540\t Accuracy 0.8466\n",
      "Epoch [5][20]\t Batch [34150][55000]\t Training Loss 0.8541\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [34200][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [34250][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [34300][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [34350][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [34400][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [34450][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [34500][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [34550][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [34600][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [34650][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [34700][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [34750][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [34800][55000]\t Training Loss 0.8537\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [34850][55000]\t Training Loss 0.8542\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [34900][55000]\t Training Loss 0.8542\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [34950][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [35000][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [35050][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [35100][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [35150][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [35200][55000]\t Training Loss 0.8543\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [35250][55000]\t Training Loss 0.8544\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [35300][55000]\t Training Loss 0.8544\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [35350][55000]\t Training Loss 0.8542\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [35400][55000]\t Training Loss 0.8542\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [35450][55000]\t Training Loss 0.8541\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [35500][55000]\t Training Loss 0.8541\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [35550][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [35600][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [35650][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [35700][55000]\t Training Loss 0.8541\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [35750][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [35800][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [35850][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [35900][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [35950][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [36000][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [36050][55000]\t Training Loss 0.8541\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [36100][55000]\t Training Loss 0.8543\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [36150][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [36200][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [36250][55000]\t Training Loss 0.8539\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [36300][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [36350][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [36400][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [36450][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [36500][55000]\t Training Loss 0.8537\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [36550][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [36600][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [36650][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [36700][55000]\t Training Loss 0.8530\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [36750][55000]\t Training Loss 0.8528\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [36800][55000]\t Training Loss 0.8526\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [36850][55000]\t Training Loss 0.8526\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [36900][55000]\t Training Loss 0.8526\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [36950][55000]\t Training Loss 0.8525\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [37000][55000]\t Training Loss 0.8524\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [37050][55000]\t Training Loss 0.8523\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [37100][55000]\t Training Loss 0.8524\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [37150][55000]\t Training Loss 0.8523\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [37200][55000]\t Training Loss 0.8522\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [37250][55000]\t Training Loss 0.8522\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [37300][55000]\t Training Loss 0.8522\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [37350][55000]\t Training Loss 0.8523\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [37400][55000]\t Training Loss 0.8525\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [37450][55000]\t Training Loss 0.8529\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [37500][55000]\t Training Loss 0.8530\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [37550][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [37600][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [37650][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [37700][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [37750][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [37800][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [37850][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [37900][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [37950][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [38000][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [38050][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [38100][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [38150][55000]\t Training Loss 0.8533\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [38200][55000]\t Training Loss 0.8532\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [38250][55000]\t Training Loss 0.8532\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [38300][55000]\t Training Loss 0.8533\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [38350][55000]\t Training Loss 0.8535\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [38400][55000]\t Training Loss 0.8536\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [38450][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [38500][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [38550][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [38600][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [38650][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [38700][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [38750][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [38800][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [38850][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [38900][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [38950][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [39000][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [39050][55000]\t Training Loss 0.8530\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [39100][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [39150][55000]\t Training Loss 0.8526\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [39200][55000]\t Training Loss 0.8524\t Accuracy 0.8476\n",
      "Epoch [5][20]\t Batch [39250][55000]\t Training Loss 0.8522\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [39300][55000]\t Training Loss 0.8522\t Accuracy 0.8477\n",
      "Epoch [5][20]\t Batch [39350][55000]\t Training Loss 0.8525\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [39400][55000]\t Training Loss 0.8525\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [39450][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [5][20]\t Batch [39500][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [39550][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [39600][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [39650][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [39700][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [39750][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [39800][55000]\t Training Loss 0.8533\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [39850][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [39900][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [39950][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40000][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40050][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [40100][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [40150][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40200][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40250][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40300][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40350][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40400][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40450][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40500][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40550][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40600][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40650][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [40700][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [40750][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40800][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [40850][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [40900][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [40950][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [41000][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [41050][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [41100][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [41150][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [41200][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [41250][55000]\t Training Loss 0.8530\t Accuracy 0.8473\n",
      "Epoch [5][20]\t Batch [41300][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [41350][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [41400][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [5][20]\t Batch [41450][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [41500][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [41550][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [41600][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [41650][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [41700][55000]\t Training Loss 0.8541\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [41750][55000]\t Training Loss 0.8542\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [41800][55000]\t Training Loss 0.8545\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [41850][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [41900][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [41950][55000]\t Training Loss 0.8545\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [42000][55000]\t Training Loss 0.8544\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [42050][55000]\t Training Loss 0.8544\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [42100][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [5][20]\t Batch [42150][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [42200][55000]\t Training Loss 0.8544\t Accuracy 0.8468\n",
      "Epoch [5][20]\t Batch [42250][55000]\t Training Loss 0.8545\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [42300][55000]\t Training Loss 0.8545\t Accuracy 0.8467\n",
      "Epoch [5][20]\t Batch [42350][55000]\t Training Loss 0.8547\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [42400][55000]\t Training Loss 0.8549\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [42450][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [42500][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [42550][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [42600][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [42650][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [42700][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [42750][55000]\t Training Loss 0.8550\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [42800][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [42850][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [42900][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [42950][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [43000][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [43050][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [43100][55000]\t Training Loss 0.8555\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [43150][55000]\t Training Loss 0.8556\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [43200][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [43250][55000]\t Training Loss 0.8555\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [43300][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [43350][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [43400][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [43450][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [43500][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [43550][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [43600][55000]\t Training Loss 0.8546\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [43650][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [43700][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [43750][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [43800][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [43850][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [43900][55000]\t Training Loss 0.8545\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [43950][55000]\t Training Loss 0.8546\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [44000][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [44050][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [44100][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [44150][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [44200][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [44250][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [44300][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [44350][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [44400][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [44450][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [44500][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [44550][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [44600][55000]\t Training Loss 0.8550\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [44650][55000]\t Training Loss 0.8549\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [44700][55000]\t Training Loss 0.8548\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [44750][55000]\t Training Loss 0.8548\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [44800][55000]\t Training Loss 0.8549\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [44850][55000]\t Training Loss 0.8549\t Accuracy 0.8465\n",
      "Epoch [5][20]\t Batch [44900][55000]\t Training Loss 0.8551\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [44950][55000]\t Training Loss 0.8551\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45000][55000]\t Training Loss 0.8551\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45050][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45100][55000]\t Training Loss 0.8553\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45150][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45200][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45250][55000]\t Training Loss 0.8554\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45300][55000]\t Training Loss 0.8553\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45350][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45400][55000]\t Training Loss 0.8551\t Accuracy 0.8464\n",
      "Epoch [5][20]\t Batch [45450][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45500][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45550][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45600][55000]\t Training Loss 0.8554\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45650][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45700][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45750][55000]\t Training Loss 0.8553\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45800][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45850][55000]\t Training Loss 0.8555\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [45900][55000]\t Training Loss 0.8556\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [45950][55000]\t Training Loss 0.8556\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [46000][55000]\t Training Loss 0.8557\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [46050][55000]\t Training Loss 0.8558\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [46100][55000]\t Training Loss 0.8560\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [46150][55000]\t Training Loss 0.8560\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [46200][55000]\t Training Loss 0.8560\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [46250][55000]\t Training Loss 0.8560\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46300][55000]\t Training Loss 0.8561\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46350][55000]\t Training Loss 0.8562\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [46400][55000]\t Training Loss 0.8563\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [46450][55000]\t Training Loss 0.8564\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [46500][55000]\t Training Loss 0.8563\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [46550][55000]\t Training Loss 0.8562\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46600][55000]\t Training Loss 0.8561\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46650][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [46700][55000]\t Training Loss 0.8560\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46750][55000]\t Training Loss 0.8561\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46800][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46850][55000]\t Training Loss 0.8558\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46900][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [46950][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [47000][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47050][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47100][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [47150][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [47200][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47250][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47300][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47350][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47400][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47450][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47500][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [47550][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [47600][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47650][55000]\t Training Loss 0.8558\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [47700][55000]\t Training Loss 0.8558\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [47750][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [47800][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [47850][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47900][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [47950][55000]\t Training Loss 0.8556\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [48000][55000]\t Training Loss 0.8556\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [48050][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [48100][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [48150][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48200][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48250][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [48300][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48350][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [48400][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48450][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48500][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [48550][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48600][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48650][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48700][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [48750][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [48800][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [48850][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [48900][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [5][20]\t Batch [48950][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [49000][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [5][20]\t Batch [49050][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [5][20]\t Batch [49100][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [5][20]\t Batch [49150][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [5][20]\t Batch [49200][55000]\t Training Loss 0.8546\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [49250][55000]\t Training Loss 0.8548\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [49300][55000]\t Training Loss 0.8546\t Accuracy 0.8456\n",
      "Epoch [5][20]\t Batch [49350][55000]\t Training Loss 0.8545\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [49400][55000]\t Training Loss 0.8543\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [49450][55000]\t Training Loss 0.8543\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [49500][55000]\t Training Loss 0.8545\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [49550][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [49600][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [49650][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [49700][55000]\t Training Loss 0.8554\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [49750][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [49800][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [49850][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [49900][55000]\t Training Loss 0.8556\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [49950][55000]\t Training Loss 0.8557\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [50000][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [50050][55000]\t Training Loss 0.8558\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [50100][55000]\t Training Loss 0.8558\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [50150][55000]\t Training Loss 0.8558\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [50200][55000]\t Training Loss 0.8557\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [50250][55000]\t Training Loss 0.8559\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [50300][55000]\t Training Loss 0.8558\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [50350][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [50400][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [50450][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [50500][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [50550][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [50600][55000]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [50650][55000]\t Training Loss 0.8566\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [50700][55000]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [50750][55000]\t Training Loss 0.8566\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [50800][55000]\t Training Loss 0.8566\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [50850][55000]\t Training Loss 0.8566\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [50900][55000]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [50950][55000]\t Training Loss 0.8564\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [51000][55000]\t Training Loss 0.8562\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [51050][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [51100][55000]\t Training Loss 0.8560\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [51150][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [51200][55000]\t Training Loss 0.8561\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [51250][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [51300][55000]\t Training Loss 0.8562\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [51350][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [51400][55000]\t Training Loss 0.8561\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [51450][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [51500][55000]\t Training Loss 0.8559\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [51550][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [51600][55000]\t Training Loss 0.8556\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [51650][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [51700][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [51750][55000]\t Training Loss 0.8556\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [51800][55000]\t Training Loss 0.8556\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [51850][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [51900][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [51950][55000]\t Training Loss 0.8554\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [52000][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [52050][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [52100][55000]\t Training Loss 0.8557\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [52150][55000]\t Training Loss 0.8560\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [52200][55000]\t Training Loss 0.8561\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [52250][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [52300][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [52350][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [52400][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [52450][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [52500][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [52550][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [5][20]\t Batch [52600][55000]\t Training Loss 0.8556\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [52650][55000]\t Training Loss 0.8554\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [52700][55000]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [52750][55000]\t Training Loss 0.8557\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [52800][55000]\t Training Loss 0.8558\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [52850][55000]\t Training Loss 0.8558\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [52900][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [52950][55000]\t Training Loss 0.8562\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [53000][55000]\t Training Loss 0.8563\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [53050][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53100][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53150][55000]\t Training Loss 0.8562\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [53200][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53250][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53300][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53350][55000]\t Training Loss 0.8563\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53400][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [53450][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [53500][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [53550][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53600][55000]\t Training Loss 0.8562\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [53650][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53700][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53750][55000]\t Training Loss 0.8561\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53800][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [53850][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53900][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [53950][55000]\t Training Loss 0.8560\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [54000][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [54050][55000]\t Training Loss 0.8562\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [54100][55000]\t Training Loss 0.8564\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [54150][55000]\t Training Loss 0.8563\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [54200][55000]\t Training Loss 0.8564\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [54250][55000]\t Training Loss 0.8563\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [54300][55000]\t Training Loss 0.8562\t Accuracy 0.8447\n",
      "Epoch [5][20]\t Batch [54350][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [54400][55000]\t Training Loss 0.8561\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [54450][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [54500][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [54550][55000]\t Training Loss 0.8559\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [54600][55000]\t Training Loss 0.8559\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [54650][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [54700][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [54750][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [54800][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [5][20]\t Batch [54850][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [54900][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [54950][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8559\t Average training accuracy 0.8447\n",
      "Epoch [5]\t Average validation loss 0.7935\t Average validation accuracy 0.8696\n",
      "\n",
      "Epoch [6][20]\t Batch [0][55000]\t Training Loss 1.4714\t Accuracy 0.0000\n",
      "Epoch [6][20]\t Batch [50][55000]\t Training Loss 0.9410\t Accuracy 0.7255\n",
      "Epoch [6][20]\t Batch [100][55000]\t Training Loss 0.8508\t Accuracy 0.8020\n",
      "Epoch [6][20]\t Batch [150][55000]\t Training Loss 0.8518\t Accuracy 0.8146\n",
      "Epoch [6][20]\t Batch [200][55000]\t Training Loss 0.8623\t Accuracy 0.8159\n",
      "Epoch [6][20]\t Batch [250][55000]\t Training Loss 0.8446\t Accuracy 0.8247\n",
      "Epoch [6][20]\t Batch [300][55000]\t Training Loss 0.8460\t Accuracy 0.8239\n",
      "Epoch [6][20]\t Batch [350][55000]\t Training Loss 0.8264\t Accuracy 0.8262\n",
      "Epoch [6][20]\t Batch [400][55000]\t Training Loss 0.8126\t Accuracy 0.8304\n",
      "Epoch [6][20]\t Batch [450][55000]\t Training Loss 0.8183\t Accuracy 0.8315\n",
      "Epoch [6][20]\t Batch [500][55000]\t Training Loss 0.8246\t Accuracy 0.8343\n",
      "Epoch [6][20]\t Batch [550][55000]\t Training Loss 0.8418\t Accuracy 0.8312\n",
      "Epoch [6][20]\t Batch [600][55000]\t Training Loss 0.8462\t Accuracy 0.8353\n",
      "Epoch [6][20]\t Batch [650][55000]\t Training Loss 0.8661\t Accuracy 0.8249\n",
      "Epoch [6][20]\t Batch [700][55000]\t Training Loss 0.8637\t Accuracy 0.8302\n",
      "Epoch [6][20]\t Batch [750][55000]\t Training Loss 0.8600\t Accuracy 0.8309\n",
      "Epoch [6][20]\t Batch [800][55000]\t Training Loss 0.8540\t Accuracy 0.8327\n",
      "Epoch [6][20]\t Batch [850][55000]\t Training Loss 0.8524\t Accuracy 0.8343\n",
      "Epoch [6][20]\t Batch [900][55000]\t Training Loss 0.8589\t Accuracy 0.8335\n",
      "Epoch [6][20]\t Batch [950][55000]\t Training Loss 0.8657\t Accuracy 0.8286\n",
      "Epoch [6][20]\t Batch [1000][55000]\t Training Loss 0.8651\t Accuracy 0.8312\n",
      "Epoch [6][20]\t Batch [1050][55000]\t Training Loss 0.8728\t Accuracy 0.8297\n",
      "Epoch [6][20]\t Batch [1100][55000]\t Training Loss 0.8812\t Accuracy 0.8292\n",
      "Epoch [6][20]\t Batch [1150][55000]\t Training Loss 0.8905\t Accuracy 0.8262\n",
      "Epoch [6][20]\t Batch [1200][55000]\t Training Loss 0.8847\t Accuracy 0.8293\n",
      "Epoch [6][20]\t Batch [1250][55000]\t Training Loss 0.8860\t Accuracy 0.8289\n",
      "Epoch [6][20]\t Batch [1300][55000]\t Training Loss 0.8863\t Accuracy 0.8286\n",
      "Epoch [6][20]\t Batch [1350][55000]\t Training Loss 0.8831\t Accuracy 0.8298\n",
      "Epoch [6][20]\t Batch [1400][55000]\t Training Loss 0.8837\t Accuracy 0.8287\n",
      "Epoch [6][20]\t Batch [1450][55000]\t Training Loss 0.8846\t Accuracy 0.8277\n",
      "Epoch [6][20]\t Batch [1500][55000]\t Training Loss 0.8827\t Accuracy 0.8308\n",
      "Epoch [6][20]\t Batch [1550][55000]\t Training Loss 0.8823\t Accuracy 0.8311\n",
      "Epoch [6][20]\t Batch [1600][55000]\t Training Loss 0.8849\t Accuracy 0.8301\n",
      "Epoch [6][20]\t Batch [1650][55000]\t Training Loss 0.8818\t Accuracy 0.8316\n",
      "Epoch [6][20]\t Batch [1700][55000]\t Training Loss 0.8801\t Accuracy 0.8325\n",
      "Epoch [6][20]\t Batch [1750][55000]\t Training Loss 0.8745\t Accuracy 0.8332\n",
      "Epoch [6][20]\t Batch [1800][55000]\t Training Loss 0.8723\t Accuracy 0.8351\n",
      "Epoch [6][20]\t Batch [1850][55000]\t Training Loss 0.8707\t Accuracy 0.8358\n",
      "Epoch [6][20]\t Batch [1900][55000]\t Training Loss 0.8685\t Accuracy 0.8359\n",
      "Epoch [6][20]\t Batch [1950][55000]\t Training Loss 0.8670\t Accuracy 0.8370\n",
      "Epoch [6][20]\t Batch [2000][55000]\t Training Loss 0.8646\t Accuracy 0.8381\n",
      "Epoch [6][20]\t Batch [2050][55000]\t Training Loss 0.8637\t Accuracy 0.8386\n",
      "Epoch [6][20]\t Batch [2100][55000]\t Training Loss 0.8608\t Accuracy 0.8391\n",
      "Epoch [6][20]\t Batch [2150][55000]\t Training Loss 0.8575\t Accuracy 0.8410\n",
      "Epoch [6][20]\t Batch [2200][55000]\t Training Loss 0.8527\t Accuracy 0.8428\n",
      "Epoch [6][20]\t Batch [2250][55000]\t Training Loss 0.8519\t Accuracy 0.8432\n",
      "Epoch [6][20]\t Batch [2300][55000]\t Training Loss 0.8490\t Accuracy 0.8427\n",
      "Epoch [6][20]\t Batch [2350][55000]\t Training Loss 0.8474\t Accuracy 0.8430\n",
      "Epoch [6][20]\t Batch [2400][55000]\t Training Loss 0.8484\t Accuracy 0.8413\n",
      "Epoch [6][20]\t Batch [2450][55000]\t Training Loss 0.8513\t Accuracy 0.8401\n",
      "Epoch [6][20]\t Batch [2500][55000]\t Training Loss 0.8491\t Accuracy 0.8421\n",
      "Epoch [6][20]\t Batch [2550][55000]\t Training Loss 0.8478\t Accuracy 0.8420\n",
      "Epoch [6][20]\t Batch [2600][55000]\t Training Loss 0.8472\t Accuracy 0.8420\n",
      "Epoch [6][20]\t Batch [2650][55000]\t Training Loss 0.8460\t Accuracy 0.8423\n",
      "Epoch [6][20]\t Batch [2700][55000]\t Training Loss 0.8458\t Accuracy 0.8427\n",
      "Epoch [6][20]\t Batch [2750][55000]\t Training Loss 0.8457\t Accuracy 0.8426\n",
      "Epoch [6][20]\t Batch [2800][55000]\t Training Loss 0.8459\t Accuracy 0.8408\n",
      "Epoch [6][20]\t Batch [2850][55000]\t Training Loss 0.8448\t Accuracy 0.8404\n",
      "Epoch [6][20]\t Batch [2900][55000]\t Training Loss 0.8416\t Accuracy 0.8414\n",
      "Epoch [6][20]\t Batch [2950][55000]\t Training Loss 0.8416\t Accuracy 0.8411\n",
      "Epoch [6][20]\t Batch [3000][55000]\t Training Loss 0.8415\t Accuracy 0.8417\n",
      "Epoch [6][20]\t Batch [3050][55000]\t Training Loss 0.8426\t Accuracy 0.8414\n",
      "Epoch [6][20]\t Batch [3100][55000]\t Training Loss 0.8449\t Accuracy 0.8410\n",
      "Epoch [6][20]\t Batch [3150][55000]\t Training Loss 0.8447\t Accuracy 0.8413\n",
      "Epoch [6][20]\t Batch [3200][55000]\t Training Loss 0.8445\t Accuracy 0.8425\n",
      "Epoch [6][20]\t Batch [3250][55000]\t Training Loss 0.8439\t Accuracy 0.8419\n",
      "Epoch [6][20]\t Batch [3300][55000]\t Training Loss 0.8452\t Accuracy 0.8419\n",
      "Epoch [6][20]\t Batch [3350][55000]\t Training Loss 0.8434\t Accuracy 0.8433\n",
      "Epoch [6][20]\t Batch [3400][55000]\t Training Loss 0.8457\t Accuracy 0.8418\n",
      "Epoch [6][20]\t Batch [3450][55000]\t Training Loss 0.8454\t Accuracy 0.8427\n",
      "Epoch [6][20]\t Batch [3500][55000]\t Training Loss 0.8454\t Accuracy 0.8423\n",
      "Epoch [6][20]\t Batch [3550][55000]\t Training Loss 0.8470\t Accuracy 0.8417\n",
      "Epoch [6][20]\t Batch [3600][55000]\t Training Loss 0.8468\t Accuracy 0.8414\n",
      "Epoch [6][20]\t Batch [3650][55000]\t Training Loss 0.8454\t Accuracy 0.8422\n",
      "Epoch [6][20]\t Batch [3700][55000]\t Training Loss 0.8462\t Accuracy 0.8417\n",
      "Epoch [6][20]\t Batch [3750][55000]\t Training Loss 0.8461\t Accuracy 0.8419\n",
      "Epoch [6][20]\t Batch [3800][55000]\t Training Loss 0.8464\t Accuracy 0.8419\n",
      "Epoch [6][20]\t Batch [3850][55000]\t Training Loss 0.8460\t Accuracy 0.8424\n",
      "Epoch [6][20]\t Batch [3900][55000]\t Training Loss 0.8448\t Accuracy 0.8436\n",
      "Epoch [6][20]\t Batch [3950][55000]\t Training Loss 0.8437\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [4000][55000]\t Training Loss 0.8434\t Accuracy 0.8455\n",
      "Epoch [6][20]\t Batch [4050][55000]\t Training Loss 0.8418\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [4100][55000]\t Training Loss 0.8423\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [4150][55000]\t Training Loss 0.8427\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [4200][55000]\t Training Loss 0.8432\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [4250][55000]\t Training Loss 0.8418\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [4300][55000]\t Training Loss 0.8418\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [4350][55000]\t Training Loss 0.8423\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [4400][55000]\t Training Loss 0.8422\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [4450][55000]\t Training Loss 0.8424\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [4500][55000]\t Training Loss 0.8427\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [4550][55000]\t Training Loss 0.8410\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [4600][55000]\t Training Loss 0.8393\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [4650][55000]\t Training Loss 0.8391\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [4700][55000]\t Training Loss 0.8388\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [4750][55000]\t Training Loss 0.8376\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [4800][55000]\t Training Loss 0.8383\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [4850][55000]\t Training Loss 0.8394\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [4900][55000]\t Training Loss 0.8380\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [4950][55000]\t Training Loss 0.8382\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [5000][55000]\t Training Loss 0.8384\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [5050][55000]\t Training Loss 0.8379\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [5100][55000]\t Training Loss 0.8379\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [5150][55000]\t Training Loss 0.8382\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [5200][55000]\t Training Loss 0.8397\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [5250][55000]\t Training Loss 0.8388\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [5300][55000]\t Training Loss 0.8387\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [5350][55000]\t Training Loss 0.8393\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [5400][55000]\t Training Loss 0.8385\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [5450][55000]\t Training Loss 0.8378\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [5500][55000]\t Training Loss 0.8360\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [5550][55000]\t Training Loss 0.8360\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [5600][55000]\t Training Loss 0.8351\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [5650][55000]\t Training Loss 0.8356\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [5700][55000]\t Training Loss 0.8353\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [5750][55000]\t Training Loss 0.8358\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [5800][55000]\t Training Loss 0.8361\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [5850][55000]\t Training Loss 0.8363\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [5900][55000]\t Training Loss 0.8369\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [5950][55000]\t Training Loss 0.8367\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [6000][55000]\t Training Loss 0.8357\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [6050][55000]\t Training Loss 0.8343\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [6100][55000]\t Training Loss 0.8328\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [6150][55000]\t Training Loss 0.8308\t Accuracy 0.8485\n",
      "Epoch [6][20]\t Batch [6200][55000]\t Training Loss 0.8309\t Accuracy 0.8486\n",
      "Epoch [6][20]\t Batch [6250][55000]\t Training Loss 0.8297\t Accuracy 0.8490\n",
      "Epoch [6][20]\t Batch [6300][55000]\t Training Loss 0.8301\t Accuracy 0.8488\n",
      "Epoch [6][20]\t Batch [6350][55000]\t Training Loss 0.8297\t Accuracy 0.8492\n",
      "Epoch [6][20]\t Batch [6400][55000]\t Training Loss 0.8291\t Accuracy 0.8496\n",
      "Epoch [6][20]\t Batch [6450][55000]\t Training Loss 0.8286\t Accuracy 0.8498\n",
      "Epoch [6][20]\t Batch [6500][55000]\t Training Loss 0.8294\t Accuracy 0.8494\n",
      "Epoch [6][20]\t Batch [6550][55000]\t Training Loss 0.8284\t Accuracy 0.8496\n",
      "Epoch [6][20]\t Batch [6600][55000]\t Training Loss 0.8271\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [6650][55000]\t Training Loss 0.8259\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [6700][55000]\t Training Loss 0.8260\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [6750][55000]\t Training Loss 0.8263\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [6800][55000]\t Training Loss 0.8267\t Accuracy 0.8509\n",
      "Epoch [6][20]\t Batch [6850][55000]\t Training Loss 0.8289\t Accuracy 0.8505\n",
      "Epoch [6][20]\t Batch [6900][55000]\t Training Loss 0.8288\t Accuracy 0.8505\n",
      "Epoch [6][20]\t Batch [6950][55000]\t Training Loss 0.8295\t Accuracy 0.8494\n",
      "Epoch [6][20]\t Batch [7000][55000]\t Training Loss 0.8293\t Accuracy 0.8495\n",
      "Epoch [6][20]\t Batch [7050][55000]\t Training Loss 0.8299\t Accuracy 0.8490\n",
      "Epoch [6][20]\t Batch [7100][55000]\t Training Loss 0.8301\t Accuracy 0.8489\n",
      "Epoch [6][20]\t Batch [7150][55000]\t Training Loss 0.8303\t Accuracy 0.8494\n",
      "Epoch [6][20]\t Batch [7200][55000]\t Training Loss 0.8311\t Accuracy 0.8493\n",
      "Epoch [6][20]\t Batch [7250][55000]\t Training Loss 0.8335\t Accuracy 0.8487\n",
      "Epoch [6][20]\t Batch [7300][55000]\t Training Loss 0.8353\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [7350][55000]\t Training Loss 0.8368\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [7400][55000]\t Training Loss 0.8378\t Accuracy 0.8484\n",
      "Epoch [6][20]\t Batch [7450][55000]\t Training Loss 0.8378\t Accuracy 0.8486\n",
      "Epoch [6][20]\t Batch [7500][55000]\t Training Loss 0.8378\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [7550][55000]\t Training Loss 0.8383\t Accuracy 0.8481\n",
      "Epoch [6][20]\t Batch [7600][55000]\t Training Loss 0.8378\t Accuracy 0.8487\n",
      "Epoch [6][20]\t Batch [7650][55000]\t Training Loss 0.8383\t Accuracy 0.8488\n",
      "Epoch [6][20]\t Batch [7700][55000]\t Training Loss 0.8388\t Accuracy 0.8487\n",
      "Epoch [6][20]\t Batch [7750][55000]\t Training Loss 0.8389\t Accuracy 0.8487\n",
      "Epoch [6][20]\t Batch [7800][55000]\t Training Loss 0.8396\t Accuracy 0.8485\n",
      "Epoch [6][20]\t Batch [7850][55000]\t Training Loss 0.8398\t Accuracy 0.8487\n",
      "Epoch [6][20]\t Batch [7900][55000]\t Training Loss 0.8400\t Accuracy 0.8485\n",
      "Epoch [6][20]\t Batch [7950][55000]\t Training Loss 0.8400\t Accuracy 0.8481\n",
      "Epoch [6][20]\t Batch [8000][55000]\t Training Loss 0.8400\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [8050][55000]\t Training Loss 0.8406\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [8100][55000]\t Training Loss 0.8391\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [8150][55000]\t Training Loss 0.8399\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [8200][55000]\t Training Loss 0.8399\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [8250][55000]\t Training Loss 0.8412\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [8300][55000]\t Training Loss 0.8415\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [8350][55000]\t Training Loss 0.8421\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [8400][55000]\t Training Loss 0.8414\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [8450][55000]\t Training Loss 0.8430\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [8500][55000]\t Training Loss 0.8422\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [8550][55000]\t Training Loss 0.8412\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [8600][55000]\t Training Loss 0.8404\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [8650][55000]\t Training Loss 0.8405\t Accuracy 0.8481\n",
      "Epoch [6][20]\t Batch [8700][55000]\t Training Loss 0.8411\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [8750][55000]\t Training Loss 0.8426\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [8800][55000]\t Training Loss 0.8434\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [8850][55000]\t Training Loss 0.8434\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [8900][55000]\t Training Loss 0.8451\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [8950][55000]\t Training Loss 0.8447\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [9000][55000]\t Training Loss 0.8440\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [9050][55000]\t Training Loss 0.8431\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [9100][55000]\t Training Loss 0.8429\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [9150][55000]\t Training Loss 0.8434\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [9200][55000]\t Training Loss 0.8430\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [9250][55000]\t Training Loss 0.8433\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [9300][55000]\t Training Loss 0.8435\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [9350][55000]\t Training Loss 0.8437\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [9400][55000]\t Training Loss 0.8440\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [9450][55000]\t Training Loss 0.8442\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [9500][55000]\t Training Loss 0.8433\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [9550][55000]\t Training Loss 0.8431\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [9600][55000]\t Training Loss 0.8435\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [9650][55000]\t Training Loss 0.8434\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [9700][55000]\t Training Loss 0.8428\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [9750][55000]\t Training Loss 0.8420\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [9800][55000]\t Training Loss 0.8425\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [9850][55000]\t Training Loss 0.8421\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [9900][55000]\t Training Loss 0.8416\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [9950][55000]\t Training Loss 0.8410\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [10000][55000]\t Training Loss 0.8407\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [10050][55000]\t Training Loss 0.8409\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [10100][55000]\t Training Loss 0.8407\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [10150][55000]\t Training Loss 0.8405\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [10200][55000]\t Training Loss 0.8407\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [10250][55000]\t Training Loss 0.8408\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [10300][55000]\t Training Loss 0.8407\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [10350][55000]\t Training Loss 0.8398\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [10400][55000]\t Training Loss 0.8390\t Accuracy 0.8481\n",
      "Epoch [6][20]\t Batch [10450][55000]\t Training Loss 0.8388\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [10500][55000]\t Training Loss 0.8378\t Accuracy 0.8485\n",
      "Epoch [6][20]\t Batch [10550][55000]\t Training Loss 0.8374\t Accuracy 0.8488\n",
      "Epoch [6][20]\t Batch [10600][55000]\t Training Loss 0.8369\t Accuracy 0.8491\n",
      "Epoch [6][20]\t Batch [10650][55000]\t Training Loss 0.8367\t Accuracy 0.8494\n",
      "Epoch [6][20]\t Batch [10700][55000]\t Training Loss 0.8364\t Accuracy 0.8497\n",
      "Epoch [6][20]\t Batch [10750][55000]\t Training Loss 0.8369\t Accuracy 0.8494\n",
      "Epoch [6][20]\t Batch [10800][55000]\t Training Loss 0.8374\t Accuracy 0.8492\n",
      "Epoch [6][20]\t Batch [10850][55000]\t Training Loss 0.8367\t Accuracy 0.8494\n",
      "Epoch [6][20]\t Batch [10900][55000]\t Training Loss 0.8363\t Accuracy 0.8496\n",
      "Epoch [6][20]\t Batch [10950][55000]\t Training Loss 0.8359\t Accuracy 0.8494\n",
      "Epoch [6][20]\t Batch [11000][55000]\t Training Loss 0.8358\t Accuracy 0.8497\n",
      "Epoch [6][20]\t Batch [11050][55000]\t Training Loss 0.8351\t Accuracy 0.8499\n",
      "Epoch [6][20]\t Batch [11100][55000]\t Training Loss 0.8344\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [11150][55000]\t Training Loss 0.8344\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [11200][55000]\t Training Loss 0.8340\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [11250][55000]\t Training Loss 0.8344\t Accuracy 0.8501\n",
      "Epoch [6][20]\t Batch [11300][55000]\t Training Loss 0.8340\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [11350][55000]\t Training Loss 0.8335\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [11400][55000]\t Training Loss 0.8336\t Accuracy 0.8501\n",
      "Epoch [6][20]\t Batch [11450][55000]\t Training Loss 0.8333\t Accuracy 0.8501\n",
      "Epoch [6][20]\t Batch [11500][55000]\t Training Loss 0.8330\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [11550][55000]\t Training Loss 0.8331\t Accuracy 0.8499\n",
      "Epoch [6][20]\t Batch [11600][55000]\t Training Loss 0.8343\t Accuracy 0.8492\n",
      "Epoch [6][20]\t Batch [11650][55000]\t Training Loss 0.8348\t Accuracy 0.8489\n",
      "Epoch [6][20]\t Batch [11700][55000]\t Training Loss 0.8348\t Accuracy 0.8491\n",
      "Epoch [6][20]\t Batch [11750][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [6][20]\t Batch [11800][55000]\t Training Loss 0.8359\t Accuracy 0.8485\n",
      "Epoch [6][20]\t Batch [11850][55000]\t Training Loss 0.8360\t Accuracy 0.8486\n",
      "Epoch [6][20]\t Batch [11900][55000]\t Training Loss 0.8363\t Accuracy 0.8486\n",
      "Epoch [6][20]\t Batch [11950][55000]\t Training Loss 0.8364\t Accuracy 0.8488\n",
      "Epoch [6][20]\t Batch [12000][55000]\t Training Loss 0.8364\t Accuracy 0.8491\n",
      "Epoch [6][20]\t Batch [12050][55000]\t Training Loss 0.8362\t Accuracy 0.8494\n",
      "Epoch [6][20]\t Batch [12100][55000]\t Training Loss 0.8361\t Accuracy 0.8493\n",
      "Epoch [6][20]\t Batch [12150][55000]\t Training Loss 0.8355\t Accuracy 0.8497\n",
      "Epoch [6][20]\t Batch [12200][55000]\t Training Loss 0.8358\t Accuracy 0.8497\n",
      "Epoch [6][20]\t Batch [12250][55000]\t Training Loss 0.8357\t Accuracy 0.8497\n",
      "Epoch [6][20]\t Batch [12300][55000]\t Training Loss 0.8357\t Accuracy 0.8497\n",
      "Epoch [6][20]\t Batch [12350][55000]\t Training Loss 0.8359\t Accuracy 0.8498\n",
      "Epoch [6][20]\t Batch [12400][55000]\t Training Loss 0.8362\t Accuracy 0.8499\n",
      "Epoch [6][20]\t Batch [12450][55000]\t Training Loss 0.8364\t Accuracy 0.8495\n",
      "Epoch [6][20]\t Batch [12500][55000]\t Training Loss 0.8366\t Accuracy 0.8492\n",
      "Epoch [6][20]\t Batch [12550][55000]\t Training Loss 0.8366\t Accuracy 0.8493\n",
      "Epoch [6][20]\t Batch [12600][55000]\t Training Loss 0.8374\t Accuracy 0.8491\n",
      "Epoch [6][20]\t Batch [12650][55000]\t Training Loss 0.8379\t Accuracy 0.8487\n",
      "Epoch [6][20]\t Batch [12700][55000]\t Training Loss 0.8386\t Accuracy 0.8484\n",
      "Epoch [6][20]\t Batch [12750][55000]\t Training Loss 0.8381\t Accuracy 0.8486\n",
      "Epoch [6][20]\t Batch [12800][55000]\t Training Loss 0.8387\t Accuracy 0.8485\n",
      "Epoch [6][20]\t Batch [12850][55000]\t Training Loss 0.8389\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [12900][55000]\t Training Loss 0.8389\t Accuracy 0.8485\n",
      "Epoch [6][20]\t Batch [12950][55000]\t Training Loss 0.8394\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [13000][55000]\t Training Loss 0.8394\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [13050][55000]\t Training Loss 0.8400\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [13100][55000]\t Training Loss 0.8407\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [13150][55000]\t Training Loss 0.8410\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [13200][55000]\t Training Loss 0.8412\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [13250][55000]\t Training Loss 0.8407\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [13300][55000]\t Training Loss 0.8406\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [13350][55000]\t Training Loss 0.8410\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [13400][55000]\t Training Loss 0.8413\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [13450][55000]\t Training Loss 0.8408\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [13500][55000]\t Training Loss 0.8404\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [13550][55000]\t Training Loss 0.8400\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [13600][55000]\t Training Loss 0.8392\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [13650][55000]\t Training Loss 0.8390\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [13700][55000]\t Training Loss 0.8398\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [13750][55000]\t Training Loss 0.8405\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [13800][55000]\t Training Loss 0.8406\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [13850][55000]\t Training Loss 0.8405\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [13900][55000]\t Training Loss 0.8407\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [13950][55000]\t Training Loss 0.8411\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [14000][55000]\t Training Loss 0.8418\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [14050][55000]\t Training Loss 0.8420\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [14100][55000]\t Training Loss 0.8421\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [14150][55000]\t Training Loss 0.8423\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [14200][55000]\t Training Loss 0.8422\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [14250][55000]\t Training Loss 0.8425\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [14300][55000]\t Training Loss 0.8428\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [14350][55000]\t Training Loss 0.8432\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [14400][55000]\t Training Loss 0.8440\t Accuracy 0.8455\n",
      "Epoch [6][20]\t Batch [14450][55000]\t Training Loss 0.8442\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [14500][55000]\t Training Loss 0.8443\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [14550][55000]\t Training Loss 0.8450\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [14600][55000]\t Training Loss 0.8451\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [14650][55000]\t Training Loss 0.8460\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [14700][55000]\t Training Loss 0.8468\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [14750][55000]\t Training Loss 0.8474\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [14800][55000]\t Training Loss 0.8484\t Accuracy 0.8444\n",
      "Epoch [6][20]\t Batch [14850][55000]\t Training Loss 0.8489\t Accuracy 0.8441\n",
      "Epoch [6][20]\t Batch [14900][55000]\t Training Loss 0.8489\t Accuracy 0.8440\n",
      "Epoch [6][20]\t Batch [14950][55000]\t Training Loss 0.8490\t Accuracy 0.8440\n",
      "Epoch [6][20]\t Batch [15000][55000]\t Training Loss 0.8489\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [15050][55000]\t Training Loss 0.8485\t Accuracy 0.8443\n",
      "Epoch [6][20]\t Batch [15100][55000]\t Training Loss 0.8483\t Accuracy 0.8444\n",
      "Epoch [6][20]\t Batch [15150][55000]\t Training Loss 0.8487\t Accuracy 0.8443\n",
      "Epoch [6][20]\t Batch [15200][55000]\t Training Loss 0.8489\t Accuracy 0.8443\n",
      "Epoch [6][20]\t Batch [15250][55000]\t Training Loss 0.8488\t Accuracy 0.8444\n",
      "Epoch [6][20]\t Batch [15300][55000]\t Training Loss 0.8488\t Accuracy 0.8444\n",
      "Epoch [6][20]\t Batch [15350][55000]\t Training Loss 0.8486\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [15400][55000]\t Training Loss 0.8489\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [15450][55000]\t Training Loss 0.8490\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [15500][55000]\t Training Loss 0.8488\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [15550][55000]\t Training Loss 0.8487\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [15600][55000]\t Training Loss 0.8485\t Accuracy 0.8455\n",
      "Epoch [6][20]\t Batch [15650][55000]\t Training Loss 0.8484\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [15700][55000]\t Training Loss 0.8483\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [15750][55000]\t Training Loss 0.8492\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [15800][55000]\t Training Loss 0.8496\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [15850][55000]\t Training Loss 0.8500\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [15900][55000]\t Training Loss 0.8505\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [15950][55000]\t Training Loss 0.8506\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [16000][55000]\t Training Loss 0.8506\t Accuracy 0.8445\n",
      "Epoch [6][20]\t Batch [16050][55000]\t Training Loss 0.8516\t Accuracy 0.8441\n",
      "Epoch [6][20]\t Batch [16100][55000]\t Training Loss 0.8515\t Accuracy 0.8441\n",
      "Epoch [6][20]\t Batch [16150][55000]\t Training Loss 0.8515\t Accuracy 0.8442\n",
      "Epoch [6][20]\t Batch [16200][55000]\t Training Loss 0.8516\t Accuracy 0.8440\n",
      "Epoch [6][20]\t Batch [16250][55000]\t Training Loss 0.8514\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [16300][55000]\t Training Loss 0.8511\t Accuracy 0.8440\n",
      "Epoch [6][20]\t Batch [16350][55000]\t Training Loss 0.8508\t Accuracy 0.8443\n",
      "Epoch [6][20]\t Batch [16400][55000]\t Training Loss 0.8508\t Accuracy 0.8443\n",
      "Epoch [6][20]\t Batch [16450][55000]\t Training Loss 0.8505\t Accuracy 0.8445\n",
      "Epoch [6][20]\t Batch [16500][55000]\t Training Loss 0.8504\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [16550][55000]\t Training Loss 0.8499\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [16600][55000]\t Training Loss 0.8500\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [16650][55000]\t Training Loss 0.8500\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [16700][55000]\t Training Loss 0.8502\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [16750][55000]\t Training Loss 0.8500\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [16800][55000]\t Training Loss 0.8506\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [16850][55000]\t Training Loss 0.8512\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [16900][55000]\t Training Loss 0.8514\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [16950][55000]\t Training Loss 0.8514\t Accuracy 0.8444\n",
      "Epoch [6][20]\t Batch [17000][55000]\t Training Loss 0.8522\t Accuracy 0.8441\n",
      "Epoch [6][20]\t Batch [17050][55000]\t Training Loss 0.8521\t Accuracy 0.8441\n",
      "Epoch [6][20]\t Batch [17100][55000]\t Training Loss 0.8525\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [17150][55000]\t Training Loss 0.8523\t Accuracy 0.8440\n",
      "Epoch [6][20]\t Batch [17200][55000]\t Training Loss 0.8524\t Accuracy 0.8440\n",
      "Epoch [6][20]\t Batch [17250][55000]\t Training Loss 0.8529\t Accuracy 0.8438\n",
      "Epoch [6][20]\t Batch [17300][55000]\t Training Loss 0.8527\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [17350][55000]\t Training Loss 0.8521\t Accuracy 0.8442\n",
      "Epoch [6][20]\t Batch [17400][55000]\t Training Loss 0.8523\t Accuracy 0.8441\n",
      "Epoch [6][20]\t Batch [17450][55000]\t Training Loss 0.8524\t Accuracy 0.8441\n",
      "Epoch [6][20]\t Batch [17500][55000]\t Training Loss 0.8524\t Accuracy 0.8441\n",
      "Epoch [6][20]\t Batch [17550][55000]\t Training Loss 0.8531\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [17600][55000]\t Training Loss 0.8537\t Accuracy 0.8436\n",
      "Epoch [6][20]\t Batch [17650][55000]\t Training Loss 0.8539\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [17700][55000]\t Training Loss 0.8546\t Accuracy 0.8436\n",
      "Epoch [6][20]\t Batch [17750][55000]\t Training Loss 0.8550\t Accuracy 0.8436\n",
      "Epoch [6][20]\t Batch [17800][55000]\t Training Loss 0.8554\t Accuracy 0.8434\n",
      "Epoch [6][20]\t Batch [17850][55000]\t Training Loss 0.8557\t Accuracy 0.8431\n",
      "Epoch [6][20]\t Batch [17900][55000]\t Training Loss 0.8560\t Accuracy 0.8430\n",
      "Epoch [6][20]\t Batch [17950][55000]\t Training Loss 0.8558\t Accuracy 0.8428\n",
      "Epoch [6][20]\t Batch [18000][55000]\t Training Loss 0.8555\t Accuracy 0.8431\n",
      "Epoch [6][20]\t Batch [18050][55000]\t Training Loss 0.8557\t Accuracy 0.8431\n",
      "Epoch [6][20]\t Batch [18100][55000]\t Training Loss 0.8556\t Accuracy 0.8432\n",
      "Epoch [6][20]\t Batch [18150][55000]\t Training Loss 0.8552\t Accuracy 0.8433\n",
      "Epoch [6][20]\t Batch [18200][55000]\t Training Loss 0.8547\t Accuracy 0.8435\n",
      "Epoch [6][20]\t Batch [18250][55000]\t Training Loss 0.8547\t Accuracy 0.8436\n",
      "Epoch [6][20]\t Batch [18300][55000]\t Training Loss 0.8542\t Accuracy 0.8437\n",
      "Epoch [6][20]\t Batch [18350][55000]\t Training Loss 0.8541\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [18400][55000]\t Training Loss 0.8541\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [18450][55000]\t Training Loss 0.8545\t Accuracy 0.8437\n",
      "Epoch [6][20]\t Batch [18500][55000]\t Training Loss 0.8546\t Accuracy 0.8437\n",
      "Epoch [6][20]\t Batch [18550][55000]\t Training Loss 0.8543\t Accuracy 0.8439\n",
      "Epoch [6][20]\t Batch [18600][55000]\t Training Loss 0.8543\t Accuracy 0.8442\n",
      "Epoch [6][20]\t Batch [18650][55000]\t Training Loss 0.8542\t Accuracy 0.8444\n",
      "Epoch [6][20]\t Batch [18700][55000]\t Training Loss 0.8544\t Accuracy 0.8442\n",
      "Epoch [6][20]\t Batch [18750][55000]\t Training Loss 0.8547\t Accuracy 0.8442\n",
      "Epoch [6][20]\t Batch [18800][55000]\t Training Loss 0.8542\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [18850][55000]\t Training Loss 0.8545\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [18900][55000]\t Training Loss 0.8540\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [18950][55000]\t Training Loss 0.8538\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [19000][55000]\t Training Loss 0.8537\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [19050][55000]\t Training Loss 0.8541\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [19100][55000]\t Training Loss 0.8544\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [19150][55000]\t Training Loss 0.8545\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [19200][55000]\t Training Loss 0.8546\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [19250][55000]\t Training Loss 0.8545\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [19300][55000]\t Training Loss 0.8543\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [19350][55000]\t Training Loss 0.8543\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [19400][55000]\t Training Loss 0.8541\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [19450][55000]\t Training Loss 0.8538\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [19500][55000]\t Training Loss 0.8535\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [19550][55000]\t Training Loss 0.8536\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [19600][55000]\t Training Loss 0.8535\t Accuracy 0.8445\n",
      "Epoch [6][20]\t Batch [19650][55000]\t Training Loss 0.8531\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [19700][55000]\t Training Loss 0.8526\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [19750][55000]\t Training Loss 0.8520\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [19800][55000]\t Training Loss 0.8514\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [19850][55000]\t Training Loss 0.8514\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [19900][55000]\t Training Loss 0.8512\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [19950][55000]\t Training Loss 0.8514\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [20000][55000]\t Training Loss 0.8513\t Accuracy 0.8455\n",
      "Epoch [6][20]\t Batch [20050][55000]\t Training Loss 0.8518\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [20100][55000]\t Training Loss 0.8519\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [20150][55000]\t Training Loss 0.8518\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [20200][55000]\t Training Loss 0.8521\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [20250][55000]\t Training Loss 0.8522\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [20300][55000]\t Training Loss 0.8523\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [20350][55000]\t Training Loss 0.8523\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [20400][55000]\t Training Loss 0.8520\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [20450][55000]\t Training Loss 0.8516\t Accuracy 0.8455\n",
      "Epoch [6][20]\t Batch [20500][55000]\t Training Loss 0.8514\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [20550][55000]\t Training Loss 0.8513\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [20600][55000]\t Training Loss 0.8513\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [20650][55000]\t Training Loss 0.8509\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [20700][55000]\t Training Loss 0.8507\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [20750][55000]\t Training Loss 0.8507\t Accuracy 0.8455\n",
      "Epoch [6][20]\t Batch [20800][55000]\t Training Loss 0.8508\t Accuracy 0.8455\n",
      "Epoch [6][20]\t Batch [20850][55000]\t Training Loss 0.8507\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [20900][55000]\t Training Loss 0.8510\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [20950][55000]\t Training Loss 0.8515\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [21000][55000]\t Training Loss 0.8517\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [21050][55000]\t Training Loss 0.8520\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [21100][55000]\t Training Loss 0.8517\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [21150][55000]\t Training Loss 0.8517\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [21200][55000]\t Training Loss 0.8514\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [21250][55000]\t Training Loss 0.8512\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [21300][55000]\t Training Loss 0.8508\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [21350][55000]\t Training Loss 0.8509\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [21400][55000]\t Training Loss 0.8509\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [21450][55000]\t Training Loss 0.8510\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [21500][55000]\t Training Loss 0.8506\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [21550][55000]\t Training Loss 0.8504\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [21600][55000]\t Training Loss 0.8506\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [21650][55000]\t Training Loss 0.8506\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [21700][55000]\t Training Loss 0.8505\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [21750][55000]\t Training Loss 0.8505\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [21800][55000]\t Training Loss 0.8499\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [21850][55000]\t Training Loss 0.8495\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [21900][55000]\t Training Loss 0.8491\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [21950][55000]\t Training Loss 0.8487\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [22000][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [22050][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [22100][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [22150][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [22200][55000]\t Training Loss 0.8487\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [22250][55000]\t Training Loss 0.8487\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [22300][55000]\t Training Loss 0.8489\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [22350][55000]\t Training Loss 0.8487\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [22400][55000]\t Training Loss 0.8486\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [22450][55000]\t Training Loss 0.8486\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [22500][55000]\t Training Loss 0.8490\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [22550][55000]\t Training Loss 0.8496\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [22600][55000]\t Training Loss 0.8500\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [22650][55000]\t Training Loss 0.8502\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [22700][55000]\t Training Loss 0.8501\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [22750][55000]\t Training Loss 0.8499\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [22800][55000]\t Training Loss 0.8499\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [22850][55000]\t Training Loss 0.8499\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [22900][55000]\t Training Loss 0.8496\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [22950][55000]\t Training Loss 0.8494\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [23000][55000]\t Training Loss 0.8492\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [23050][55000]\t Training Loss 0.8490\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [23100][55000]\t Training Loss 0.8491\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [23150][55000]\t Training Loss 0.8490\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [23200][55000]\t Training Loss 0.8491\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [23250][55000]\t Training Loss 0.8488\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [23300][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [23350][55000]\t Training Loss 0.8483\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [23400][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [23450][55000]\t Training Loss 0.8480\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [23500][55000]\t Training Loss 0.8478\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [23550][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [23600][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [23650][55000]\t Training Loss 0.8479\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [23700][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [23750][55000]\t Training Loss 0.8486\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [23800][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [23850][55000]\t Training Loss 0.8482\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [23900][55000]\t Training Loss 0.8484\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [23950][55000]\t Training Loss 0.8484\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [24000][55000]\t Training Loss 0.8484\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [24050][55000]\t Training Loss 0.8483\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [24100][55000]\t Training Loss 0.8482\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [24150][55000]\t Training Loss 0.8481\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [24200][55000]\t Training Loss 0.8479\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [24250][55000]\t Training Loss 0.8480\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [24300][55000]\t Training Loss 0.8482\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [24350][55000]\t Training Loss 0.8482\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [24400][55000]\t Training Loss 0.8482\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [24450][55000]\t Training Loss 0.8482\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [24500][55000]\t Training Loss 0.8482\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [24550][55000]\t Training Loss 0.8482\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [24600][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [24650][55000]\t Training Loss 0.8484\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [24700][55000]\t Training Loss 0.8483\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [24750][55000]\t Training Loss 0.8486\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [24800][55000]\t Training Loss 0.8489\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [24850][55000]\t Training Loss 0.8488\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [24900][55000]\t Training Loss 0.8489\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [24950][55000]\t Training Loss 0.8492\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [25000][55000]\t Training Loss 0.8494\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [25050][55000]\t Training Loss 0.8491\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [25100][55000]\t Training Loss 0.8491\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [25150][55000]\t Training Loss 0.8489\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [25200][55000]\t Training Loss 0.8488\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [25250][55000]\t Training Loss 0.8487\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [25300][55000]\t Training Loss 0.8486\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [25350][55000]\t Training Loss 0.8488\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [25400][55000]\t Training Loss 0.8483\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [25450][55000]\t Training Loss 0.8478\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [25500][55000]\t Training Loss 0.8477\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [25550][55000]\t Training Loss 0.8473\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [25600][55000]\t Training Loss 0.8472\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [25650][55000]\t Training Loss 0.8471\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [25700][55000]\t Training Loss 0.8469\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [25750][55000]\t Training Loss 0.8467\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [25800][55000]\t Training Loss 0.8466\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [25850][55000]\t Training Loss 0.8467\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [25900][55000]\t Training Loss 0.8468\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [25950][55000]\t Training Loss 0.8468\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [26000][55000]\t Training Loss 0.8468\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [26050][55000]\t Training Loss 0.8466\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [26100][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [26150][55000]\t Training Loss 0.8461\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [26200][55000]\t Training Loss 0.8459\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [26250][55000]\t Training Loss 0.8459\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [26300][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [26350][55000]\t Training Loss 0.8460\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [26400][55000]\t Training Loss 0.8463\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [26450][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [26500][55000]\t Training Loss 0.8467\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [26550][55000]\t Training Loss 0.8469\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [26600][55000]\t Training Loss 0.8471\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [26650][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [26700][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [26750][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [26800][55000]\t Training Loss 0.8475\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [26850][55000]\t Training Loss 0.8474\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [26900][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [26950][55000]\t Training Loss 0.8475\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [27000][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [27050][55000]\t Training Loss 0.8471\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [27100][55000]\t Training Loss 0.8470\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [27150][55000]\t Training Loss 0.8470\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [27200][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [27250][55000]\t Training Loss 0.8472\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [27300][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [27350][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [27400][55000]\t Training Loss 0.8472\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [27450][55000]\t Training Loss 0.8471\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [27500][55000]\t Training Loss 0.8471\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [27550][55000]\t Training Loss 0.8472\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [27600][55000]\t Training Loss 0.8470\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [27650][55000]\t Training Loss 0.8470\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [27700][55000]\t Training Loss 0.8470\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [27750][55000]\t Training Loss 0.8471\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [27800][55000]\t Training Loss 0.8472\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [27850][55000]\t Training Loss 0.8473\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [27900][55000]\t Training Loss 0.8471\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [27950][55000]\t Training Loss 0.8469\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [28000][55000]\t Training Loss 0.8467\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [28050][55000]\t Training Loss 0.8464\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [28100][55000]\t Training Loss 0.8460\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [28150][55000]\t Training Loss 0.8458\t Accuracy 0.8481\n",
      "Epoch [6][20]\t Batch [28200][55000]\t Training Loss 0.8459\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [28250][55000]\t Training Loss 0.8455\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [28300][55000]\t Training Loss 0.8454\t Accuracy 0.8481\n",
      "Epoch [6][20]\t Batch [28350][55000]\t Training Loss 0.8452\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [28400][55000]\t Training Loss 0.8457\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [28450][55000]\t Training Loss 0.8454\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [28500][55000]\t Training Loss 0.8452\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [28550][55000]\t Training Loss 0.8451\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [28600][55000]\t Training Loss 0.8450\t Accuracy 0.8484\n",
      "Epoch [6][20]\t Batch [28650][55000]\t Training Loss 0.8453\t Accuracy 0.8484\n",
      "Epoch [6][20]\t Batch [28700][55000]\t Training Loss 0.8455\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [28750][55000]\t Training Loss 0.8456\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [28800][55000]\t Training Loss 0.8456\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [28850][55000]\t Training Loss 0.8455\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [28900][55000]\t Training Loss 0.8454\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [28950][55000]\t Training Loss 0.8454\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [29000][55000]\t Training Loss 0.8453\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [29050][55000]\t Training Loss 0.8453\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [29100][55000]\t Training Loss 0.8454\t Accuracy 0.8482\n",
      "Epoch [6][20]\t Batch [29150][55000]\t Training Loss 0.8458\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [29200][55000]\t Training Loss 0.8462\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [29250][55000]\t Training Loss 0.8464\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [29300][55000]\t Training Loss 0.8462\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [29350][55000]\t Training Loss 0.8463\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [29400][55000]\t Training Loss 0.8463\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [29450][55000]\t Training Loss 0.8458\t Accuracy 0.8480\n",
      "Epoch [6][20]\t Batch [29500][55000]\t Training Loss 0.8456\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [29550][55000]\t Training Loss 0.8456\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [29600][55000]\t Training Loss 0.8456\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [29650][55000]\t Training Loss 0.8455\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [29700][55000]\t Training Loss 0.8456\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [29750][55000]\t Training Loss 0.8460\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [29800][55000]\t Training Loss 0.8461\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [29850][55000]\t Training Loss 0.8464\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [29900][55000]\t Training Loss 0.8467\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [29950][55000]\t Training Loss 0.8470\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [30000][55000]\t Training Loss 0.8474\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [30050][55000]\t Training Loss 0.8475\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [30100][55000]\t Training Loss 0.8479\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [30150][55000]\t Training Loss 0.8483\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [30200][55000]\t Training Loss 0.8486\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [30250][55000]\t Training Loss 0.8487\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [30300][55000]\t Training Loss 0.8485\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [30350][55000]\t Training Loss 0.8484\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [30400][55000]\t Training Loss 0.8483\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [30450][55000]\t Training Loss 0.8483\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [30500][55000]\t Training Loss 0.8485\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [30550][55000]\t Training Loss 0.8489\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [30600][55000]\t Training Loss 0.8490\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [30650][55000]\t Training Loss 0.8494\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [30700][55000]\t Training Loss 0.8497\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [30750][55000]\t Training Loss 0.8498\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [30800][55000]\t Training Loss 0.8500\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [30850][55000]\t Training Loss 0.8499\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [30900][55000]\t Training Loss 0.8503\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [30950][55000]\t Training Loss 0.8502\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [31000][55000]\t Training Loss 0.8501\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [31050][55000]\t Training Loss 0.8503\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [31100][55000]\t Training Loss 0.8503\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [31150][55000]\t Training Loss 0.8503\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [31200][55000]\t Training Loss 0.8503\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [31250][55000]\t Training Loss 0.8503\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [31300][55000]\t Training Loss 0.8506\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [31350][55000]\t Training Loss 0.8513\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [31400][55000]\t Training Loss 0.8514\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [31450][55000]\t Training Loss 0.8517\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [31500][55000]\t Training Loss 0.8517\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [31550][55000]\t Training Loss 0.8518\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [31600][55000]\t Training Loss 0.8519\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [31650][55000]\t Training Loss 0.8521\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [31700][55000]\t Training Loss 0.8524\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [31750][55000]\t Training Loss 0.8528\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [31800][55000]\t Training Loss 0.8529\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [31850][55000]\t Training Loss 0.8529\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [31900][55000]\t Training Loss 0.8528\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [31950][55000]\t Training Loss 0.8528\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [32000][55000]\t Training Loss 0.8528\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [32050][55000]\t Training Loss 0.8528\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [32100][55000]\t Training Loss 0.8527\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [32150][55000]\t Training Loss 0.8529\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [32200][55000]\t Training Loss 0.8531\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [32250][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [32300][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [32350][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [32400][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [32450][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [32500][55000]\t Training Loss 0.8546\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [32550][55000]\t Training Loss 0.8549\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [32600][55000]\t Training Loss 0.8548\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [32650][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [32700][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [32750][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [32800][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [32850][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [32900][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [32950][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33000][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33050][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33100][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33150][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [33200][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [33250][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33300][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33350][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [33400][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33450][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [33500][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [33550][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [33600][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33650][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [33700][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [33750][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [33800][55000]\t Training Loss 0.8546\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [33850][55000]\t Training Loss 0.8543\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [33900][55000]\t Training Loss 0.8539\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [33950][55000]\t Training Loss 0.8536\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [34000][55000]\t Training Loss 0.8536\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [34050][55000]\t Training Loss 0.8538\t Accuracy 0.8467\n",
      "Epoch [6][20]\t Batch [34100][55000]\t Training Loss 0.8538\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [34150][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [34200][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [34250][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [34300][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [34350][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [34400][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [34450][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [34500][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [34550][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [34600][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [34650][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [34700][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [34750][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [34800][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [34850][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [34900][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [34950][55000]\t Training Loss 0.8541\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [35000][55000]\t Training Loss 0.8539\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [35050][55000]\t Training Loss 0.8538\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [35100][55000]\t Training Loss 0.8539\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [35150][55000]\t Training Loss 0.8539\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [35200][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [35250][55000]\t Training Loss 0.8541\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [35300][55000]\t Training Loss 0.8542\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [35350][55000]\t Training Loss 0.8540\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [35400][55000]\t Training Loss 0.8540\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [35450][55000]\t Training Loss 0.8539\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [35500][55000]\t Training Loss 0.8539\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [35550][55000]\t Training Loss 0.8538\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [35600][55000]\t Training Loss 0.8536\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [35650][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [35700][55000]\t Training Loss 0.8539\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [35750][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [35800][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [35850][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [35900][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [35950][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [36000][55000]\t Training Loss 0.8537\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [36050][55000]\t Training Loss 0.8539\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [36100][55000]\t Training Loss 0.8541\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [36150][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [36200][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [36250][55000]\t Training Loss 0.8537\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [36300][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [36350][55000]\t Training Loss 0.8533\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [36400][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [36450][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [36500][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [36550][55000]\t Training Loss 0.8534\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [36600][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [36650][55000]\t Training Loss 0.8530\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [36700][55000]\t Training Loss 0.8528\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [36750][55000]\t Training Loss 0.8525\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [36800][55000]\t Training Loss 0.8524\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [36850][55000]\t Training Loss 0.8524\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [36900][55000]\t Training Loss 0.8524\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [36950][55000]\t Training Loss 0.8523\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [37000][55000]\t Training Loss 0.8522\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [37050][55000]\t Training Loss 0.8520\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [37100][55000]\t Training Loss 0.8521\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [37150][55000]\t Training Loss 0.8521\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [37200][55000]\t Training Loss 0.8520\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [37250][55000]\t Training Loss 0.8519\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [37300][55000]\t Training Loss 0.8520\t Accuracy 0.8479\n",
      "Epoch [6][20]\t Batch [37350][55000]\t Training Loss 0.8521\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [37400][55000]\t Training Loss 0.8523\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [37450][55000]\t Training Loss 0.8526\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [37500][55000]\t Training Loss 0.8527\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [37550][55000]\t Training Loss 0.8529\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [37600][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [37650][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [37700][55000]\t Training Loss 0.8530\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [37750][55000]\t Training Loss 0.8530\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [37800][55000]\t Training Loss 0.8530\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [37850][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [37900][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [37950][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [38000][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [38050][55000]\t Training Loss 0.8531\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [38100][55000]\t Training Loss 0.8532\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [38150][55000]\t Training Loss 0.8531\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [38200][55000]\t Training Loss 0.8530\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [38250][55000]\t Training Loss 0.8530\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [38300][55000]\t Training Loss 0.8531\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [38350][55000]\t Training Loss 0.8533\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [38400][55000]\t Training Loss 0.8534\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [38450][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [38500][55000]\t Training Loss 0.8532\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [38550][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [38600][55000]\t Training Loss 0.8534\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [38650][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [38700][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [38750][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [38800][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [38850][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [38900][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [38950][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [39000][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [39050][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [39100][55000]\t Training Loss 0.8525\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [39150][55000]\t Training Loss 0.8524\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [39200][55000]\t Training Loss 0.8522\t Accuracy 0.8477\n",
      "Epoch [6][20]\t Batch [39250][55000]\t Training Loss 0.8520\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [39300][55000]\t Training Loss 0.8520\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [39350][55000]\t Training Loss 0.8523\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [39400][55000]\t Training Loss 0.8523\t Accuracy 0.8476\n",
      "Epoch [6][20]\t Batch [39450][55000]\t Training Loss 0.8526\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [39500][55000]\t Training Loss 0.8527\t Accuracy 0.8475\n",
      "Epoch [6][20]\t Batch [39550][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [39600][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [39650][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [39700][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [39750][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [39800][55000]\t Training Loss 0.8530\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [39850][55000]\t Training Loss 0.8532\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [39900][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [39950][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40000][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40050][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40100][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40150][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40200][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40250][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40300][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40350][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40400][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40450][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40500][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40550][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40600][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40650][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40700][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40750][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [40800][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40850][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40900][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [40950][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [41000][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [41050][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [41100][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [41150][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [41200][55000]\t Training Loss 0.8529\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [41250][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [6][20]\t Batch [41300][55000]\t Training Loss 0.8530\t Accuracy 0.8473\n",
      "Epoch [6][20]\t Batch [41350][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [41400][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [41450][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [41500][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [41550][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [41600][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [41650][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [41700][55000]\t Training Loss 0.8539\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [41750][55000]\t Training Loss 0.8540\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [41800][55000]\t Training Loss 0.8543\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [41850][55000]\t Training Loss 0.8543\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [41900][55000]\t Training Loss 0.8542\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [41950][55000]\t Training Loss 0.8543\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [42000][55000]\t Training Loss 0.8542\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [42050][55000]\t Training Loss 0.8542\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [42100][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [6][20]\t Batch [42150][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [42200][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [6][20]\t Batch [42250][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [42300][55000]\t Training Loss 0.8543\t Accuracy 0.8468\n",
      "Epoch [6][20]\t Batch [42350][55000]\t Training Loss 0.8545\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [42400][55000]\t Training Loss 0.8547\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [42450][55000]\t Training Loss 0.8548\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [42500][55000]\t Training Loss 0.8549\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [42550][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [42600][55000]\t Training Loss 0.8550\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [42650][55000]\t Training Loss 0.8548\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [42700][55000]\t Training Loss 0.8548\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [42750][55000]\t Training Loss 0.8548\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [42800][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [42850][55000]\t Training Loss 0.8546\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [42900][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [42950][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [43000][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [43050][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [43100][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [43150][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [43200][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [43250][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [43300][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [43350][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [43400][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [43450][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [43500][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [43550][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [43600][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [43650][55000]\t Training Loss 0.8542\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [43700][55000]\t Training Loss 0.8542\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [43750][55000]\t Training Loss 0.8542\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [43800][55000]\t Training Loss 0.8542\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [43850][55000]\t Training Loss 0.8542\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [43900][55000]\t Training Loss 0.8543\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [43950][55000]\t Training Loss 0.8544\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [44000][55000]\t Training Loss 0.8545\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [44050][55000]\t Training Loss 0.8545\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [44100][55000]\t Training Loss 0.8545\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [44150][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [44200][55000]\t Training Loss 0.8548\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [44250][55000]\t Training Loss 0.8548\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [44300][55000]\t Training Loss 0.8550\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [44350][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [44400][55000]\t Training Loss 0.8551\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [44450][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [44500][55000]\t Training Loss 0.8551\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [44550][55000]\t Training Loss 0.8550\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [44600][55000]\t Training Loss 0.8548\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [44650][55000]\t Training Loss 0.8547\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [44700][55000]\t Training Loss 0.8546\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [44750][55000]\t Training Loss 0.8546\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [44800][55000]\t Training Loss 0.8547\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [44850][55000]\t Training Loss 0.8547\t Accuracy 0.8466\n",
      "Epoch [6][20]\t Batch [44900][55000]\t Training Loss 0.8548\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [44950][55000]\t Training Loss 0.8549\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [45000][55000]\t Training Loss 0.8549\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [45050][55000]\t Training Loss 0.8550\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45100][55000]\t Training Loss 0.8551\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45150][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [45200][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45250][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45300][55000]\t Training Loss 0.8551\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [45350][55000]\t Training Loss 0.8550\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [45400][55000]\t Training Loss 0.8549\t Accuracy 0.8465\n",
      "Epoch [6][20]\t Batch [45450][55000]\t Training Loss 0.8550\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [45500][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45550][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45600][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45650][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45700][55000]\t Training Loss 0.8551\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45750][55000]\t Training Loss 0.8551\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [45800][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45850][55000]\t Training Loss 0.8553\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [45900][55000]\t Training Loss 0.8554\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [45950][55000]\t Training Loss 0.8554\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [46000][55000]\t Training Loss 0.8555\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [46050][55000]\t Training Loss 0.8556\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [46100][55000]\t Training Loss 0.8558\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [46150][55000]\t Training Loss 0.8558\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [46200][55000]\t Training Loss 0.8558\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [46250][55000]\t Training Loss 0.8558\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [46300][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [46350][55000]\t Training Loss 0.8560\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [46400][55000]\t Training Loss 0.8561\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [46450][55000]\t Training Loss 0.8562\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [46500][55000]\t Training Loss 0.8561\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [46550][55000]\t Training Loss 0.8560\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [46600][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [46650][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [46700][55000]\t Training Loss 0.8558\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [46750][55000]\t Training Loss 0.8559\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [46800][55000]\t Training Loss 0.8557\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [46850][55000]\t Training Loss 0.8556\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [46900][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [46950][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47000][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [47050][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47100][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47150][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47200][55000]\t Training Loss 0.8551\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [47250][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [47300][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [47350][55000]\t Training Loss 0.8554\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [47400][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47450][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47500][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47550][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47600][55000]\t Training Loss 0.8555\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [47650][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47700][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [47750][55000]\t Training Loss 0.8555\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [47800][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [47850][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47900][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [47950][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [48000][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [48050][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [48100][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [48150][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48200][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48250][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48300][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48350][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48400][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [48450][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48500][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [48550][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48600][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48650][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48700][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [48750][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [48800][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [48850][55000]\t Training Loss 0.8536\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [48900][55000]\t Training Loss 0.8535\t Accuracy 0.8464\n",
      "Epoch [6][20]\t Batch [48950][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [6][20]\t Batch [49000][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [6][20]\t Batch [49050][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [49100][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [6][20]\t Batch [49150][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [6][20]\t Batch [49200][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [49250][55000]\t Training Loss 0.8545\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [49300][55000]\t Training Loss 0.8544\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [49350][55000]\t Training Loss 0.8543\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [49400][55000]\t Training Loss 0.8541\t Accuracy 0.8458\n",
      "Epoch [6][20]\t Batch [49450][55000]\t Training Loss 0.8541\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [49500][55000]\t Training Loss 0.8543\t Accuracy 0.8456\n",
      "Epoch [6][20]\t Batch [49550][55000]\t Training Loss 0.8547\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [49600][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [49650][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [49700][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [49750][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [49800][55000]\t Training Loss 0.8552\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [49850][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [49900][55000]\t Training Loss 0.8554\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [49950][55000]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [50000][55000]\t Training Loss 0.8556\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [50050][55000]\t Training Loss 0.8556\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [50100][55000]\t Training Loss 0.8556\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [50150][55000]\t Training Loss 0.8556\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [50200][55000]\t Training Loss 0.8555\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [50250][55000]\t Training Loss 0.8557\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [50300][55000]\t Training Loss 0.8556\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [50350][55000]\t Training Loss 0.8556\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [50400][55000]\t Training Loss 0.8559\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [50450][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [50500][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [50550][55000]\t Training Loss 0.8562\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [50600][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [50650][55000]\t Training Loss 0.8564\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [50700][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [50750][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [50800][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [50850][55000]\t Training Loss 0.8564\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [50900][55000]\t Training Loss 0.8563\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [50950][55000]\t Training Loss 0.8562\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [51000][55000]\t Training Loss 0.8560\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [51050][55000]\t Training Loss 0.8559\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [51100][55000]\t Training Loss 0.8558\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [51150][55000]\t Training Loss 0.8557\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [51200][55000]\t Training Loss 0.8558\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [51250][55000]\t Training Loss 0.8559\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [51300][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [51350][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [51400][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [51450][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [51500][55000]\t Training Loss 0.8557\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [51550][55000]\t Training Loss 0.8556\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [51600][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [51650][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [51700][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [51750][55000]\t Training Loss 0.8554\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [51800][55000]\t Training Loss 0.8554\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [51850][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [51900][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [51950][55000]\t Training Loss 0.8552\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [52000][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [52050][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [52100][55000]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [52150][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [52200][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [52250][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [52300][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [52350][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [52400][55000]\t Training Loss 0.8561\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [52450][55000]\t Training Loss 0.8558\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [52500][55000]\t Training Loss 0.8557\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [52550][55000]\t Training Loss 0.8556\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [52600][55000]\t Training Loss 0.8554\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [52650][55000]\t Training Loss 0.8552\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [52700][55000]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [6][20]\t Batch [52750][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [52800][55000]\t Training Loss 0.8556\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [52850][55000]\t Training Loss 0.8556\t Accuracy 0.8452\n",
      "Epoch [6][20]\t Batch [52900][55000]\t Training Loss 0.8558\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [52950][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [53000][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [53050][55000]\t Training Loss 0.8561\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [53100][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53150][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [53200][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53250][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53300][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53350][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53400][55000]\t Training Loss 0.8559\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [53450][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [53500][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53550][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53600][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [53650][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [53700][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [53750][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53800][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53850][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53900][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [53950][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [54000][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [54050][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [54100][55000]\t Training Loss 0.8562\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [54150][55000]\t Training Loss 0.8561\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [54200][55000]\t Training Loss 0.8562\t Accuracy 0.8447\n",
      "Epoch [6][20]\t Batch [54250][55000]\t Training Loss 0.8561\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [54300][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [54350][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [54400][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [54450][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [54500][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [54550][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [54600][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [54650][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [54700][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [6][20]\t Batch [54750][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [54800][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [54850][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [54900][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [54950][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8557\t Average training accuracy 0.8448\n",
      "Epoch [6]\t Average validation loss 0.7932\t Average validation accuracy 0.8698\n",
      "\n",
      "Epoch [7][20]\t Batch [0][55000]\t Training Loss 1.4737\t Accuracy 0.0000\n",
      "Epoch [7][20]\t Batch [50][55000]\t Training Loss 0.9406\t Accuracy 0.7255\n",
      "Epoch [7][20]\t Batch [100][55000]\t Training Loss 0.8504\t Accuracy 0.8020\n",
      "Epoch [7][20]\t Batch [150][55000]\t Training Loss 0.8515\t Accuracy 0.8146\n",
      "Epoch [7][20]\t Batch [200][55000]\t Training Loss 0.8621\t Accuracy 0.8159\n",
      "Epoch [7][20]\t Batch [250][55000]\t Training Loss 0.8444\t Accuracy 0.8247\n",
      "Epoch [7][20]\t Batch [300][55000]\t Training Loss 0.8458\t Accuracy 0.8239\n",
      "Epoch [7][20]\t Batch [350][55000]\t Training Loss 0.8262\t Accuracy 0.8262\n",
      "Epoch [7][20]\t Batch [400][55000]\t Training Loss 0.8123\t Accuracy 0.8304\n",
      "Epoch [7][20]\t Batch [450][55000]\t Training Loss 0.8181\t Accuracy 0.8315\n",
      "Epoch [7][20]\t Batch [500][55000]\t Training Loss 0.8244\t Accuracy 0.8343\n",
      "Epoch [7][20]\t Batch [550][55000]\t Training Loss 0.8416\t Accuracy 0.8312\n",
      "Epoch [7][20]\t Batch [600][55000]\t Training Loss 0.8461\t Accuracy 0.8336\n",
      "Epoch [7][20]\t Batch [650][55000]\t Training Loss 0.8659\t Accuracy 0.8233\n",
      "Epoch [7][20]\t Batch [700][55000]\t Training Loss 0.8635\t Accuracy 0.8288\n",
      "Epoch [7][20]\t Batch [750][55000]\t Training Loss 0.8598\t Accuracy 0.8296\n",
      "Epoch [7][20]\t Batch [800][55000]\t Training Loss 0.8538\t Accuracy 0.8315\n",
      "Epoch [7][20]\t Batch [850][55000]\t Training Loss 0.8522\t Accuracy 0.8331\n",
      "Epoch [7][20]\t Batch [900][55000]\t Training Loss 0.8587\t Accuracy 0.8324\n",
      "Epoch [7][20]\t Batch [950][55000]\t Training Loss 0.8655\t Accuracy 0.8275\n",
      "Epoch [7][20]\t Batch [1000][55000]\t Training Loss 0.8649\t Accuracy 0.8302\n",
      "Epoch [7][20]\t Batch [1050][55000]\t Training Loss 0.8726\t Accuracy 0.8287\n",
      "Epoch [7][20]\t Batch [1100][55000]\t Training Loss 0.8810\t Accuracy 0.8283\n",
      "Epoch [7][20]\t Batch [1150][55000]\t Training Loss 0.8903\t Accuracy 0.8254\n",
      "Epoch [7][20]\t Batch [1200][55000]\t Training Loss 0.8845\t Accuracy 0.8285\n",
      "Epoch [7][20]\t Batch [1250][55000]\t Training Loss 0.8858\t Accuracy 0.8281\n",
      "Epoch [7][20]\t Batch [1300][55000]\t Training Loss 0.8861\t Accuracy 0.8278\n",
      "Epoch [7][20]\t Batch [1350][55000]\t Training Loss 0.8829\t Accuracy 0.8290\n",
      "Epoch [7][20]\t Batch [1400][55000]\t Training Loss 0.8835\t Accuracy 0.8280\n",
      "Epoch [7][20]\t Batch [1450][55000]\t Training Loss 0.8844\t Accuracy 0.8270\n",
      "Epoch [7][20]\t Batch [1500][55000]\t Training Loss 0.8825\t Accuracy 0.8301\n",
      "Epoch [7][20]\t Batch [1550][55000]\t Training Loss 0.8820\t Accuracy 0.8304\n",
      "Epoch [7][20]\t Batch [1600][55000]\t Training Loss 0.8847\t Accuracy 0.8295\n",
      "Epoch [7][20]\t Batch [1650][55000]\t Training Loss 0.8816\t Accuracy 0.8310\n",
      "Epoch [7][20]\t Batch [1700][55000]\t Training Loss 0.8799\t Accuracy 0.8325\n",
      "Epoch [7][20]\t Batch [1750][55000]\t Training Loss 0.8743\t Accuracy 0.8332\n",
      "Epoch [7][20]\t Batch [1800][55000]\t Training Loss 0.8721\t Accuracy 0.8351\n",
      "Epoch [7][20]\t Batch [1850][55000]\t Training Loss 0.8705\t Accuracy 0.8358\n",
      "Epoch [7][20]\t Batch [1900][55000]\t Training Loss 0.8683\t Accuracy 0.8359\n",
      "Epoch [7][20]\t Batch [1950][55000]\t Training Loss 0.8668\t Accuracy 0.8370\n",
      "Epoch [7][20]\t Batch [2000][55000]\t Training Loss 0.8643\t Accuracy 0.8381\n",
      "Epoch [7][20]\t Batch [2050][55000]\t Training Loss 0.8634\t Accuracy 0.8386\n",
      "Epoch [7][20]\t Batch [2100][55000]\t Training Loss 0.8606\t Accuracy 0.8391\n",
      "Epoch [7][20]\t Batch [2150][55000]\t Training Loss 0.8573\t Accuracy 0.8410\n",
      "Epoch [7][20]\t Batch [2200][55000]\t Training Loss 0.8525\t Accuracy 0.8428\n",
      "Epoch [7][20]\t Batch [2250][55000]\t Training Loss 0.8517\t Accuracy 0.8432\n",
      "Epoch [7][20]\t Batch [2300][55000]\t Training Loss 0.8488\t Accuracy 0.8427\n",
      "Epoch [7][20]\t Batch [2350][55000]\t Training Loss 0.8472\t Accuracy 0.8430\n",
      "Epoch [7][20]\t Batch [2400][55000]\t Training Loss 0.8482\t Accuracy 0.8413\n",
      "Epoch [7][20]\t Batch [2450][55000]\t Training Loss 0.8510\t Accuracy 0.8401\n",
      "Epoch [7][20]\t Batch [2500][55000]\t Training Loss 0.8489\t Accuracy 0.8421\n",
      "Epoch [7][20]\t Batch [2550][55000]\t Training Loss 0.8476\t Accuracy 0.8420\n",
      "Epoch [7][20]\t Batch [2600][55000]\t Training Loss 0.8470\t Accuracy 0.8416\n",
      "Epoch [7][20]\t Batch [2650][55000]\t Training Loss 0.8458\t Accuracy 0.8419\n",
      "Epoch [7][20]\t Batch [2700][55000]\t Training Loss 0.8456\t Accuracy 0.8423\n",
      "Epoch [7][20]\t Batch [2750][55000]\t Training Loss 0.8455\t Accuracy 0.8422\n",
      "Epoch [7][20]\t Batch [2800][55000]\t Training Loss 0.8457\t Accuracy 0.8404\n",
      "Epoch [7][20]\t Batch [2850][55000]\t Training Loss 0.8446\t Accuracy 0.8401\n",
      "Epoch [7][20]\t Batch [2900][55000]\t Training Loss 0.8414\t Accuracy 0.8411\n",
      "Epoch [7][20]\t Batch [2950][55000]\t Training Loss 0.8414\t Accuracy 0.8407\n",
      "Epoch [7][20]\t Batch [3000][55000]\t Training Loss 0.8413\t Accuracy 0.8414\n",
      "Epoch [7][20]\t Batch [3050][55000]\t Training Loss 0.8424\t Accuracy 0.8410\n",
      "Epoch [7][20]\t Batch [3100][55000]\t Training Loss 0.8447\t Accuracy 0.8407\n",
      "Epoch [7][20]\t Batch [3150][55000]\t Training Loss 0.8445\t Accuracy 0.8410\n",
      "Epoch [7][20]\t Batch [3200][55000]\t Training Loss 0.8443\t Accuracy 0.8422\n",
      "Epoch [7][20]\t Batch [3250][55000]\t Training Loss 0.8437\t Accuracy 0.8416\n",
      "Epoch [7][20]\t Batch [3300][55000]\t Training Loss 0.8450\t Accuracy 0.8416\n",
      "Epoch [7][20]\t Batch [3350][55000]\t Training Loss 0.8432\t Accuracy 0.8430\n",
      "Epoch [7][20]\t Batch [3400][55000]\t Training Loss 0.8456\t Accuracy 0.8415\n",
      "Epoch [7][20]\t Batch [3450][55000]\t Training Loss 0.8452\t Accuracy 0.8424\n",
      "Epoch [7][20]\t Batch [3500][55000]\t Training Loss 0.8452\t Accuracy 0.8418\n",
      "Epoch [7][20]\t Batch [3550][55000]\t Training Loss 0.8468\t Accuracy 0.8412\n",
      "Epoch [7][20]\t Batch [3600][55000]\t Training Loss 0.8466\t Accuracy 0.8406\n",
      "Epoch [7][20]\t Batch [3650][55000]\t Training Loss 0.8452\t Accuracy 0.8414\n",
      "Epoch [7][20]\t Batch [3700][55000]\t Training Loss 0.8460\t Accuracy 0.8409\n",
      "Epoch [7][20]\t Batch [3750][55000]\t Training Loss 0.8459\t Accuracy 0.8411\n",
      "Epoch [7][20]\t Batch [3800][55000]\t Training Loss 0.8462\t Accuracy 0.8411\n",
      "Epoch [7][20]\t Batch [3850][55000]\t Training Loss 0.8458\t Accuracy 0.8416\n",
      "Epoch [7][20]\t Batch [3900][55000]\t Training Loss 0.8447\t Accuracy 0.8429\n",
      "Epoch [7][20]\t Batch [3950][55000]\t Training Loss 0.8435\t Accuracy 0.8441\n",
      "Epoch [7][20]\t Batch [4000][55000]\t Training Loss 0.8432\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [4050][55000]\t Training Loss 0.8416\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [4100][55000]\t Training Loss 0.8421\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [4150][55000]\t Training Loss 0.8425\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [4200][55000]\t Training Loss 0.8430\t Accuracy 0.8438\n",
      "Epoch [7][20]\t Batch [4250][55000]\t Training Loss 0.8416\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [4300][55000]\t Training Loss 0.8416\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [4350][55000]\t Training Loss 0.8421\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [4400][55000]\t Training Loss 0.8420\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [4450][55000]\t Training Loss 0.8422\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [4500][55000]\t Training Loss 0.8425\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [4550][55000]\t Training Loss 0.8408\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [4600][55000]\t Training Loss 0.8391\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [4650][55000]\t Training Loss 0.8390\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [4700][55000]\t Training Loss 0.8386\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [4750][55000]\t Training Loss 0.8374\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [4800][55000]\t Training Loss 0.8381\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [4850][55000]\t Training Loss 0.8392\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [4900][55000]\t Training Loss 0.8378\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [4950][55000]\t Training Loss 0.8381\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [5000][55000]\t Training Loss 0.8382\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [5050][55000]\t Training Loss 0.8377\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [5100][55000]\t Training Loss 0.8377\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [5150][55000]\t Training Loss 0.8380\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [5200][55000]\t Training Loss 0.8395\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [5250][55000]\t Training Loss 0.8386\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [5300][55000]\t Training Loss 0.8385\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [5350][55000]\t Training Loss 0.8391\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [5400][55000]\t Training Loss 0.8384\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [5450][55000]\t Training Loss 0.8377\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [5500][55000]\t Training Loss 0.8359\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [5550][55000]\t Training Loss 0.8358\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [5600][55000]\t Training Loss 0.8349\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [5650][55000]\t Training Loss 0.8355\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [5700][55000]\t Training Loss 0.8352\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [5750][55000]\t Training Loss 0.8356\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [5800][55000]\t Training Loss 0.8360\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [5850][55000]\t Training Loss 0.8361\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [5900][55000]\t Training Loss 0.8367\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [5950][55000]\t Training Loss 0.8365\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [6000][55000]\t Training Loss 0.8355\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [6050][55000]\t Training Loss 0.8341\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [6100][55000]\t Training Loss 0.8327\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [6150][55000]\t Training Loss 0.8306\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [6200][55000]\t Training Loss 0.8307\t Accuracy 0.8479\n",
      "Epoch [7][20]\t Batch [6250][55000]\t Training Loss 0.8295\t Accuracy 0.8483\n",
      "Epoch [7][20]\t Batch [6300][55000]\t Training Loss 0.8299\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [6350][55000]\t Training Loss 0.8295\t Accuracy 0.8484\n",
      "Epoch [7][20]\t Batch [6400][55000]\t Training Loss 0.8289\t Accuracy 0.8488\n",
      "Epoch [7][20]\t Batch [6450][55000]\t Training Loss 0.8284\t Accuracy 0.8490\n",
      "Epoch [7][20]\t Batch [6500][55000]\t Training Loss 0.8292\t Accuracy 0.8486\n",
      "Epoch [7][20]\t Batch [6550][55000]\t Training Loss 0.8282\t Accuracy 0.8489\n",
      "Epoch [7][20]\t Batch [6600][55000]\t Training Loss 0.8269\t Accuracy 0.8494\n",
      "Epoch [7][20]\t Batch [6650][55000]\t Training Loss 0.8257\t Accuracy 0.8495\n",
      "Epoch [7][20]\t Batch [6700][55000]\t Training Loss 0.8259\t Accuracy 0.8494\n",
      "Epoch [7][20]\t Batch [6750][55000]\t Training Loss 0.8261\t Accuracy 0.8502\n",
      "Epoch [7][20]\t Batch [6800][55000]\t Training Loss 0.8265\t Accuracy 0.8502\n",
      "Epoch [7][20]\t Batch [6850][55000]\t Training Loss 0.8287\t Accuracy 0.8498\n",
      "Epoch [7][20]\t Batch [6900][55000]\t Training Loss 0.8286\t Accuracy 0.8497\n",
      "Epoch [7][20]\t Batch [6950][55000]\t Training Loss 0.8293\t Accuracy 0.8487\n",
      "Epoch [7][20]\t Batch [7000][55000]\t Training Loss 0.8292\t Accuracy 0.8487\n",
      "Epoch [7][20]\t Batch [7050][55000]\t Training Loss 0.8297\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [7100][55000]\t Training Loss 0.8300\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [7150][55000]\t Training Loss 0.8302\t Accuracy 0.8487\n",
      "Epoch [7][20]\t Batch [7200][55000]\t Training Loss 0.8309\t Accuracy 0.8486\n",
      "Epoch [7][20]\t Batch [7250][55000]\t Training Loss 0.8333\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [7300][55000]\t Training Loss 0.8351\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [7350][55000]\t Training Loss 0.8366\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [7400][55000]\t Training Loss 0.8376\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [7450][55000]\t Training Loss 0.8376\t Accuracy 0.8479\n",
      "Epoch [7][20]\t Batch [7500][55000]\t Training Loss 0.8376\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [7550][55000]\t Training Loss 0.8381\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [7600][55000]\t Training Loss 0.8376\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [7650][55000]\t Training Loss 0.8381\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [7700][55000]\t Training Loss 0.8386\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [7750][55000]\t Training Loss 0.8387\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [7800][55000]\t Training Loss 0.8395\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [7850][55000]\t Training Loss 0.8396\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [7900][55000]\t Training Loss 0.8398\t Accuracy 0.8479\n",
      "Epoch [7][20]\t Batch [7950][55000]\t Training Loss 0.8398\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [8000][55000]\t Training Loss 0.8399\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [8050][55000]\t Training Loss 0.8404\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [8100][55000]\t Training Loss 0.8389\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [8150][55000]\t Training Loss 0.8397\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [8200][55000]\t Training Loss 0.8397\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [8250][55000]\t Training Loss 0.8410\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [8300][55000]\t Training Loss 0.8413\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [8350][55000]\t Training Loss 0.8419\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [8400][55000]\t Training Loss 0.8413\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [8450][55000]\t Training Loss 0.8428\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [8500][55000]\t Training Loss 0.8420\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [8550][55000]\t Training Loss 0.8410\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [8600][55000]\t Training Loss 0.8402\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [8650][55000]\t Training Loss 0.8403\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [8700][55000]\t Training Loss 0.8409\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [8750][55000]\t Training Loss 0.8425\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [8800][55000]\t Training Loss 0.8433\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [8850][55000]\t Training Loss 0.8432\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [8900][55000]\t Training Loss 0.8450\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [8950][55000]\t Training Loss 0.8445\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [9000][55000]\t Training Loss 0.8438\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [9050][55000]\t Training Loss 0.8430\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [9100][55000]\t Training Loss 0.8427\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [9150][55000]\t Training Loss 0.8432\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [9200][55000]\t Training Loss 0.8428\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [9250][55000]\t Training Loss 0.8431\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [9300][55000]\t Training Loss 0.8433\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [9350][55000]\t Training Loss 0.8435\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [9400][55000]\t Training Loss 0.8438\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [9450][55000]\t Training Loss 0.8440\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [9500][55000]\t Training Loss 0.8431\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [9550][55000]\t Training Loss 0.8429\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [9600][55000]\t Training Loss 0.8434\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [9650][55000]\t Training Loss 0.8432\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [9700][55000]\t Training Loss 0.8426\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [9750][55000]\t Training Loss 0.8418\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [9800][55000]\t Training Loss 0.8424\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [9850][55000]\t Training Loss 0.8419\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [9900][55000]\t Training Loss 0.8414\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [9950][55000]\t Training Loss 0.8408\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [10000][55000]\t Training Loss 0.8406\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [10050][55000]\t Training Loss 0.8407\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [10100][55000]\t Training Loss 0.8405\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [10150][55000]\t Training Loss 0.8404\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [10200][55000]\t Training Loss 0.8405\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [10250][55000]\t Training Loss 0.8407\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [10300][55000]\t Training Loss 0.8406\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [10350][55000]\t Training Loss 0.8396\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [10400][55000]\t Training Loss 0.8388\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [10450][55000]\t Training Loss 0.8386\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [10500][55000]\t Training Loss 0.8376\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [10550][55000]\t Training Loss 0.8372\t Accuracy 0.8483\n",
      "Epoch [7][20]\t Batch [10600][55000]\t Training Loss 0.8368\t Accuracy 0.8485\n",
      "Epoch [7][20]\t Batch [10650][55000]\t Training Loss 0.8365\t Accuracy 0.8488\n",
      "Epoch [7][20]\t Batch [10700][55000]\t Training Loss 0.8362\t Accuracy 0.8492\n",
      "Epoch [7][20]\t Batch [10750][55000]\t Training Loss 0.8368\t Accuracy 0.8489\n",
      "Epoch [7][20]\t Batch [10800][55000]\t Training Loss 0.8372\t Accuracy 0.8486\n",
      "Epoch [7][20]\t Batch [10850][55000]\t Training Loss 0.8365\t Accuracy 0.8489\n",
      "Epoch [7][20]\t Batch [10900][55000]\t Training Loss 0.8361\t Accuracy 0.8491\n",
      "Epoch [7][20]\t Batch [10950][55000]\t Training Loss 0.8358\t Accuracy 0.8489\n",
      "Epoch [7][20]\t Batch [11000][55000]\t Training Loss 0.8356\t Accuracy 0.8491\n",
      "Epoch [7][20]\t Batch [11050][55000]\t Training Loss 0.8349\t Accuracy 0.8493\n",
      "Epoch [7][20]\t Batch [11100][55000]\t Training Loss 0.8342\t Accuracy 0.8495\n",
      "Epoch [7][20]\t Batch [11150][55000]\t Training Loss 0.8342\t Accuracy 0.8497\n",
      "Epoch [7][20]\t Batch [11200][55000]\t Training Loss 0.8339\t Accuracy 0.8497\n",
      "Epoch [7][20]\t Batch [11250][55000]\t Training Loss 0.8342\t Accuracy 0.8496\n",
      "Epoch [7][20]\t Batch [11300][55000]\t Training Loss 0.8338\t Accuracy 0.8497\n",
      "Epoch [7][20]\t Batch [11350][55000]\t Training Loss 0.8333\t Accuracy 0.8498\n",
      "Epoch [7][20]\t Batch [11400][55000]\t Training Loss 0.8335\t Accuracy 0.8497\n",
      "Epoch [7][20]\t Batch [11450][55000]\t Training Loss 0.8331\t Accuracy 0.8497\n",
      "Epoch [7][20]\t Batch [11500][55000]\t Training Loss 0.8329\t Accuracy 0.8496\n",
      "Epoch [7][20]\t Batch [11550][55000]\t Training Loss 0.8329\t Accuracy 0.8495\n",
      "Epoch [7][20]\t Batch [11600][55000]\t Training Loss 0.8341\t Accuracy 0.8488\n",
      "Epoch [7][20]\t Batch [11650][55000]\t Training Loss 0.8347\t Accuracy 0.8485\n",
      "Epoch [7][20]\t Batch [11700][55000]\t Training Loss 0.8346\t Accuracy 0.8486\n",
      "Epoch [7][20]\t Batch [11750][55000]\t Training Loss 0.8356\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [11800][55000]\t Training Loss 0.8358\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [11850][55000]\t Training Loss 0.8358\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [11900][55000]\t Training Loss 0.8361\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [11950][55000]\t Training Loss 0.8362\t Accuracy 0.8484\n",
      "Epoch [7][20]\t Batch [12000][55000]\t Training Loss 0.8362\t Accuracy 0.8487\n",
      "Epoch [7][20]\t Batch [12050][55000]\t Training Loss 0.8360\t Accuracy 0.8490\n",
      "Epoch [7][20]\t Batch [12100][55000]\t Training Loss 0.8360\t Accuracy 0.8489\n",
      "Epoch [7][20]\t Batch [12150][55000]\t Training Loss 0.8353\t Accuracy 0.8493\n",
      "Epoch [7][20]\t Batch [12200][55000]\t Training Loss 0.8357\t Accuracy 0.8493\n",
      "Epoch [7][20]\t Batch [12250][55000]\t Training Loss 0.8355\t Accuracy 0.8493\n",
      "Epoch [7][20]\t Batch [12300][55000]\t Training Loss 0.8355\t Accuracy 0.8493\n",
      "Epoch [7][20]\t Batch [12350][55000]\t Training Loss 0.8357\t Accuracy 0.8494\n",
      "Epoch [7][20]\t Batch [12400][55000]\t Training Loss 0.8361\t Accuracy 0.8494\n",
      "Epoch [7][20]\t Batch [12450][55000]\t Training Loss 0.8363\t Accuracy 0.8491\n",
      "Epoch [7][20]\t Batch [12500][55000]\t Training Loss 0.8365\t Accuracy 0.8487\n",
      "Epoch [7][20]\t Batch [12550][55000]\t Training Loss 0.8365\t Accuracy 0.8489\n",
      "Epoch [7][20]\t Batch [12600][55000]\t Training Loss 0.8372\t Accuracy 0.8486\n",
      "Epoch [7][20]\t Batch [12650][55000]\t Training Loss 0.8377\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [12700][55000]\t Training Loss 0.8384\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [12750][55000]\t Training Loss 0.8380\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [12800][55000]\t Training Loss 0.8385\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [12850][55000]\t Training Loss 0.8388\t Accuracy 0.8479\n",
      "Epoch [7][20]\t Batch [12900][55000]\t Training Loss 0.8387\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [12950][55000]\t Training Loss 0.8392\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [13000][55000]\t Training Loss 0.8392\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [13050][55000]\t Training Loss 0.8399\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [13100][55000]\t Training Loss 0.8405\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [13150][55000]\t Training Loss 0.8409\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [13200][55000]\t Training Loss 0.8410\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [13250][55000]\t Training Loss 0.8405\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [13300][55000]\t Training Loss 0.8405\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [13350][55000]\t Training Loss 0.8409\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [13400][55000]\t Training Loss 0.8411\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [13450][55000]\t Training Loss 0.8406\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [13500][55000]\t Training Loss 0.8403\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [13550][55000]\t Training Loss 0.8398\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [13600][55000]\t Training Loss 0.8390\t Accuracy 0.8479\n",
      "Epoch [7][20]\t Batch [13650][55000]\t Training Loss 0.8388\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [13700][55000]\t Training Loss 0.8397\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [13750][55000]\t Training Loss 0.8403\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [13800][55000]\t Training Loss 0.8404\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [13850][55000]\t Training Loss 0.8404\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [13900][55000]\t Training Loss 0.8406\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [13950][55000]\t Training Loss 0.8409\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [14000][55000]\t Training Loss 0.8417\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [14050][55000]\t Training Loss 0.8418\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [14100][55000]\t Training Loss 0.8419\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [14150][55000]\t Training Loss 0.8422\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [14200][55000]\t Training Loss 0.8421\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [14250][55000]\t Training Loss 0.8423\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [14300][55000]\t Training Loss 0.8427\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [14350][55000]\t Training Loss 0.8430\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [14400][55000]\t Training Loss 0.8439\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [14450][55000]\t Training Loss 0.8441\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [14500][55000]\t Training Loss 0.8442\t Accuracy 0.8453\n",
      "Epoch [7][20]\t Batch [14550][55000]\t Training Loss 0.8449\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [14600][55000]\t Training Loss 0.8449\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [14650][55000]\t Training Loss 0.8459\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [14700][55000]\t Training Loss 0.8466\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [14750][55000]\t Training Loss 0.8472\t Accuracy 0.8442\n",
      "Epoch [7][20]\t Batch [14800][55000]\t Training Loss 0.8482\t Accuracy 0.8440\n",
      "Epoch [7][20]\t Batch [14850][55000]\t Training Loss 0.8488\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [14900][55000]\t Training Loss 0.8487\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [14950][55000]\t Training Loss 0.8488\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [15000][55000]\t Training Loss 0.8487\t Accuracy 0.8435\n",
      "Epoch [7][20]\t Batch [15050][55000]\t Training Loss 0.8484\t Accuracy 0.8439\n",
      "Epoch [7][20]\t Batch [15100][55000]\t Training Loss 0.8481\t Accuracy 0.8440\n",
      "Epoch [7][20]\t Batch [15150][55000]\t Training Loss 0.8485\t Accuracy 0.8439\n",
      "Epoch [7][20]\t Batch [15200][55000]\t Training Loss 0.8487\t Accuracy 0.8439\n",
      "Epoch [7][20]\t Batch [15250][55000]\t Training Loss 0.8487\t Accuracy 0.8440\n",
      "Epoch [7][20]\t Batch [15300][55000]\t Training Loss 0.8487\t Accuracy 0.8440\n",
      "Epoch [7][20]\t Batch [15350][55000]\t Training Loss 0.8484\t Accuracy 0.8442\n",
      "Epoch [7][20]\t Batch [15400][55000]\t Training Loss 0.8487\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [15450][55000]\t Training Loss 0.8488\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [15500][55000]\t Training Loss 0.8486\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [15550][55000]\t Training Loss 0.8485\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [15600][55000]\t Training Loss 0.8483\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [15650][55000]\t Training Loss 0.8483\t Accuracy 0.8453\n",
      "Epoch [7][20]\t Batch [15700][55000]\t Training Loss 0.8481\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [15750][55000]\t Training Loss 0.8490\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [15800][55000]\t Training Loss 0.8494\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [15850][55000]\t Training Loss 0.8498\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [15900][55000]\t Training Loss 0.8503\t Accuracy 0.8443\n",
      "Epoch [7][20]\t Batch [15950][55000]\t Training Loss 0.8504\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [16000][55000]\t Training Loss 0.8504\t Accuracy 0.8441\n",
      "Epoch [7][20]\t Batch [16050][55000]\t Training Loss 0.8514\t Accuracy 0.8437\n",
      "Epoch [7][20]\t Batch [16100][55000]\t Training Loss 0.8513\t Accuracy 0.8437\n",
      "Epoch [7][20]\t Batch [16150][55000]\t Training Loss 0.8513\t Accuracy 0.8438\n",
      "Epoch [7][20]\t Batch [16200][55000]\t Training Loss 0.8514\t Accuracy 0.8437\n",
      "Epoch [7][20]\t Batch [16250][55000]\t Training Loss 0.8512\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [16300][55000]\t Training Loss 0.8509\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [16350][55000]\t Training Loss 0.8506\t Accuracy 0.8439\n",
      "Epoch [7][20]\t Batch [16400][55000]\t Training Loss 0.8506\t Accuracy 0.8439\n",
      "Epoch [7][20]\t Batch [16450][55000]\t Training Loss 0.8504\t Accuracy 0.8441\n",
      "Epoch [7][20]\t Batch [16500][55000]\t Training Loss 0.8502\t Accuracy 0.8443\n",
      "Epoch [7][20]\t Batch [16550][55000]\t Training Loss 0.8498\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [16600][55000]\t Training Loss 0.8498\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [16650][55000]\t Training Loss 0.8498\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [16700][55000]\t Training Loss 0.8500\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [16750][55000]\t Training Loss 0.8498\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [16800][55000]\t Training Loss 0.8504\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [16850][55000]\t Training Loss 0.8510\t Accuracy 0.8442\n",
      "Epoch [7][20]\t Batch [16900][55000]\t Training Loss 0.8512\t Accuracy 0.8442\n",
      "Epoch [7][20]\t Batch [16950][55000]\t Training Loss 0.8512\t Accuracy 0.8441\n",
      "Epoch [7][20]\t Batch [17000][55000]\t Training Loss 0.8520\t Accuracy 0.8437\n",
      "Epoch [7][20]\t Batch [17050][55000]\t Training Loss 0.8519\t Accuracy 0.8437\n",
      "Epoch [7][20]\t Batch [17100][55000]\t Training Loss 0.8523\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [17150][55000]\t Training Loss 0.8521\t Accuracy 0.8437\n",
      "Epoch [7][20]\t Batch [17200][55000]\t Training Loss 0.8522\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [17250][55000]\t Training Loss 0.8527\t Accuracy 0.8435\n",
      "Epoch [7][20]\t Batch [17300][55000]\t Training Loss 0.8525\t Accuracy 0.8435\n",
      "Epoch [7][20]\t Batch [17350][55000]\t Training Loss 0.8520\t Accuracy 0.8438\n",
      "Epoch [7][20]\t Batch [17400][55000]\t Training Loss 0.8521\t Accuracy 0.8438\n",
      "Epoch [7][20]\t Batch [17450][55000]\t Training Loss 0.8522\t Accuracy 0.8437\n",
      "Epoch [7][20]\t Batch [17500][55000]\t Training Loss 0.8523\t Accuracy 0.8438\n",
      "Epoch [7][20]\t Batch [17550][55000]\t Training Loss 0.8529\t Accuracy 0.8435\n",
      "Epoch [7][20]\t Batch [17600][55000]\t Training Loss 0.8536\t Accuracy 0.8433\n",
      "Epoch [7][20]\t Batch [17650][55000]\t Training Loss 0.8538\t Accuracy 0.8435\n",
      "Epoch [7][20]\t Batch [17700][55000]\t Training Loss 0.8545\t Accuracy 0.8433\n",
      "Epoch [7][20]\t Batch [17750][55000]\t Training Loss 0.8548\t Accuracy 0.8433\n",
      "Epoch [7][20]\t Batch [17800][55000]\t Training Loss 0.8552\t Accuracy 0.8430\n",
      "Epoch [7][20]\t Batch [17850][55000]\t Training Loss 0.8555\t Accuracy 0.8428\n",
      "Epoch [7][20]\t Batch [17900][55000]\t Training Loss 0.8558\t Accuracy 0.8427\n",
      "Epoch [7][20]\t Batch [17950][55000]\t Training Loss 0.8556\t Accuracy 0.8425\n",
      "Epoch [7][20]\t Batch [18000][55000]\t Training Loss 0.8553\t Accuracy 0.8427\n",
      "Epoch [7][20]\t Batch [18050][55000]\t Training Loss 0.8555\t Accuracy 0.8427\n",
      "Epoch [7][20]\t Batch [18100][55000]\t Training Loss 0.8554\t Accuracy 0.8428\n",
      "Epoch [7][20]\t Batch [18150][55000]\t Training Loss 0.8550\t Accuracy 0.8429\n",
      "Epoch [7][20]\t Batch [18200][55000]\t Training Loss 0.8545\t Accuracy 0.8431\n",
      "Epoch [7][20]\t Batch [18250][55000]\t Training Loss 0.8545\t Accuracy 0.8432\n",
      "Epoch [7][20]\t Batch [18300][55000]\t Training Loss 0.8540\t Accuracy 0.8433\n",
      "Epoch [7][20]\t Batch [18350][55000]\t Training Loss 0.8539\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [18400][55000]\t Training Loss 0.8539\t Accuracy 0.8435\n",
      "Epoch [7][20]\t Batch [18450][55000]\t Training Loss 0.8544\t Accuracy 0.8433\n",
      "Epoch [7][20]\t Batch [18500][55000]\t Training Loss 0.8544\t Accuracy 0.8433\n",
      "Epoch [7][20]\t Batch [18550][55000]\t Training Loss 0.8542\t Accuracy 0.8436\n",
      "Epoch [7][20]\t Batch [18600][55000]\t Training Loss 0.8542\t Accuracy 0.8438\n",
      "Epoch [7][20]\t Batch [18650][55000]\t Training Loss 0.8540\t Accuracy 0.8440\n",
      "Epoch [7][20]\t Batch [18700][55000]\t Training Loss 0.8542\t Accuracy 0.8438\n",
      "Epoch [7][20]\t Batch [18750][55000]\t Training Loss 0.8545\t Accuracy 0.8438\n",
      "Epoch [7][20]\t Batch [18800][55000]\t Training Loss 0.8541\t Accuracy 0.8443\n",
      "Epoch [7][20]\t Batch [18850][55000]\t Training Loss 0.8544\t Accuracy 0.8443\n",
      "Epoch [7][20]\t Batch [18900][55000]\t Training Loss 0.8538\t Accuracy 0.8446\n",
      "Epoch [7][20]\t Batch [18950][55000]\t Training Loss 0.8536\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [19000][55000]\t Training Loss 0.8535\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [19050][55000]\t Training Loss 0.8539\t Accuracy 0.8446\n",
      "Epoch [7][20]\t Batch [19100][55000]\t Training Loss 0.8542\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [19150][55000]\t Training Loss 0.8543\t Accuracy 0.8443\n",
      "Epoch [7][20]\t Batch [19200][55000]\t Training Loss 0.8544\t Accuracy 0.8442\n",
      "Epoch [7][20]\t Batch [19250][55000]\t Training Loss 0.8543\t Accuracy 0.8443\n",
      "Epoch [7][20]\t Batch [19300][55000]\t Training Loss 0.8541\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [19350][55000]\t Training Loss 0.8541\t Accuracy 0.8443\n",
      "Epoch [7][20]\t Batch [19400][55000]\t Training Loss 0.8539\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [19450][55000]\t Training Loss 0.8537\t Accuracy 0.8443\n",
      "Epoch [7][20]\t Batch [19500][55000]\t Training Loss 0.8533\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [19550][55000]\t Training Loss 0.8534\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [19600][55000]\t Training Loss 0.8533\t Accuracy 0.8441\n",
      "Epoch [7][20]\t Batch [19650][55000]\t Training Loss 0.8529\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [19700][55000]\t Training Loss 0.8524\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [19750][55000]\t Training Loss 0.8518\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [19800][55000]\t Training Loss 0.8512\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [19850][55000]\t Training Loss 0.8513\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [19900][55000]\t Training Loss 0.8510\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [19950][55000]\t Training Loss 0.8512\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [20000][55000]\t Training Loss 0.8511\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [20050][55000]\t Training Loss 0.8516\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [20100][55000]\t Training Loss 0.8518\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [20150][55000]\t Training Loss 0.8516\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [20200][55000]\t Training Loss 0.8519\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [20250][55000]\t Training Loss 0.8520\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [20300][55000]\t Training Loss 0.8521\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [20350][55000]\t Training Loss 0.8521\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [20400][55000]\t Training Loss 0.8518\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [20450][55000]\t Training Loss 0.8515\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [20500][55000]\t Training Loss 0.8512\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [20550][55000]\t Training Loss 0.8511\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [20600][55000]\t Training Loss 0.8511\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [20650][55000]\t Training Loss 0.8508\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [20700][55000]\t Training Loss 0.8506\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [20750][55000]\t Training Loss 0.8505\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [20800][55000]\t Training Loss 0.8506\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [20850][55000]\t Training Loss 0.8505\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [20900][55000]\t Training Loss 0.8508\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [20950][55000]\t Training Loss 0.8513\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [21000][55000]\t Training Loss 0.8515\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [21050][55000]\t Training Loss 0.8518\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [21100][55000]\t Training Loss 0.8515\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [21150][55000]\t Training Loss 0.8515\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [21200][55000]\t Training Loss 0.8512\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [21250][55000]\t Training Loss 0.8511\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [21300][55000]\t Training Loss 0.8506\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [21350][55000]\t Training Loss 0.8507\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [21400][55000]\t Training Loss 0.8507\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [21450][55000]\t Training Loss 0.8508\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [21500][55000]\t Training Loss 0.8504\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [21550][55000]\t Training Loss 0.8502\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [21600][55000]\t Training Loss 0.8504\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [21650][55000]\t Training Loss 0.8504\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [21700][55000]\t Training Loss 0.8503\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [21750][55000]\t Training Loss 0.8503\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [21800][55000]\t Training Loss 0.8498\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [21850][55000]\t Training Loss 0.8494\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [21900][55000]\t Training Loss 0.8490\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [21950][55000]\t Training Loss 0.8485\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [22000][55000]\t Training Loss 0.8481\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [22050][55000]\t Training Loss 0.8477\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [22100][55000]\t Training Loss 0.8479\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [22150][55000]\t Training Loss 0.8483\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [22200][55000]\t Training Loss 0.8485\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [22250][55000]\t Training Loss 0.8486\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [22300][55000]\t Training Loss 0.8487\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [22350][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [22400][55000]\t Training Loss 0.8484\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [22450][55000]\t Training Loss 0.8484\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [22500][55000]\t Training Loss 0.8488\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [22550][55000]\t Training Loss 0.8494\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [22600][55000]\t Training Loss 0.8498\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [22650][55000]\t Training Loss 0.8500\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [22700][55000]\t Training Loss 0.8499\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [22750][55000]\t Training Loss 0.8498\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [22800][55000]\t Training Loss 0.8497\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [22850][55000]\t Training Loss 0.8497\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [22900][55000]\t Training Loss 0.8494\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [22950][55000]\t Training Loss 0.8493\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [23000][55000]\t Training Loss 0.8490\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [23050][55000]\t Training Loss 0.8488\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [23100][55000]\t Training Loss 0.8489\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [23150][55000]\t Training Loss 0.8489\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [23200][55000]\t Training Loss 0.8489\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [23250][55000]\t Training Loss 0.8486\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [23300][55000]\t Training Loss 0.8483\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [23350][55000]\t Training Loss 0.8482\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [23400][55000]\t Training Loss 0.8479\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [23450][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [23500][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [23550][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [23600][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [23650][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [23700][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [23750][55000]\t Training Loss 0.8484\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [23800][55000]\t Training Loss 0.8481\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [23850][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [23900][55000]\t Training Loss 0.8482\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [23950][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [24000][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [24050][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [24100][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [24150][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [24200][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [24250][55000]\t Training Loss 0.8478\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [24300][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [24350][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [24400][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [24450][55000]\t Training Loss 0.8480\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [24500][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [24550][55000]\t Training Loss 0.8481\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [24600][55000]\t Training Loss 0.8481\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [24650][55000]\t Training Loss 0.8482\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [24700][55000]\t Training Loss 0.8482\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [24750][55000]\t Training Loss 0.8484\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [24800][55000]\t Training Loss 0.8488\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [24850][55000]\t Training Loss 0.8486\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [24900][55000]\t Training Loss 0.8487\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [24950][55000]\t Training Loss 0.8490\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [25000][55000]\t Training Loss 0.8492\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [25050][55000]\t Training Loss 0.8490\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [25100][55000]\t Training Loss 0.8489\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [25150][55000]\t Training Loss 0.8487\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [25200][55000]\t Training Loss 0.8486\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [25250][55000]\t Training Loss 0.8485\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [25300][55000]\t Training Loss 0.8485\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [25350][55000]\t Training Loss 0.8486\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [25400][55000]\t Training Loss 0.8481\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [25450][55000]\t Training Loss 0.8477\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [25500][55000]\t Training Loss 0.8475\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [25550][55000]\t Training Loss 0.8471\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [25600][55000]\t Training Loss 0.8471\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [25650][55000]\t Training Loss 0.8469\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [25700][55000]\t Training Loss 0.8468\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [25750][55000]\t Training Loss 0.8465\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [25800][55000]\t Training Loss 0.8464\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [25850][55000]\t Training Loss 0.8466\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [25900][55000]\t Training Loss 0.8466\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [25950][55000]\t Training Loss 0.8466\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [26000][55000]\t Training Loss 0.8466\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [26050][55000]\t Training Loss 0.8464\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [26100][55000]\t Training Loss 0.8461\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [26150][55000]\t Training Loss 0.8459\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [26200][55000]\t Training Loss 0.8457\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [26250][55000]\t Training Loss 0.8457\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [26300][55000]\t Training Loss 0.8459\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [26350][55000]\t Training Loss 0.8458\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [26400][55000]\t Training Loss 0.8462\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [26450][55000]\t Training Loss 0.8464\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [26500][55000]\t Training Loss 0.8465\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [26550][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [26600][55000]\t Training Loss 0.8469\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [26650][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [26700][55000]\t Training Loss 0.8472\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [26750][55000]\t Training Loss 0.8474\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [26800][55000]\t Training Loss 0.8473\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [26850][55000]\t Training Loss 0.8472\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [26900][55000]\t Training Loss 0.8475\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [26950][55000]\t Training Loss 0.8474\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [27000][55000]\t Training Loss 0.8472\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [27050][55000]\t Training Loss 0.8469\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [27100][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [27150][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [27200][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [27250][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [27300][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [27350][55000]\t Training Loss 0.8470\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [27400][55000]\t Training Loss 0.8470\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [27450][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [27500][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [27550][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [27600][55000]\t Training Loss 0.8468\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [27650][55000]\t Training Loss 0.8468\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [27700][55000]\t Training Loss 0.8469\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [27750][55000]\t Training Loss 0.8470\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [27800][55000]\t Training Loss 0.8471\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [27850][55000]\t Training Loss 0.8471\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [27900][55000]\t Training Loss 0.8470\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [27950][55000]\t Training Loss 0.8467\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [28000][55000]\t Training Loss 0.8465\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [28050][55000]\t Training Loss 0.8462\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [28100][55000]\t Training Loss 0.8458\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [28150][55000]\t Training Loss 0.8456\t Accuracy 0.8479\n",
      "Epoch [7][20]\t Batch [28200][55000]\t Training Loss 0.8457\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [28250][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [28300][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [28350][55000]\t Training Loss 0.8450\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [28400][55000]\t Training Loss 0.8455\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [28450][55000]\t Training Loss 0.8452\t Accuracy 0.8479\n",
      "Epoch [7][20]\t Batch [28500][55000]\t Training Loss 0.8450\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [28550][55000]\t Training Loss 0.8449\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [28600][55000]\t Training Loss 0.8448\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [28650][55000]\t Training Loss 0.8451\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [28700][55000]\t Training Loss 0.8453\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [28750][55000]\t Training Loss 0.8454\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [28800][55000]\t Training Loss 0.8454\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [28850][55000]\t Training Loss 0.8453\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [28900][55000]\t Training Loss 0.8452\t Accuracy 0.8481\n",
      "Epoch [7][20]\t Batch [28950][55000]\t Training Loss 0.8452\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [29000][55000]\t Training Loss 0.8451\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [29050][55000]\t Training Loss 0.8451\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [29100][55000]\t Training Loss 0.8453\t Accuracy 0.8480\n",
      "Epoch [7][20]\t Batch [29150][55000]\t Training Loss 0.8456\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [29200][55000]\t Training Loss 0.8460\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [29250][55000]\t Training Loss 0.8462\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [29300][55000]\t Training Loss 0.8460\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [29350][55000]\t Training Loss 0.8462\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [29400][55000]\t Training Loss 0.8461\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [29450][55000]\t Training Loss 0.8457\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [29500][55000]\t Training Loss 0.8455\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [29550][55000]\t Training Loss 0.8454\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [29600][55000]\t Training Loss 0.8454\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [29650][55000]\t Training Loss 0.8453\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [29700][55000]\t Training Loss 0.8454\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [29750][55000]\t Training Loss 0.8458\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [29800][55000]\t Training Loss 0.8459\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [29850][55000]\t Training Loss 0.8462\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [29900][55000]\t Training Loss 0.8466\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [29950][55000]\t Training Loss 0.8468\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [30000][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [30050][55000]\t Training Loss 0.8473\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [30100][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [30150][55000]\t Training Loss 0.8482\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [30200][55000]\t Training Loss 0.8484\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [30250][55000]\t Training Loss 0.8485\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [30300][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [30350][55000]\t Training Loss 0.8483\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [30400][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [30450][55000]\t Training Loss 0.8481\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [30500][55000]\t Training Loss 0.8484\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [30550][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [30600][55000]\t Training Loss 0.8488\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [30650][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [30700][55000]\t Training Loss 0.8495\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [30750][55000]\t Training Loss 0.8496\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [30800][55000]\t Training Loss 0.8498\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [30850][55000]\t Training Loss 0.8497\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [30900][55000]\t Training Loss 0.8501\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [30950][55000]\t Training Loss 0.8500\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [31000][55000]\t Training Loss 0.8500\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [31050][55000]\t Training Loss 0.8501\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [31100][55000]\t Training Loss 0.8501\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [31150][55000]\t Training Loss 0.8501\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [31200][55000]\t Training Loss 0.8502\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [31250][55000]\t Training Loss 0.8501\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [31300][55000]\t Training Loss 0.8504\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [31350][55000]\t Training Loss 0.8511\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [31400][55000]\t Training Loss 0.8512\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [31450][55000]\t Training Loss 0.8515\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [31500][55000]\t Training Loss 0.8516\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [31550][55000]\t Training Loss 0.8516\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [31600][55000]\t Training Loss 0.8518\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [31650][55000]\t Training Loss 0.8519\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [31700][55000]\t Training Loss 0.8522\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [31750][55000]\t Training Loss 0.8526\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [31800][55000]\t Training Loss 0.8527\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [31850][55000]\t Training Loss 0.8527\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [31900][55000]\t Training Loss 0.8527\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [31950][55000]\t Training Loss 0.8526\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [32000][55000]\t Training Loss 0.8526\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [32050][55000]\t Training Loss 0.8526\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [32100][55000]\t Training Loss 0.8525\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [32150][55000]\t Training Loss 0.8528\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [32200][55000]\t Training Loss 0.8529\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [32250][55000]\t Training Loss 0.8532\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [32300][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [32350][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [32400][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [32450][55000]\t Training Loss 0.8542\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [32500][55000]\t Training Loss 0.8544\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [32550][55000]\t Training Loss 0.8547\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [32600][55000]\t Training Loss 0.8546\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [32650][55000]\t Training Loss 0.8545\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [32700][55000]\t Training Loss 0.8545\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [32750][55000]\t Training Loss 0.8545\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [32800][55000]\t Training Loss 0.8546\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [32850][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [32900][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [32950][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [33000][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [33050][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [33100][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [33150][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [33200][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [33250][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [33300][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [33350][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [33400][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [33450][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [33500][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [33550][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [33600][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [33650][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [33700][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [33750][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [33800][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [33850][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [33900][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [33950][55000]\t Training Loss 0.8534\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [34000][55000]\t Training Loss 0.8534\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [34050][55000]\t Training Loss 0.8536\t Accuracy 0.8465\n",
      "Epoch [7][20]\t Batch [34100][55000]\t Training Loss 0.8537\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [34150][55000]\t Training Loss 0.8537\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [34200][55000]\t Training Loss 0.8535\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [34250][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [34300][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [34350][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [34400][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [34450][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [34500][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [34550][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [34600][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [34650][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [34700][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [34750][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [34800][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [34850][55000]\t Training Loss 0.8538\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [34900][55000]\t Training Loss 0.8538\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [34950][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [35000][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [35050][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [35100][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [35150][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [35200][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [35250][55000]\t Training Loss 0.8540\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [35300][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [35350][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [35400][55000]\t Training Loss 0.8538\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [35450][55000]\t Training Loss 0.8537\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [35500][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [35550][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [35600][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [35650][55000]\t Training Loss 0.8538\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [35700][55000]\t Training Loss 0.8538\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [35750][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [35800][55000]\t Training Loss 0.8536\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [35850][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [35900][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [35950][55000]\t Training Loss 0.8535\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [36000][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [36050][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [36100][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [36150][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [36200][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [36250][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [36300][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [36350][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [36400][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [36450][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [36500][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [36550][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [36600][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [36650][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [36700][55000]\t Training Loss 0.8526\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [36750][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [36800][55000]\t Training Loss 0.8523\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [36850][55000]\t Training Loss 0.8523\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [36900][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [36950][55000]\t Training Loss 0.8521\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [37000][55000]\t Training Loss 0.8520\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [37050][55000]\t Training Loss 0.8519\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [37100][55000]\t Training Loss 0.8520\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [37150][55000]\t Training Loss 0.8520\t Accuracy 0.8477\n",
      "Epoch [7][20]\t Batch [37200][55000]\t Training Loss 0.8519\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [37250][55000]\t Training Loss 0.8518\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [37300][55000]\t Training Loss 0.8518\t Accuracy 0.8478\n",
      "Epoch [7][20]\t Batch [37350][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [37400][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [37450][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [37500][55000]\t Training Loss 0.8526\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [37550][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [37600][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [37650][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [37700][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [37750][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [37800][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [37850][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [37900][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [37950][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [38000][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [38050][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [38100][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [38150][55000]\t Training Loss 0.8529\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [38200][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [38250][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [38300][55000]\t Training Loss 0.8530\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [38350][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [38400][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [38450][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [38500][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [38550][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [38600][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [38650][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [38700][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [38750][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [38800][55000]\t Training Loss 0.8533\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [38850][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [38900][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [38950][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [39000][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [39050][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [39100][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [39150][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [39200][55000]\t Training Loss 0.8520\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [39250][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [39300][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [7][20]\t Batch [39350][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [7][20]\t Batch [39400][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [7][20]\t Batch [39450][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [39500][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [39550][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [39600][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [39650][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [39700][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [39750][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [39800][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [39850][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [39900][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [39950][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40000][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40050][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [40100][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [40150][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [40200][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40250][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [40300][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40350][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40400][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40450][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40500][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [40550][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40600][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [40650][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [40700][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [40750][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40800][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40850][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [40900][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [40950][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [41000][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [41050][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [41100][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [41150][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [41200][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [41250][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [41300][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [7][20]\t Batch [41350][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [41400][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [41450][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [41500][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [41550][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [41600][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [41650][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [41700][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [7][20]\t Batch [41750][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [41800][55000]\t Training Loss 0.8541\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [41850][55000]\t Training Loss 0.8542\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [41900][55000]\t Training Loss 0.8541\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [41950][55000]\t Training Loss 0.8541\t Accuracy 0.8469\n",
      "Epoch [7][20]\t Batch [42000][55000]\t Training Loss 0.8541\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [42050][55000]\t Training Loss 0.8540\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [42100][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [7][20]\t Batch [42150][55000]\t Training Loss 0.8540\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [42200][55000]\t Training Loss 0.8540\t Accuracy 0.8467\n",
      "Epoch [7][20]\t Batch [42250][55000]\t Training Loss 0.8541\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [42300][55000]\t Training Loss 0.8541\t Accuracy 0.8466\n",
      "Epoch [7][20]\t Batch [42350][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [42400][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [42450][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [42500][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [42550][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [42600][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [42650][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [42700][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [42750][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [42800][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [42850][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [42900][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [42950][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [43000][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [43050][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [43100][55000]\t Training Loss 0.8552\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [43150][55000]\t Training Loss 0.8552\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [43200][55000]\t Training Loss 0.8551\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [43250][55000]\t Training Loss 0.8551\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [43300][55000]\t Training Loss 0.8550\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [43350][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [43400][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [43450][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [43500][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [43550][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [43600][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [43650][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [43700][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [43750][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [43800][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [43850][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [43900][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [43950][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [44000][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [44050][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [44100][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [44150][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [44200][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [44250][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [44300][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [44350][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [44400][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [44450][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [44500][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [44550][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [44600][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [44650][55000]\t Training Loss 0.8546\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [44700][55000]\t Training Loss 0.8545\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [44750][55000]\t Training Loss 0.8545\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [44800][55000]\t Training Loss 0.8545\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [44850][55000]\t Training Loss 0.8546\t Accuracy 0.8464\n",
      "Epoch [7][20]\t Batch [44900][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [44950][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [45000][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [45050][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45100][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45150][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [45200][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45250][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45300][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [45350][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [45400][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [45450][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [45500][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45550][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45600][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45650][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45700][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45750][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [45800][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45850][55000]\t Training Loss 0.8551\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [45900][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [45950][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [46000][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [46050][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [46100][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [46150][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46200][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46250][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46300][55000]\t Training Loss 0.8557\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [46350][55000]\t Training Loss 0.8558\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [46400][55000]\t Training Loss 0.8560\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [46450][55000]\t Training Loss 0.8561\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [46500][55000]\t Training Loss 0.8560\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [46550][55000]\t Training Loss 0.8559\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46600][55000]\t Training Loss 0.8558\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46650][55000]\t Training Loss 0.8558\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [46700][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46750][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46800][55000]\t Training Loss 0.8555\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46850][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46900][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [46950][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [47000][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47050][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47100][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [47150][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [47200][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47250][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47300][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47350][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47400][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47450][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [47500][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [47550][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [47600][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47650][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [47700][55000]\t Training Loss 0.8554\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [47750][55000]\t Training Loss 0.8553\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [47800][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [47850][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47900][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [47950][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [48000][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [48050][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [48100][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [48150][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [48200][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [48250][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [48300][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [48350][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [48400][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [48450][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [48500][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [48550][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [48600][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [48650][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [48700][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [48750][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [48800][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [48850][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [48900][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [48950][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [7][20]\t Batch [49000][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [7][20]\t Batch [49050][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [7][20]\t Batch [49100][55000]\t Training Loss 0.8541\t Accuracy 0.8458\n",
      "Epoch [7][20]\t Batch [49150][55000]\t Training Loss 0.8542\t Accuracy 0.8457\n",
      "Epoch [7][20]\t Batch [49200][55000]\t Training Loss 0.8542\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [49250][55000]\t Training Loss 0.8544\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [49300][55000]\t Training Loss 0.8543\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [49350][55000]\t Training Loss 0.8541\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [49400][55000]\t Training Loss 0.8540\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [49450][55000]\t Training Loss 0.8539\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [49500][55000]\t Training Loss 0.8541\t Accuracy 0.8454\n",
      "Epoch [7][20]\t Batch [49550][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [49600][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [49650][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [49700][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [49750][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [49800][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [49850][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [49900][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [49950][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [50000][55000]\t Training Loss 0.8554\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [50050][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [50100][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [50150][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [50200][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [50250][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [50300][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [50350][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [50400][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [50450][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [50500][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [50550][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [50600][55000]\t Training Loss 0.8561\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [50650][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [50700][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [50750][55000]\t Training Loss 0.8562\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [50800][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [50850][55000]\t Training Loss 0.8562\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [50900][55000]\t Training Loss 0.8561\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [50950][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [51000][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [51050][55000]\t Training Loss 0.8557\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [51100][55000]\t Training Loss 0.8556\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [51150][55000]\t Training Loss 0.8556\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [51200][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [51250][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [51300][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [51350][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [51400][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [51450][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [51500][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [51550][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [51600][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [51650][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [51700][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [51750][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [51800][55000]\t Training Loss 0.8553\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [51850][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [51900][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [51950][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [52000][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [52050][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [52100][55000]\t Training Loss 0.8553\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [52150][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [52200][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [52250][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [52300][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [52350][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [52400][55000]\t Training Loss 0.8559\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [52450][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [52500][55000]\t Training Loss 0.8556\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [52550][55000]\t Training Loss 0.8554\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [52600][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [52650][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [7][20]\t Batch [52700][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [52750][55000]\t Training Loss 0.8553\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [52800][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [52850][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [52900][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [52950][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [53000][55000]\t Training Loss 0.8559\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53050][55000]\t Training Loss 0.8559\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [53100][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53150][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53200][55000]\t Training Loss 0.8559\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53250][55000]\t Training Loss 0.8560\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53300][55000]\t Training Loss 0.8560\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [53350][55000]\t Training Loss 0.8559\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53400][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53450][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53500][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53550][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53600][55000]\t Training Loss 0.8558\t Accuracy 0.8446\n",
      "Epoch [7][20]\t Batch [53650][55000]\t Training Loss 0.8558\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [53700][55000]\t Training Loss 0.8558\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [53750][55000]\t Training Loss 0.8557\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [53800][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53850][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [53900][55000]\t Training Loss 0.8556\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [53950][55000]\t Training Loss 0.8556\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [54000][55000]\t Training Loss 0.8558\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [54050][55000]\t Training Loss 0.8559\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [54100][55000]\t Training Loss 0.8560\t Accuracy 0.8446\n",
      "Epoch [7][20]\t Batch [54150][55000]\t Training Loss 0.8560\t Accuracy 0.8446\n",
      "Epoch [7][20]\t Batch [54200][55000]\t Training Loss 0.8560\t Accuracy 0.8445\n",
      "Epoch [7][20]\t Batch [54250][55000]\t Training Loss 0.8559\t Accuracy 0.8446\n",
      "Epoch [7][20]\t Batch [54300][55000]\t Training Loss 0.8559\t Accuracy 0.8446\n",
      "Epoch [7][20]\t Batch [54350][55000]\t Training Loss 0.8558\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [54400][55000]\t Training Loss 0.8558\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [54450][55000]\t Training Loss 0.8557\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [54500][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [54550][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [54600][55000]\t Training Loss 0.8555\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [54650][55000]\t Training Loss 0.8554\t Accuracy 0.8447\n",
      "Epoch [7][20]\t Batch [54700][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [54750][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [54800][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [54850][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [54900][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [7][20]\t Batch [54950][55000]\t Training Loss 0.8554\t Accuracy 0.8447\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8555\t Average training accuracy 0.8447\n",
      "Epoch [7]\t Average validation loss 0.7930\t Average validation accuracy 0.8698\n",
      "\n",
      "Epoch [8][20]\t Batch [0][55000]\t Training Loss 1.4743\t Accuracy 0.0000\n",
      "Epoch [8][20]\t Batch [50][55000]\t Training Loss 0.9402\t Accuracy 0.7255\n",
      "Epoch [8][20]\t Batch [100][55000]\t Training Loss 0.8501\t Accuracy 0.8020\n",
      "Epoch [8][20]\t Batch [150][55000]\t Training Loss 0.8512\t Accuracy 0.8146\n",
      "Epoch [8][20]\t Batch [200][55000]\t Training Loss 0.8619\t Accuracy 0.8159\n",
      "Epoch [8][20]\t Batch [250][55000]\t Training Loss 0.8442\t Accuracy 0.8247\n",
      "Epoch [8][20]\t Batch [300][55000]\t Training Loss 0.8457\t Accuracy 0.8239\n",
      "Epoch [8][20]\t Batch [350][55000]\t Training Loss 0.8260\t Accuracy 0.8262\n",
      "Epoch [8][20]\t Batch [400][55000]\t Training Loss 0.8121\t Accuracy 0.8304\n",
      "Epoch [8][20]\t Batch [450][55000]\t Training Loss 0.8179\t Accuracy 0.8315\n",
      "Epoch [8][20]\t Batch [500][55000]\t Training Loss 0.8242\t Accuracy 0.8343\n",
      "Epoch [8][20]\t Batch [550][55000]\t Training Loss 0.8415\t Accuracy 0.8312\n",
      "Epoch [8][20]\t Batch [600][55000]\t Training Loss 0.8459\t Accuracy 0.8336\n",
      "Epoch [8][20]\t Batch [650][55000]\t Training Loss 0.8658\t Accuracy 0.8233\n",
      "Epoch [8][20]\t Batch [700][55000]\t Training Loss 0.8634\t Accuracy 0.8288\n",
      "Epoch [8][20]\t Batch [750][55000]\t Training Loss 0.8597\t Accuracy 0.8296\n",
      "Epoch [8][20]\t Batch [800][55000]\t Training Loss 0.8537\t Accuracy 0.8315\n",
      "Epoch [8][20]\t Batch [850][55000]\t Training Loss 0.8521\t Accuracy 0.8331\n",
      "Epoch [8][20]\t Batch [900][55000]\t Training Loss 0.8586\t Accuracy 0.8324\n",
      "Epoch [8][20]\t Batch [950][55000]\t Training Loss 0.8654\t Accuracy 0.8275\n",
      "Epoch [8][20]\t Batch [1000][55000]\t Training Loss 0.8648\t Accuracy 0.8302\n",
      "Epoch [8][20]\t Batch [1050][55000]\t Training Loss 0.8725\t Accuracy 0.8287\n",
      "Epoch [8][20]\t Batch [1100][55000]\t Training Loss 0.8809\t Accuracy 0.8283\n",
      "Epoch [8][20]\t Batch [1150][55000]\t Training Loss 0.8901\t Accuracy 0.8254\n",
      "Epoch [8][20]\t Batch [1200][55000]\t Training Loss 0.8844\t Accuracy 0.8285\n",
      "Epoch [8][20]\t Batch [1250][55000]\t Training Loss 0.8857\t Accuracy 0.8281\n",
      "Epoch [8][20]\t Batch [1300][55000]\t Training Loss 0.8860\t Accuracy 0.8278\n",
      "Epoch [8][20]\t Batch [1350][55000]\t Training Loss 0.8828\t Accuracy 0.8290\n",
      "Epoch [8][20]\t Batch [1400][55000]\t Training Loss 0.8834\t Accuracy 0.8280\n",
      "Epoch [8][20]\t Batch [1450][55000]\t Training Loss 0.8843\t Accuracy 0.8270\n",
      "Epoch [8][20]\t Batch [1500][55000]\t Training Loss 0.8824\t Accuracy 0.8301\n",
      "Epoch [8][20]\t Batch [1550][55000]\t Training Loss 0.8819\t Accuracy 0.8304\n",
      "Epoch [8][20]\t Batch [1600][55000]\t Training Loss 0.8845\t Accuracy 0.8295\n",
      "Epoch [8][20]\t Batch [1650][55000]\t Training Loss 0.8814\t Accuracy 0.8310\n",
      "Epoch [8][20]\t Batch [1700][55000]\t Training Loss 0.8797\t Accuracy 0.8325\n",
      "Epoch [8][20]\t Batch [1750][55000]\t Training Loss 0.8741\t Accuracy 0.8332\n",
      "Epoch [8][20]\t Batch [1800][55000]\t Training Loss 0.8719\t Accuracy 0.8351\n",
      "Epoch [8][20]\t Batch [1850][55000]\t Training Loss 0.8703\t Accuracy 0.8358\n",
      "Epoch [8][20]\t Batch [1900][55000]\t Training Loss 0.8681\t Accuracy 0.8359\n",
      "Epoch [8][20]\t Batch [1950][55000]\t Training Loss 0.8666\t Accuracy 0.8370\n",
      "Epoch [8][20]\t Batch [2000][55000]\t Training Loss 0.8642\t Accuracy 0.8381\n",
      "Epoch [8][20]\t Batch [2050][55000]\t Training Loss 0.8632\t Accuracy 0.8386\n",
      "Epoch [8][20]\t Batch [2100][55000]\t Training Loss 0.8604\t Accuracy 0.8391\n",
      "Epoch [8][20]\t Batch [2150][55000]\t Training Loss 0.8571\t Accuracy 0.8410\n",
      "Epoch [8][20]\t Batch [2200][55000]\t Training Loss 0.8523\t Accuracy 0.8428\n",
      "Epoch [8][20]\t Batch [2250][55000]\t Training Loss 0.8515\t Accuracy 0.8432\n",
      "Epoch [8][20]\t Batch [2300][55000]\t Training Loss 0.8486\t Accuracy 0.8427\n",
      "Epoch [8][20]\t Batch [2350][55000]\t Training Loss 0.8470\t Accuracy 0.8430\n",
      "Epoch [8][20]\t Batch [2400][55000]\t Training Loss 0.8480\t Accuracy 0.8413\n",
      "Epoch [8][20]\t Batch [2450][55000]\t Training Loss 0.8509\t Accuracy 0.8401\n",
      "Epoch [8][20]\t Batch [2500][55000]\t Training Loss 0.8487\t Accuracy 0.8421\n",
      "Epoch [8][20]\t Batch [2550][55000]\t Training Loss 0.8474\t Accuracy 0.8420\n",
      "Epoch [8][20]\t Batch [2600][55000]\t Training Loss 0.8468\t Accuracy 0.8416\n",
      "Epoch [8][20]\t Batch [2650][55000]\t Training Loss 0.8456\t Accuracy 0.8419\n",
      "Epoch [8][20]\t Batch [2700][55000]\t Training Loss 0.8454\t Accuracy 0.8423\n",
      "Epoch [8][20]\t Batch [2750][55000]\t Training Loss 0.8453\t Accuracy 0.8422\n",
      "Epoch [8][20]\t Batch [2800][55000]\t Training Loss 0.8456\t Accuracy 0.8404\n",
      "Epoch [8][20]\t Batch [2850][55000]\t Training Loss 0.8444\t Accuracy 0.8404\n",
      "Epoch [8][20]\t Batch [2900][55000]\t Training Loss 0.8412\t Accuracy 0.8414\n",
      "Epoch [8][20]\t Batch [2950][55000]\t Training Loss 0.8413\t Accuracy 0.8411\n",
      "Epoch [8][20]\t Batch [3000][55000]\t Training Loss 0.8411\t Accuracy 0.8417\n",
      "Epoch [8][20]\t Batch [3050][55000]\t Training Loss 0.8422\t Accuracy 0.8414\n",
      "Epoch [8][20]\t Batch [3100][55000]\t Training Loss 0.8445\t Accuracy 0.8410\n",
      "Epoch [8][20]\t Batch [3150][55000]\t Training Loss 0.8444\t Accuracy 0.8413\n",
      "Epoch [8][20]\t Batch [3200][55000]\t Training Loss 0.8441\t Accuracy 0.8425\n",
      "Epoch [8][20]\t Batch [3250][55000]\t Training Loss 0.8435\t Accuracy 0.8419\n",
      "Epoch [8][20]\t Batch [3300][55000]\t Training Loss 0.8448\t Accuracy 0.8419\n",
      "Epoch [8][20]\t Batch [3350][55000]\t Training Loss 0.8430\t Accuracy 0.8433\n",
      "Epoch [8][20]\t Batch [3400][55000]\t Training Loss 0.8454\t Accuracy 0.8418\n",
      "Epoch [8][20]\t Batch [3450][55000]\t Training Loss 0.8450\t Accuracy 0.8427\n",
      "Epoch [8][20]\t Batch [3500][55000]\t Training Loss 0.8450\t Accuracy 0.8423\n",
      "Epoch [8][20]\t Batch [3550][55000]\t Training Loss 0.8466\t Accuracy 0.8417\n",
      "Epoch [8][20]\t Batch [3600][55000]\t Training Loss 0.8464\t Accuracy 0.8412\n",
      "Epoch [8][20]\t Batch [3650][55000]\t Training Loss 0.8450\t Accuracy 0.8420\n",
      "Epoch [8][20]\t Batch [3700][55000]\t Training Loss 0.8458\t Accuracy 0.8414\n",
      "Epoch [8][20]\t Batch [3750][55000]\t Training Loss 0.8457\t Accuracy 0.8416\n",
      "Epoch [8][20]\t Batch [3800][55000]\t Training Loss 0.8460\t Accuracy 0.8416\n",
      "Epoch [8][20]\t Batch [3850][55000]\t Training Loss 0.8456\t Accuracy 0.8421\n",
      "Epoch [8][20]\t Batch [3900][55000]\t Training Loss 0.8445\t Accuracy 0.8434\n",
      "Epoch [8][20]\t Batch [3950][55000]\t Training Loss 0.8434\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [4000][55000]\t Training Loss 0.8430\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [4050][55000]\t Training Loss 0.8415\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [4100][55000]\t Training Loss 0.8419\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [4150][55000]\t Training Loss 0.8424\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [4200][55000]\t Training Loss 0.8429\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [4250][55000]\t Training Loss 0.8414\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [4300][55000]\t Training Loss 0.8415\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [4350][55000]\t Training Loss 0.8420\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [4400][55000]\t Training Loss 0.8419\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [4450][55000]\t Training Loss 0.8421\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [4500][55000]\t Training Loss 0.8423\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [4550][55000]\t Training Loss 0.8406\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [4600][55000]\t Training Loss 0.8389\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [4650][55000]\t Training Loss 0.8388\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [4700][55000]\t Training Loss 0.8384\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [4750][55000]\t Training Loss 0.8372\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [4800][55000]\t Training Loss 0.8379\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [4850][55000]\t Training Loss 0.8391\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [4900][55000]\t Training Loss 0.8376\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [4950][55000]\t Training Loss 0.8379\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [5000][55000]\t Training Loss 0.8380\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [5050][55000]\t Training Loss 0.8375\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [5100][55000]\t Training Loss 0.8375\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [5150][55000]\t Training Loss 0.8379\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [5200][55000]\t Training Loss 0.8393\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [5250][55000]\t Training Loss 0.8384\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [5300][55000]\t Training Loss 0.8383\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [5350][55000]\t Training Loss 0.8390\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [5400][55000]\t Training Loss 0.8382\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [5450][55000]\t Training Loss 0.8375\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [5500][55000]\t Training Loss 0.8357\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [5550][55000]\t Training Loss 0.8357\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [5600][55000]\t Training Loss 0.8348\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [5650][55000]\t Training Loss 0.8353\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [5700][55000]\t Training Loss 0.8350\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [5750][55000]\t Training Loss 0.8355\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [5800][55000]\t Training Loss 0.8358\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [5850][55000]\t Training Loss 0.8360\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [5900][55000]\t Training Loss 0.8365\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [5950][55000]\t Training Loss 0.8364\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [6000][55000]\t Training Loss 0.8354\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [6050][55000]\t Training Loss 0.8340\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [6100][55000]\t Training Loss 0.8325\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [6150][55000]\t Training Loss 0.8304\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [6200][55000]\t Training Loss 0.8305\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [6250][55000]\t Training Loss 0.8293\t Accuracy 0.8488\n",
      "Epoch [8][20]\t Batch [6300][55000]\t Training Loss 0.8297\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [6350][55000]\t Training Loss 0.8294\t Accuracy 0.8488\n",
      "Epoch [8][20]\t Batch [6400][55000]\t Training Loss 0.8288\t Accuracy 0.8492\n",
      "Epoch [8][20]\t Batch [6450][55000]\t Training Loss 0.8282\t Accuracy 0.8495\n",
      "Epoch [8][20]\t Batch [6500][55000]\t Training Loss 0.8290\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [6550][55000]\t Training Loss 0.8280\t Accuracy 0.8493\n",
      "Epoch [8][20]\t Batch [6600][55000]\t Training Loss 0.8267\t Accuracy 0.8499\n",
      "Epoch [8][20]\t Batch [6650][55000]\t Training Loss 0.8255\t Accuracy 0.8499\n",
      "Epoch [8][20]\t Batch [6700][55000]\t Training Loss 0.8257\t Accuracy 0.8499\n",
      "Epoch [8][20]\t Batch [6750][55000]\t Training Loss 0.8259\t Accuracy 0.8507\n",
      "Epoch [8][20]\t Batch [6800][55000]\t Training Loss 0.8264\t Accuracy 0.8506\n",
      "Epoch [8][20]\t Batch [6850][55000]\t Training Loss 0.8285\t Accuracy 0.8502\n",
      "Epoch [8][20]\t Batch [6900][55000]\t Training Loss 0.8285\t Accuracy 0.8502\n",
      "Epoch [8][20]\t Batch [6950][55000]\t Training Loss 0.8291\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [7000][55000]\t Training Loss 0.8290\t Accuracy 0.8492\n",
      "Epoch [8][20]\t Batch [7050][55000]\t Training Loss 0.8296\t Accuracy 0.8487\n",
      "Epoch [8][20]\t Batch [7100][55000]\t Training Loss 0.8298\t Accuracy 0.8486\n",
      "Epoch [8][20]\t Batch [7150][55000]\t Training Loss 0.8300\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [7200][55000]\t Training Loss 0.8308\t Accuracy 0.8490\n",
      "Epoch [8][20]\t Batch [7250][55000]\t Training Loss 0.8331\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [7300][55000]\t Training Loss 0.8350\t Accuracy 0.8480\n",
      "Epoch [8][20]\t Batch [7350][55000]\t Training Loss 0.8364\t Accuracy 0.8480\n",
      "Epoch [8][20]\t Batch [7400][55000]\t Training Loss 0.8375\t Accuracy 0.8481\n",
      "Epoch [8][20]\t Batch [7450][55000]\t Training Loss 0.8374\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [7500][55000]\t Training Loss 0.8374\t Accuracy 0.8480\n",
      "Epoch [8][20]\t Batch [7550][55000]\t Training Loss 0.8379\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [7600][55000]\t Training Loss 0.8375\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [7650][55000]\t Training Loss 0.8379\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [7700][55000]\t Training Loss 0.8384\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [7750][55000]\t Training Loss 0.8385\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [7800][55000]\t Training Loss 0.8393\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [7850][55000]\t Training Loss 0.8394\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [7900][55000]\t Training Loss 0.8396\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [7950][55000]\t Training Loss 0.8397\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [8000][55000]\t Training Loss 0.8397\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [8050][55000]\t Training Loss 0.8402\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [8100][55000]\t Training Loss 0.8388\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [8150][55000]\t Training Loss 0.8396\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [8200][55000]\t Training Loss 0.8396\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [8250][55000]\t Training Loss 0.8408\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [8300][55000]\t Training Loss 0.8411\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [8350][55000]\t Training Loss 0.8418\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [8400][55000]\t Training Loss 0.8411\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [8450][55000]\t Training Loss 0.8426\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [8500][55000]\t Training Loss 0.8418\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [8550][55000]\t Training Loss 0.8409\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [8600][55000]\t Training Loss 0.8400\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [8650][55000]\t Training Loss 0.8401\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [8700][55000]\t Training Loss 0.8407\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [8750][55000]\t Training Loss 0.8423\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [8800][55000]\t Training Loss 0.8431\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [8850][55000]\t Training Loss 0.8431\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [8900][55000]\t Training Loss 0.8448\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [8950][55000]\t Training Loss 0.8444\t Accuracy 0.8455\n",
      "Epoch [8][20]\t Batch [9000][55000]\t Training Loss 0.8437\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [9050][55000]\t Training Loss 0.8428\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [9100][55000]\t Training Loss 0.8426\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [9150][55000]\t Training Loss 0.8431\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [9200][55000]\t Training Loss 0.8427\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [9250][55000]\t Training Loss 0.8430\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [9300][55000]\t Training Loss 0.8432\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [9350][55000]\t Training Loss 0.8434\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [9400][55000]\t Training Loss 0.8437\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [9450][55000]\t Training Loss 0.8439\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [9500][55000]\t Training Loss 0.8430\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [9550][55000]\t Training Loss 0.8428\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [9600][55000]\t Training Loss 0.8432\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [9650][55000]\t Training Loss 0.8430\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [9700][55000]\t Training Loss 0.8425\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [9750][55000]\t Training Loss 0.8417\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [9800][55000]\t Training Loss 0.8422\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [9850][55000]\t Training Loss 0.8418\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [9900][55000]\t Training Loss 0.8413\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [9950][55000]\t Training Loss 0.8407\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [10000][55000]\t Training Loss 0.8404\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [10050][55000]\t Training Loss 0.8406\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [10100][55000]\t Training Loss 0.8404\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [10150][55000]\t Training Loss 0.8402\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [10200][55000]\t Training Loss 0.8404\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [10250][55000]\t Training Loss 0.8405\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [10300][55000]\t Training Loss 0.8404\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [10350][55000]\t Training Loss 0.8395\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [10400][55000]\t Training Loss 0.8387\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [10450][55000]\t Training Loss 0.8385\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [10500][55000]\t Training Loss 0.8375\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [10550][55000]\t Training Loss 0.8371\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [10600][55000]\t Training Loss 0.8366\t Accuracy 0.8488\n",
      "Epoch [8][20]\t Batch [10650][55000]\t Training Loss 0.8363\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [10700][55000]\t Training Loss 0.8361\t Accuracy 0.8495\n",
      "Epoch [8][20]\t Batch [10750][55000]\t Training Loss 0.8366\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [10800][55000]\t Training Loss 0.8371\t Accuracy 0.8489\n",
      "Epoch [8][20]\t Batch [10850][55000]\t Training Loss 0.8364\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [10900][55000]\t Training Loss 0.8359\t Accuracy 0.8494\n",
      "Epoch [8][20]\t Batch [10950][55000]\t Training Loss 0.8356\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [11000][55000]\t Training Loss 0.8355\t Accuracy 0.8494\n",
      "Epoch [8][20]\t Batch [11050][55000]\t Training Loss 0.8348\t Accuracy 0.8496\n",
      "Epoch [8][20]\t Batch [11100][55000]\t Training Loss 0.8341\t Accuracy 0.8497\n",
      "Epoch [8][20]\t Batch [11150][55000]\t Training Loss 0.8341\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [11200][55000]\t Training Loss 0.8337\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [11250][55000]\t Training Loss 0.8341\t Accuracy 0.8499\n",
      "Epoch [8][20]\t Batch [11300][55000]\t Training Loss 0.8337\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [11350][55000]\t Training Loss 0.8332\t Accuracy 0.8501\n",
      "Epoch [8][20]\t Batch [11400][55000]\t Training Loss 0.8333\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [11450][55000]\t Training Loss 0.8330\t Accuracy 0.8501\n",
      "Epoch [8][20]\t Batch [11500][55000]\t Training Loss 0.8327\t Accuracy 0.8499\n",
      "Epoch [8][20]\t Batch [11550][55000]\t Training Loss 0.8328\t Accuracy 0.8498\n",
      "Epoch [8][20]\t Batch [11600][55000]\t Training Loss 0.8340\t Accuracy 0.8492\n",
      "Epoch [8][20]\t Batch [11650][55000]\t Training Loss 0.8345\t Accuracy 0.8489\n",
      "Epoch [8][20]\t Batch [11700][55000]\t Training Loss 0.8345\t Accuracy 0.8490\n",
      "Epoch [8][20]\t Batch [11750][55000]\t Training Loss 0.8354\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [11800][55000]\t Training Loss 0.8357\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [11850][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [11900][55000]\t Training Loss 0.8360\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [11950][55000]\t Training Loss 0.8361\t Accuracy 0.8487\n",
      "Epoch [8][20]\t Batch [12000][55000]\t Training Loss 0.8361\t Accuracy 0.8490\n",
      "Epoch [8][20]\t Batch [12050][55000]\t Training Loss 0.8359\t Accuracy 0.8493\n",
      "Epoch [8][20]\t Batch [12100][55000]\t Training Loss 0.8358\t Accuracy 0.8492\n",
      "Epoch [8][20]\t Batch [12150][55000]\t Training Loss 0.8352\t Accuracy 0.8496\n",
      "Epoch [8][20]\t Batch [12200][55000]\t Training Loss 0.8355\t Accuracy 0.8496\n",
      "Epoch [8][20]\t Batch [12250][55000]\t Training Loss 0.8354\t Accuracy 0.8496\n",
      "Epoch [8][20]\t Batch [12300][55000]\t Training Loss 0.8354\t Accuracy 0.8496\n",
      "Epoch [8][20]\t Batch [12350][55000]\t Training Loss 0.8356\t Accuracy 0.8497\n",
      "Epoch [8][20]\t Batch [12400][55000]\t Training Loss 0.8359\t Accuracy 0.8498\n",
      "Epoch [8][20]\t Batch [12450][55000]\t Training Loss 0.8361\t Accuracy 0.8494\n",
      "Epoch [8][20]\t Batch [12500][55000]\t Training Loss 0.8364\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [12550][55000]\t Training Loss 0.8364\t Accuracy 0.8492\n",
      "Epoch [8][20]\t Batch [12600][55000]\t Training Loss 0.8371\t Accuracy 0.8489\n",
      "Epoch [8][20]\t Batch [12650][55000]\t Training Loss 0.8376\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [12700][55000]\t Training Loss 0.8383\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [12750][55000]\t Training Loss 0.8378\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [12800][55000]\t Training Loss 0.8384\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [12850][55000]\t Training Loss 0.8386\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [12900][55000]\t Training Loss 0.8386\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [12950][55000]\t Training Loss 0.8391\t Accuracy 0.8481\n",
      "Epoch [8][20]\t Batch [13000][55000]\t Training Loss 0.8391\t Accuracy 0.8481\n",
      "Epoch [8][20]\t Batch [13050][55000]\t Training Loss 0.8397\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [13100][55000]\t Training Loss 0.8404\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [13150][55000]\t Training Loss 0.8407\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [13200][55000]\t Training Loss 0.8409\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [13250][55000]\t Training Loss 0.8404\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [13300][55000]\t Training Loss 0.8403\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [13350][55000]\t Training Loss 0.8407\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [13400][55000]\t Training Loss 0.8410\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [13450][55000]\t Training Loss 0.8405\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [13500][55000]\t Training Loss 0.8401\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [13550][55000]\t Training Loss 0.8397\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [13600][55000]\t Training Loss 0.8389\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [13650][55000]\t Training Loss 0.8387\t Accuracy 0.8481\n",
      "Epoch [8][20]\t Batch [13700][55000]\t Training Loss 0.8395\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [13750][55000]\t Training Loss 0.8402\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [13800][55000]\t Training Loss 0.8403\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [13850][55000]\t Training Loss 0.8402\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [13900][55000]\t Training Loss 0.8404\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [13950][55000]\t Training Loss 0.8408\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [14000][55000]\t Training Loss 0.8415\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [14050][55000]\t Training Loss 0.8417\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [14100][55000]\t Training Loss 0.8418\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [14150][55000]\t Training Loss 0.8420\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [14200][55000]\t Training Loss 0.8419\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [14250][55000]\t Training Loss 0.8422\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [14300][55000]\t Training Loss 0.8425\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [14350][55000]\t Training Loss 0.8429\t Accuracy 0.8455\n",
      "Epoch [8][20]\t Batch [14400][55000]\t Training Loss 0.8437\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [14450][55000]\t Training Loss 0.8439\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [14500][55000]\t Training Loss 0.8440\t Accuracy 0.8455\n",
      "Epoch [8][20]\t Batch [14550][55000]\t Training Loss 0.8447\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [14600][55000]\t Training Loss 0.8448\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [14650][55000]\t Training Loss 0.8457\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [14700][55000]\t Training Loss 0.8465\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [14750][55000]\t Training Loss 0.8471\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [14800][55000]\t Training Loss 0.8480\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [14850][55000]\t Training Loss 0.8486\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [14900][55000]\t Training Loss 0.8485\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [14950][55000]\t Training Loss 0.8487\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [15000][55000]\t Training Loss 0.8486\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [15050][55000]\t Training Loss 0.8482\t Accuracy 0.8441\n",
      "Epoch [8][20]\t Batch [15100][55000]\t Training Loss 0.8480\t Accuracy 0.8442\n",
      "Epoch [8][20]\t Batch [15150][55000]\t Training Loss 0.8484\t Accuracy 0.8442\n",
      "Epoch [8][20]\t Batch [15200][55000]\t Training Loss 0.8486\t Accuracy 0.8442\n",
      "Epoch [8][20]\t Batch [15250][55000]\t Training Loss 0.8485\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [15300][55000]\t Training Loss 0.8485\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [15350][55000]\t Training Loss 0.8483\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [15400][55000]\t Training Loss 0.8486\t Accuracy 0.8447\n",
      "Epoch [8][20]\t Batch [15450][55000]\t Training Loss 0.8487\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [15500][55000]\t Training Loss 0.8485\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [15550][55000]\t Training Loss 0.8483\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [15600][55000]\t Training Loss 0.8482\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [15650][55000]\t Training Loss 0.8481\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [15700][55000]\t Training Loss 0.8480\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [15750][55000]\t Training Loss 0.8489\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [15800][55000]\t Training Loss 0.8493\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [15850][55000]\t Training Loss 0.8496\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [15900][55000]\t Training Loss 0.8502\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [15950][55000]\t Training Loss 0.8502\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [16000][55000]\t Training Loss 0.8503\t Accuracy 0.8444\n",
      "Epoch [8][20]\t Batch [16050][55000]\t Training Loss 0.8512\t Accuracy 0.8440\n",
      "Epoch [8][20]\t Batch [16100][55000]\t Training Loss 0.8512\t Accuracy 0.8440\n",
      "Epoch [8][20]\t Batch [16150][55000]\t Training Loss 0.8511\t Accuracy 0.8441\n",
      "Epoch [8][20]\t Batch [16200][55000]\t Training Loss 0.8513\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [16250][55000]\t Training Loss 0.8511\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [16300][55000]\t Training Loss 0.8508\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [16350][55000]\t Training Loss 0.8504\t Accuracy 0.8441\n",
      "Epoch [8][20]\t Batch [16400][55000]\t Training Loss 0.8505\t Accuracy 0.8441\n",
      "Epoch [8][20]\t Batch [16450][55000]\t Training Loss 0.8502\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [16500][55000]\t Training Loss 0.8500\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [16550][55000]\t Training Loss 0.8496\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [16600][55000]\t Training Loss 0.8497\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [16650][55000]\t Training Loss 0.8497\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [16700][55000]\t Training Loss 0.8498\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [16750][55000]\t Training Loss 0.8497\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [16800][55000]\t Training Loss 0.8503\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [16850][55000]\t Training Loss 0.8509\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [16900][55000]\t Training Loss 0.8511\t Accuracy 0.8444\n",
      "Epoch [8][20]\t Batch [16950][55000]\t Training Loss 0.8511\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [17000][55000]\t Training Loss 0.8518\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [17050][55000]\t Training Loss 0.8517\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [17100][55000]\t Training Loss 0.8522\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [17150][55000]\t Training Loss 0.8520\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [17200][55000]\t Training Loss 0.8520\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [17250][55000]\t Training Loss 0.8525\t Accuracy 0.8437\n",
      "Epoch [8][20]\t Batch [17300][55000]\t Training Loss 0.8524\t Accuracy 0.8437\n",
      "Epoch [8][20]\t Batch [17350][55000]\t Training Loss 0.8518\t Accuracy 0.8440\n",
      "Epoch [8][20]\t Batch [17400][55000]\t Training Loss 0.8519\t Accuracy 0.8440\n",
      "Epoch [8][20]\t Batch [17450][55000]\t Training Loss 0.8521\t Accuracy 0.8439\n",
      "Epoch [8][20]\t Batch [17500][55000]\t Training Loss 0.8521\t Accuracy 0.8440\n",
      "Epoch [8][20]\t Batch [17550][55000]\t Training Loss 0.8528\t Accuracy 0.8437\n",
      "Epoch [8][20]\t Batch [17600][55000]\t Training Loss 0.8534\t Accuracy 0.8435\n",
      "Epoch [8][20]\t Batch [17650][55000]\t Training Loss 0.8536\t Accuracy 0.8437\n",
      "Epoch [8][20]\t Batch [17700][55000]\t Training Loss 0.8543\t Accuracy 0.8435\n",
      "Epoch [8][20]\t Batch [17750][55000]\t Training Loss 0.8547\t Accuracy 0.8435\n",
      "Epoch [8][20]\t Batch [17800][55000]\t Training Loss 0.8551\t Accuracy 0.8433\n",
      "Epoch [8][20]\t Batch [17850][55000]\t Training Loss 0.8553\t Accuracy 0.8430\n",
      "Epoch [8][20]\t Batch [17900][55000]\t Training Loss 0.8557\t Accuracy 0.8429\n",
      "Epoch [8][20]\t Batch [17950][55000]\t Training Loss 0.8554\t Accuracy 0.8427\n",
      "Epoch [8][20]\t Batch [18000][55000]\t Training Loss 0.8551\t Accuracy 0.8429\n",
      "Epoch [8][20]\t Batch [18050][55000]\t Training Loss 0.8554\t Accuracy 0.8429\n",
      "Epoch [8][20]\t Batch [18100][55000]\t Training Loss 0.8552\t Accuracy 0.8430\n",
      "Epoch [8][20]\t Batch [18150][55000]\t Training Loss 0.8548\t Accuracy 0.8431\n",
      "Epoch [8][20]\t Batch [18200][55000]\t Training Loss 0.8544\t Accuracy 0.8434\n",
      "Epoch [8][20]\t Batch [18250][55000]\t Training Loss 0.8544\t Accuracy 0.8434\n",
      "Epoch [8][20]\t Batch [18300][55000]\t Training Loss 0.8539\t Accuracy 0.8435\n",
      "Epoch [8][20]\t Batch [18350][55000]\t Training Loss 0.8538\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [18400][55000]\t Training Loss 0.8538\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [18450][55000]\t Training Loss 0.8542\t Accuracy 0.8436\n",
      "Epoch [8][20]\t Batch [18500][55000]\t Training Loss 0.8543\t Accuracy 0.8436\n",
      "Epoch [8][20]\t Batch [18550][55000]\t Training Loss 0.8540\t Accuracy 0.8438\n",
      "Epoch [8][20]\t Batch [18600][55000]\t Training Loss 0.8540\t Accuracy 0.8440\n",
      "Epoch [8][20]\t Batch [18650][55000]\t Training Loss 0.8539\t Accuracy 0.8442\n",
      "Epoch [8][20]\t Batch [18700][55000]\t Training Loss 0.8541\t Accuracy 0.8440\n",
      "Epoch [8][20]\t Batch [18750][55000]\t Training Loss 0.8543\t Accuracy 0.8441\n",
      "Epoch [8][20]\t Batch [18800][55000]\t Training Loss 0.8539\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [18850][55000]\t Training Loss 0.8542\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [18900][55000]\t Training Loss 0.8537\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [18950][55000]\t Training Loss 0.8535\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [19000][55000]\t Training Loss 0.8534\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [19050][55000]\t Training Loss 0.8538\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [19100][55000]\t Training Loss 0.8541\t Accuracy 0.8447\n",
      "Epoch [8][20]\t Batch [19150][55000]\t Training Loss 0.8542\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [19200][55000]\t Training Loss 0.8543\t Accuracy 0.8444\n",
      "Epoch [8][20]\t Batch [19250][55000]\t Training Loss 0.8541\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [19300][55000]\t Training Loss 0.8540\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [19350][55000]\t Training Loss 0.8540\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [19400][55000]\t Training Loss 0.8538\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [19450][55000]\t Training Loss 0.8535\t Accuracy 0.8445\n",
      "Epoch [8][20]\t Batch [19500][55000]\t Training Loss 0.8532\t Accuracy 0.8447\n",
      "Epoch [8][20]\t Batch [19550][55000]\t Training Loss 0.8532\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [19600][55000]\t Training Loss 0.8532\t Accuracy 0.8443\n",
      "Epoch [8][20]\t Batch [19650][55000]\t Training Loss 0.8528\t Accuracy 0.8446\n",
      "Epoch [8][20]\t Batch [19700][55000]\t Training Loss 0.8522\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [19750][55000]\t Training Loss 0.8517\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [19800][55000]\t Training Loss 0.8511\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [19850][55000]\t Training Loss 0.8511\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [19900][55000]\t Training Loss 0.8509\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [19950][55000]\t Training Loss 0.8511\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [20000][55000]\t Training Loss 0.8510\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [20050][55000]\t Training Loss 0.8515\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [20100][55000]\t Training Loss 0.8516\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [20150][55000]\t Training Loss 0.8514\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [20200][55000]\t Training Loss 0.8518\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [20250][55000]\t Training Loss 0.8519\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [20300][55000]\t Training Loss 0.8520\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [20350][55000]\t Training Loss 0.8520\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [20400][55000]\t Training Loss 0.8517\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [20450][55000]\t Training Loss 0.8513\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [20500][55000]\t Training Loss 0.8510\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [20550][55000]\t Training Loss 0.8510\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [20600][55000]\t Training Loss 0.8510\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [20650][55000]\t Training Loss 0.8506\t Accuracy 0.8455\n",
      "Epoch [8][20]\t Batch [20700][55000]\t Training Loss 0.8504\t Accuracy 0.8455\n",
      "Epoch [8][20]\t Batch [20750][55000]\t Training Loss 0.8504\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [20800][55000]\t Training Loss 0.8504\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [20850][55000]\t Training Loss 0.8504\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [20900][55000]\t Training Loss 0.8507\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [20950][55000]\t Training Loss 0.8512\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [21000][55000]\t Training Loss 0.8514\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [21050][55000]\t Training Loss 0.8516\t Accuracy 0.8447\n",
      "Epoch [8][20]\t Batch [21100][55000]\t Training Loss 0.8514\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [21150][55000]\t Training Loss 0.8514\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [21200][55000]\t Training Loss 0.8511\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [21250][55000]\t Training Loss 0.8509\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [21300][55000]\t Training Loss 0.8505\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [21350][55000]\t Training Loss 0.8506\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [21400][55000]\t Training Loss 0.8506\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [21450][55000]\t Training Loss 0.8507\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [21500][55000]\t Training Loss 0.8503\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [21550][55000]\t Training Loss 0.8501\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [21600][55000]\t Training Loss 0.8502\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [21650][55000]\t Training Loss 0.8503\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [21700][55000]\t Training Loss 0.8502\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [21750][55000]\t Training Loss 0.8502\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [21800][55000]\t Training Loss 0.8496\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [21850][55000]\t Training Loss 0.8492\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [21900][55000]\t Training Loss 0.8488\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [21950][55000]\t Training Loss 0.8483\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [22000][55000]\t Training Loss 0.8479\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [22050][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [22100][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [22150][55000]\t Training Loss 0.8481\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [22200][55000]\t Training Loss 0.8484\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [22250][55000]\t Training Loss 0.8484\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [22300][55000]\t Training Loss 0.8486\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [22350][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [22400][55000]\t Training Loss 0.8483\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [22450][55000]\t Training Loss 0.8482\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [22500][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [22550][55000]\t Training Loss 0.8493\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [22600][55000]\t Training Loss 0.8497\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [22650][55000]\t Training Loss 0.8499\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [22700][55000]\t Training Loss 0.8498\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [22750][55000]\t Training Loss 0.8496\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [22800][55000]\t Training Loss 0.8495\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [22850][55000]\t Training Loss 0.8496\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [22900][55000]\t Training Loss 0.8493\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [22950][55000]\t Training Loss 0.8491\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [23000][55000]\t Training Loss 0.8489\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [23050][55000]\t Training Loss 0.8487\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [23100][55000]\t Training Loss 0.8488\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [23150][55000]\t Training Loss 0.8487\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [23200][55000]\t Training Loss 0.8488\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [23250][55000]\t Training Loss 0.8485\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [23300][55000]\t Training Loss 0.8482\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [23350][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [23400][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [23450][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [23500][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [23550][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [23600][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [23650][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [23700][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [23750][55000]\t Training Loss 0.8483\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [23800][55000]\t Training Loss 0.8479\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [23850][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [23900][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [23950][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [24000][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [24050][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [24100][55000]\t Training Loss 0.8479\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [24150][55000]\t Training Loss 0.8478\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [24200][55000]\t Training Loss 0.8476\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [24250][55000]\t Training Loss 0.8477\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [24300][55000]\t Training Loss 0.8479\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [24350][55000]\t Training Loss 0.8479\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [24400][55000]\t Training Loss 0.8479\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [24450][55000]\t Training Loss 0.8479\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [24500][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [24550][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [24600][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [24650][55000]\t Training Loss 0.8481\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [24700][55000]\t Training Loss 0.8480\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [24750][55000]\t Training Loss 0.8483\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [24800][55000]\t Training Loss 0.8486\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [24850][55000]\t Training Loss 0.8485\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [24900][55000]\t Training Loss 0.8486\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [24950][55000]\t Training Loss 0.8489\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [25000][55000]\t Training Loss 0.8491\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [25050][55000]\t Training Loss 0.8488\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [25100][55000]\t Training Loss 0.8488\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [25150][55000]\t Training Loss 0.8486\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [25200][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [25250][55000]\t Training Loss 0.8484\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [25300][55000]\t Training Loss 0.8483\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [25350][55000]\t Training Loss 0.8485\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [25400][55000]\t Training Loss 0.8480\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [25450][55000]\t Training Loss 0.8475\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [25500][55000]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [25550][55000]\t Training Loss 0.8470\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [25600][55000]\t Training Loss 0.8469\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [25650][55000]\t Training Loss 0.8468\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [25700][55000]\t Training Loss 0.8466\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [25750][55000]\t Training Loss 0.8464\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [25800][55000]\t Training Loss 0.8463\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [25850][55000]\t Training Loss 0.8464\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [25900][55000]\t Training Loss 0.8465\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [25950][55000]\t Training Loss 0.8465\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [26000][55000]\t Training Loss 0.8465\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [26050][55000]\t Training Loss 0.8463\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [26100][55000]\t Training Loss 0.8460\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [26150][55000]\t Training Loss 0.8458\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [26200][55000]\t Training Loss 0.8456\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [26250][55000]\t Training Loss 0.8456\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [26300][55000]\t Training Loss 0.8458\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [26350][55000]\t Training Loss 0.8456\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [26400][55000]\t Training Loss 0.8460\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [26450][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [26500][55000]\t Training Loss 0.8464\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [26550][55000]\t Training Loss 0.8466\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [26600][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [26650][55000]\t Training Loss 0.8472\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [26700][55000]\t Training Loss 0.8470\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [26750][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [26800][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [26850][55000]\t Training Loss 0.8470\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [26900][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [26950][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [27000][55000]\t Training Loss 0.8470\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [27050][55000]\t Training Loss 0.8468\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [27100][55000]\t Training Loss 0.8467\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [27150][55000]\t Training Loss 0.8467\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [27200][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [27250][55000]\t Training Loss 0.8469\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [27300][55000]\t Training Loss 0.8470\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [27350][55000]\t Training Loss 0.8469\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [27400][55000]\t Training Loss 0.8468\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [27450][55000]\t Training Loss 0.8468\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [27500][55000]\t Training Loss 0.8468\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [27550][55000]\t Training Loss 0.8469\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [27600][55000]\t Training Loss 0.8467\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [27650][55000]\t Training Loss 0.8467\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [27700][55000]\t Training Loss 0.8467\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [27750][55000]\t Training Loss 0.8468\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [27800][55000]\t Training Loss 0.8469\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [27850][55000]\t Training Loss 0.8470\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [27900][55000]\t Training Loss 0.8468\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [27950][55000]\t Training Loss 0.8466\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [28000][55000]\t Training Loss 0.8464\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [28050][55000]\t Training Loss 0.8461\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [28100][55000]\t Training Loss 0.8457\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [28150][55000]\t Training Loss 0.8455\t Accuracy 0.8480\n",
      "Epoch [8][20]\t Batch [28200][55000]\t Training Loss 0.8456\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [28250][55000]\t Training Loss 0.8452\t Accuracy 0.8480\n",
      "Epoch [8][20]\t Batch [28300][55000]\t Training Loss 0.8451\t Accuracy 0.8480\n",
      "Epoch [8][20]\t Batch [28350][55000]\t Training Loss 0.8449\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [28400][55000]\t Training Loss 0.8453\t Accuracy 0.8480\n",
      "Epoch [8][20]\t Batch [28450][55000]\t Training Loss 0.8450\t Accuracy 0.8481\n",
      "Epoch [8][20]\t Batch [28500][55000]\t Training Loss 0.8449\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [28550][55000]\t Training Loss 0.8448\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [28600][55000]\t Training Loss 0.8447\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [28650][55000]\t Training Loss 0.8449\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [28700][55000]\t Training Loss 0.8452\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [28750][55000]\t Training Loss 0.8453\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [28800][55000]\t Training Loss 0.8453\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [28850][55000]\t Training Loss 0.8452\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [28900][55000]\t Training Loss 0.8451\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [28950][55000]\t Training Loss 0.8450\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [29000][55000]\t Training Loss 0.8450\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [29050][55000]\t Training Loss 0.8450\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [29100][55000]\t Training Loss 0.8451\t Accuracy 0.8481\n",
      "Epoch [8][20]\t Batch [29150][55000]\t Training Loss 0.8455\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [29200][55000]\t Training Loss 0.8458\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [29250][55000]\t Training Loss 0.8461\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [29300][55000]\t Training Loss 0.8459\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [29350][55000]\t Training Loss 0.8460\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [29400][55000]\t Training Loss 0.8459\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [29450][55000]\t Training Loss 0.8455\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [29500][55000]\t Training Loss 0.8453\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [29550][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [29600][55000]\t Training Loss 0.8453\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [29650][55000]\t Training Loss 0.8452\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [29700][55000]\t Training Loss 0.8453\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [29750][55000]\t Training Loss 0.8457\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [29800][55000]\t Training Loss 0.8458\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [29850][55000]\t Training Loss 0.8461\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [29900][55000]\t Training Loss 0.8464\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [29950][55000]\t Training Loss 0.8467\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [30000][55000]\t Training Loss 0.8471\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [30050][55000]\t Training Loss 0.8472\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [30100][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [30150][55000]\t Training Loss 0.8480\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [30200][55000]\t Training Loss 0.8483\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [30250][55000]\t Training Loss 0.8484\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [30300][55000]\t Training Loss 0.8481\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [30350][55000]\t Training Loss 0.8481\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [30400][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [30450][55000]\t Training Loss 0.8480\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [30500][55000]\t Training Loss 0.8482\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [30550][55000]\t Training Loss 0.8486\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [30600][55000]\t Training Loss 0.8487\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [30650][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [30700][55000]\t Training Loss 0.8493\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [30750][55000]\t Training Loss 0.8495\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [30800][55000]\t Training Loss 0.8497\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [30850][55000]\t Training Loss 0.8496\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [30900][55000]\t Training Loss 0.8500\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [30950][55000]\t Training Loss 0.8498\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [31000][55000]\t Training Loss 0.8498\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [31050][55000]\t Training Loss 0.8500\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [31100][55000]\t Training Loss 0.8499\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [31150][55000]\t Training Loss 0.8500\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [31200][55000]\t Training Loss 0.8500\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [31250][55000]\t Training Loss 0.8500\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [31300][55000]\t Training Loss 0.8503\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [31350][55000]\t Training Loss 0.8510\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [31400][55000]\t Training Loss 0.8511\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [31450][55000]\t Training Loss 0.8514\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [31500][55000]\t Training Loss 0.8514\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [31550][55000]\t Training Loss 0.8514\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [31600][55000]\t Training Loss 0.8516\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [31650][55000]\t Training Loss 0.8518\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [31700][55000]\t Training Loss 0.8521\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [31750][55000]\t Training Loss 0.8525\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [31800][55000]\t Training Loss 0.8526\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [31850][55000]\t Training Loss 0.8526\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [31900][55000]\t Training Loss 0.8525\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [31950][55000]\t Training Loss 0.8525\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [32000][55000]\t Training Loss 0.8525\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [32050][55000]\t Training Loss 0.8525\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [32100][55000]\t Training Loss 0.8524\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [32150][55000]\t Training Loss 0.8526\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [32200][55000]\t Training Loss 0.8528\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [32250][55000]\t Training Loss 0.8531\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [32300][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [32350][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [32400][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [32450][55000]\t Training Loss 0.8540\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [32500][55000]\t Training Loss 0.8542\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [32550][55000]\t Training Loss 0.8545\t Accuracy 0.8455\n",
      "Epoch [8][20]\t Batch [32600][55000]\t Training Loss 0.8545\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [32650][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [32700][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [32750][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [32800][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [32850][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [32900][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [32950][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [33000][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [33050][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [33100][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [33150][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [33200][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [33250][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [33300][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [33350][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [33400][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [33450][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [33500][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [33550][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [33600][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [33650][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [33700][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [33750][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [33800][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [33850][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [33900][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [33950][55000]\t Training Loss 0.8533\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [34000][55000]\t Training Loss 0.8533\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [34050][55000]\t Training Loss 0.8535\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [34100][55000]\t Training Loss 0.8535\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [34150][55000]\t Training Loss 0.8536\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [34200][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [34250][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [34300][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [34350][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [34400][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [34450][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [34500][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [34550][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [34600][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [34650][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [34700][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [34750][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [34800][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [34850][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [34900][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [34950][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [35000][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [35050][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [35100][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [35150][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [35200][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [35250][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [35300][55000]\t Training Loss 0.8539\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [35350][55000]\t Training Loss 0.8537\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [35400][55000]\t Training Loss 0.8537\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [35450][55000]\t Training Loss 0.8536\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [35500][55000]\t Training Loss 0.8536\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [35550][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [35600][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [35650][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [35700][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [35750][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [35800][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [35850][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [35900][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [35950][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [36000][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [36050][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [36100][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [36150][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [36200][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [36250][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [36300][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [36350][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [36400][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [36450][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [36500][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [36550][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [36600][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [36650][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [36700][55000]\t Training Loss 0.8524\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [36750][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [36800][55000]\t Training Loss 0.8521\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [36850][55000]\t Training Loss 0.8521\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [36900][55000]\t Training Loss 0.8521\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [36950][55000]\t Training Loss 0.8520\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [37000][55000]\t Training Loss 0.8519\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [37050][55000]\t Training Loss 0.8517\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [37100][55000]\t Training Loss 0.8518\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [37150][55000]\t Training Loss 0.8518\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [37200][55000]\t Training Loss 0.8517\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [37250][55000]\t Training Loss 0.8516\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [37300][55000]\t Training Loss 0.8517\t Accuracy 0.8479\n",
      "Epoch [8][20]\t Batch [37350][55000]\t Training Loss 0.8518\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [37400][55000]\t Training Loss 0.8520\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [37450][55000]\t Training Loss 0.8523\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [37500][55000]\t Training Loss 0.8524\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [37550][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [37600][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [37650][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [37700][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [37750][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [37800][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [37850][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [37900][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [37950][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [38000][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [38050][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [38100][55000]\t Training Loss 0.8529\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [38150][55000]\t Training Loss 0.8528\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [38200][55000]\t Training Loss 0.8527\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [38250][55000]\t Training Loss 0.8527\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [38300][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [38350][55000]\t Training Loss 0.8530\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [38400][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [38450][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [38500][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [38550][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [38600][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [38650][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [38700][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [38750][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [38800][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [38850][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [38900][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [38950][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [39000][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [39050][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [39100][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [39150][55000]\t Training Loss 0.8521\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [39200][55000]\t Training Loss 0.8519\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [39250][55000]\t Training Loss 0.8517\t Accuracy 0.8477\n",
      "Epoch [8][20]\t Batch [39300][55000]\t Training Loss 0.8517\t Accuracy 0.8478\n",
      "Epoch [8][20]\t Batch [39350][55000]\t Training Loss 0.8520\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [39400][55000]\t Training Loss 0.8520\t Accuracy 0.8476\n",
      "Epoch [8][20]\t Batch [39450][55000]\t Training Loss 0.8523\t Accuracy 0.8475\n",
      "Epoch [8][20]\t Batch [39500][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [39550][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [39600][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [39650][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [39700][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [39750][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [39800][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [39850][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [39900][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [39950][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40000][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40050][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [40100][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [40150][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40200][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40250][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40300][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40350][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40400][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40450][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40500][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40550][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40600][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40650][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [40700][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [40750][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40800][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [40850][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [40900][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [40950][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [41000][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [41050][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [41100][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [41150][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [41200][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [41250][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [8][20]\t Batch [41300][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [41350][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [41400][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [41450][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [41500][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [41550][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [41600][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [41650][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [41700][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [8][20]\t Batch [41750][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [41800][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [41850][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [41900][55000]\t Training Loss 0.8539\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [41950][55000]\t Training Loss 0.8540\t Accuracy 0.8470\n",
      "Epoch [8][20]\t Batch [42000][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [42050][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [42100][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [8][20]\t Batch [42150][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [42200][55000]\t Training Loss 0.8539\t Accuracy 0.8468\n",
      "Epoch [8][20]\t Batch [42250][55000]\t Training Loss 0.8540\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [42300][55000]\t Training Loss 0.8540\t Accuracy 0.8467\n",
      "Epoch [8][20]\t Batch [42350][55000]\t Training Loss 0.8542\t Accuracy 0.8466\n",
      "Epoch [8][20]\t Batch [42400][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [42450][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [42500][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [42550][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [42600][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [42650][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [42700][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [42750][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [42800][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [42850][55000]\t Training Loss 0.8543\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [42900][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [42950][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [43000][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [43050][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [43100][55000]\t Training Loss 0.8550\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [43150][55000]\t Training Loss 0.8551\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [43200][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [43250][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [43300][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [43350][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [43400][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [43450][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [43500][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [43550][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [43600][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [43650][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [43700][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [43750][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [43800][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [43850][55000]\t Training Loss 0.8539\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [43900][55000]\t Training Loss 0.8540\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [43950][55000]\t Training Loss 0.8541\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [44000][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [44050][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [44100][55000]\t Training Loss 0.8542\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [44150][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [44200][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [44250][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [44300][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [44350][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [44400][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [44450][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [44500][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [44550][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [44600][55000]\t Training Loss 0.8545\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [44650][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [44700][55000]\t Training Loss 0.8543\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [44750][55000]\t Training Loss 0.8543\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [44800][55000]\t Training Loss 0.8544\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [44850][55000]\t Training Loss 0.8544\t Accuracy 0.8465\n",
      "Epoch [8][20]\t Batch [44900][55000]\t Training Loss 0.8546\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [44950][55000]\t Training Loss 0.8546\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45000][55000]\t Training Loss 0.8546\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45050][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45100][55000]\t Training Loss 0.8548\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45150][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45200][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45250][55000]\t Training Loss 0.8549\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45300][55000]\t Training Loss 0.8548\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45350][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45400][55000]\t Training Loss 0.8546\t Accuracy 0.8464\n",
      "Epoch [8][20]\t Batch [45450][55000]\t Training Loss 0.8547\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45500][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45550][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45600][55000]\t Training Loss 0.8549\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45650][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45700][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45750][55000]\t Training Loss 0.8548\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45800][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45850][55000]\t Training Loss 0.8550\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [45900][55000]\t Training Loss 0.8551\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [45950][55000]\t Training Loss 0.8551\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [46000][55000]\t Training Loss 0.8552\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [46050][55000]\t Training Loss 0.8553\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [46100][55000]\t Training Loss 0.8555\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [46150][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46200][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46250][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46300][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [46350][55000]\t Training Loss 0.8557\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [46400][55000]\t Training Loss 0.8558\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [46450][55000]\t Training Loss 0.8559\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [46500][55000]\t Training Loss 0.8558\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [46550][55000]\t Training Loss 0.8557\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46600][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46650][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [46700][55000]\t Training Loss 0.8555\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46750][55000]\t Training Loss 0.8556\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46800][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46850][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46900][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [46950][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [47000][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47050][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47100][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [47150][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [47200][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47250][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47300][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47350][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47400][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47450][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47500][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [47550][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [47600][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47650][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [47700][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [47750][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [47800][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [47850][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47900][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [47950][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [48000][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [48050][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [48100][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [48150][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [48200][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [48250][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [48300][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [48350][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [48400][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [48450][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [48500][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [48550][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [48600][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [48650][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [48700][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [48750][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [48800][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [48850][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [48900][55000]\t Training Loss 0.8532\t Accuracy 0.8463\n",
      "Epoch [8][20]\t Batch [48950][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [49000][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [8][20]\t Batch [49050][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [8][20]\t Batch [49100][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [8][20]\t Batch [49150][55000]\t Training Loss 0.8541\t Accuracy 0.8458\n",
      "Epoch [8][20]\t Batch [49200][55000]\t Training Loss 0.8541\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [49250][55000]\t Training Loss 0.8542\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [49300][55000]\t Training Loss 0.8541\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [49350][55000]\t Training Loss 0.8540\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [49400][55000]\t Training Loss 0.8538\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [49450][55000]\t Training Loss 0.8538\t Accuracy 0.8457\n",
      "Epoch [8][20]\t Batch [49500][55000]\t Training Loss 0.8540\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [49550][55000]\t Training Loss 0.8544\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [49600][55000]\t Training Loss 0.8546\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [49650][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [49700][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [49750][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [49800][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [49850][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [49900][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [49950][55000]\t Training Loss 0.8552\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [50000][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [50050][55000]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [50100][55000]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [50150][55000]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [50200][55000]\t Training Loss 0.8552\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [50250][55000]\t Training Loss 0.8554\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [50300][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [50350][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [50400][55000]\t Training Loss 0.8556\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [50450][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [50500][55000]\t Training Loss 0.8559\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [50550][55000]\t Training Loss 0.8559\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [50600][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [50650][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [50700][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [50750][55000]\t Training Loss 0.8561\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [50800][55000]\t Training Loss 0.8561\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [50850][55000]\t Training Loss 0.8561\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [50900][55000]\t Training Loss 0.8560\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [50950][55000]\t Training Loss 0.8559\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [51000][55000]\t Training Loss 0.8557\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [51050][55000]\t Training Loss 0.8556\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [51100][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [51150][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [51200][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [51250][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [51300][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [51350][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [51400][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [51450][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [51500][55000]\t Training Loss 0.8554\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [51550][55000]\t Training Loss 0.8553\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [51600][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [51650][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [51700][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [51750][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [51800][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [51850][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [51900][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [51950][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [52000][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [52050][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [52100][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [52150][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [52200][55000]\t Training Loss 0.8556\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [52250][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [52300][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [52350][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [52400][55000]\t Training Loss 0.8558\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [52450][55000]\t Training Loss 0.8555\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [52500][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [52550][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [52600][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [52650][55000]\t Training Loss 0.8549\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [52700][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [8][20]\t Batch [52750][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [52800][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [52850][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [52900][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [52950][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [53000][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53050][55000]\t Training Loss 0.8558\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [53100][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53150][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [53200][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53250][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53300][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53350][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53400][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [53450][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [53500][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53550][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53600][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [53650][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [53700][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [53750][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53800][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53850][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53900][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [53950][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [54000][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [54050][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [54100][55000]\t Training Loss 0.8559\t Accuracy 0.8447\n",
      "Epoch [8][20]\t Batch [54150][55000]\t Training Loss 0.8558\t Accuracy 0.8447\n",
      "Epoch [8][20]\t Batch [54200][55000]\t Training Loss 0.8559\t Accuracy 0.8447\n",
      "Epoch [8][20]\t Batch [54250][55000]\t Training Loss 0.8557\t Accuracy 0.8447\n",
      "Epoch [8][20]\t Batch [54300][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [54350][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [54400][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [54450][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [54500][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [54550][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [54600][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [54650][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [54700][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [54750][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [54800][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [54850][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [54900][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [8][20]\t Batch [54950][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8554\t Average training accuracy 0.8448\n",
      "Epoch [8]\t Average validation loss 0.7929\t Average validation accuracy 0.8696\n",
      "\n",
      "Epoch [9][20]\t Batch [0][55000]\t Training Loss 1.4745\t Accuracy 0.0000\n",
      "Epoch [9][20]\t Batch [50][55000]\t Training Loss 0.9399\t Accuracy 0.7255\n",
      "Epoch [9][20]\t Batch [100][55000]\t Training Loss 0.8499\t Accuracy 0.8020\n",
      "Epoch [9][20]\t Batch [150][55000]\t Training Loss 0.8511\t Accuracy 0.8146\n",
      "Epoch [9][20]\t Batch [200][55000]\t Training Loss 0.8618\t Accuracy 0.8159\n",
      "Epoch [9][20]\t Batch [250][55000]\t Training Loss 0.8441\t Accuracy 0.8247\n",
      "Epoch [9][20]\t Batch [300][55000]\t Training Loss 0.8456\t Accuracy 0.8206\n",
      "Epoch [9][20]\t Batch [350][55000]\t Training Loss 0.8259\t Accuracy 0.8234\n",
      "Epoch [9][20]\t Batch [400][55000]\t Training Loss 0.8120\t Accuracy 0.8279\n",
      "Epoch [9][20]\t Batch [450][55000]\t Training Loss 0.8178\t Accuracy 0.8293\n",
      "Epoch [9][20]\t Batch [500][55000]\t Training Loss 0.8241\t Accuracy 0.8323\n",
      "Epoch [9][20]\t Batch [550][55000]\t Training Loss 0.8414\t Accuracy 0.8294\n",
      "Epoch [9][20]\t Batch [600][55000]\t Training Loss 0.8459\t Accuracy 0.8319\n",
      "Epoch [9][20]\t Batch [650][55000]\t Training Loss 0.8657\t Accuracy 0.8218\n",
      "Epoch [9][20]\t Batch [700][55000]\t Training Loss 0.8633\t Accuracy 0.8274\n",
      "Epoch [9][20]\t Batch [750][55000]\t Training Loss 0.8596\t Accuracy 0.8282\n",
      "Epoch [9][20]\t Batch [800][55000]\t Training Loss 0.8536\t Accuracy 0.8302\n",
      "Epoch [9][20]\t Batch [850][55000]\t Training Loss 0.8520\t Accuracy 0.8320\n",
      "Epoch [9][20]\t Batch [900][55000]\t Training Loss 0.8585\t Accuracy 0.8313\n",
      "Epoch [9][20]\t Batch [950][55000]\t Training Loss 0.8653\t Accuracy 0.8265\n",
      "Epoch [9][20]\t Batch [1000][55000]\t Training Loss 0.8647\t Accuracy 0.8292\n",
      "Epoch [9][20]\t Batch [1050][55000]\t Training Loss 0.8724\t Accuracy 0.8278\n",
      "Epoch [9][20]\t Batch [1100][55000]\t Training Loss 0.8808\t Accuracy 0.8274\n",
      "Epoch [9][20]\t Batch [1150][55000]\t Training Loss 0.8900\t Accuracy 0.8245\n",
      "Epoch [9][20]\t Batch [1200][55000]\t Training Loss 0.8843\t Accuracy 0.8276\n",
      "Epoch [9][20]\t Batch [1250][55000]\t Training Loss 0.8856\t Accuracy 0.8273\n",
      "Epoch [9][20]\t Batch [1300][55000]\t Training Loss 0.8859\t Accuracy 0.8271\n",
      "Epoch [9][20]\t Batch [1350][55000]\t Training Loss 0.8827\t Accuracy 0.8283\n",
      "Epoch [9][20]\t Batch [1400][55000]\t Training Loss 0.8833\t Accuracy 0.8273\n",
      "Epoch [9][20]\t Batch [1450][55000]\t Training Loss 0.8842\t Accuracy 0.8263\n",
      "Epoch [9][20]\t Batch [1500][55000]\t Training Loss 0.8823\t Accuracy 0.8288\n",
      "Epoch [9][20]\t Batch [1550][55000]\t Training Loss 0.8818\t Accuracy 0.8291\n",
      "Epoch [9][20]\t Batch [1600][55000]\t Training Loss 0.8844\t Accuracy 0.8282\n",
      "Epoch [9][20]\t Batch [1650][55000]\t Training Loss 0.8813\t Accuracy 0.8298\n",
      "Epoch [9][20]\t Batch [1700][55000]\t Training Loss 0.8796\t Accuracy 0.8313\n",
      "Epoch [9][20]\t Batch [1750][55000]\t Training Loss 0.8740\t Accuracy 0.8321\n",
      "Epoch [9][20]\t Batch [1800][55000]\t Training Loss 0.8718\t Accuracy 0.8340\n",
      "Epoch [9][20]\t Batch [1850][55000]\t Training Loss 0.8702\t Accuracy 0.8347\n",
      "Epoch [9][20]\t Batch [1900][55000]\t Training Loss 0.8680\t Accuracy 0.8348\n",
      "Epoch [9][20]\t Batch [1950][55000]\t Training Loss 0.8665\t Accuracy 0.8360\n",
      "Epoch [9][20]\t Batch [2000][55000]\t Training Loss 0.8640\t Accuracy 0.8371\n",
      "Epoch [9][20]\t Batch [2050][55000]\t Training Loss 0.8631\t Accuracy 0.8376\n",
      "Epoch [9][20]\t Batch [2100][55000]\t Training Loss 0.8602\t Accuracy 0.8382\n",
      "Epoch [9][20]\t Batch [2150][55000]\t Training Loss 0.8570\t Accuracy 0.8401\n",
      "Epoch [9][20]\t Batch [2200][55000]\t Training Loss 0.8522\t Accuracy 0.8419\n",
      "Epoch [9][20]\t Batch [2250][55000]\t Training Loss 0.8514\t Accuracy 0.8423\n",
      "Epoch [9][20]\t Batch [2300][55000]\t Training Loss 0.8484\t Accuracy 0.8418\n",
      "Epoch [9][20]\t Batch [2350][55000]\t Training Loss 0.8469\t Accuracy 0.8422\n",
      "Epoch [9][20]\t Batch [2400][55000]\t Training Loss 0.8479\t Accuracy 0.8405\n",
      "Epoch [9][20]\t Batch [2450][55000]\t Training Loss 0.8507\t Accuracy 0.8392\n",
      "Epoch [9][20]\t Batch [2500][55000]\t Training Loss 0.8485\t Accuracy 0.8413\n",
      "Epoch [9][20]\t Batch [2550][55000]\t Training Loss 0.8473\t Accuracy 0.8412\n",
      "Epoch [9][20]\t Batch [2600][55000]\t Training Loss 0.8467\t Accuracy 0.8404\n",
      "Epoch [9][20]\t Batch [2650][55000]\t Training Loss 0.8455\t Accuracy 0.8408\n",
      "Epoch [9][20]\t Batch [2700][55000]\t Training Loss 0.8453\t Accuracy 0.8412\n",
      "Epoch [9][20]\t Batch [2750][55000]\t Training Loss 0.8452\t Accuracy 0.8411\n",
      "Epoch [9][20]\t Batch [2800][55000]\t Training Loss 0.8454\t Accuracy 0.8393\n",
      "Epoch [9][20]\t Batch [2850][55000]\t Training Loss 0.8443\t Accuracy 0.8394\n",
      "Epoch [9][20]\t Batch [2900][55000]\t Training Loss 0.8410\t Accuracy 0.8404\n",
      "Epoch [9][20]\t Batch [2950][55000]\t Training Loss 0.8411\t Accuracy 0.8401\n",
      "Epoch [9][20]\t Batch [3000][55000]\t Training Loss 0.8410\t Accuracy 0.8407\n",
      "Epoch [9][20]\t Batch [3050][55000]\t Training Loss 0.8420\t Accuracy 0.8404\n",
      "Epoch [9][20]\t Batch [3100][55000]\t Training Loss 0.8443\t Accuracy 0.8401\n",
      "Epoch [9][20]\t Batch [3150][55000]\t Training Loss 0.8442\t Accuracy 0.8404\n",
      "Epoch [9][20]\t Batch [3200][55000]\t Training Loss 0.8440\t Accuracy 0.8416\n",
      "Epoch [9][20]\t Batch [3250][55000]\t Training Loss 0.8434\t Accuracy 0.8410\n",
      "Epoch [9][20]\t Batch [3300][55000]\t Training Loss 0.8447\t Accuracy 0.8410\n",
      "Epoch [9][20]\t Batch [3350][55000]\t Training Loss 0.8429\t Accuracy 0.8424\n",
      "Epoch [9][20]\t Batch [3400][55000]\t Training Loss 0.8452\t Accuracy 0.8409\n",
      "Epoch [9][20]\t Batch [3450][55000]\t Training Loss 0.8449\t Accuracy 0.8418\n",
      "Epoch [9][20]\t Batch [3500][55000]\t Training Loss 0.8449\t Accuracy 0.8415\n",
      "Epoch [9][20]\t Batch [3550][55000]\t Training Loss 0.8465\t Accuracy 0.8409\n",
      "Epoch [9][20]\t Batch [3600][55000]\t Training Loss 0.8463\t Accuracy 0.8403\n",
      "Epoch [9][20]\t Batch [3650][55000]\t Training Loss 0.8449\t Accuracy 0.8411\n",
      "Epoch [9][20]\t Batch [3700][55000]\t Training Loss 0.8457\t Accuracy 0.8406\n",
      "Epoch [9][20]\t Batch [3750][55000]\t Training Loss 0.8456\t Accuracy 0.8408\n",
      "Epoch [9][20]\t Batch [3800][55000]\t Training Loss 0.8459\t Accuracy 0.8408\n",
      "Epoch [9][20]\t Batch [3850][55000]\t Training Loss 0.8455\t Accuracy 0.8413\n",
      "Epoch [9][20]\t Batch [3900][55000]\t Training Loss 0.8444\t Accuracy 0.8426\n",
      "Epoch [9][20]\t Batch [3950][55000]\t Training Loss 0.8433\t Accuracy 0.8438\n",
      "Epoch [9][20]\t Batch [4000][55000]\t Training Loss 0.8429\t Accuracy 0.8445\n",
      "Epoch [9][20]\t Batch [4050][55000]\t Training Loss 0.8414\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [4100][55000]\t Training Loss 0.8418\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [4150][55000]\t Training Loss 0.8423\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [4200][55000]\t Training Loss 0.8427\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [4250][55000]\t Training Loss 0.8413\t Accuracy 0.8445\n",
      "Epoch [9][20]\t Batch [4300][55000]\t Training Loss 0.8414\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [4350][55000]\t Training Loss 0.8418\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [4400][55000]\t Training Loss 0.8418\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [4450][55000]\t Training Loss 0.8419\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [4500][55000]\t Training Loss 0.8422\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [4550][55000]\t Training Loss 0.8405\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [4600][55000]\t Training Loss 0.8388\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [4650][55000]\t Training Loss 0.8387\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [4700][55000]\t Training Loss 0.8383\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [4750][55000]\t Training Loss 0.8371\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [4800][55000]\t Training Loss 0.8378\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [4850][55000]\t Training Loss 0.8390\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [4900][55000]\t Training Loss 0.8375\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [4950][55000]\t Training Loss 0.8378\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [5000][55000]\t Training Loss 0.8379\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [5050][55000]\t Training Loss 0.8374\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [5100][55000]\t Training Loss 0.8374\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [5150][55000]\t Training Loss 0.8377\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [5200][55000]\t Training Loss 0.8392\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [5250][55000]\t Training Loss 0.8383\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [5300][55000]\t Training Loss 0.8382\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [5350][55000]\t Training Loss 0.8389\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [5400][55000]\t Training Loss 0.8381\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [5450][55000]\t Training Loss 0.8374\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [5500][55000]\t Training Loss 0.8356\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [5550][55000]\t Training Loss 0.8355\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [5600][55000]\t Training Loss 0.8346\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [5650][55000]\t Training Loss 0.8352\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [5700][55000]\t Training Loss 0.8349\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [5750][55000]\t Training Loss 0.8354\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [5800][55000]\t Training Loss 0.8357\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [5850][55000]\t Training Loss 0.8359\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [5900][55000]\t Training Loss 0.8364\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [5950][55000]\t Training Loss 0.8363\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [6000][55000]\t Training Loss 0.8352\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [6050][55000]\t Training Loss 0.8339\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [6100][55000]\t Training Loss 0.8324\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [6150][55000]\t Training Loss 0.8303\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [6200][55000]\t Training Loss 0.8304\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [6250][55000]\t Training Loss 0.8292\t Accuracy 0.8483\n",
      "Epoch [9][20]\t Batch [6300][55000]\t Training Loss 0.8296\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [6350][55000]\t Training Loss 0.8293\t Accuracy 0.8484\n",
      "Epoch [9][20]\t Batch [6400][55000]\t Training Loss 0.8287\t Accuracy 0.8488\n",
      "Epoch [9][20]\t Batch [6450][55000]\t Training Loss 0.8281\t Accuracy 0.8490\n",
      "Epoch [9][20]\t Batch [6500][55000]\t Training Loss 0.8289\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [6550][55000]\t Training Loss 0.8279\t Accuracy 0.8489\n",
      "Epoch [9][20]\t Batch [6600][55000]\t Training Loss 0.8266\t Accuracy 0.8494\n",
      "Epoch [9][20]\t Batch [6650][55000]\t Training Loss 0.8254\t Accuracy 0.8495\n",
      "Epoch [9][20]\t Batch [6700][55000]\t Training Loss 0.8256\t Accuracy 0.8494\n",
      "Epoch [9][20]\t Batch [6750][55000]\t Training Loss 0.8258\t Accuracy 0.8502\n",
      "Epoch [9][20]\t Batch [6800][55000]\t Training Loss 0.8262\t Accuracy 0.8502\n",
      "Epoch [9][20]\t Batch [6850][55000]\t Training Loss 0.8284\t Accuracy 0.8498\n",
      "Epoch [9][20]\t Batch [6900][55000]\t Training Loss 0.8284\t Accuracy 0.8497\n",
      "Epoch [9][20]\t Batch [6950][55000]\t Training Loss 0.8290\t Accuracy 0.8487\n",
      "Epoch [9][20]\t Batch [7000][55000]\t Training Loss 0.8289\t Accuracy 0.8487\n",
      "Epoch [9][20]\t Batch [7050][55000]\t Training Loss 0.8294\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [7100][55000]\t Training Loss 0.8297\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [7150][55000]\t Training Loss 0.8299\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [7200][55000]\t Training Loss 0.8307\t Accuracy 0.8485\n",
      "Epoch [9][20]\t Batch [7250][55000]\t Training Loss 0.8330\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [7300][55000]\t Training Loss 0.8348\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [7350][55000]\t Training Loss 0.8363\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [7400][55000]\t Training Loss 0.8374\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [7450][55000]\t Training Loss 0.8373\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [7500][55000]\t Training Loss 0.8373\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [7550][55000]\t Training Loss 0.8378\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [7600][55000]\t Training Loss 0.8374\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [7650][55000]\t Training Loss 0.8378\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [7700][55000]\t Training Loss 0.8383\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [7750][55000]\t Training Loss 0.8384\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [7800][55000]\t Training Loss 0.8392\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [7850][55000]\t Training Loss 0.8393\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [7900][55000]\t Training Loss 0.8395\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [7950][55000]\t Training Loss 0.8396\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [8000][55000]\t Training Loss 0.8396\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [8050][55000]\t Training Loss 0.8401\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [8100][55000]\t Training Loss 0.8387\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [8150][55000]\t Training Loss 0.8395\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [8200][55000]\t Training Loss 0.8395\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [8250][55000]\t Training Loss 0.8407\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [8300][55000]\t Training Loss 0.8410\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [8350][55000]\t Training Loss 0.8417\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [8400][55000]\t Training Loss 0.8410\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [8450][55000]\t Training Loss 0.8425\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [8500][55000]\t Training Loss 0.8417\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [8550][55000]\t Training Loss 0.8408\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [8600][55000]\t Training Loss 0.8399\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [8650][55000]\t Training Loss 0.8400\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [8700][55000]\t Training Loss 0.8406\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [8750][55000]\t Training Loss 0.8422\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [8800][55000]\t Training Loss 0.8430\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [8850][55000]\t Training Loss 0.8430\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [8900][55000]\t Training Loss 0.8447\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [8950][55000]\t Training Loss 0.8443\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [9000][55000]\t Training Loss 0.8436\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [9050][55000]\t Training Loss 0.8427\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [9100][55000]\t Training Loss 0.8425\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [9150][55000]\t Training Loss 0.8430\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [9200][55000]\t Training Loss 0.8426\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [9250][55000]\t Training Loss 0.8429\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [9300][55000]\t Training Loss 0.8431\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [9350][55000]\t Training Loss 0.8433\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [9400][55000]\t Training Loss 0.8436\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [9450][55000]\t Training Loss 0.8438\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [9500][55000]\t Training Loss 0.8429\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [9550][55000]\t Training Loss 0.8427\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [9600][55000]\t Training Loss 0.8431\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [9650][55000]\t Training Loss 0.8429\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [9700][55000]\t Training Loss 0.8424\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [9750][55000]\t Training Loss 0.8416\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [9800][55000]\t Training Loss 0.8421\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [9850][55000]\t Training Loss 0.8417\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [9900][55000]\t Training Loss 0.8412\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [9950][55000]\t Training Loss 0.8406\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [10000][55000]\t Training Loss 0.8403\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [10050][55000]\t Training Loss 0.8405\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [10100][55000]\t Training Loss 0.8403\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [10150][55000]\t Training Loss 0.8401\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [10200][55000]\t Training Loss 0.8403\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [10250][55000]\t Training Loss 0.8404\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [10300][55000]\t Training Loss 0.8403\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [10350][55000]\t Training Loss 0.8394\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [10400][55000]\t Training Loss 0.8386\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [10450][55000]\t Training Loss 0.8384\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [10500][55000]\t Training Loss 0.8374\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [10550][55000]\t Training Loss 0.8370\t Accuracy 0.8483\n",
      "Epoch [9][20]\t Batch [10600][55000]\t Training Loss 0.8365\t Accuracy 0.8485\n",
      "Epoch [9][20]\t Batch [10650][55000]\t Training Loss 0.8362\t Accuracy 0.8488\n",
      "Epoch [9][20]\t Batch [10700][55000]\t Training Loss 0.8360\t Accuracy 0.8492\n",
      "Epoch [9][20]\t Batch [10750][55000]\t Training Loss 0.8365\t Accuracy 0.8489\n",
      "Epoch [9][20]\t Batch [10800][55000]\t Training Loss 0.8370\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [10850][55000]\t Training Loss 0.8363\t Accuracy 0.8489\n",
      "Epoch [9][20]\t Batch [10900][55000]\t Training Loss 0.8359\t Accuracy 0.8491\n",
      "Epoch [9][20]\t Batch [10950][55000]\t Training Loss 0.8355\t Accuracy 0.8489\n",
      "Epoch [9][20]\t Batch [11000][55000]\t Training Loss 0.8354\t Accuracy 0.8491\n",
      "Epoch [9][20]\t Batch [11050][55000]\t Training Loss 0.8347\t Accuracy 0.8493\n",
      "Epoch [9][20]\t Batch [11100][55000]\t Training Loss 0.8340\t Accuracy 0.8495\n",
      "Epoch [9][20]\t Batch [11150][55000]\t Training Loss 0.8340\t Accuracy 0.8497\n",
      "Epoch [9][20]\t Batch [11200][55000]\t Training Loss 0.8336\t Accuracy 0.8497\n",
      "Epoch [9][20]\t Batch [11250][55000]\t Training Loss 0.8340\t Accuracy 0.8496\n",
      "Epoch [9][20]\t Batch [11300][55000]\t Training Loss 0.8336\t Accuracy 0.8497\n",
      "Epoch [9][20]\t Batch [11350][55000]\t Training Loss 0.8331\t Accuracy 0.8498\n",
      "Epoch [9][20]\t Batch [11400][55000]\t Training Loss 0.8332\t Accuracy 0.8497\n",
      "Epoch [9][20]\t Batch [11450][55000]\t Training Loss 0.8329\t Accuracy 0.8497\n",
      "Epoch [9][20]\t Batch [11500][55000]\t Training Loss 0.8327\t Accuracy 0.8496\n",
      "Epoch [9][20]\t Batch [11550][55000]\t Training Loss 0.8327\t Accuracy 0.8495\n",
      "Epoch [9][20]\t Batch [11600][55000]\t Training Loss 0.8339\t Accuracy 0.8488\n",
      "Epoch [9][20]\t Batch [11650][55000]\t Training Loss 0.8344\t Accuracy 0.8485\n",
      "Epoch [9][20]\t Batch [11700][55000]\t Training Loss 0.8344\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [11750][55000]\t Training Loss 0.8353\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [11800][55000]\t Training Loss 0.8356\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [11850][55000]\t Training Loss 0.8356\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [11900][55000]\t Training Loss 0.8359\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [11950][55000]\t Training Loss 0.8360\t Accuracy 0.8484\n",
      "Epoch [9][20]\t Batch [12000][55000]\t Training Loss 0.8360\t Accuracy 0.8487\n",
      "Epoch [9][20]\t Batch [12050][55000]\t Training Loss 0.8358\t Accuracy 0.8490\n",
      "Epoch [9][20]\t Batch [12100][55000]\t Training Loss 0.8358\t Accuracy 0.8489\n",
      "Epoch [9][20]\t Batch [12150][55000]\t Training Loss 0.8351\t Accuracy 0.8493\n",
      "Epoch [9][20]\t Batch [12200][55000]\t Training Loss 0.8354\t Accuracy 0.8493\n",
      "Epoch [9][20]\t Batch [12250][55000]\t Training Loss 0.8353\t Accuracy 0.8493\n",
      "Epoch [9][20]\t Batch [12300][55000]\t Training Loss 0.8353\t Accuracy 0.8493\n",
      "Epoch [9][20]\t Batch [12350][55000]\t Training Loss 0.8355\t Accuracy 0.8494\n",
      "Epoch [9][20]\t Batch [12400][55000]\t Training Loss 0.8358\t Accuracy 0.8494\n",
      "Epoch [9][20]\t Batch [12450][55000]\t Training Loss 0.8360\t Accuracy 0.8491\n",
      "Epoch [9][20]\t Batch [12500][55000]\t Training Loss 0.8363\t Accuracy 0.8487\n",
      "Epoch [9][20]\t Batch [12550][55000]\t Training Loss 0.8363\t Accuracy 0.8489\n",
      "Epoch [9][20]\t Batch [12600][55000]\t Training Loss 0.8370\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [12650][55000]\t Training Loss 0.8375\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [12700][55000]\t Training Loss 0.8382\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [12750][55000]\t Training Loss 0.8377\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [12800][55000]\t Training Loss 0.8383\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [12850][55000]\t Training Loss 0.8385\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [12900][55000]\t Training Loss 0.8385\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [12950][55000]\t Training Loss 0.8390\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [13000][55000]\t Training Loss 0.8390\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [13050][55000]\t Training Loss 0.8396\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [13100][55000]\t Training Loss 0.8403\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [13150][55000]\t Training Loss 0.8406\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [13200][55000]\t Training Loss 0.8408\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [13250][55000]\t Training Loss 0.8403\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [13300][55000]\t Training Loss 0.8402\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [13350][55000]\t Training Loss 0.8406\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [13400][55000]\t Training Loss 0.8409\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [13450][55000]\t Training Loss 0.8404\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [13500][55000]\t Training Loss 0.8400\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [13550][55000]\t Training Loss 0.8396\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [13600][55000]\t Training Loss 0.8388\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [13650][55000]\t Training Loss 0.8386\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [13700][55000]\t Training Loss 0.8394\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [13750][55000]\t Training Loss 0.8401\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [13800][55000]\t Training Loss 0.8402\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [13850][55000]\t Training Loss 0.8401\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [13900][55000]\t Training Loss 0.8403\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [13950][55000]\t Training Loss 0.8407\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [14000][55000]\t Training Loss 0.8414\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [14050][55000]\t Training Loss 0.8416\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [14100][55000]\t Training Loss 0.8417\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [14150][55000]\t Training Loss 0.8419\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [14200][55000]\t Training Loss 0.8418\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [14250][55000]\t Training Loss 0.8421\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [14300][55000]\t Training Loss 0.8424\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [14350][55000]\t Training Loss 0.8428\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [14400][55000]\t Training Loss 0.8436\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [14450][55000]\t Training Loss 0.8438\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [14500][55000]\t Training Loss 0.8439\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [14550][55000]\t Training Loss 0.8446\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [14600][55000]\t Training Loss 0.8447\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [14650][55000]\t Training Loss 0.8456\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [14700][55000]\t Training Loss 0.8464\t Accuracy 0.8444\n",
      "Epoch [9][20]\t Batch [14750][55000]\t Training Loss 0.8470\t Accuracy 0.8441\n",
      "Epoch [9][20]\t Batch [14800][55000]\t Training Loss 0.8479\t Accuracy 0.8439\n",
      "Epoch [9][20]\t Batch [14850][55000]\t Training Loss 0.8485\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [14900][55000]\t Training Loss 0.8484\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [14950][55000]\t Training Loss 0.8486\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [15000][55000]\t Training Loss 0.8485\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [15050][55000]\t Training Loss 0.8481\t Accuracy 0.8437\n",
      "Epoch [9][20]\t Batch [15100][55000]\t Training Loss 0.8479\t Accuracy 0.8439\n",
      "Epoch [9][20]\t Batch [15150][55000]\t Training Loss 0.8483\t Accuracy 0.8438\n",
      "Epoch [9][20]\t Batch [15200][55000]\t Training Loss 0.8485\t Accuracy 0.8438\n",
      "Epoch [9][20]\t Batch [15250][55000]\t Training Loss 0.8484\t Accuracy 0.8439\n",
      "Epoch [9][20]\t Batch [15300][55000]\t Training Loss 0.8484\t Accuracy 0.8439\n",
      "Epoch [9][20]\t Batch [15350][55000]\t Training Loss 0.8482\t Accuracy 0.8441\n",
      "Epoch [9][20]\t Batch [15400][55000]\t Training Loss 0.8485\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [15450][55000]\t Training Loss 0.8486\t Accuracy 0.8444\n",
      "Epoch [9][20]\t Batch [15500][55000]\t Training Loss 0.8483\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [15550][55000]\t Training Loss 0.8482\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [15600][55000]\t Training Loss 0.8481\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [15650][55000]\t Training Loss 0.8480\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [15700][55000]\t Training Loss 0.8479\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [15750][55000]\t Training Loss 0.8488\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [15800][55000]\t Training Loss 0.8492\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [15850][55000]\t Training Loss 0.8495\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [15900][55000]\t Training Loss 0.8500\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [15950][55000]\t Training Loss 0.8501\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [16000][55000]\t Training Loss 0.8502\t Accuracy 0.8440\n",
      "Epoch [9][20]\t Batch [16050][55000]\t Training Loss 0.8511\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [16100][55000]\t Training Loss 0.8511\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [16150][55000]\t Training Loss 0.8510\t Accuracy 0.8437\n",
      "Epoch [9][20]\t Batch [16200][55000]\t Training Loss 0.8512\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [16250][55000]\t Training Loss 0.8510\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [16300][55000]\t Training Loss 0.8507\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [16350][55000]\t Training Loss 0.8503\t Accuracy 0.8437\n",
      "Epoch [9][20]\t Batch [16400][55000]\t Training Loss 0.8504\t Accuracy 0.8437\n",
      "Epoch [9][20]\t Batch [16450][55000]\t Training Loss 0.8501\t Accuracy 0.8440\n",
      "Epoch [9][20]\t Batch [16500][55000]\t Training Loss 0.8499\t Accuracy 0.8441\n",
      "Epoch [9][20]\t Batch [16550][55000]\t Training Loss 0.8495\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [16600][55000]\t Training Loss 0.8496\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [16650][55000]\t Training Loss 0.8496\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [16700][55000]\t Training Loss 0.8497\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [16750][55000]\t Training Loss 0.8496\t Accuracy 0.8444\n",
      "Epoch [9][20]\t Batch [16800][55000]\t Training Loss 0.8502\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [16850][55000]\t Training Loss 0.8507\t Accuracy 0.8440\n",
      "Epoch [9][20]\t Batch [16900][55000]\t Training Loss 0.8510\t Accuracy 0.8440\n",
      "Epoch [9][20]\t Batch [16950][55000]\t Training Loss 0.8510\t Accuracy 0.8439\n",
      "Epoch [9][20]\t Batch [17000][55000]\t Training Loss 0.8517\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [17050][55000]\t Training Loss 0.8516\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [17100][55000]\t Training Loss 0.8521\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [17150][55000]\t Training Loss 0.8519\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [17200][55000]\t Training Loss 0.8519\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [17250][55000]\t Training Loss 0.8524\t Accuracy 0.8433\n",
      "Epoch [9][20]\t Batch [17300][55000]\t Training Loss 0.8522\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [17350][55000]\t Training Loss 0.8517\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [17400][55000]\t Training Loss 0.8518\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [17450][55000]\t Training Loss 0.8519\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [17500][55000]\t Training Loss 0.8520\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [17550][55000]\t Training Loss 0.8527\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [17600][55000]\t Training Loss 0.8533\t Accuracy 0.8431\n",
      "Epoch [9][20]\t Batch [17650][55000]\t Training Loss 0.8535\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [17700][55000]\t Training Loss 0.8542\t Accuracy 0.8432\n",
      "Epoch [9][20]\t Batch [17750][55000]\t Training Loss 0.8546\t Accuracy 0.8432\n",
      "Epoch [9][20]\t Batch [17800][55000]\t Training Loss 0.8549\t Accuracy 0.8430\n",
      "Epoch [9][20]\t Batch [17850][55000]\t Training Loss 0.8552\t Accuracy 0.8428\n",
      "Epoch [9][20]\t Batch [17900][55000]\t Training Loss 0.8556\t Accuracy 0.8426\n",
      "Epoch [9][20]\t Batch [17950][55000]\t Training Loss 0.8553\t Accuracy 0.8425\n",
      "Epoch [9][20]\t Batch [18000][55000]\t Training Loss 0.8550\t Accuracy 0.8426\n",
      "Epoch [9][20]\t Batch [18050][55000]\t Training Loss 0.8553\t Accuracy 0.8426\n",
      "Epoch [9][20]\t Batch [18100][55000]\t Training Loss 0.8551\t Accuracy 0.8428\n",
      "Epoch [9][20]\t Batch [18150][55000]\t Training Loss 0.8547\t Accuracy 0.8428\n",
      "Epoch [9][20]\t Batch [18200][55000]\t Training Loss 0.8543\t Accuracy 0.8431\n",
      "Epoch [9][20]\t Batch [18250][55000]\t Training Loss 0.8542\t Accuracy 0.8431\n",
      "Epoch [9][20]\t Batch [18300][55000]\t Training Loss 0.8538\t Accuracy 0.8432\n",
      "Epoch [9][20]\t Batch [18350][55000]\t Training Loss 0.8537\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [18400][55000]\t Training Loss 0.8537\t Accuracy 0.8435\n",
      "Epoch [9][20]\t Batch [18450][55000]\t Training Loss 0.8541\t Accuracy 0.8433\n",
      "Epoch [9][20]\t Batch [18500][55000]\t Training Loss 0.8541\t Accuracy 0.8433\n",
      "Epoch [9][20]\t Batch [18550][55000]\t Training Loss 0.8539\t Accuracy 0.8436\n",
      "Epoch [9][20]\t Batch [18600][55000]\t Training Loss 0.8539\t Accuracy 0.8438\n",
      "Epoch [9][20]\t Batch [18650][55000]\t Training Loss 0.8538\t Accuracy 0.8439\n",
      "Epoch [9][20]\t Batch [18700][55000]\t Training Loss 0.8540\t Accuracy 0.8438\n",
      "Epoch [9][20]\t Batch [18750][55000]\t Training Loss 0.8542\t Accuracy 0.8438\n",
      "Epoch [9][20]\t Batch [18800][55000]\t Training Loss 0.8538\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [18850][55000]\t Training Loss 0.8541\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [18900][55000]\t Training Loss 0.8536\t Accuracy 0.8445\n",
      "Epoch [9][20]\t Batch [18950][55000]\t Training Loss 0.8533\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [19000][55000]\t Training Loss 0.8533\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [19050][55000]\t Training Loss 0.8537\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [19100][55000]\t Training Loss 0.8540\t Accuracy 0.8444\n",
      "Epoch [9][20]\t Batch [19150][55000]\t Training Loss 0.8541\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [19200][55000]\t Training Loss 0.8542\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [19250][55000]\t Training Loss 0.8540\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [19300][55000]\t Training Loss 0.8538\t Accuracy 0.8444\n",
      "Epoch [9][20]\t Batch [19350][55000]\t Training Loss 0.8539\t Accuracy 0.8442\n",
      "Epoch [9][20]\t Batch [19400][55000]\t Training Loss 0.8537\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [19450][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [19500][55000]\t Training Loss 0.8531\t Accuracy 0.8444\n",
      "Epoch [9][20]\t Batch [19550][55000]\t Training Loss 0.8531\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [19600][55000]\t Training Loss 0.8531\t Accuracy 0.8441\n",
      "Epoch [9][20]\t Batch [19650][55000]\t Training Loss 0.8527\t Accuracy 0.8443\n",
      "Epoch [9][20]\t Batch [19700][55000]\t Training Loss 0.8521\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [19750][55000]\t Training Loss 0.8516\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [19800][55000]\t Training Loss 0.8510\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [19850][55000]\t Training Loss 0.8510\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [19900][55000]\t Training Loss 0.8508\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [19950][55000]\t Training Loss 0.8509\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [20000][55000]\t Training Loss 0.8509\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [20050][55000]\t Training Loss 0.8514\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [20100][55000]\t Training Loss 0.8515\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [20150][55000]\t Training Loss 0.8513\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [20200][55000]\t Training Loss 0.8517\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [20250][55000]\t Training Loss 0.8518\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [20300][55000]\t Training Loss 0.8519\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [20350][55000]\t Training Loss 0.8519\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [20400][55000]\t Training Loss 0.8516\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [20450][55000]\t Training Loss 0.8512\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [20500][55000]\t Training Loss 0.8509\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [20550][55000]\t Training Loss 0.8509\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [20600][55000]\t Training Loss 0.8508\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [20650][55000]\t Training Loss 0.8505\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [20700][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [20750][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [20800][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [20850][55000]\t Training Loss 0.8503\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [20900][55000]\t Training Loss 0.8506\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [20950][55000]\t Training Loss 0.8511\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [21000][55000]\t Training Loss 0.8513\t Accuracy 0.8445\n",
      "Epoch [9][20]\t Batch [21050][55000]\t Training Loss 0.8515\t Accuracy 0.8445\n",
      "Epoch [9][20]\t Batch [21100][55000]\t Training Loss 0.8513\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [21150][55000]\t Training Loss 0.8512\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [21200][55000]\t Training Loss 0.8510\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [21250][55000]\t Training Loss 0.8508\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [21300][55000]\t Training Loss 0.8504\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [21350][55000]\t Training Loss 0.8505\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [21400][55000]\t Training Loss 0.8505\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [21450][55000]\t Training Loss 0.8506\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [21500][55000]\t Training Loss 0.8502\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [21550][55000]\t Training Loss 0.8500\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [21600][55000]\t Training Loss 0.8501\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [21650][55000]\t Training Loss 0.8501\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [21700][55000]\t Training Loss 0.8501\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [21750][55000]\t Training Loss 0.8501\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [21800][55000]\t Training Loss 0.8495\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [21850][55000]\t Training Loss 0.8491\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [21900][55000]\t Training Loss 0.8487\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [21950][55000]\t Training Loss 0.8482\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [22000][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [22050][55000]\t Training Loss 0.8474\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [22100][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [22150][55000]\t Training Loss 0.8480\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [22200][55000]\t Training Loss 0.8483\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [22250][55000]\t Training Loss 0.8483\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [22300][55000]\t Training Loss 0.8485\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [22350][55000]\t Training Loss 0.8482\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [22400][55000]\t Training Loss 0.8481\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [22450][55000]\t Training Loss 0.8481\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [22500][55000]\t Training Loss 0.8486\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [22550][55000]\t Training Loss 0.8492\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [22600][55000]\t Training Loss 0.8496\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [22650][55000]\t Training Loss 0.8498\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [22700][55000]\t Training Loss 0.8496\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [22750][55000]\t Training Loss 0.8495\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [22800][55000]\t Training Loss 0.8494\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [22850][55000]\t Training Loss 0.8495\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [22900][55000]\t Training Loss 0.8492\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [22950][55000]\t Training Loss 0.8490\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [23000][55000]\t Training Loss 0.8488\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [23050][55000]\t Training Loss 0.8486\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [23100][55000]\t Training Loss 0.8487\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [23150][55000]\t Training Loss 0.8486\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [23200][55000]\t Training Loss 0.8487\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [23250][55000]\t Training Loss 0.8484\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [23300][55000]\t Training Loss 0.8480\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [23350][55000]\t Training Loss 0.8479\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [23400][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [23450][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [23500][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [23550][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [23600][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [23650][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [23700][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [23750][55000]\t Training Loss 0.8482\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [23800][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [23850][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [23900][55000]\t Training Loss 0.8479\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [23950][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [24000][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [24050][55000]\t Training Loss 0.8479\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [24100][55000]\t Training Loss 0.8478\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [24150][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [24200][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [24250][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [24300][55000]\t Training Loss 0.8478\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [24350][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [24400][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [24450][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [24500][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [24550][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [24600][55000]\t Training Loss 0.8479\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [24650][55000]\t Training Loss 0.8480\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [24700][55000]\t Training Loss 0.8479\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [24750][55000]\t Training Loss 0.8482\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [24800][55000]\t Training Loss 0.8485\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [24850][55000]\t Training Loss 0.8484\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [24900][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [24950][55000]\t Training Loss 0.8488\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [25000][55000]\t Training Loss 0.8490\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [25050][55000]\t Training Loss 0.8487\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [25100][55000]\t Training Loss 0.8487\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [25150][55000]\t Training Loss 0.8485\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [25200][55000]\t Training Loss 0.8484\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [25250][55000]\t Training Loss 0.8483\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [25300][55000]\t Training Loss 0.8482\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [25350][55000]\t Training Loss 0.8484\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [25400][55000]\t Training Loss 0.8479\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [25450][55000]\t Training Loss 0.8474\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [25500][55000]\t Training Loss 0.8473\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [25550][55000]\t Training Loss 0.8469\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [25600][55000]\t Training Loss 0.8468\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [25650][55000]\t Training Loss 0.8467\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [25700][55000]\t Training Loss 0.8465\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [25750][55000]\t Training Loss 0.8463\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [25800][55000]\t Training Loss 0.8462\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [25850][55000]\t Training Loss 0.8463\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [25900][55000]\t Training Loss 0.8464\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [25950][55000]\t Training Loss 0.8464\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [26000][55000]\t Training Loss 0.8464\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [26050][55000]\t Training Loss 0.8462\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [26100][55000]\t Training Loss 0.8459\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [26150][55000]\t Training Loss 0.8457\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [26200][55000]\t Training Loss 0.8455\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [26250][55000]\t Training Loss 0.8455\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [26300][55000]\t Training Loss 0.8457\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [26350][55000]\t Training Loss 0.8455\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [26400][55000]\t Training Loss 0.8459\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [26450][55000]\t Training Loss 0.8461\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [26500][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [26550][55000]\t Training Loss 0.8465\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [26600][55000]\t Training Loss 0.8467\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [26650][55000]\t Training Loss 0.8471\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [26700][55000]\t Training Loss 0.8469\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [26750][55000]\t Training Loss 0.8472\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [26800][55000]\t Training Loss 0.8471\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [26850][55000]\t Training Loss 0.8469\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [26900][55000]\t Training Loss 0.8472\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [26950][55000]\t Training Loss 0.8471\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [27000][55000]\t Training Loss 0.8469\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [27050][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [27100][55000]\t Training Loss 0.8466\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [27150][55000]\t Training Loss 0.8466\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [27200][55000]\t Training Loss 0.8469\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [27250][55000]\t Training Loss 0.8468\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [27300][55000]\t Training Loss 0.8468\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [27350][55000]\t Training Loss 0.8468\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [27400][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [27450][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [27500][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [27550][55000]\t Training Loss 0.8468\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [27600][55000]\t Training Loss 0.8466\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [27650][55000]\t Training Loss 0.8466\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [27700][55000]\t Training Loss 0.8466\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [27750][55000]\t Training Loss 0.8467\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [27800][55000]\t Training Loss 0.8468\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [27850][55000]\t Training Loss 0.8469\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [27900][55000]\t Training Loss 0.8467\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [27950][55000]\t Training Loss 0.8465\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [28000][55000]\t Training Loss 0.8463\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [28050][55000]\t Training Loss 0.8460\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [28100][55000]\t Training Loss 0.8456\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [28150][55000]\t Training Loss 0.8454\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [28200][55000]\t Training Loss 0.8455\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [28250][55000]\t Training Loss 0.8451\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [28300][55000]\t Training Loss 0.8450\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [28350][55000]\t Training Loss 0.8448\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [28400][55000]\t Training Loss 0.8452\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [28450][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [9][20]\t Batch [28500][55000]\t Training Loss 0.8448\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [28550][55000]\t Training Loss 0.8447\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [28600][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [28650][55000]\t Training Loss 0.8448\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [28700][55000]\t Training Loss 0.8451\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [28750][55000]\t Training Loss 0.8452\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [28800][55000]\t Training Loss 0.8451\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [28850][55000]\t Training Loss 0.8451\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [28900][55000]\t Training Loss 0.8449\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [28950][55000]\t Training Loss 0.8449\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [29000][55000]\t Training Loss 0.8449\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [29050][55000]\t Training Loss 0.8449\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [29100][55000]\t Training Loss 0.8450\t Accuracy 0.8480\n",
      "Epoch [9][20]\t Batch [29150][55000]\t Training Loss 0.8454\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [29200][55000]\t Training Loss 0.8457\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [29250][55000]\t Training Loss 0.8459\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [29300][55000]\t Training Loss 0.8458\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [29350][55000]\t Training Loss 0.8459\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [29400][55000]\t Training Loss 0.8458\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [29450][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [29500][55000]\t Training Loss 0.8452\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [29550][55000]\t Training Loss 0.8452\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [29600][55000]\t Training Loss 0.8451\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [29650][55000]\t Training Loss 0.8451\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [29700][55000]\t Training Loss 0.8452\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [29750][55000]\t Training Loss 0.8455\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [29800][55000]\t Training Loss 0.8457\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [29850][55000]\t Training Loss 0.8460\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [29900][55000]\t Training Loss 0.8463\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [29950][55000]\t Training Loss 0.8466\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [30000][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [30050][55000]\t Training Loss 0.8471\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [30100][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [30150][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [30200][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [30250][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [30300][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [30350][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [30400][55000]\t Training Loss 0.8479\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [30450][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [30500][55000]\t Training Loss 0.8481\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [30550][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [30600][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [30650][55000]\t Training Loss 0.8490\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [30700][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [30750][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [30800][55000]\t Training Loss 0.8496\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [30850][55000]\t Training Loss 0.8495\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [30900][55000]\t Training Loss 0.8499\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [30950][55000]\t Training Loss 0.8497\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [31000][55000]\t Training Loss 0.8497\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [31050][55000]\t Training Loss 0.8498\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [31100][55000]\t Training Loss 0.8498\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [31150][55000]\t Training Loss 0.8499\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [31200][55000]\t Training Loss 0.8499\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [31250][55000]\t Training Loss 0.8499\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [31300][55000]\t Training Loss 0.8502\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [31350][55000]\t Training Loss 0.8509\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [31400][55000]\t Training Loss 0.8510\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [31450][55000]\t Training Loss 0.8513\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [31500][55000]\t Training Loss 0.8513\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [31550][55000]\t Training Loss 0.8513\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [31600][55000]\t Training Loss 0.8515\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [31650][55000]\t Training Loss 0.8517\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [31700][55000]\t Training Loss 0.8520\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [31750][55000]\t Training Loss 0.8524\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [31800][55000]\t Training Loss 0.8525\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [31850][55000]\t Training Loss 0.8524\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [31900][55000]\t Training Loss 0.8524\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [31950][55000]\t Training Loss 0.8524\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [32000][55000]\t Training Loss 0.8524\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [32050][55000]\t Training Loss 0.8524\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [32100][55000]\t Training Loss 0.8523\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [32150][55000]\t Training Loss 0.8525\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [32200][55000]\t Training Loss 0.8527\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [32250][55000]\t Training Loss 0.8530\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [32300][55000]\t Training Loss 0.8534\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [32350][55000]\t Training Loss 0.8537\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [32400][55000]\t Training Loss 0.8537\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [32450][55000]\t Training Loss 0.8539\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [32500][55000]\t Training Loss 0.8541\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [32550][55000]\t Training Loss 0.8544\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [32600][55000]\t Training Loss 0.8544\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [32650][55000]\t Training Loss 0.8542\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [32700][55000]\t Training Loss 0.8542\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [32750][55000]\t Training Loss 0.8543\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [32800][55000]\t Training Loss 0.8544\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [32850][55000]\t Training Loss 0.8544\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [32900][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [32950][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33000][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33050][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33100][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33150][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [33200][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [33250][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33300][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33350][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33400][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [33450][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [33500][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [33550][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [33600][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33650][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [33700][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [33750][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [33800][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [33850][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [33900][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [33950][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [34000][55000]\t Training Loss 0.8532\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [34050][55000]\t Training Loss 0.8533\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [34100][55000]\t Training Loss 0.8534\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [34150][55000]\t Training Loss 0.8535\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [34200][55000]\t Training Loss 0.8533\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [34250][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [34300][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [34350][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [34400][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [34450][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [34500][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [34550][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [34600][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [34650][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [34700][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [34750][55000]\t Training Loss 0.8532\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [34800][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [34850][55000]\t Training Loss 0.8536\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [34900][55000]\t Training Loss 0.8536\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [34950][55000]\t Training Loss 0.8537\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [35000][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [35050][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [35100][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [35150][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [35200][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [35250][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [35300][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [35350][55000]\t Training Loss 0.8536\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [35400][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [35450][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [35500][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [35550][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [35600][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [35650][55000]\t Training Loss 0.8536\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [35700][55000]\t Training Loss 0.8535\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [35750][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [35800][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [35850][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [35900][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [35950][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [36000][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [36050][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [36100][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [36150][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [36200][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [36250][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [36300][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [36350][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [36400][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [36450][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [36500][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [36550][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [36600][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [36650][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [36700][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [36750][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [36800][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [36850][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [36900][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [36950][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [37000][55000]\t Training Loss 0.8518\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [37050][55000]\t Training Loss 0.8516\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [37100][55000]\t Training Loss 0.8517\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [37150][55000]\t Training Loss 0.8517\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [37200][55000]\t Training Loss 0.8516\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [37250][55000]\t Training Loss 0.8515\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [37300][55000]\t Training Loss 0.8516\t Accuracy 0.8478\n",
      "Epoch [9][20]\t Batch [37350][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [37400][55000]\t Training Loss 0.8519\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [37450][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [37500][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [37550][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [37600][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [37650][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [37700][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [37750][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [37800][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [37850][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [37900][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [37950][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [38000][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [38050][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [38100][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [38150][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [38200][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [38250][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [38300][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [38350][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [38400][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [38450][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [38500][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [38550][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [38600][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [38650][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [38700][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [38750][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [38800][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [38850][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [38900][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [38950][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [39000][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [39050][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [39100][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [39150][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [39200][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [39250][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [9][20]\t Batch [39300][55000]\t Training Loss 0.8516\t Accuracy 0.8477\n",
      "Epoch [9][20]\t Batch [39350][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [39400][55000]\t Training Loss 0.8519\t Accuracy 0.8475\n",
      "Epoch [9][20]\t Batch [39450][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [9][20]\t Batch [39500][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [39550][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [39600][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [39650][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [39700][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [39750][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [39800][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [39850][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [39900][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [39950][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40000][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40050][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [40100][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [40150][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40200][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40250][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40300][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40350][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40400][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40450][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40500][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40550][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40600][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40650][55000]\t Training Loss 0.8527\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [40700][55000]\t Training Loss 0.8527\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [40750][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40800][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [40850][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [40900][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [40950][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [41000][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [41050][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [41100][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [41150][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [41200][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [41250][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [41300][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [9][20]\t Batch [41350][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [41400][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [41450][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [41500][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [41550][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [41600][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [41650][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [41700][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [9][20]\t Batch [41750][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [41800][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [41850][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [41900][55000]\t Training Loss 0.8538\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [41950][55000]\t Training Loss 0.8539\t Accuracy 0.8469\n",
      "Epoch [9][20]\t Batch [42000][55000]\t Training Loss 0.8538\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [42050][55000]\t Training Loss 0.8537\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [42100][55000]\t Training Loss 0.8536\t Accuracy 0.8468\n",
      "Epoch [9][20]\t Batch [42150][55000]\t Training Loss 0.8538\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [42200][55000]\t Training Loss 0.8538\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [42250][55000]\t Training Loss 0.8539\t Accuracy 0.8467\n",
      "Epoch [9][20]\t Batch [42300][55000]\t Training Loss 0.8539\t Accuracy 0.8466\n",
      "Epoch [9][20]\t Batch [42350][55000]\t Training Loss 0.8541\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [42400][55000]\t Training Loss 0.8543\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [42450][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [42500][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [42550][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [42600][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [42650][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [42700][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [42750][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [42800][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [42850][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [42900][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [42950][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [43000][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [43050][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [43100][55000]\t Training Loss 0.8549\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [43150][55000]\t Training Loss 0.8549\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [43200][55000]\t Training Loss 0.8548\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [43250][55000]\t Training Loss 0.8549\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [43300][55000]\t Training Loss 0.8548\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [43350][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [43400][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [43450][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [43500][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [43550][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [43600][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [43650][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [43700][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [43750][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [43800][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [43850][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [43900][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [43950][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [44000][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [44050][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [44100][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [44150][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [44200][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [44250][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [44300][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [44350][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [44400][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [44450][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [44500][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [44550][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [44600][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [44650][55000]\t Training Loss 0.8543\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [44700][55000]\t Training Loss 0.8542\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [44750][55000]\t Training Loss 0.8542\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [44800][55000]\t Training Loss 0.8542\t Accuracy 0.8465\n",
      "Epoch [9][20]\t Batch [44850][55000]\t Training Loss 0.8543\t Accuracy 0.8464\n",
      "Epoch [9][20]\t Batch [44900][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [44950][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45000][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45050][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [45100][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45150][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [45200][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [45250][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45300][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45350][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45400][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [45450][55000]\t Training Loss 0.8546\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [45500][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [45550][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45600][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45650][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [45700][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [45750][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45800][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45850][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [45900][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [45950][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [46000][55000]\t Training Loss 0.8551\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [46050][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [46100][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [46150][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46200][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46250][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46300][55000]\t Training Loss 0.8555\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [46350][55000]\t Training Loss 0.8555\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [46400][55000]\t Training Loss 0.8557\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [46450][55000]\t Training Loss 0.8558\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [46500][55000]\t Training Loss 0.8557\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [46550][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46600][55000]\t Training Loss 0.8555\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46650][55000]\t Training Loss 0.8555\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46700][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46750][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46800][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [46850][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46900][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [46950][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [47000][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47050][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47100][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [47150][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [47200][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [47250][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [47300][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47350][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47400][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47450][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47500][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [47550][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47600][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47650][55000]\t Training Loss 0.8552\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [47700][55000]\t Training Loss 0.8552\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [47750][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [47800][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [47850][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47900][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [47950][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [48000][55000]\t Training Loss 0.8549\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [48050][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [48100][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [48150][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [48200][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48250][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48300][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48350][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48400][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [48450][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48500][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48550][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48600][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [48650][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48700][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [48750][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [48800][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [48850][55000]\t Training Loss 0.8532\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [48900][55000]\t Training Loss 0.8531\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [48950][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [9][20]\t Batch [49000][55000]\t Training Loss 0.8534\t Accuracy 0.8460\n",
      "Epoch [9][20]\t Batch [49050][55000]\t Training Loss 0.8537\t Accuracy 0.8459\n",
      "Epoch [9][20]\t Batch [49100][55000]\t Training Loss 0.8539\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [49150][55000]\t Training Loss 0.8539\t Accuracy 0.8458\n",
      "Epoch [9][20]\t Batch [49200][55000]\t Training Loss 0.8539\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [49250][55000]\t Training Loss 0.8541\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [49300][55000]\t Training Loss 0.8540\t Accuracy 0.8456\n",
      "Epoch [9][20]\t Batch [49350][55000]\t Training Loss 0.8539\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [49400][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [49450][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [9][20]\t Batch [49500][55000]\t Training Loss 0.8539\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [49550][55000]\t Training Loss 0.8542\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [49600][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [49650][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [49700][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [49750][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [49800][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [49850][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [49900][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [49950][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [50000][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [50050][55000]\t Training Loss 0.8552\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [50100][55000]\t Training Loss 0.8552\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [50150][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [50200][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [50250][55000]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [50300][55000]\t Training Loss 0.8552\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [50350][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [50400][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [50450][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [50500][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [50550][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [50600][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [50650][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [50700][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [50750][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [50800][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [50850][55000]\t Training Loss 0.8560\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [50900][55000]\t Training Loss 0.8559\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [50950][55000]\t Training Loss 0.8558\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [51000][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [51050][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [51100][55000]\t Training Loss 0.8554\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [51150][55000]\t Training Loss 0.8553\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [51200][55000]\t Training Loss 0.8554\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [51250][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [51300][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [51350][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [51400][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [51450][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [51500][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [51550][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [51600][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [51650][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [51700][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [51750][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [51800][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [51850][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [51900][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [51950][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [52000][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [52050][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [52100][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [52150][55000]\t Training Loss 0.8554\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [52200][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [52250][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [52300][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [52350][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [52400][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [52450][55000]\t Training Loss 0.8554\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [52500][55000]\t Training Loss 0.8553\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [52550][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [52600][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [52650][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [52700][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [9][20]\t Batch [52750][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [52800][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [52850][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [52900][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [52950][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [53000][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [53050][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53100][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53150][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [53200][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [53250][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53300][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53350][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53400][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [53450][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [53500][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [53550][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53600][55000]\t Training Loss 0.8556\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [53650][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53700][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53750][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53800][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [53850][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53900][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [53950][55000]\t Training Loss 0.8554\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [54000][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [54050][55000]\t Training Loss 0.8556\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [54100][55000]\t Training Loss 0.8558\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [54150][55000]\t Training Loss 0.8557\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [54200][55000]\t Training Loss 0.8558\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [54250][55000]\t Training Loss 0.8556\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [54300][55000]\t Training Loss 0.8556\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [54350][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [54400][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [54450][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [54500][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [54550][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [54600][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [54650][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [54700][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [54750][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [54800][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [54850][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [54900][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [9][20]\t Batch [54950][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8553\t Average training accuracy 0.8447\n",
      "Epoch [9]\t Average validation loss 0.7927\t Average validation accuracy 0.8696\n",
      "\n",
      "Epoch [10][20]\t Batch [0][55000]\t Training Loss 1.4736\t Accuracy 0.0000\n",
      "Epoch [10][20]\t Batch [50][55000]\t Training Loss 0.9396\t Accuracy 0.7255\n",
      "Epoch [10][20]\t Batch [100][55000]\t Training Loss 0.8497\t Accuracy 0.8020\n",
      "Epoch [10][20]\t Batch [150][55000]\t Training Loss 0.8509\t Accuracy 0.8146\n",
      "Epoch [10][20]\t Batch [200][55000]\t Training Loss 0.8617\t Accuracy 0.8159\n",
      "Epoch [10][20]\t Batch [250][55000]\t Training Loss 0.8439\t Accuracy 0.8247\n",
      "Epoch [10][20]\t Batch [300][55000]\t Training Loss 0.8455\t Accuracy 0.8206\n",
      "Epoch [10][20]\t Batch [350][55000]\t Training Loss 0.8257\t Accuracy 0.8234\n",
      "Epoch [10][20]\t Batch [400][55000]\t Training Loss 0.8119\t Accuracy 0.8279\n",
      "Epoch [10][20]\t Batch [450][55000]\t Training Loss 0.8177\t Accuracy 0.8293\n",
      "Epoch [10][20]\t Batch [500][55000]\t Training Loss 0.8240\t Accuracy 0.8323\n",
      "Epoch [10][20]\t Batch [550][55000]\t Training Loss 0.8413\t Accuracy 0.8294\n",
      "Epoch [10][20]\t Batch [600][55000]\t Training Loss 0.8457\t Accuracy 0.8319\n",
      "Epoch [10][20]\t Batch [650][55000]\t Training Loss 0.8655\t Accuracy 0.8218\n",
      "Epoch [10][20]\t Batch [700][55000]\t Training Loss 0.8632\t Accuracy 0.8274\n",
      "Epoch [10][20]\t Batch [750][55000]\t Training Loss 0.8595\t Accuracy 0.8282\n",
      "Epoch [10][20]\t Batch [800][55000]\t Training Loss 0.8534\t Accuracy 0.8302\n",
      "Epoch [10][20]\t Batch [850][55000]\t Training Loss 0.8519\t Accuracy 0.8320\n",
      "Epoch [10][20]\t Batch [900][55000]\t Training Loss 0.8584\t Accuracy 0.8313\n",
      "Epoch [10][20]\t Batch [950][55000]\t Training Loss 0.8652\t Accuracy 0.8265\n",
      "Epoch [10][20]\t Batch [1000][55000]\t Training Loss 0.8646\t Accuracy 0.8292\n",
      "Epoch [10][20]\t Batch [1050][55000]\t Training Loss 0.8723\t Accuracy 0.8278\n",
      "Epoch [10][20]\t Batch [1100][55000]\t Training Loss 0.8807\t Accuracy 0.8274\n",
      "Epoch [10][20]\t Batch [1150][55000]\t Training Loss 0.8899\t Accuracy 0.8245\n",
      "Epoch [10][20]\t Batch [1200][55000]\t Training Loss 0.8842\t Accuracy 0.8276\n",
      "Epoch [10][20]\t Batch [1250][55000]\t Training Loss 0.8855\t Accuracy 0.8273\n",
      "Epoch [10][20]\t Batch [1300][55000]\t Training Loss 0.8858\t Accuracy 0.8271\n",
      "Epoch [10][20]\t Batch [1350][55000]\t Training Loss 0.8826\t Accuracy 0.8283\n",
      "Epoch [10][20]\t Batch [1400][55000]\t Training Loss 0.8831\t Accuracy 0.8273\n",
      "Epoch [10][20]\t Batch [1450][55000]\t Training Loss 0.8840\t Accuracy 0.8263\n",
      "Epoch [10][20]\t Batch [1500][55000]\t Training Loss 0.8821\t Accuracy 0.8288\n",
      "Epoch [10][20]\t Batch [1550][55000]\t Training Loss 0.8816\t Accuracy 0.8291\n",
      "Epoch [10][20]\t Batch [1600][55000]\t Training Loss 0.8842\t Accuracy 0.8282\n",
      "Epoch [10][20]\t Batch [1650][55000]\t Training Loss 0.8811\t Accuracy 0.8298\n",
      "Epoch [10][20]\t Batch [1700][55000]\t Training Loss 0.8794\t Accuracy 0.8313\n",
      "Epoch [10][20]\t Batch [1750][55000]\t Training Loss 0.8738\t Accuracy 0.8327\n",
      "Epoch [10][20]\t Batch [1800][55000]\t Training Loss 0.8716\t Accuracy 0.8345\n",
      "Epoch [10][20]\t Batch [1850][55000]\t Training Loss 0.8700\t Accuracy 0.8352\n",
      "Epoch [10][20]\t Batch [1900][55000]\t Training Loss 0.8678\t Accuracy 0.8353\n",
      "Epoch [10][20]\t Batch [1950][55000]\t Training Loss 0.8663\t Accuracy 0.8365\n",
      "Epoch [10][20]\t Batch [2000][55000]\t Training Loss 0.8639\t Accuracy 0.8376\n",
      "Epoch [10][20]\t Batch [2050][55000]\t Training Loss 0.8629\t Accuracy 0.8381\n",
      "Epoch [10][20]\t Batch [2100][55000]\t Training Loss 0.8601\t Accuracy 0.8386\n",
      "Epoch [10][20]\t Batch [2150][55000]\t Training Loss 0.8568\t Accuracy 0.8405\n",
      "Epoch [10][20]\t Batch [2200][55000]\t Training Loss 0.8520\t Accuracy 0.8423\n",
      "Epoch [10][20]\t Batch [2250][55000]\t Training Loss 0.8512\t Accuracy 0.8427\n",
      "Epoch [10][20]\t Batch [2300][55000]\t Training Loss 0.8483\t Accuracy 0.8422\n",
      "Epoch [10][20]\t Batch [2350][55000]\t Training Loss 0.8467\t Accuracy 0.8426\n",
      "Epoch [10][20]\t Batch [2400][55000]\t Training Loss 0.8477\t Accuracy 0.8409\n",
      "Epoch [10][20]\t Batch [2450][55000]\t Training Loss 0.8506\t Accuracy 0.8397\n",
      "Epoch [10][20]\t Batch [2500][55000]\t Training Loss 0.8484\t Accuracy 0.8417\n",
      "Epoch [10][20]\t Batch [2550][55000]\t Training Loss 0.8471\t Accuracy 0.8416\n",
      "Epoch [10][20]\t Batch [2600][55000]\t Training Loss 0.8465\t Accuracy 0.8408\n",
      "Epoch [10][20]\t Batch [2650][55000]\t Training Loss 0.8453\t Accuracy 0.8412\n",
      "Epoch [10][20]\t Batch [2700][55000]\t Training Loss 0.8451\t Accuracy 0.8415\n",
      "Epoch [10][20]\t Batch [2750][55000]\t Training Loss 0.8450\t Accuracy 0.8415\n",
      "Epoch [10][20]\t Batch [2800][55000]\t Training Loss 0.8453\t Accuracy 0.8397\n",
      "Epoch [10][20]\t Batch [2850][55000]\t Training Loss 0.8442\t Accuracy 0.8397\n",
      "Epoch [10][20]\t Batch [2900][55000]\t Training Loss 0.8409\t Accuracy 0.8407\n",
      "Epoch [10][20]\t Batch [2950][55000]\t Training Loss 0.8410\t Accuracy 0.8404\n",
      "Epoch [10][20]\t Batch [3000][55000]\t Training Loss 0.8408\t Accuracy 0.8411\n",
      "Epoch [10][20]\t Batch [3050][55000]\t Training Loss 0.8419\t Accuracy 0.8407\n",
      "Epoch [10][20]\t Batch [3100][55000]\t Training Loss 0.8442\t Accuracy 0.8404\n",
      "Epoch [10][20]\t Batch [3150][55000]\t Training Loss 0.8441\t Accuracy 0.8407\n",
      "Epoch [10][20]\t Batch [3200][55000]\t Training Loss 0.8439\t Accuracy 0.8419\n",
      "Epoch [10][20]\t Batch [3250][55000]\t Training Loss 0.8432\t Accuracy 0.8413\n",
      "Epoch [10][20]\t Batch [3300][55000]\t Training Loss 0.8445\t Accuracy 0.8413\n",
      "Epoch [10][20]\t Batch [3350][55000]\t Training Loss 0.8428\t Accuracy 0.8427\n",
      "Epoch [10][20]\t Batch [3400][55000]\t Training Loss 0.8451\t Accuracy 0.8412\n",
      "Epoch [10][20]\t Batch [3450][55000]\t Training Loss 0.8448\t Accuracy 0.8421\n",
      "Epoch [10][20]\t Batch [3500][55000]\t Training Loss 0.8447\t Accuracy 0.8418\n",
      "Epoch [10][20]\t Batch [3550][55000]\t Training Loss 0.8463\t Accuracy 0.8412\n",
      "Epoch [10][20]\t Batch [3600][55000]\t Training Loss 0.8462\t Accuracy 0.8406\n",
      "Epoch [10][20]\t Batch [3650][55000]\t Training Loss 0.8448\t Accuracy 0.8414\n",
      "Epoch [10][20]\t Batch [3700][55000]\t Training Loss 0.8455\t Accuracy 0.8409\n",
      "Epoch [10][20]\t Batch [3750][55000]\t Training Loss 0.8455\t Accuracy 0.8411\n",
      "Epoch [10][20]\t Batch [3800][55000]\t Training Loss 0.8458\t Accuracy 0.8411\n",
      "Epoch [10][20]\t Batch [3850][55000]\t Training Loss 0.8454\t Accuracy 0.8416\n",
      "Epoch [10][20]\t Batch [3900][55000]\t Training Loss 0.8442\t Accuracy 0.8429\n",
      "Epoch [10][20]\t Batch [3950][55000]\t Training Loss 0.8431\t Accuracy 0.8441\n",
      "Epoch [10][20]\t Batch [4000][55000]\t Training Loss 0.8427\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [4050][55000]\t Training Loss 0.8412\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [4100][55000]\t Training Loss 0.8417\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [4150][55000]\t Training Loss 0.8421\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [4200][55000]\t Training Loss 0.8426\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [4250][55000]\t Training Loss 0.8412\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [4300][55000]\t Training Loss 0.8412\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [4350][55000]\t Training Loss 0.8417\t Accuracy 0.8456\n",
      "Epoch [10][20]\t Batch [4400][55000]\t Training Loss 0.8416\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [4450][55000]\t Training Loss 0.8418\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [4500][55000]\t Training Loss 0.8421\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [4550][55000]\t Training Loss 0.8404\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [4600][55000]\t Training Loss 0.8387\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [4650][55000]\t Training Loss 0.8385\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [4700][55000]\t Training Loss 0.8381\t Accuracy 0.8456\n",
      "Epoch [10][20]\t Batch [4750][55000]\t Training Loss 0.8369\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [4800][55000]\t Training Loss 0.8376\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [4850][55000]\t Training Loss 0.8388\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [4900][55000]\t Training Loss 0.8374\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [4950][55000]\t Training Loss 0.8376\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [5000][55000]\t Training Loss 0.8378\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [5050][55000]\t Training Loss 0.8373\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [5100][55000]\t Training Loss 0.8373\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [5150][55000]\t Training Loss 0.8376\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [5200][55000]\t Training Loss 0.8391\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [5250][55000]\t Training Loss 0.8382\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [5300][55000]\t Training Loss 0.8381\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [5350][55000]\t Training Loss 0.8387\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [5400][55000]\t Training Loss 0.8380\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [5450][55000]\t Training Loss 0.8372\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [5500][55000]\t Training Loss 0.8355\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [5550][55000]\t Training Loss 0.8354\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [5600][55000]\t Training Loss 0.8345\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [5650][55000]\t Training Loss 0.8351\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [5700][55000]\t Training Loss 0.8348\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [5750][55000]\t Training Loss 0.8352\t Accuracy 0.8456\n",
      "Epoch [10][20]\t Batch [5800][55000]\t Training Loss 0.8356\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [5850][55000]\t Training Loss 0.8357\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [5900][55000]\t Training Loss 0.8363\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [5950][55000]\t Training Loss 0.8361\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [6000][55000]\t Training Loss 0.8351\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [6050][55000]\t Training Loss 0.8337\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [6100][55000]\t Training Loss 0.8323\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [6150][55000]\t Training Loss 0.8302\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [6200][55000]\t Training Loss 0.8303\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [6250][55000]\t Training Loss 0.8291\t Accuracy 0.8487\n",
      "Epoch [10][20]\t Batch [6300][55000]\t Training Loss 0.8295\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [6350][55000]\t Training Loss 0.8291\t Accuracy 0.8487\n",
      "Epoch [10][20]\t Batch [6400][55000]\t Training Loss 0.8285\t Accuracy 0.8491\n",
      "Epoch [10][20]\t Batch [6450][55000]\t Training Loss 0.8280\t Accuracy 0.8493\n",
      "Epoch [10][20]\t Batch [6500][55000]\t Training Loss 0.8288\t Accuracy 0.8489\n",
      "Epoch [10][20]\t Batch [6550][55000]\t Training Loss 0.8278\t Accuracy 0.8492\n",
      "Epoch [10][20]\t Batch [6600][55000]\t Training Loss 0.8265\t Accuracy 0.8497\n",
      "Epoch [10][20]\t Batch [6650][55000]\t Training Loss 0.8253\t Accuracy 0.8498\n",
      "Epoch [10][20]\t Batch [6700][55000]\t Training Loss 0.8255\t Accuracy 0.8497\n",
      "Epoch [10][20]\t Batch [6750][55000]\t Training Loss 0.8257\t Accuracy 0.8505\n",
      "Epoch [10][20]\t Batch [6800][55000]\t Training Loss 0.8261\t Accuracy 0.8505\n",
      "Epoch [10][20]\t Batch [6850][55000]\t Training Loss 0.8283\t Accuracy 0.8501\n",
      "Epoch [10][20]\t Batch [6900][55000]\t Training Loss 0.8282\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [6950][55000]\t Training Loss 0.8289\t Accuracy 0.8489\n",
      "Epoch [10][20]\t Batch [7000][55000]\t Training Loss 0.8288\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [7050][55000]\t Training Loss 0.8293\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [7100][55000]\t Training Loss 0.8296\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [7150][55000]\t Training Loss 0.8298\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [7200][55000]\t Training Loss 0.8305\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [7250][55000]\t Training Loss 0.8329\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [7300][55000]\t Training Loss 0.8347\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [7350][55000]\t Training Loss 0.8362\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [7400][55000]\t Training Loss 0.8372\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [7450][55000]\t Training Loss 0.8372\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [7500][55000]\t Training Loss 0.8372\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [7550][55000]\t Training Loss 0.8377\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [7600][55000]\t Training Loss 0.8372\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [7650][55000]\t Training Loss 0.8377\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [7700][55000]\t Training Loss 0.8382\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [7750][55000]\t Training Loss 0.8383\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [7800][55000]\t Training Loss 0.8391\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [7850][55000]\t Training Loss 0.8392\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [7900][55000]\t Training Loss 0.8394\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [7950][55000]\t Training Loss 0.8395\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [8000][55000]\t Training Loss 0.8395\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [8050][55000]\t Training Loss 0.8400\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [8100][55000]\t Training Loss 0.8386\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [8150][55000]\t Training Loss 0.8394\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [8200][55000]\t Training Loss 0.8394\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [8250][55000]\t Training Loss 0.8406\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [8300][55000]\t Training Loss 0.8409\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [8350][55000]\t Training Loss 0.8415\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [8400][55000]\t Training Loss 0.8409\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [8450][55000]\t Training Loss 0.8424\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [8500][55000]\t Training Loss 0.8416\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [8550][55000]\t Training Loss 0.8406\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [8600][55000]\t Training Loss 0.8398\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [8650][55000]\t Training Loss 0.8399\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [8700][55000]\t Training Loss 0.8405\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [8750][55000]\t Training Loss 0.8421\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [8800][55000]\t Training Loss 0.8429\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [8850][55000]\t Training Loss 0.8428\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [8900][55000]\t Training Loss 0.8446\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [8950][55000]\t Training Loss 0.8442\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [9000][55000]\t Training Loss 0.8434\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [9050][55000]\t Training Loss 0.8426\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [9100][55000]\t Training Loss 0.8424\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [9150][55000]\t Training Loss 0.8429\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [9200][55000]\t Training Loss 0.8424\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [9250][55000]\t Training Loss 0.8428\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [9300][55000]\t Training Loss 0.8430\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [9350][55000]\t Training Loss 0.8432\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [9400][55000]\t Training Loss 0.8434\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [9450][55000]\t Training Loss 0.8437\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [9500][55000]\t Training Loss 0.8428\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [9550][55000]\t Training Loss 0.8425\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [9600][55000]\t Training Loss 0.8430\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [9650][55000]\t Training Loss 0.8428\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [9700][55000]\t Training Loss 0.8423\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [9750][55000]\t Training Loss 0.8415\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [9800][55000]\t Training Loss 0.8420\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [9850][55000]\t Training Loss 0.8416\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [9900][55000]\t Training Loss 0.8411\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [9950][55000]\t Training Loss 0.8405\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [10000][55000]\t Training Loss 0.8402\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [10050][55000]\t Training Loss 0.8404\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [10100][55000]\t Training Loss 0.8402\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [10150][55000]\t Training Loss 0.8400\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [10200][55000]\t Training Loss 0.8402\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [10250][55000]\t Training Loss 0.8403\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [10300][55000]\t Training Loss 0.8402\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [10350][55000]\t Training Loss 0.8393\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [10400][55000]\t Training Loss 0.8385\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [10450][55000]\t Training Loss 0.8383\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [10500][55000]\t Training Loss 0.8373\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [10550][55000]\t Training Loss 0.8368\t Accuracy 0.8485\n",
      "Epoch [10][20]\t Batch [10600][55000]\t Training Loss 0.8364\t Accuracy 0.8487\n",
      "Epoch [10][20]\t Batch [10650][55000]\t Training Loss 0.8361\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [10700][55000]\t Training Loss 0.8359\t Accuracy 0.8494\n",
      "Epoch [10][20]\t Batch [10750][55000]\t Training Loss 0.8364\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [10800][55000]\t Training Loss 0.8369\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [10850][55000]\t Training Loss 0.8362\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [10900][55000]\t Training Loss 0.8357\t Accuracy 0.8493\n",
      "Epoch [10][20]\t Batch [10950][55000]\t Training Loss 0.8354\t Accuracy 0.8491\n",
      "Epoch [10][20]\t Batch [11000][55000]\t Training Loss 0.8353\t Accuracy 0.8493\n",
      "Epoch [10][20]\t Batch [11050][55000]\t Training Loss 0.8346\t Accuracy 0.8495\n",
      "Epoch [10][20]\t Batch [11100][55000]\t Training Loss 0.8339\t Accuracy 0.8497\n",
      "Epoch [10][20]\t Batch [11150][55000]\t Training Loss 0.8339\t Accuracy 0.8499\n",
      "Epoch [10][20]\t Batch [11200][55000]\t Training Loss 0.8335\t Accuracy 0.8499\n",
      "Epoch [10][20]\t Batch [11250][55000]\t Training Loss 0.8339\t Accuracy 0.8498\n",
      "Epoch [10][20]\t Batch [11300][55000]\t Training Loss 0.8335\t Accuracy 0.8499\n",
      "Epoch [10][20]\t Batch [11350][55000]\t Training Loss 0.8330\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [11400][55000]\t Training Loss 0.8331\t Accuracy 0.8498\n",
      "Epoch [10][20]\t Batch [11450][55000]\t Training Loss 0.8328\t Accuracy 0.8499\n",
      "Epoch [10][20]\t Batch [11500][55000]\t Training Loss 0.8325\t Accuracy 0.8498\n",
      "Epoch [10][20]\t Batch [11550][55000]\t Training Loss 0.8326\t Accuracy 0.8496\n",
      "Epoch [10][20]\t Batch [11600][55000]\t Training Loss 0.8338\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [11650][55000]\t Training Loss 0.8343\t Accuracy 0.8487\n",
      "Epoch [10][20]\t Batch [11700][55000]\t Training Loss 0.8343\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [11750][55000]\t Training Loss 0.8352\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [11800][55000]\t Training Loss 0.8355\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [11850][55000]\t Training Loss 0.8355\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [11900][55000]\t Training Loss 0.8358\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [11950][55000]\t Training Loss 0.8359\t Accuracy 0.8485\n",
      "Epoch [10][20]\t Batch [12000][55000]\t Training Loss 0.8359\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [12050][55000]\t Training Loss 0.8357\t Accuracy 0.8491\n",
      "Epoch [10][20]\t Batch [12100][55000]\t Training Loss 0.8356\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [12150][55000]\t Training Loss 0.8350\t Accuracy 0.8495\n",
      "Epoch [10][20]\t Batch [12200][55000]\t Training Loss 0.8353\t Accuracy 0.8494\n",
      "Epoch [10][20]\t Batch [12250][55000]\t Training Loss 0.8352\t Accuracy 0.8495\n",
      "Epoch [10][20]\t Batch [12300][55000]\t Training Loss 0.8352\t Accuracy 0.8494\n",
      "Epoch [10][20]\t Batch [12350][55000]\t Training Loss 0.8354\t Accuracy 0.8496\n",
      "Epoch [10][20]\t Batch [12400][55000]\t Training Loss 0.8357\t Accuracy 0.8496\n",
      "Epoch [10][20]\t Batch [12450][55000]\t Training Loss 0.8359\t Accuracy 0.8492\n",
      "Epoch [10][20]\t Batch [12500][55000]\t Training Loss 0.8361\t Accuracy 0.8489\n",
      "Epoch [10][20]\t Batch [12550][55000]\t Training Loss 0.8361\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [12600][55000]\t Training Loss 0.8369\t Accuracy 0.8487\n",
      "Epoch [10][20]\t Batch [12650][55000]\t Training Loss 0.8374\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [12700][55000]\t Training Loss 0.8381\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [12750][55000]\t Training Loss 0.8376\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [12800][55000]\t Training Loss 0.8382\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [12850][55000]\t Training Loss 0.8384\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [12900][55000]\t Training Loss 0.8384\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [12950][55000]\t Training Loss 0.8389\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [13000][55000]\t Training Loss 0.8389\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [13050][55000]\t Training Loss 0.8395\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [13100][55000]\t Training Loss 0.8402\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [13150][55000]\t Training Loss 0.8405\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [13200][55000]\t Training Loss 0.8407\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [13250][55000]\t Training Loss 0.8402\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [13300][55000]\t Training Loss 0.8401\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [13350][55000]\t Training Loss 0.8405\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [13400][55000]\t Training Loss 0.8408\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [13450][55000]\t Training Loss 0.8403\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [13500][55000]\t Training Loss 0.8399\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [13550][55000]\t Training Loss 0.8395\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [13600][55000]\t Training Loss 0.8386\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [13650][55000]\t Training Loss 0.8385\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [13700][55000]\t Training Loss 0.8393\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [13750][55000]\t Training Loss 0.8400\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [13800][55000]\t Training Loss 0.8401\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [13850][55000]\t Training Loss 0.8400\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [13900][55000]\t Training Loss 0.8402\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [13950][55000]\t Training Loss 0.8406\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [14000][55000]\t Training Loss 0.8413\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [14050][55000]\t Training Loss 0.8415\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [14100][55000]\t Training Loss 0.8416\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [14150][55000]\t Training Loss 0.8418\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [14200][55000]\t Training Loss 0.8417\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [14250][55000]\t Training Loss 0.8420\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [14300][55000]\t Training Loss 0.8423\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [14350][55000]\t Training Loss 0.8427\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [14400][55000]\t Training Loss 0.8435\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [14450][55000]\t Training Loss 0.8437\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [14500][55000]\t Training Loss 0.8438\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [14550][55000]\t Training Loss 0.8445\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [14600][55000]\t Training Loss 0.8445\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [14650][55000]\t Training Loss 0.8455\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [14700][55000]\t Training Loss 0.8463\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [14750][55000]\t Training Loss 0.8469\t Accuracy 0.8444\n",
      "Epoch [10][20]\t Batch [14800][55000]\t Training Loss 0.8478\t Accuracy 0.8442\n",
      "Epoch [10][20]\t Batch [14850][55000]\t Training Loss 0.8484\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [14900][55000]\t Training Loss 0.8483\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [14950][55000]\t Training Loss 0.8484\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [15000][55000]\t Training Loss 0.8484\t Accuracy 0.8437\n",
      "Epoch [10][20]\t Batch [15050][55000]\t Training Loss 0.8480\t Accuracy 0.8441\n",
      "Epoch [10][20]\t Batch [15100][55000]\t Training Loss 0.8478\t Accuracy 0.8442\n",
      "Epoch [10][20]\t Batch [15150][55000]\t Training Loss 0.8482\t Accuracy 0.8442\n",
      "Epoch [10][20]\t Batch [15200][55000]\t Training Loss 0.8484\t Accuracy 0.8442\n",
      "Epoch [10][20]\t Batch [15250][55000]\t Training Loss 0.8483\t Accuracy 0.8443\n",
      "Epoch [10][20]\t Batch [15300][55000]\t Training Loss 0.8483\t Accuracy 0.8443\n",
      "Epoch [10][20]\t Batch [15350][55000]\t Training Loss 0.8481\t Accuracy 0.8445\n",
      "Epoch [10][20]\t Batch [15400][55000]\t Training Loss 0.8484\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [15450][55000]\t Training Loss 0.8485\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [15500][55000]\t Training Loss 0.8482\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [15550][55000]\t Training Loss 0.8481\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [15600][55000]\t Training Loss 0.8480\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [15650][55000]\t Training Loss 0.8479\t Accuracy 0.8456\n",
      "Epoch [10][20]\t Batch [15700][55000]\t Training Loss 0.8478\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [15750][55000]\t Training Loss 0.8486\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [15800][55000]\t Training Loss 0.8491\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [15850][55000]\t Training Loss 0.8494\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [15900][55000]\t Training Loss 0.8499\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [15950][55000]\t Training Loss 0.8500\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [16000][55000]\t Training Loss 0.8500\t Accuracy 0.8444\n",
      "Epoch [10][20]\t Batch [16050][55000]\t Training Loss 0.8510\t Accuracy 0.8440\n",
      "Epoch [10][20]\t Batch [16100][55000]\t Training Loss 0.8510\t Accuracy 0.8440\n",
      "Epoch [10][20]\t Batch [16150][55000]\t Training Loss 0.8509\t Accuracy 0.8441\n",
      "Epoch [10][20]\t Batch [16200][55000]\t Training Loss 0.8510\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [16250][55000]\t Training Loss 0.8509\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [16300][55000]\t Training Loss 0.8506\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [16350][55000]\t Training Loss 0.8502\t Accuracy 0.8441\n",
      "Epoch [10][20]\t Batch [16400][55000]\t Training Loss 0.8503\t Accuracy 0.8441\n",
      "Epoch [10][20]\t Batch [16450][55000]\t Training Loss 0.8500\t Accuracy 0.8443\n",
      "Epoch [10][20]\t Batch [16500][55000]\t Training Loss 0.8498\t Accuracy 0.8445\n",
      "Epoch [10][20]\t Batch [16550][55000]\t Training Loss 0.8494\t Accuracy 0.8445\n",
      "Epoch [10][20]\t Batch [16600][55000]\t Training Loss 0.8495\t Accuracy 0.8445\n",
      "Epoch [10][20]\t Batch [16650][55000]\t Training Loss 0.8495\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [16700][55000]\t Training Loss 0.8496\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [16750][55000]\t Training Loss 0.8495\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [16800][55000]\t Training Loss 0.8501\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [16850][55000]\t Training Loss 0.8506\t Accuracy 0.8443\n",
      "Epoch [10][20]\t Batch [16900][55000]\t Training Loss 0.8509\t Accuracy 0.8444\n",
      "Epoch [10][20]\t Batch [16950][55000]\t Training Loss 0.8508\t Accuracy 0.8443\n",
      "Epoch [10][20]\t Batch [17000][55000]\t Training Loss 0.8516\t Accuracy 0.8439\n",
      "Epoch [10][20]\t Batch [17050][55000]\t Training Loss 0.8515\t Accuracy 0.8439\n",
      "Epoch [10][20]\t Batch [17100][55000]\t Training Loss 0.8520\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [17150][55000]\t Training Loss 0.8517\t Accuracy 0.8439\n",
      "Epoch [10][20]\t Batch [17200][55000]\t Training Loss 0.8518\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [17250][55000]\t Training Loss 0.8523\t Accuracy 0.8437\n",
      "Epoch [10][20]\t Batch [17300][55000]\t Training Loss 0.8521\t Accuracy 0.8437\n",
      "Epoch [10][20]\t Batch [17350][55000]\t Training Loss 0.8516\t Accuracy 0.8440\n",
      "Epoch [10][20]\t Batch [17400][55000]\t Training Loss 0.8517\t Accuracy 0.8440\n",
      "Epoch [10][20]\t Batch [17450][55000]\t Training Loss 0.8518\t Accuracy 0.8440\n",
      "Epoch [10][20]\t Batch [17500][55000]\t Training Loss 0.8519\t Accuracy 0.8440\n",
      "Epoch [10][20]\t Batch [17550][55000]\t Training Loss 0.8526\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [17600][55000]\t Training Loss 0.8532\t Accuracy 0.8435\n",
      "Epoch [10][20]\t Batch [17650][55000]\t Training Loss 0.8534\t Accuracy 0.8437\n",
      "Epoch [10][20]\t Batch [17700][55000]\t Training Loss 0.8541\t Accuracy 0.8435\n",
      "Epoch [10][20]\t Batch [17750][55000]\t Training Loss 0.8544\t Accuracy 0.8436\n",
      "Epoch [10][20]\t Batch [17800][55000]\t Training Loss 0.8548\t Accuracy 0.8433\n",
      "Epoch [10][20]\t Batch [17850][55000]\t Training Loss 0.8551\t Accuracy 0.8431\n",
      "Epoch [10][20]\t Batch [17900][55000]\t Training Loss 0.8555\t Accuracy 0.8430\n",
      "Epoch [10][20]\t Batch [17950][55000]\t Training Loss 0.8552\t Accuracy 0.8428\n",
      "Epoch [10][20]\t Batch [18000][55000]\t Training Loss 0.8549\t Accuracy 0.8430\n",
      "Epoch [10][20]\t Batch [18050][55000]\t Training Loss 0.8551\t Accuracy 0.8429\n",
      "Epoch [10][20]\t Batch [18100][55000]\t Training Loss 0.8550\t Accuracy 0.8431\n",
      "Epoch [10][20]\t Batch [18150][55000]\t Training Loss 0.8546\t Accuracy 0.8431\n",
      "Epoch [10][20]\t Batch [18200][55000]\t Training Loss 0.8541\t Accuracy 0.8434\n",
      "Epoch [10][20]\t Batch [18250][55000]\t Training Loss 0.8541\t Accuracy 0.8435\n",
      "Epoch [10][20]\t Batch [18300][55000]\t Training Loss 0.8537\t Accuracy 0.8436\n",
      "Epoch [10][20]\t Batch [18350][55000]\t Training Loss 0.8536\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [18400][55000]\t Training Loss 0.8536\t Accuracy 0.8438\n",
      "Epoch [10][20]\t Batch [18450][55000]\t Training Loss 0.8540\t Accuracy 0.8436\n",
      "Epoch [10][20]\t Batch [18500][55000]\t Training Loss 0.8540\t Accuracy 0.8436\n",
      "Epoch [10][20]\t Batch [18550][55000]\t Training Loss 0.8538\t Accuracy 0.8439\n",
      "Epoch [10][20]\t Batch [18600][55000]\t Training Loss 0.8538\t Accuracy 0.8441\n",
      "Epoch [10][20]\t Batch [18650][55000]\t Training Loss 0.8537\t Accuracy 0.8442\n",
      "Epoch [10][20]\t Batch [18700][55000]\t Training Loss 0.8539\t Accuracy 0.8441\n",
      "Epoch [10][20]\t Batch [18750][55000]\t Training Loss 0.8541\t Accuracy 0.8441\n",
      "Epoch [10][20]\t Batch [18800][55000]\t Training Loss 0.8537\t Accuracy 0.8445\n",
      "Epoch [10][20]\t Batch [18850][55000]\t Training Loss 0.8540\t Accuracy 0.8445\n",
      "Epoch [10][20]\t Batch [18900][55000]\t Training Loss 0.8534\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [18950][55000]\t Training Loss 0.8532\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [19000][55000]\t Training Loss 0.8531\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [19050][55000]\t Training Loss 0.8535\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [19100][55000]\t Training Loss 0.8538\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [19150][55000]\t Training Loss 0.8539\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [19200][55000]\t Training Loss 0.8540\t Accuracy 0.8445\n",
      "Epoch [10][20]\t Batch [19250][55000]\t Training Loss 0.8539\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [19300][55000]\t Training Loss 0.8537\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [19350][55000]\t Training Loss 0.8538\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [19400][55000]\t Training Loss 0.8536\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [19450][55000]\t Training Loss 0.8533\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [19500][55000]\t Training Loss 0.8529\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [19550][55000]\t Training Loss 0.8530\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [19600][55000]\t Training Loss 0.8530\t Accuracy 0.8444\n",
      "Epoch [10][20]\t Batch [19650][55000]\t Training Loss 0.8526\t Accuracy 0.8446\n",
      "Epoch [10][20]\t Batch [19700][55000]\t Training Loss 0.8520\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [19750][55000]\t Training Loss 0.8515\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [19800][55000]\t Training Loss 0.8508\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [19850][55000]\t Training Loss 0.8509\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [19900][55000]\t Training Loss 0.8506\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [19950][55000]\t Training Loss 0.8508\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [20000][55000]\t Training Loss 0.8507\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [20050][55000]\t Training Loss 0.8513\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [20100][55000]\t Training Loss 0.8514\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [20150][55000]\t Training Loss 0.8512\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [20200][55000]\t Training Loss 0.8516\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [20250][55000]\t Training Loss 0.8517\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [20300][55000]\t Training Loss 0.8518\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [20350][55000]\t Training Loss 0.8518\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [20400][55000]\t Training Loss 0.8515\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [20450][55000]\t Training Loss 0.8511\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [20500][55000]\t Training Loss 0.8508\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [20550][55000]\t Training Loss 0.8507\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [20600][55000]\t Training Loss 0.8507\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [20650][55000]\t Training Loss 0.8504\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [20700][55000]\t Training Loss 0.8502\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [20750][55000]\t Training Loss 0.8502\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [20800][55000]\t Training Loss 0.8502\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [20850][55000]\t Training Loss 0.8502\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [20900][55000]\t Training Loss 0.8504\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [20950][55000]\t Training Loss 0.8510\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [21000][55000]\t Training Loss 0.8512\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [21050][55000]\t Training Loss 0.8514\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [21100][55000]\t Training Loss 0.8512\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [21150][55000]\t Training Loss 0.8511\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [21200][55000]\t Training Loss 0.8509\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [21250][55000]\t Training Loss 0.8507\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [21300][55000]\t Training Loss 0.8502\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [21350][55000]\t Training Loss 0.8503\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [21400][55000]\t Training Loss 0.8504\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [21450][55000]\t Training Loss 0.8505\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [21500][55000]\t Training Loss 0.8500\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [21550][55000]\t Training Loss 0.8498\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [21600][55000]\t Training Loss 0.8500\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [21650][55000]\t Training Loss 0.8500\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [21700][55000]\t Training Loss 0.8500\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [21750][55000]\t Training Loss 0.8499\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [21800][55000]\t Training Loss 0.8494\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [21850][55000]\t Training Loss 0.8490\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [21900][55000]\t Training Loss 0.8486\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [21950][55000]\t Training Loss 0.8481\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [22000][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [22050][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [22100][55000]\t Training Loss 0.8475\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [22150][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [22200][55000]\t Training Loss 0.8481\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [22250][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [22300][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [22350][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [22400][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [22450][55000]\t Training Loss 0.8480\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [22500][55000]\t Training Loss 0.8484\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [22550][55000]\t Training Loss 0.8490\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [22600][55000]\t Training Loss 0.8495\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [22650][55000]\t Training Loss 0.8496\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [22700][55000]\t Training Loss 0.8495\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [22750][55000]\t Training Loss 0.8494\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [22800][55000]\t Training Loss 0.8493\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [22850][55000]\t Training Loss 0.8493\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [22900][55000]\t Training Loss 0.8491\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [22950][55000]\t Training Loss 0.8489\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [23000][55000]\t Training Loss 0.8486\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [23050][55000]\t Training Loss 0.8485\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [23100][55000]\t Training Loss 0.8486\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [23150][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [23200][55000]\t Training Loss 0.8485\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [23250][55000]\t Training Loss 0.8483\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [23300][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [23350][55000]\t Training Loss 0.8478\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [23400][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [23450][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [23500][55000]\t Training Loss 0.8473\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [23550][55000]\t Training Loss 0.8471\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [23600][55000]\t Training Loss 0.8472\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [23650][55000]\t Training Loss 0.8473\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [23700][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [23750][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [23800][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [23850][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [23900][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [23950][55000]\t Training Loss 0.8478\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [24000][55000]\t Training Loss 0.8479\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [24050][55000]\t Training Loss 0.8478\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [24100][55000]\t Training Loss 0.8477\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [24150][55000]\t Training Loss 0.8475\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [24200][55000]\t Training Loss 0.8473\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [24250][55000]\t Training Loss 0.8475\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [24300][55000]\t Training Loss 0.8477\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [24350][55000]\t Training Loss 0.8476\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [24400][55000]\t Training Loss 0.8476\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [24450][55000]\t Training Loss 0.8476\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [24500][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [24550][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [24600][55000]\t Training Loss 0.8478\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [24650][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [24700][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [24750][55000]\t Training Loss 0.8481\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [24800][55000]\t Training Loss 0.8484\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [24850][55000]\t Training Loss 0.8483\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [24900][55000]\t Training Loss 0.8484\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [24950][55000]\t Training Loss 0.8487\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [25000][55000]\t Training Loss 0.8489\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [25050][55000]\t Training Loss 0.8486\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [25100][55000]\t Training Loss 0.8486\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [25150][55000]\t Training Loss 0.8484\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [25200][55000]\t Training Loss 0.8482\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [25250][55000]\t Training Loss 0.8482\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [25300][55000]\t Training Loss 0.8481\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [25350][55000]\t Training Loss 0.8482\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [25400][55000]\t Training Loss 0.8478\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [25450][55000]\t Training Loss 0.8473\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [25500][55000]\t Training Loss 0.8471\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [25550][55000]\t Training Loss 0.8468\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [25600][55000]\t Training Loss 0.8467\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [25650][55000]\t Training Loss 0.8466\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [25700][55000]\t Training Loss 0.8464\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [25750][55000]\t Training Loss 0.8461\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [25800][55000]\t Training Loss 0.8461\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [25850][55000]\t Training Loss 0.8462\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [25900][55000]\t Training Loss 0.8463\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [25950][55000]\t Training Loss 0.8463\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [26000][55000]\t Training Loss 0.8462\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [26050][55000]\t Training Loss 0.8460\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [26100][55000]\t Training Loss 0.8458\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [26150][55000]\t Training Loss 0.8456\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [26200][55000]\t Training Loss 0.8454\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [26250][55000]\t Training Loss 0.8453\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [26300][55000]\t Training Loss 0.8455\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [26350][55000]\t Training Loss 0.8454\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [26400][55000]\t Training Loss 0.8458\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [26450][55000]\t Training Loss 0.8460\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [26500][55000]\t Training Loss 0.8462\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [26550][55000]\t Training Loss 0.8464\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [26600][55000]\t Training Loss 0.8466\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [26650][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [26700][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [26750][55000]\t Training Loss 0.8470\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [26800][55000]\t Training Loss 0.8469\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [26850][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [26900][55000]\t Training Loss 0.8471\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [26950][55000]\t Training Loss 0.8470\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [27000][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [27050][55000]\t Training Loss 0.8466\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [27100][55000]\t Training Loss 0.8465\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [27150][55000]\t Training Loss 0.8464\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [27200][55000]\t Training Loss 0.8468\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [27250][55000]\t Training Loss 0.8467\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [27300][55000]\t Training Loss 0.8467\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [27350][55000]\t Training Loss 0.8467\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [27400][55000]\t Training Loss 0.8466\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [27450][55000]\t Training Loss 0.8466\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [27500][55000]\t Training Loss 0.8466\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [27550][55000]\t Training Loss 0.8466\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [27600][55000]\t Training Loss 0.8464\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [27650][55000]\t Training Loss 0.8465\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [27700][55000]\t Training Loss 0.8465\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [27750][55000]\t Training Loss 0.8466\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [27800][55000]\t Training Loss 0.8467\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [27850][55000]\t Training Loss 0.8467\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [27900][55000]\t Training Loss 0.8466\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [27950][55000]\t Training Loss 0.8464\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [28000][55000]\t Training Loss 0.8461\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [28050][55000]\t Training Loss 0.8458\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [28100][55000]\t Training Loss 0.8455\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [28150][55000]\t Training Loss 0.8453\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [28200][55000]\t Training Loss 0.8454\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [28250][55000]\t Training Loss 0.8450\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [28300][55000]\t Training Loss 0.8449\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [28350][55000]\t Training Loss 0.8446\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [28400][55000]\t Training Loss 0.8451\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [28450][55000]\t Training Loss 0.8448\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [28500][55000]\t Training Loss 0.8447\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [28550][55000]\t Training Loss 0.8446\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [28600][55000]\t Training Loss 0.8444\t Accuracy 0.8485\n",
      "Epoch [10][20]\t Batch [28650][55000]\t Training Loss 0.8447\t Accuracy 0.8485\n",
      "Epoch [10][20]\t Batch [28700][55000]\t Training Loss 0.8450\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [28750][55000]\t Training Loss 0.8451\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [28800][55000]\t Training Loss 0.8450\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [28850][55000]\t Training Loss 0.8450\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [28900][55000]\t Training Loss 0.8448\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [28950][55000]\t Training Loss 0.8448\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [29000][55000]\t Training Loss 0.8447\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [29050][55000]\t Training Loss 0.8447\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [29100][55000]\t Training Loss 0.8449\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [29150][55000]\t Training Loss 0.8453\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [29200][55000]\t Training Loss 0.8456\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [29250][55000]\t Training Loss 0.8458\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [29300][55000]\t Training Loss 0.8457\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [29350][55000]\t Training Loss 0.8458\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [29400][55000]\t Training Loss 0.8457\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [29450][55000]\t Training Loss 0.8453\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [29500][55000]\t Training Loss 0.8451\t Accuracy 0.8480\n",
      "Epoch [10][20]\t Batch [29550][55000]\t Training Loss 0.8450\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [29600][55000]\t Training Loss 0.8450\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [29650][55000]\t Training Loss 0.8450\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [29700][55000]\t Training Loss 0.8450\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [29750][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [29800][55000]\t Training Loss 0.8456\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [29850][55000]\t Training Loss 0.8459\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [29900][55000]\t Training Loss 0.8462\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [29950][55000]\t Training Loss 0.8465\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [30000][55000]\t Training Loss 0.8469\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [30050][55000]\t Training Loss 0.8470\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [30100][55000]\t Training Loss 0.8473\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [30150][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [30200][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [30250][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [30300][55000]\t Training Loss 0.8479\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [30350][55000]\t Training Loss 0.8479\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [30400][55000]\t Training Loss 0.8477\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [30450][55000]\t Training Loss 0.8478\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [30500][55000]\t Training Loss 0.8480\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [30550][55000]\t Training Loss 0.8484\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [30600][55000]\t Training Loss 0.8484\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [30650][55000]\t Training Loss 0.8488\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [30700][55000]\t Training Loss 0.8491\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [30750][55000]\t Training Loss 0.8493\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [30800][55000]\t Training Loss 0.8495\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [30850][55000]\t Training Loss 0.8494\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [30900][55000]\t Training Loss 0.8497\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [30950][55000]\t Training Loss 0.8496\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [31000][55000]\t Training Loss 0.8496\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [31050][55000]\t Training Loss 0.8497\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [31100][55000]\t Training Loss 0.8497\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [31150][55000]\t Training Loss 0.8498\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [31200][55000]\t Training Loss 0.8498\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [31250][55000]\t Training Loss 0.8498\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [31300][55000]\t Training Loss 0.8501\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [31350][55000]\t Training Loss 0.8507\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [31400][55000]\t Training Loss 0.8509\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [31450][55000]\t Training Loss 0.8512\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [31500][55000]\t Training Loss 0.8512\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [31550][55000]\t Training Loss 0.8512\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [31600][55000]\t Training Loss 0.8514\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [31650][55000]\t Training Loss 0.8516\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [31700][55000]\t Training Loss 0.8518\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [31750][55000]\t Training Loss 0.8522\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [31800][55000]\t Training Loss 0.8524\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [31850][55000]\t Training Loss 0.8523\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [31900][55000]\t Training Loss 0.8523\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [31950][55000]\t Training Loss 0.8522\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [32000][55000]\t Training Loss 0.8523\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [32050][55000]\t Training Loss 0.8522\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [32100][55000]\t Training Loss 0.8521\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [32150][55000]\t Training Loss 0.8524\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [32200][55000]\t Training Loss 0.8526\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [32250][55000]\t Training Loss 0.8529\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [32300][55000]\t Training Loss 0.8532\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [32350][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [32400][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [32450][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [32500][55000]\t Training Loss 0.8540\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [32550][55000]\t Training Loss 0.8543\t Accuracy 0.8456\n",
      "Epoch [10][20]\t Batch [32600][55000]\t Training Loss 0.8543\t Accuracy 0.8456\n",
      "Epoch [10][20]\t Batch [32650][55000]\t Training Loss 0.8541\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [32700][55000]\t Training Loss 0.8541\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [32750][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [32800][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [32850][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [32900][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [32950][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [33000][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [33050][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [33100][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [33150][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [33200][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [33250][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [33300][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [33350][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [33400][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [33450][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [33500][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [33550][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [33600][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [33650][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [33700][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [33750][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [33800][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [33850][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [33900][55000]\t Training Loss 0.8534\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [33950][55000]\t Training Loss 0.8531\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [34000][55000]\t Training Loss 0.8530\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [34050][55000]\t Training Loss 0.8532\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [34100][55000]\t Training Loss 0.8533\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [34150][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [34200][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [34250][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [34300][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [34350][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [34400][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [34450][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [34500][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [34550][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [34600][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [34650][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [34700][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [34750][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [34800][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [34850][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [34900][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [34950][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [35000][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [35050][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [35100][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [35150][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [35200][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [35250][55000]\t Training Loss 0.8536\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [35300][55000]\t Training Loss 0.8536\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [35350][55000]\t Training Loss 0.8535\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [35400][55000]\t Training Loss 0.8534\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [35450][55000]\t Training Loss 0.8533\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [35500][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [35550][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [35600][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [35650][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [35700][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [35750][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [35800][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [35850][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [35900][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [35950][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [36000][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [36050][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [36100][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [36150][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [36200][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [36250][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [36300][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [36350][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [36400][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [36450][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [36500][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [36550][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [36600][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [36650][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [36700][55000]\t Training Loss 0.8522\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [36750][55000]\t Training Loss 0.8520\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [36800][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [36850][55000]\t Training Loss 0.8519\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [36900][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [36950][55000]\t Training Loss 0.8518\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [37000][55000]\t Training Loss 0.8516\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [37050][55000]\t Training Loss 0.8515\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [37100][55000]\t Training Loss 0.8516\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [37150][55000]\t Training Loss 0.8516\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [37200][55000]\t Training Loss 0.8515\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [37250][55000]\t Training Loss 0.8514\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [37300][55000]\t Training Loss 0.8514\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [37350][55000]\t Training Loss 0.8516\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [37400][55000]\t Training Loss 0.8518\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [37450][55000]\t Training Loss 0.8521\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [37500][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [37550][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [37600][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [37650][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [37700][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [37750][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [37800][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [37850][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [37900][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [37950][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [38000][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [38050][55000]\t Training Loss 0.8526\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [38100][55000]\t Training Loss 0.8527\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [38150][55000]\t Training Loss 0.8526\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [38200][55000]\t Training Loss 0.8525\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [38250][55000]\t Training Loss 0.8525\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [38300][55000]\t Training Loss 0.8526\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [38350][55000]\t Training Loss 0.8528\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [38400][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [38450][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [38500][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [38550][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [38600][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [38650][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [38700][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [38750][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [38800][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [38850][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [38900][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [38950][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [39000][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [39050][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [39100][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [39150][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [39200][55000]\t Training Loss 0.8517\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [39250][55000]\t Training Loss 0.8515\t Accuracy 0.8477\n",
      "Epoch [10][20]\t Batch [39300][55000]\t Training Loss 0.8515\t Accuracy 0.8478\n",
      "Epoch [10][20]\t Batch [39350][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [39400][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [39450][55000]\t Training Loss 0.8521\t Accuracy 0.8475\n",
      "Epoch [10][20]\t Batch [39500][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [39550][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [39600][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [39650][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [39700][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [39750][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [39800][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [10][20]\t Batch [39850][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [39900][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [39950][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40000][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40050][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [40100][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [40150][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40200][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40250][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40300][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40350][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40400][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40450][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40500][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40550][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40600][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40650][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [40700][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [40750][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40800][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [40850][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [40900][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [40950][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [41000][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [41050][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [41100][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [41150][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [41200][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [41250][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [10][20]\t Batch [41300][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [41350][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [41400][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [41450][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [41500][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [41550][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [41600][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [41650][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [41700][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [10][20]\t Batch [41750][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [41800][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [41850][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [41900][55000]\t Training Loss 0.8537\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [41950][55000]\t Training Loss 0.8538\t Accuracy 0.8470\n",
      "Epoch [10][20]\t Batch [42000][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [42050][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [42100][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [10][20]\t Batch [42150][55000]\t Training Loss 0.8537\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [42200][55000]\t Training Loss 0.8537\t Accuracy 0.8468\n",
      "Epoch [10][20]\t Batch [42250][55000]\t Training Loss 0.8537\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [42300][55000]\t Training Loss 0.8538\t Accuracy 0.8467\n",
      "Epoch [10][20]\t Batch [42350][55000]\t Training Loss 0.8540\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [42400][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [42450][55000]\t Training Loss 0.8543\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [42500][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [42550][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [42600][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [42650][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [42700][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [42750][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [42800][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [42850][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [42900][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [42950][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [43000][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [43050][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [43100][55000]\t Training Loss 0.8548\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [43150][55000]\t Training Loss 0.8548\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [43200][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [43250][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [43300][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [43350][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [43400][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [43450][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [43500][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [43550][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [43600][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [43650][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [43700][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [43750][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [43800][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [43850][55000]\t Training Loss 0.8537\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [43900][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [43950][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [44000][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [44050][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [44100][55000]\t Training Loss 0.8540\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [44150][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [44200][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [44250][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [44300][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [44350][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [44400][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [44450][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [44500][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [44550][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [44600][55000]\t Training Loss 0.8543\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [44650][55000]\t Training Loss 0.8542\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [44700][55000]\t Training Loss 0.8541\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [44750][55000]\t Training Loss 0.8541\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [44800][55000]\t Training Loss 0.8541\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [44850][55000]\t Training Loss 0.8542\t Accuracy 0.8465\n",
      "Epoch [10][20]\t Batch [44900][55000]\t Training Loss 0.8543\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [44950][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [45000][55000]\t Training Loss 0.8543\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [45050][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45100][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45150][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [45200][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45250][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45300][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [45350][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [45400][55000]\t Training Loss 0.8544\t Accuracy 0.8464\n",
      "Epoch [10][20]\t Batch [45450][55000]\t Training Loss 0.8545\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [45500][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45550][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45600][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45650][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45700][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45750][55000]\t Training Loss 0.8546\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [45800][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45850][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45900][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [45950][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [46000][55000]\t Training Loss 0.8550\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [46050][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46100][55000]\t Training Loss 0.8552\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [46150][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46200][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46250][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46300][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [46350][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [46400][55000]\t Training Loss 0.8556\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [46450][55000]\t Training Loss 0.8557\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [46500][55000]\t Training Loss 0.8556\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [46550][55000]\t Training Loss 0.8555\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [46600][55000]\t Training Loss 0.8554\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46650][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [46700][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46750][55000]\t Training Loss 0.8553\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46800][55000]\t Training Loss 0.8552\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46850][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46900][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [46950][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47000][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [47050][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47100][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47150][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47200][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [47250][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [47300][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [47350][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [47400][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47450][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47500][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47550][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47600][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [47650][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47700][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [47750][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47800][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [47850][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [47900][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [47950][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [48000][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [48050][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [48100][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [48150][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [48200][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [48250][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [48300][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [48350][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [48400][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [48450][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [48500][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [48550][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [48600][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [48650][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [48700][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [48750][55000]\t Training Loss 0.8532\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [48800][55000]\t Training Loss 0.8531\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [48850][55000]\t Training Loss 0.8530\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [48900][55000]\t Training Loss 0.8529\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [48950][55000]\t Training Loss 0.8531\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [49000][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [49050][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [10][20]\t Batch [49100][55000]\t Training Loss 0.8537\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [49150][55000]\t Training Loss 0.8538\t Accuracy 0.8459\n",
      "Epoch [10][20]\t Batch [49200][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [49250][55000]\t Training Loss 0.8540\t Accuracy 0.8456\n",
      "Epoch [10][20]\t Batch [49300][55000]\t Training Loss 0.8539\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [49350][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [49400][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [10][20]\t Batch [49450][55000]\t Training Loss 0.8535\t Accuracy 0.8457\n",
      "Epoch [10][20]\t Batch [49500][55000]\t Training Loss 0.8538\t Accuracy 0.8456\n",
      "Epoch [10][20]\t Batch [49550][55000]\t Training Loss 0.8541\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [49600][55000]\t Training Loss 0.8544\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [49650][55000]\t Training Loss 0.8544\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [49700][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [49750][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [49800][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [49850][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [49900][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [49950][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [50000][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [50050][55000]\t Training Loss 0.8551\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [50100][55000]\t Training Loss 0.8551\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [50150][55000]\t Training Loss 0.8550\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [50200][55000]\t Training Loss 0.8549\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [50250][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [50300][55000]\t Training Loss 0.8550\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [50350][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [50400][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [50450][55000]\t Training Loss 0.8557\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [50500][55000]\t Training Loss 0.8556\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [50550][55000]\t Training Loss 0.8557\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [50600][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [50650][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [50700][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [50750][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [50800][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [50850][55000]\t Training Loss 0.8558\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [50900][55000]\t Training Loss 0.8557\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [50950][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [51000][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [51050][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [51100][55000]\t Training Loss 0.8552\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [51150][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [51200][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [51250][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [51300][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [51350][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [51400][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [51450][55000]\t Training Loss 0.8553\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [51500][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [51550][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [51600][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [51650][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [51700][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [51750][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [51800][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [51850][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [51900][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [51950][55000]\t Training Loss 0.8546\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [52000][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [52050][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [52100][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [52150][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [52200][55000]\t Training Loss 0.8554\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [52250][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [52300][55000]\t Training Loss 0.8556\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [52350][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [52400][55000]\t Training Loss 0.8555\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [52450][55000]\t Training Loss 0.8553\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [52500][55000]\t Training Loss 0.8552\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [52550][55000]\t Training Loss 0.8551\t Accuracy 0.8453\n",
      "Epoch [10][20]\t Batch [52600][55000]\t Training Loss 0.8548\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [52650][55000]\t Training Loss 0.8547\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [52700][55000]\t Training Loss 0.8548\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [52750][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [52800][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [52850][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [10][20]\t Batch [52900][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [52950][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [53000][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [53050][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53100][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53150][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [53200][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53250][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53300][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53350][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53400][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [53450][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [53500][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [53550][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53600][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [53650][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [53700][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [53750][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53800][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [53850][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53900][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [53950][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [54000][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [54050][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [54100][55000]\t Training Loss 0.8557\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [54150][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [54200][55000]\t Training Loss 0.8556\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [54250][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [54300][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [54350][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [54400][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [54450][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [54500][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [54550][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [54600][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [54650][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [54700][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [54750][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [54800][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [54850][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [54900][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [10][20]\t Batch [54950][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8552\t Average training accuracy 0.8448\n",
      "Epoch [10]\t Average validation loss 0.7926\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [11][20]\t Batch [0][55000]\t Training Loss 1.4736\t Accuracy 0.0000\n",
      "Epoch [11][20]\t Batch [50][55000]\t Training Loss 0.9394\t Accuracy 0.7255\n",
      "Epoch [11][20]\t Batch [100][55000]\t Training Loss 0.8495\t Accuracy 0.8020\n",
      "Epoch [11][20]\t Batch [150][55000]\t Training Loss 0.8508\t Accuracy 0.8146\n",
      "Epoch [11][20]\t Batch [200][55000]\t Training Loss 0.8616\t Accuracy 0.8159\n",
      "Epoch [11][20]\t Batch [250][55000]\t Training Loss 0.8438\t Accuracy 0.8247\n",
      "Epoch [11][20]\t Batch [300][55000]\t Training Loss 0.8453\t Accuracy 0.8206\n",
      "Epoch [11][20]\t Batch [350][55000]\t Training Loss 0.8256\t Accuracy 0.8234\n",
      "Epoch [11][20]\t Batch [400][55000]\t Training Loss 0.8117\t Accuracy 0.8279\n",
      "Epoch [11][20]\t Batch [450][55000]\t Training Loss 0.8175\t Accuracy 0.8293\n",
      "Epoch [11][20]\t Batch [500][55000]\t Training Loss 0.8238\t Accuracy 0.8303\n",
      "Epoch [11][20]\t Batch [550][55000]\t Training Loss 0.8411\t Accuracy 0.8276\n",
      "Epoch [11][20]\t Batch [600][55000]\t Training Loss 0.8456\t Accuracy 0.8303\n",
      "Epoch [11][20]\t Batch [650][55000]\t Training Loss 0.8654\t Accuracy 0.8203\n",
      "Epoch [11][20]\t Batch [700][55000]\t Training Loss 0.8631\t Accuracy 0.8260\n",
      "Epoch [11][20]\t Batch [750][55000]\t Training Loss 0.8593\t Accuracy 0.8269\n",
      "Epoch [11][20]\t Batch [800][55000]\t Training Loss 0.8533\t Accuracy 0.8290\n",
      "Epoch [11][20]\t Batch [850][55000]\t Training Loss 0.8518\t Accuracy 0.8308\n",
      "Epoch [11][20]\t Batch [900][55000]\t Training Loss 0.8582\t Accuracy 0.8302\n",
      "Epoch [11][20]\t Batch [950][55000]\t Training Loss 0.8650\t Accuracy 0.8254\n",
      "Epoch [11][20]\t Batch [1000][55000]\t Training Loss 0.8644\t Accuracy 0.8282\n",
      "Epoch [11][20]\t Batch [1050][55000]\t Training Loss 0.8721\t Accuracy 0.8268\n",
      "Epoch [11][20]\t Batch [1100][55000]\t Training Loss 0.8805\t Accuracy 0.8265\n",
      "Epoch [11][20]\t Batch [1150][55000]\t Training Loss 0.8898\t Accuracy 0.8236\n",
      "Epoch [11][20]\t Batch [1200][55000]\t Training Loss 0.8840\t Accuracy 0.8268\n",
      "Epoch [11][20]\t Batch [1250][55000]\t Training Loss 0.8853\t Accuracy 0.8265\n",
      "Epoch [11][20]\t Batch [1300][55000]\t Training Loss 0.8856\t Accuracy 0.8263\n",
      "Epoch [11][20]\t Batch [1350][55000]\t Training Loss 0.8824\t Accuracy 0.8275\n",
      "Epoch [11][20]\t Batch [1400][55000]\t Training Loss 0.8830\t Accuracy 0.8266\n",
      "Epoch [11][20]\t Batch [1450][55000]\t Training Loss 0.8839\t Accuracy 0.8256\n",
      "Epoch [11][20]\t Batch [1500][55000]\t Training Loss 0.8819\t Accuracy 0.8281\n",
      "Epoch [11][20]\t Batch [1550][55000]\t Training Loss 0.8814\t Accuracy 0.8285\n",
      "Epoch [11][20]\t Batch [1600][55000]\t Training Loss 0.8841\t Accuracy 0.8276\n",
      "Epoch [11][20]\t Batch [1650][55000]\t Training Loss 0.8809\t Accuracy 0.8292\n",
      "Epoch [11][20]\t Batch [1700][55000]\t Training Loss 0.8793\t Accuracy 0.8307\n",
      "Epoch [11][20]\t Batch [1750][55000]\t Training Loss 0.8736\t Accuracy 0.8321\n",
      "Epoch [11][20]\t Batch [1800][55000]\t Training Loss 0.8714\t Accuracy 0.8340\n",
      "Epoch [11][20]\t Batch [1850][55000]\t Training Loss 0.8698\t Accuracy 0.8347\n",
      "Epoch [11][20]\t Batch [1900][55000]\t Training Loss 0.8676\t Accuracy 0.8348\n",
      "Epoch [11][20]\t Batch [1950][55000]\t Training Loss 0.8662\t Accuracy 0.8360\n",
      "Epoch [11][20]\t Batch [2000][55000]\t Training Loss 0.8637\t Accuracy 0.8371\n",
      "Epoch [11][20]\t Batch [2050][55000]\t Training Loss 0.8628\t Accuracy 0.8376\n",
      "Epoch [11][20]\t Batch [2100][55000]\t Training Loss 0.8599\t Accuracy 0.8382\n",
      "Epoch [11][20]\t Batch [2150][55000]\t Training Loss 0.8566\t Accuracy 0.8401\n",
      "Epoch [11][20]\t Batch [2200][55000]\t Training Loss 0.8518\t Accuracy 0.8419\n",
      "Epoch [11][20]\t Batch [2250][55000]\t Training Loss 0.8510\t Accuracy 0.8423\n",
      "Epoch [11][20]\t Batch [2300][55000]\t Training Loss 0.8481\t Accuracy 0.8418\n",
      "Epoch [11][20]\t Batch [2350][55000]\t Training Loss 0.8466\t Accuracy 0.8422\n",
      "Epoch [11][20]\t Batch [2400][55000]\t Training Loss 0.8475\t Accuracy 0.8405\n",
      "Epoch [11][20]\t Batch [2450][55000]\t Training Loss 0.8504\t Accuracy 0.8392\n",
      "Epoch [11][20]\t Batch [2500][55000]\t Training Loss 0.8482\t Accuracy 0.8413\n",
      "Epoch [11][20]\t Batch [2550][55000]\t Training Loss 0.8470\t Accuracy 0.8412\n",
      "Epoch [11][20]\t Batch [2600][55000]\t Training Loss 0.8464\t Accuracy 0.8408\n",
      "Epoch [11][20]\t Batch [2650][55000]\t Training Loss 0.8452\t Accuracy 0.8412\n",
      "Epoch [11][20]\t Batch [2700][55000]\t Training Loss 0.8450\t Accuracy 0.8415\n",
      "Epoch [11][20]\t Batch [2750][55000]\t Training Loss 0.8449\t Accuracy 0.8415\n",
      "Epoch [11][20]\t Batch [2800][55000]\t Training Loss 0.8451\t Accuracy 0.8397\n",
      "Epoch [11][20]\t Batch [2850][55000]\t Training Loss 0.8440\t Accuracy 0.8397\n",
      "Epoch [11][20]\t Batch [2900][55000]\t Training Loss 0.8407\t Accuracy 0.8407\n",
      "Epoch [11][20]\t Batch [2950][55000]\t Training Loss 0.8408\t Accuracy 0.8404\n",
      "Epoch [11][20]\t Batch [3000][55000]\t Training Loss 0.8407\t Accuracy 0.8411\n",
      "Epoch [11][20]\t Batch [3050][55000]\t Training Loss 0.8417\t Accuracy 0.8407\n",
      "Epoch [11][20]\t Batch [3100][55000]\t Training Loss 0.8440\t Accuracy 0.8404\n",
      "Epoch [11][20]\t Batch [3150][55000]\t Training Loss 0.8439\t Accuracy 0.8407\n",
      "Epoch [11][20]\t Batch [3200][55000]\t Training Loss 0.8437\t Accuracy 0.8419\n",
      "Epoch [11][20]\t Batch [3250][55000]\t Training Loss 0.8431\t Accuracy 0.8413\n",
      "Epoch [11][20]\t Batch [3300][55000]\t Training Loss 0.8444\t Accuracy 0.8413\n",
      "Epoch [11][20]\t Batch [3350][55000]\t Training Loss 0.8426\t Accuracy 0.8427\n",
      "Epoch [11][20]\t Batch [3400][55000]\t Training Loss 0.8450\t Accuracy 0.8412\n",
      "Epoch [11][20]\t Batch [3450][55000]\t Training Loss 0.8446\t Accuracy 0.8421\n",
      "Epoch [11][20]\t Batch [3500][55000]\t Training Loss 0.8446\t Accuracy 0.8418\n",
      "Epoch [11][20]\t Batch [3550][55000]\t Training Loss 0.8462\t Accuracy 0.8412\n",
      "Epoch [11][20]\t Batch [3600][55000]\t Training Loss 0.8460\t Accuracy 0.8406\n",
      "Epoch [11][20]\t Batch [3650][55000]\t Training Loss 0.8446\t Accuracy 0.8414\n",
      "Epoch [11][20]\t Batch [3700][55000]\t Training Loss 0.8454\t Accuracy 0.8409\n",
      "Epoch [11][20]\t Batch [3750][55000]\t Training Loss 0.8453\t Accuracy 0.8411\n",
      "Epoch [11][20]\t Batch [3800][55000]\t Training Loss 0.8456\t Accuracy 0.8411\n",
      "Epoch [11][20]\t Batch [3850][55000]\t Training Loss 0.8452\t Accuracy 0.8416\n",
      "Epoch [11][20]\t Batch [3900][55000]\t Training Loss 0.8441\t Accuracy 0.8429\n",
      "Epoch [11][20]\t Batch [3950][55000]\t Training Loss 0.8430\t Accuracy 0.8441\n",
      "Epoch [11][20]\t Batch [4000][55000]\t Training Loss 0.8426\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [4050][55000]\t Training Loss 0.8411\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [4100][55000]\t Training Loss 0.8415\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [4150][55000]\t Training Loss 0.8420\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [4200][55000]\t Training Loss 0.8425\t Accuracy 0.8438\n",
      "Epoch [11][20]\t Batch [4250][55000]\t Training Loss 0.8410\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [4300][55000]\t Training Loss 0.8411\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [4350][55000]\t Training Loss 0.8416\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [4400][55000]\t Training Loss 0.8415\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [4450][55000]\t Training Loss 0.8417\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [4500][55000]\t Training Loss 0.8419\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [4550][55000]\t Training Loss 0.8402\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [4600][55000]\t Training Loss 0.8385\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [4650][55000]\t Training Loss 0.8384\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [4700][55000]\t Training Loss 0.8380\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [4750][55000]\t Training Loss 0.8368\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [4800][55000]\t Training Loss 0.8375\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [4850][55000]\t Training Loss 0.8387\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [4900][55000]\t Training Loss 0.8372\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [4950][55000]\t Training Loss 0.8375\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [5000][55000]\t Training Loss 0.8376\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [5050][55000]\t Training Loss 0.8371\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [5100][55000]\t Training Loss 0.8371\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [5150][55000]\t Training Loss 0.8375\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [5200][55000]\t Training Loss 0.8389\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [5250][55000]\t Training Loss 0.8380\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [5300][55000]\t Training Loss 0.8380\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [5350][55000]\t Training Loss 0.8386\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [5400][55000]\t Training Loss 0.8378\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [5450][55000]\t Training Loss 0.8371\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [5500][55000]\t Training Loss 0.8353\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [5550][55000]\t Training Loss 0.8353\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [5600][55000]\t Training Loss 0.8344\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [5650][55000]\t Training Loss 0.8350\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [5700][55000]\t Training Loss 0.8347\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [5750][55000]\t Training Loss 0.8351\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [5800][55000]\t Training Loss 0.8355\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [5850][55000]\t Training Loss 0.8356\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [5900][55000]\t Training Loss 0.8361\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [5950][55000]\t Training Loss 0.8360\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [6000][55000]\t Training Loss 0.8350\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [6050][55000]\t Training Loss 0.8336\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [6100][55000]\t Training Loss 0.8321\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [6150][55000]\t Training Loss 0.8301\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [6200][55000]\t Training Loss 0.8301\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [6250][55000]\t Training Loss 0.8290\t Accuracy 0.8487\n",
      "Epoch [11][20]\t Batch [6300][55000]\t Training Loss 0.8293\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [6350][55000]\t Training Loss 0.8290\t Accuracy 0.8487\n",
      "Epoch [11][20]\t Batch [6400][55000]\t Training Loss 0.8284\t Accuracy 0.8491\n",
      "Epoch [11][20]\t Batch [6450][55000]\t Training Loss 0.8279\t Accuracy 0.8493\n",
      "Epoch [11][20]\t Batch [6500][55000]\t Training Loss 0.8287\t Accuracy 0.8489\n",
      "Epoch [11][20]\t Batch [6550][55000]\t Training Loss 0.8277\t Accuracy 0.8492\n",
      "Epoch [11][20]\t Batch [6600][55000]\t Training Loss 0.8264\t Accuracy 0.8497\n",
      "Epoch [11][20]\t Batch [6650][55000]\t Training Loss 0.8251\t Accuracy 0.8498\n",
      "Epoch [11][20]\t Batch [6700][55000]\t Training Loss 0.8253\t Accuracy 0.8497\n",
      "Epoch [11][20]\t Batch [6750][55000]\t Training Loss 0.8256\t Accuracy 0.8505\n",
      "Epoch [11][20]\t Batch [6800][55000]\t Training Loss 0.8260\t Accuracy 0.8505\n",
      "Epoch [11][20]\t Batch [6850][55000]\t Training Loss 0.8281\t Accuracy 0.8501\n",
      "Epoch [11][20]\t Batch [6900][55000]\t Training Loss 0.8281\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [6950][55000]\t Training Loss 0.8287\t Accuracy 0.8489\n",
      "Epoch [11][20]\t Batch [7000][55000]\t Training Loss 0.8286\t Accuracy 0.8490\n",
      "Epoch [11][20]\t Batch [7050][55000]\t Training Loss 0.8292\t Accuracy 0.8484\n",
      "Epoch [11][20]\t Batch [7100][55000]\t Training Loss 0.8294\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [7150][55000]\t Training Loss 0.8296\t Accuracy 0.8488\n",
      "Epoch [11][20]\t Batch [7200][55000]\t Training Loss 0.8304\t Accuracy 0.8488\n",
      "Epoch [11][20]\t Batch [7250][55000]\t Training Loss 0.8327\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [7300][55000]\t Training Loss 0.8346\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [7350][55000]\t Training Loss 0.8361\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [7400][55000]\t Training Loss 0.8371\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [7450][55000]\t Training Loss 0.8371\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [7500][55000]\t Training Loss 0.8371\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [7550][55000]\t Training Loss 0.8376\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [7600][55000]\t Training Loss 0.8371\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [7650][55000]\t Training Loss 0.8376\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [7700][55000]\t Training Loss 0.8381\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [7750][55000]\t Training Loss 0.8382\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [7800][55000]\t Training Loss 0.8390\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [7850][55000]\t Training Loss 0.8391\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [7900][55000]\t Training Loss 0.8393\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [7950][55000]\t Training Loss 0.8393\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [8000][55000]\t Training Loss 0.8394\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [8050][55000]\t Training Loss 0.8399\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [8100][55000]\t Training Loss 0.8385\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [8150][55000]\t Training Loss 0.8392\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [8200][55000]\t Training Loss 0.8392\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [8250][55000]\t Training Loss 0.8405\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [8300][55000]\t Training Loss 0.8408\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [8350][55000]\t Training Loss 0.8414\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [8400][55000]\t Training Loss 0.8408\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [8450][55000]\t Training Loss 0.8423\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [8500][55000]\t Training Loss 0.8415\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [8550][55000]\t Training Loss 0.8405\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [8600][55000]\t Training Loss 0.8397\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [8650][55000]\t Training Loss 0.8398\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [8700][55000]\t Training Loss 0.8404\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [8750][55000]\t Training Loss 0.8419\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [8800][55000]\t Training Loss 0.8428\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [8850][55000]\t Training Loss 0.8427\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [8900][55000]\t Training Loss 0.8445\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [8950][55000]\t Training Loss 0.8441\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [9000][55000]\t Training Loss 0.8433\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [9050][55000]\t Training Loss 0.8425\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [9100][55000]\t Training Loss 0.8423\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [9150][55000]\t Training Loss 0.8428\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [9200][55000]\t Training Loss 0.8423\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [9250][55000]\t Training Loss 0.8426\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [9300][55000]\t Training Loss 0.8429\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [9350][55000]\t Training Loss 0.8431\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [9400][55000]\t Training Loss 0.8433\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [9450][55000]\t Training Loss 0.8435\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [9500][55000]\t Training Loss 0.8427\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [9550][55000]\t Training Loss 0.8424\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [9600][55000]\t Training Loss 0.8429\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [9650][55000]\t Training Loss 0.8427\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [9700][55000]\t Training Loss 0.8422\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [9750][55000]\t Training Loss 0.8414\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [9800][55000]\t Training Loss 0.8419\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [9850][55000]\t Training Loss 0.8415\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [9900][55000]\t Training Loss 0.8410\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [9950][55000]\t Training Loss 0.8404\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [10000][55000]\t Training Loss 0.8401\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [10050][55000]\t Training Loss 0.8403\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [10100][55000]\t Training Loss 0.8401\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [10150][55000]\t Training Loss 0.8399\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [10200][55000]\t Training Loss 0.8401\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [10250][55000]\t Training Loss 0.8402\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [10300][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [10350][55000]\t Training Loss 0.8392\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [10400][55000]\t Training Loss 0.8383\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [10450][55000]\t Training Loss 0.8381\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [10500][55000]\t Training Loss 0.8372\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [10550][55000]\t Training Loss 0.8367\t Accuracy 0.8484\n",
      "Epoch [11][20]\t Batch [10600][55000]\t Training Loss 0.8363\t Accuracy 0.8486\n",
      "Epoch [11][20]\t Batch [10650][55000]\t Training Loss 0.8360\t Accuracy 0.8489\n",
      "Epoch [11][20]\t Batch [10700][55000]\t Training Loss 0.8358\t Accuracy 0.8493\n",
      "Epoch [11][20]\t Batch [10750][55000]\t Training Loss 0.8363\t Accuracy 0.8489\n",
      "Epoch [11][20]\t Batch [10800][55000]\t Training Loss 0.8368\t Accuracy 0.8487\n",
      "Epoch [11][20]\t Batch [10850][55000]\t Training Loss 0.8361\t Accuracy 0.8490\n",
      "Epoch [11][20]\t Batch [10900][55000]\t Training Loss 0.8356\t Accuracy 0.8492\n",
      "Epoch [11][20]\t Batch [10950][55000]\t Training Loss 0.8353\t Accuracy 0.8490\n",
      "Epoch [11][20]\t Batch [11000][55000]\t Training Loss 0.8352\t Accuracy 0.8492\n",
      "Epoch [11][20]\t Batch [11050][55000]\t Training Loss 0.8344\t Accuracy 0.8494\n",
      "Epoch [11][20]\t Batch [11100][55000]\t Training Loss 0.8338\t Accuracy 0.8496\n",
      "Epoch [11][20]\t Batch [11150][55000]\t Training Loss 0.8338\t Accuracy 0.8498\n",
      "Epoch [11][20]\t Batch [11200][55000]\t Training Loss 0.8334\t Accuracy 0.8498\n",
      "Epoch [11][20]\t Batch [11250][55000]\t Training Loss 0.8338\t Accuracy 0.8497\n",
      "Epoch [11][20]\t Batch [11300][55000]\t Training Loss 0.8334\t Accuracy 0.8498\n",
      "Epoch [11][20]\t Batch [11350][55000]\t Training Loss 0.8328\t Accuracy 0.8499\n",
      "Epoch [11][20]\t Batch [11400][55000]\t Training Loss 0.8330\t Accuracy 0.8498\n",
      "Epoch [11][20]\t Batch [11450][55000]\t Training Loss 0.8327\t Accuracy 0.8497\n",
      "Epoch [11][20]\t Batch [11500][55000]\t Training Loss 0.8324\t Accuracy 0.8496\n",
      "Epoch [11][20]\t Batch [11550][55000]\t Training Loss 0.8325\t Accuracy 0.8495\n",
      "Epoch [11][20]\t Batch [11600][55000]\t Training Loss 0.8337\t Accuracy 0.8488\n",
      "Epoch [11][20]\t Batch [11650][55000]\t Training Loss 0.8342\t Accuracy 0.8485\n",
      "Epoch [11][20]\t Batch [11700][55000]\t Training Loss 0.8342\t Accuracy 0.8486\n",
      "Epoch [11][20]\t Batch [11750][55000]\t Training Loss 0.8351\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [11800][55000]\t Training Loss 0.8353\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [11850][55000]\t Training Loss 0.8354\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [11900][55000]\t Training Loss 0.8357\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [11950][55000]\t Training Loss 0.8358\t Accuracy 0.8484\n",
      "Epoch [11][20]\t Batch [12000][55000]\t Training Loss 0.8358\t Accuracy 0.8487\n",
      "Epoch [11][20]\t Batch [12050][55000]\t Training Loss 0.8356\t Accuracy 0.8490\n",
      "Epoch [11][20]\t Batch [12100][55000]\t Training Loss 0.8355\t Accuracy 0.8489\n",
      "Epoch [11][20]\t Batch [12150][55000]\t Training Loss 0.8349\t Accuracy 0.8493\n",
      "Epoch [11][20]\t Batch [12200][55000]\t Training Loss 0.8352\t Accuracy 0.8493\n",
      "Epoch [11][20]\t Batch [12250][55000]\t Training Loss 0.8351\t Accuracy 0.8493\n",
      "Epoch [11][20]\t Batch [12300][55000]\t Training Loss 0.8351\t Accuracy 0.8493\n",
      "Epoch [11][20]\t Batch [12350][55000]\t Training Loss 0.8353\t Accuracy 0.8494\n",
      "Epoch [11][20]\t Batch [12400][55000]\t Training Loss 0.8356\t Accuracy 0.8494\n",
      "Epoch [11][20]\t Batch [12450][55000]\t Training Loss 0.8358\t Accuracy 0.8491\n",
      "Epoch [11][20]\t Batch [12500][55000]\t Training Loss 0.8360\t Accuracy 0.8487\n",
      "Epoch [11][20]\t Batch [12550][55000]\t Training Loss 0.8360\t Accuracy 0.8489\n",
      "Epoch [11][20]\t Batch [12600][55000]\t Training Loss 0.8368\t Accuracy 0.8486\n",
      "Epoch [11][20]\t Batch [12650][55000]\t Training Loss 0.8373\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [12700][55000]\t Training Loss 0.8379\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [12750][55000]\t Training Loss 0.8375\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [12800][55000]\t Training Loss 0.8381\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [12850][55000]\t Training Loss 0.8383\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [12900][55000]\t Training Loss 0.8383\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [12950][55000]\t Training Loss 0.8388\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [13000][55000]\t Training Loss 0.8388\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [13050][55000]\t Training Loss 0.8394\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [13100][55000]\t Training Loss 0.8400\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [13150][55000]\t Training Loss 0.8404\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [13200][55000]\t Training Loss 0.8406\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [13250][55000]\t Training Loss 0.8401\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [13300][55000]\t Training Loss 0.8400\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [13350][55000]\t Training Loss 0.8404\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [13400][55000]\t Training Loss 0.8407\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [13450][55000]\t Training Loss 0.8402\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [13500][55000]\t Training Loss 0.8398\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [13550][55000]\t Training Loss 0.8394\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [13600][55000]\t Training Loss 0.8385\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [13650][55000]\t Training Loss 0.8384\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [13700][55000]\t Training Loss 0.8392\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [13750][55000]\t Training Loss 0.8398\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [13800][55000]\t Training Loss 0.8399\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [13850][55000]\t Training Loss 0.8399\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [13900][55000]\t Training Loss 0.8401\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [13950][55000]\t Training Loss 0.8405\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [14000][55000]\t Training Loss 0.8412\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [14050][55000]\t Training Loss 0.8414\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [14100][55000]\t Training Loss 0.8414\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [14150][55000]\t Training Loss 0.8417\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [14200][55000]\t Training Loss 0.8416\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [14250][55000]\t Training Loss 0.8419\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [14300][55000]\t Training Loss 0.8422\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [14350][55000]\t Training Loss 0.8426\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [14400][55000]\t Training Loss 0.8434\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [14450][55000]\t Training Loss 0.8436\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [14500][55000]\t Training Loss 0.8437\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [14550][55000]\t Training Loss 0.8444\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [14600][55000]\t Training Loss 0.8444\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [14650][55000]\t Training Loss 0.8454\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [14700][55000]\t Training Loss 0.8462\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [14750][55000]\t Training Loss 0.8468\t Accuracy 0.8443\n",
      "Epoch [11][20]\t Batch [14800][55000]\t Training Loss 0.8477\t Accuracy 0.8441\n",
      "Epoch [11][20]\t Batch [14850][55000]\t Training Loss 0.8483\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [14900][55000]\t Training Loss 0.8482\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [14950][55000]\t Training Loss 0.8483\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [15000][55000]\t Training Loss 0.8483\t Accuracy 0.8436\n",
      "Epoch [11][20]\t Batch [15050][55000]\t Training Loss 0.8479\t Accuracy 0.8439\n",
      "Epoch [11][20]\t Batch [15100][55000]\t Training Loss 0.8477\t Accuracy 0.8441\n",
      "Epoch [11][20]\t Batch [15150][55000]\t Training Loss 0.8481\t Accuracy 0.8440\n",
      "Epoch [11][20]\t Batch [15200][55000]\t Training Loss 0.8483\t Accuracy 0.8440\n",
      "Epoch [11][20]\t Batch [15250][55000]\t Training Loss 0.8482\t Accuracy 0.8441\n",
      "Epoch [11][20]\t Batch [15300][55000]\t Training Loss 0.8482\t Accuracy 0.8441\n",
      "Epoch [11][20]\t Batch [15350][55000]\t Training Loss 0.8479\t Accuracy 0.8444\n",
      "Epoch [11][20]\t Batch [15400][55000]\t Training Loss 0.8483\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [15450][55000]\t Training Loss 0.8484\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [15500][55000]\t Training Loss 0.8481\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [15550][55000]\t Training Loss 0.8480\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [15600][55000]\t Training Loss 0.8479\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [15650][55000]\t Training Loss 0.8478\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [15700][55000]\t Training Loss 0.8477\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [15750][55000]\t Training Loss 0.8485\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [15800][55000]\t Training Loss 0.8490\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [15850][55000]\t Training Loss 0.8493\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [15900][55000]\t Training Loss 0.8498\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [15950][55000]\t Training Loss 0.8499\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [16000][55000]\t Training Loss 0.8499\t Accuracy 0.8443\n",
      "Epoch [11][20]\t Batch [16050][55000]\t Training Loss 0.8509\t Accuracy 0.8439\n",
      "Epoch [11][20]\t Batch [16100][55000]\t Training Loss 0.8509\t Accuracy 0.8439\n",
      "Epoch [11][20]\t Batch [16150][55000]\t Training Loss 0.8508\t Accuracy 0.8440\n",
      "Epoch [11][20]\t Batch [16200][55000]\t Training Loss 0.8509\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [16250][55000]\t Training Loss 0.8507\t Accuracy 0.8436\n",
      "Epoch [11][20]\t Batch [16300][55000]\t Training Loss 0.8504\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [16350][55000]\t Training Loss 0.8501\t Accuracy 0.8440\n",
      "Epoch [11][20]\t Batch [16400][55000]\t Training Loss 0.8502\t Accuracy 0.8440\n",
      "Epoch [11][20]\t Batch [16450][55000]\t Training Loss 0.8499\t Accuracy 0.8442\n",
      "Epoch [11][20]\t Batch [16500][55000]\t Training Loss 0.8497\t Accuracy 0.8444\n",
      "Epoch [11][20]\t Batch [16550][55000]\t Training Loss 0.8493\t Accuracy 0.8444\n",
      "Epoch [11][20]\t Batch [16600][55000]\t Training Loss 0.8494\t Accuracy 0.8444\n",
      "Epoch [11][20]\t Batch [16650][55000]\t Training Loss 0.8494\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [16700][55000]\t Training Loss 0.8495\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [16750][55000]\t Training Loss 0.8493\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [16800][55000]\t Training Loss 0.8500\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [16850][55000]\t Training Loss 0.8505\t Accuracy 0.8442\n",
      "Epoch [11][20]\t Batch [16900][55000]\t Training Loss 0.8507\t Accuracy 0.8443\n",
      "Epoch [11][20]\t Batch [16950][55000]\t Training Loss 0.8507\t Accuracy 0.8441\n",
      "Epoch [11][20]\t Batch [17000][55000]\t Training Loss 0.8515\t Accuracy 0.8438\n",
      "Epoch [11][20]\t Batch [17050][55000]\t Training Loss 0.8514\t Accuracy 0.8438\n",
      "Epoch [11][20]\t Batch [17100][55000]\t Training Loss 0.8519\t Accuracy 0.8436\n",
      "Epoch [11][20]\t Batch [17150][55000]\t Training Loss 0.8516\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [17200][55000]\t Training Loss 0.8517\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [17250][55000]\t Training Loss 0.8522\t Accuracy 0.8435\n",
      "Epoch [11][20]\t Batch [17300][55000]\t Training Loss 0.8520\t Accuracy 0.8436\n",
      "Epoch [11][20]\t Batch [17350][55000]\t Training Loss 0.8515\t Accuracy 0.8439\n",
      "Epoch [11][20]\t Batch [17400][55000]\t Training Loss 0.8516\t Accuracy 0.8439\n",
      "Epoch [11][20]\t Batch [17450][55000]\t Training Loss 0.8517\t Accuracy 0.8438\n",
      "Epoch [11][20]\t Batch [17500][55000]\t Training Loss 0.8518\t Accuracy 0.8439\n",
      "Epoch [11][20]\t Batch [17550][55000]\t Training Loss 0.8524\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [17600][55000]\t Training Loss 0.8531\t Accuracy 0.8434\n",
      "Epoch [11][20]\t Batch [17650][55000]\t Training Loss 0.8533\t Accuracy 0.8436\n",
      "Epoch [11][20]\t Batch [17700][55000]\t Training Loss 0.8540\t Accuracy 0.8434\n",
      "Epoch [11][20]\t Batch [17750][55000]\t Training Loss 0.8543\t Accuracy 0.8434\n",
      "Epoch [11][20]\t Batch [17800][55000]\t Training Loss 0.8547\t Accuracy 0.8432\n",
      "Epoch [11][20]\t Batch [17850][55000]\t Training Loss 0.8550\t Accuracy 0.8430\n",
      "Epoch [11][20]\t Batch [17900][55000]\t Training Loss 0.8553\t Accuracy 0.8429\n",
      "Epoch [11][20]\t Batch [17950][55000]\t Training Loss 0.8551\t Accuracy 0.8427\n",
      "Epoch [11][20]\t Batch [18000][55000]\t Training Loss 0.8548\t Accuracy 0.8428\n",
      "Epoch [11][20]\t Batch [18050][55000]\t Training Loss 0.8550\t Accuracy 0.8428\n",
      "Epoch [11][20]\t Batch [18100][55000]\t Training Loss 0.8549\t Accuracy 0.8430\n",
      "Epoch [11][20]\t Batch [18150][55000]\t Training Loss 0.8545\t Accuracy 0.8430\n",
      "Epoch [11][20]\t Batch [18200][55000]\t Training Loss 0.8540\t Accuracy 0.8433\n",
      "Epoch [11][20]\t Batch [18250][55000]\t Training Loss 0.8540\t Accuracy 0.8434\n",
      "Epoch [11][20]\t Batch [18300][55000]\t Training Loss 0.8536\t Accuracy 0.8435\n",
      "Epoch [11][20]\t Batch [18350][55000]\t Training Loss 0.8534\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [18400][55000]\t Training Loss 0.8535\t Accuracy 0.8437\n",
      "Epoch [11][20]\t Batch [18450][55000]\t Training Loss 0.8539\t Accuracy 0.8435\n",
      "Epoch [11][20]\t Batch [18500][55000]\t Training Loss 0.8539\t Accuracy 0.8435\n",
      "Epoch [11][20]\t Batch [18550][55000]\t Training Loss 0.8537\t Accuracy 0.8438\n",
      "Epoch [11][20]\t Batch [18600][55000]\t Training Loss 0.8537\t Accuracy 0.8440\n",
      "Epoch [11][20]\t Batch [18650][55000]\t Training Loss 0.8535\t Accuracy 0.8442\n",
      "Epoch [11][20]\t Batch [18700][55000]\t Training Loss 0.8538\t Accuracy 0.8440\n",
      "Epoch [11][20]\t Batch [18750][55000]\t Training Loss 0.8540\t Accuracy 0.8441\n",
      "Epoch [11][20]\t Batch [18800][55000]\t Training Loss 0.8536\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [18850][55000]\t Training Loss 0.8539\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [18900][55000]\t Training Loss 0.8533\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [18950][55000]\t Training Loss 0.8531\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [19000][55000]\t Training Loss 0.8530\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [19050][55000]\t Training Loss 0.8534\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [19100][55000]\t Training Loss 0.8537\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [19150][55000]\t Training Loss 0.8538\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [19200][55000]\t Training Loss 0.8539\t Accuracy 0.8444\n",
      "Epoch [11][20]\t Batch [19250][55000]\t Training Loss 0.8538\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [19300][55000]\t Training Loss 0.8536\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [19350][55000]\t Training Loss 0.8537\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [19400][55000]\t Training Loss 0.8535\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [19450][55000]\t Training Loss 0.8532\t Accuracy 0.8445\n",
      "Epoch [11][20]\t Batch [19500][55000]\t Training Loss 0.8528\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [19550][55000]\t Training Loss 0.8529\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [19600][55000]\t Training Loss 0.8528\t Accuracy 0.8443\n",
      "Epoch [11][20]\t Batch [19650][55000]\t Training Loss 0.8524\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [19700][55000]\t Training Loss 0.8519\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [19750][55000]\t Training Loss 0.8514\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [19800][55000]\t Training Loss 0.8507\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [19850][55000]\t Training Loss 0.8508\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [19900][55000]\t Training Loss 0.8505\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [19950][55000]\t Training Loss 0.8507\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [20000][55000]\t Training Loss 0.8506\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [20050][55000]\t Training Loss 0.8512\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [20100][55000]\t Training Loss 0.8513\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [20150][55000]\t Training Loss 0.8511\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [20200][55000]\t Training Loss 0.8515\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [20250][55000]\t Training Loss 0.8516\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [20300][55000]\t Training Loss 0.8517\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [20350][55000]\t Training Loss 0.8517\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [20400][55000]\t Training Loss 0.8513\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [20450][55000]\t Training Loss 0.8510\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [20500][55000]\t Training Loss 0.8507\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [20550][55000]\t Training Loss 0.8506\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [20600][55000]\t Training Loss 0.8506\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [20650][55000]\t Training Loss 0.8503\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [20700][55000]\t Training Loss 0.8501\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [20750][55000]\t Training Loss 0.8501\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [20800][55000]\t Training Loss 0.8501\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [20850][55000]\t Training Loss 0.8501\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [20900][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [20950][55000]\t Training Loss 0.8509\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [21000][55000]\t Training Loss 0.8510\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [21050][55000]\t Training Loss 0.8513\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [21100][55000]\t Training Loss 0.8510\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [21150][55000]\t Training Loss 0.8510\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [21200][55000]\t Training Loss 0.8507\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [21250][55000]\t Training Loss 0.8506\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [21300][55000]\t Training Loss 0.8501\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [21350][55000]\t Training Loss 0.8502\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [21400][55000]\t Training Loss 0.8503\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [21450][55000]\t Training Loss 0.8503\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [21500][55000]\t Training Loss 0.8499\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [21550][55000]\t Training Loss 0.8497\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [21600][55000]\t Training Loss 0.8499\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [21650][55000]\t Training Loss 0.8499\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [21700][55000]\t Training Loss 0.8499\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [21750][55000]\t Training Loss 0.8498\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [21800][55000]\t Training Loss 0.8493\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [21850][55000]\t Training Loss 0.8489\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [21900][55000]\t Training Loss 0.8485\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [21950][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [22000][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [22050][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [22100][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [22150][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [22200][55000]\t Training Loss 0.8480\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [22250][55000]\t Training Loss 0.8481\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [22300][55000]\t Training Loss 0.8482\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [22350][55000]\t Training Loss 0.8480\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [22400][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [22450][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [22500][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [22550][55000]\t Training Loss 0.8489\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [22600][55000]\t Training Loss 0.8494\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [22650][55000]\t Training Loss 0.8495\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [22700][55000]\t Training Loss 0.8494\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [22750][55000]\t Training Loss 0.8493\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [22800][55000]\t Training Loss 0.8492\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [22850][55000]\t Training Loss 0.8492\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [22900][55000]\t Training Loss 0.8490\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [22950][55000]\t Training Loss 0.8488\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [23000][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [23050][55000]\t Training Loss 0.8483\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [23100][55000]\t Training Loss 0.8485\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [23150][55000]\t Training Loss 0.8484\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [23200][55000]\t Training Loss 0.8484\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [23250][55000]\t Training Loss 0.8482\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [23300][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [23350][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [23400][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [23450][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [23500][55000]\t Training Loss 0.8471\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [23550][55000]\t Training Loss 0.8470\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [23600][55000]\t Training Loss 0.8471\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [23650][55000]\t Training Loss 0.8472\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [23700][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [23750][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [23800][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [23850][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [23900][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [23950][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [24000][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [24050][55000]\t Training Loss 0.8477\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [24100][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [24150][55000]\t Training Loss 0.8474\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [24200][55000]\t Training Loss 0.8472\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [24250][55000]\t Training Loss 0.8474\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [24300][55000]\t Training Loss 0.8476\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [24350][55000]\t Training Loss 0.8475\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [24400][55000]\t Training Loss 0.8475\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [24450][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [24500][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [24550][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [24600][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [24650][55000]\t Training Loss 0.8477\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [24700][55000]\t Training Loss 0.8477\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [24750][55000]\t Training Loss 0.8480\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [24800][55000]\t Training Loss 0.8483\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [24850][55000]\t Training Loss 0.8482\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [24900][55000]\t Training Loss 0.8483\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [24950][55000]\t Training Loss 0.8486\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [25000][55000]\t Training Loss 0.8488\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [25050][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [25100][55000]\t Training Loss 0.8484\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [25150][55000]\t Training Loss 0.8483\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [25200][55000]\t Training Loss 0.8481\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [25250][55000]\t Training Loss 0.8480\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [25300][55000]\t Training Loss 0.8480\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [25350][55000]\t Training Loss 0.8481\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [25400][55000]\t Training Loss 0.8477\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [25450][55000]\t Training Loss 0.8472\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [25500][55000]\t Training Loss 0.8470\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [25550][55000]\t Training Loss 0.8467\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [25600][55000]\t Training Loss 0.8466\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [25650][55000]\t Training Loss 0.8465\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [25700][55000]\t Training Loss 0.8463\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [25750][55000]\t Training Loss 0.8460\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [25800][55000]\t Training Loss 0.8460\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [25850][55000]\t Training Loss 0.8461\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [25900][55000]\t Training Loss 0.8462\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [25950][55000]\t Training Loss 0.8462\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [26000][55000]\t Training Loss 0.8461\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [26050][55000]\t Training Loss 0.8459\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [26100][55000]\t Training Loss 0.8457\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [26150][55000]\t Training Loss 0.8455\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [26200][55000]\t Training Loss 0.8453\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [26250][55000]\t Training Loss 0.8452\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [26300][55000]\t Training Loss 0.8454\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [26350][55000]\t Training Loss 0.8453\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [26400][55000]\t Training Loss 0.8457\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [26450][55000]\t Training Loss 0.8459\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [26500][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [26550][55000]\t Training Loss 0.8463\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [26600][55000]\t Training Loss 0.8465\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [26650][55000]\t Training Loss 0.8468\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [26700][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [26750][55000]\t Training Loss 0.8469\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [26800][55000]\t Training Loss 0.8468\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [26850][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [26900][55000]\t Training Loss 0.8470\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [26950][55000]\t Training Loss 0.8469\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [27000][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [27050][55000]\t Training Loss 0.8465\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [27100][55000]\t Training Loss 0.8464\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [27150][55000]\t Training Loss 0.8463\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [27200][55000]\t Training Loss 0.8467\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [27250][55000]\t Training Loss 0.8466\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [27300][55000]\t Training Loss 0.8466\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [27350][55000]\t Training Loss 0.8466\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [27400][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [27450][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [27500][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [27550][55000]\t Training Loss 0.8465\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [27600][55000]\t Training Loss 0.8463\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [27650][55000]\t Training Loss 0.8464\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [27700][55000]\t Training Loss 0.8464\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [27750][55000]\t Training Loss 0.8465\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [27800][55000]\t Training Loss 0.8466\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [27850][55000]\t Training Loss 0.8466\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [27900][55000]\t Training Loss 0.8465\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [27950][55000]\t Training Loss 0.8463\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [28000][55000]\t Training Loss 0.8460\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [28050][55000]\t Training Loss 0.8457\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [28100][55000]\t Training Loss 0.8454\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [28150][55000]\t Training Loss 0.8452\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [28200][55000]\t Training Loss 0.8453\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [28250][55000]\t Training Loss 0.8449\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [28300][55000]\t Training Loss 0.8448\t Accuracy 0.8481\n",
      "Epoch [11][20]\t Batch [28350][55000]\t Training Loss 0.8445\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [28400][55000]\t Training Loss 0.8450\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [28450][55000]\t Training Loss 0.8447\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [28500][55000]\t Training Loss 0.8446\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [28550][55000]\t Training Loss 0.8445\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [28600][55000]\t Training Loss 0.8443\t Accuracy 0.8484\n",
      "Epoch [11][20]\t Batch [28650][55000]\t Training Loss 0.8446\t Accuracy 0.8484\n",
      "Epoch [11][20]\t Batch [28700][55000]\t Training Loss 0.8449\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [28750][55000]\t Training Loss 0.8450\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [28800][55000]\t Training Loss 0.8449\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [28850][55000]\t Training Loss 0.8448\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [28900][55000]\t Training Loss 0.8447\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [28950][55000]\t Training Loss 0.8447\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [29000][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [29050][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [29100][55000]\t Training Loss 0.8448\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [29150][55000]\t Training Loss 0.8452\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [29200][55000]\t Training Loss 0.8455\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [29250][55000]\t Training Loss 0.8457\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [29300][55000]\t Training Loss 0.8455\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [29350][55000]\t Training Loss 0.8457\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [29400][55000]\t Training Loss 0.8456\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [29450][55000]\t Training Loss 0.8452\t Accuracy 0.8480\n",
      "Epoch [11][20]\t Batch [29500][55000]\t Training Loss 0.8450\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [29550][55000]\t Training Loss 0.8449\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [29600][55000]\t Training Loss 0.8449\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [29650][55000]\t Training Loss 0.8448\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [29700][55000]\t Training Loss 0.8449\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [29750][55000]\t Training Loss 0.8453\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [29800][55000]\t Training Loss 0.8455\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [29850][55000]\t Training Loss 0.8458\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [29900][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [29950][55000]\t Training Loss 0.8464\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [30000][55000]\t Training Loss 0.8467\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [30050][55000]\t Training Loss 0.8468\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [30100][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [30150][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [30200][55000]\t Training Loss 0.8479\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [30250][55000]\t Training Loss 0.8480\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [30300][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [30350][55000]\t Training Loss 0.8478\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [30400][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [30450][55000]\t Training Loss 0.8477\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [30500][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [30550][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [30600][55000]\t Training Loss 0.8483\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [30650][55000]\t Training Loss 0.8487\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [30700][55000]\t Training Loss 0.8490\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [30750][55000]\t Training Loss 0.8492\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [30800][55000]\t Training Loss 0.8494\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [30850][55000]\t Training Loss 0.8493\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [30900][55000]\t Training Loss 0.8496\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [30950][55000]\t Training Loss 0.8495\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [31000][55000]\t Training Loss 0.8495\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [31050][55000]\t Training Loss 0.8496\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [31100][55000]\t Training Loss 0.8496\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [31150][55000]\t Training Loss 0.8497\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [31200][55000]\t Training Loss 0.8497\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [31250][55000]\t Training Loss 0.8496\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [31300][55000]\t Training Loss 0.8500\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [31350][55000]\t Training Loss 0.8506\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [31400][55000]\t Training Loss 0.8507\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [31450][55000]\t Training Loss 0.8511\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [31500][55000]\t Training Loss 0.8511\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [31550][55000]\t Training Loss 0.8511\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [31600][55000]\t Training Loss 0.8513\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [31650][55000]\t Training Loss 0.8515\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [31700][55000]\t Training Loss 0.8517\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [31750][55000]\t Training Loss 0.8521\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [31800][55000]\t Training Loss 0.8523\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [31850][55000]\t Training Loss 0.8522\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [31900][55000]\t Training Loss 0.8522\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [31950][55000]\t Training Loss 0.8521\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [32000][55000]\t Training Loss 0.8522\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [32050][55000]\t Training Loss 0.8521\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [32100][55000]\t Training Loss 0.8520\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [32150][55000]\t Training Loss 0.8523\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [32200][55000]\t Training Loss 0.8524\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [32250][55000]\t Training Loss 0.8528\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [32300][55000]\t Training Loss 0.8531\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [32350][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [32400][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [32450][55000]\t Training Loss 0.8537\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [32500][55000]\t Training Loss 0.8539\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [32550][55000]\t Training Loss 0.8542\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [32600][55000]\t Training Loss 0.8542\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [32650][55000]\t Training Loss 0.8540\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [32700][55000]\t Training Loss 0.8540\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [32750][55000]\t Training Loss 0.8541\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [32800][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [32850][55000]\t Training Loss 0.8541\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [32900][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [32950][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33000][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33050][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33100][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33150][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [33200][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [33250][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33300][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33350][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [33400][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33450][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [33500][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [33550][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [33600][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33650][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [33700][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [33750][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [33800][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [33850][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [33900][55000]\t Training Loss 0.8533\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [33950][55000]\t Training Loss 0.8530\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [34000][55000]\t Training Loss 0.8529\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [34050][55000]\t Training Loss 0.8531\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [34100][55000]\t Training Loss 0.8532\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [34150][55000]\t Training Loss 0.8533\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [34200][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [34250][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [34300][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [34350][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [34400][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [34450][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [34500][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [34550][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [34600][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [34650][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [34700][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [34750][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [34800][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [34850][55000]\t Training Loss 0.8533\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [34900][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [34950][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [35000][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [35050][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [35100][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [35150][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [35200][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [35250][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [35300][55000]\t Training Loss 0.8535\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [35350][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [35400][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [35450][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [35500][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [35550][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [35600][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [35650][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [35700][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [35750][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [35800][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [35850][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [35900][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [35950][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [36000][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [36050][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [36100][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [36150][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [36200][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [36250][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [36300][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [36350][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [36400][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [36450][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [36500][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [36550][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [36600][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [36650][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [36700][55000]\t Training Loss 0.8521\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [36750][55000]\t Training Loss 0.8519\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [36800][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [36850][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [36900][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [36950][55000]\t Training Loss 0.8517\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [37000][55000]\t Training Loss 0.8515\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [37050][55000]\t Training Loss 0.8514\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [37100][55000]\t Training Loss 0.8515\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [37150][55000]\t Training Loss 0.8515\t Accuracy 0.8478\n",
      "Epoch [11][20]\t Batch [37200][55000]\t Training Loss 0.8514\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [37250][55000]\t Training Loss 0.8513\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [37300][55000]\t Training Loss 0.8513\t Accuracy 0.8479\n",
      "Epoch [11][20]\t Batch [37350][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [37400][55000]\t Training Loss 0.8517\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [37450][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [37500][55000]\t Training Loss 0.8521\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [37550][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [37600][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [37650][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [37700][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [37750][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [37800][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [37850][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [37900][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [37950][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [38000][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [38050][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [38100][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [38150][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [38200][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [38250][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [38300][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [38350][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [38400][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [38450][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [38500][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [38550][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [38600][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [38650][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [38700][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [38750][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [38800][55000]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [38850][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [38900][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [38950][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [39000][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [39050][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [39100][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [39150][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [39200][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [11][20]\t Batch [39250][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [39300][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [11][20]\t Batch [39350][55000]\t Training Loss 0.8517\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [39400][55000]\t Training Loss 0.8517\t Accuracy 0.8475\n",
      "Epoch [11][20]\t Batch [39450][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [11][20]\t Batch [39500][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [39550][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [39600][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [39650][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [39700][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [39750][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [39800][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [39850][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [39900][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [39950][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40000][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40050][55000]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [40100][55000]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [40150][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40200][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40250][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40300][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40350][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [40400][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [40450][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [40500][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40550][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40600][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40650][55000]\t Training Loss 0.8525\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [40700][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40750][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40800][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [40850][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [40900][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [40950][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [41000][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [41050][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [41100][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [41150][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [41200][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [41250][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [41300][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [11][20]\t Batch [41350][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [41400][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [41450][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [41500][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [41550][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [41600][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [41650][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [41700][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [11][20]\t Batch [41750][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [41800][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [41850][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [41900][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [41950][55000]\t Training Loss 0.8537\t Accuracy 0.8469\n",
      "Epoch [11][20]\t Batch [42000][55000]\t Training Loss 0.8536\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [42050][55000]\t Training Loss 0.8535\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [42100][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [11][20]\t Batch [42150][55000]\t Training Loss 0.8536\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [42200][55000]\t Training Loss 0.8536\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [42250][55000]\t Training Loss 0.8536\t Accuracy 0.8467\n",
      "Epoch [11][20]\t Batch [42300][55000]\t Training Loss 0.8537\t Accuracy 0.8466\n",
      "Epoch [11][20]\t Batch [42350][55000]\t Training Loss 0.8539\t Accuracy 0.8465\n",
      "Epoch [11][20]\t Batch [42400][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [42450][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [42500][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [42550][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [42600][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [42650][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [42700][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [42750][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [42800][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [42850][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [42900][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [42950][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [43000][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [43050][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [43100][55000]\t Training Loss 0.8547\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [43150][55000]\t Training Loss 0.8547\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [43200][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [43250][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [43300][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [43350][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [43400][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [43450][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [43500][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [43550][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [43600][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [43650][55000]\t Training Loss 0.8536\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [43700][55000]\t Training Loss 0.8536\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [43750][55000]\t Training Loss 0.8536\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [43800][55000]\t Training Loss 0.8535\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [43850][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [43900][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [43950][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [44000][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [44050][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [44100][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [44150][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [44200][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [44250][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [44300][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [44350][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [44400][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [44450][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [44500][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [44550][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [44600][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [44650][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [44700][55000]\t Training Loss 0.8540\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [44750][55000]\t Training Loss 0.8540\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [44800][55000]\t Training Loss 0.8540\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [44850][55000]\t Training Loss 0.8541\t Accuracy 0.8464\n",
      "Epoch [11][20]\t Batch [44900][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [44950][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45000][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45050][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [45100][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [45150][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [45200][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [45250][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [45300][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45350][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45400][55000]\t Training Loss 0.8543\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [45450][55000]\t Training Loss 0.8544\t Accuracy 0.8463\n",
      "Epoch [11][20]\t Batch [45500][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [45550][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45600][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45650][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [45700][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [45750][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45800][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45850][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [45900][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [45950][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [46000][55000]\t Training Loss 0.8549\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [46050][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [46100][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [46150][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46200][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46250][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46300][55000]\t Training Loss 0.8552\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [46350][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [46400][55000]\t Training Loss 0.8555\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [46450][55000]\t Training Loss 0.8556\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [46500][55000]\t Training Loss 0.8555\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [46550][55000]\t Training Loss 0.8554\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46600][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46650][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46700][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46750][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46800][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [46850][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46900][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [46950][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47000][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47050][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47100][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47150][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47200][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47250][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47300][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47350][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47400][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47450][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47500][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47550][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47600][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47650][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47700][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47750][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47800][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [47850][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47900][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [47950][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [48000][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [48050][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [48100][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [48150][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [48200][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [48250][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [48300][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [48350][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [48400][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [48450][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [48500][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [48550][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [48600][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [48650][55000]\t Training Loss 0.8534\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [48700][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [48750][55000]\t Training Loss 0.8531\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [48800][55000]\t Training Loss 0.8530\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [48850][55000]\t Training Loss 0.8529\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [48900][55000]\t Training Loss 0.8528\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [48950][55000]\t Training Loss 0.8530\t Accuracy 0.8461\n",
      "Epoch [11][20]\t Batch [49000][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [11][20]\t Batch [49050][55000]\t Training Loss 0.8535\t Accuracy 0.8459\n",
      "Epoch [11][20]\t Batch [49100][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [11][20]\t Batch [49150][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [11][20]\t Batch [49200][55000]\t Training Loss 0.8537\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [49250][55000]\t Training Loss 0.8539\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [49300][55000]\t Training Loss 0.8538\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [49350][55000]\t Training Loss 0.8536\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [49400][55000]\t Training Loss 0.8535\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [49450][55000]\t Training Loss 0.8534\t Accuracy 0.8456\n",
      "Epoch [11][20]\t Batch [49500][55000]\t Training Loss 0.8537\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [49550][55000]\t Training Loss 0.8540\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [49600][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [49650][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [49700][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [49750][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [49800][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [49850][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [49900][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [49950][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [50000][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [50050][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [50100][55000]\t Training Loss 0.8550\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [50150][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [50200][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [50250][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [50300][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [50350][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [50400][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [50450][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [50500][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [50550][55000]\t Training Loss 0.8556\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [50600][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [50650][55000]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [50700][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [50750][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [50800][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [50850][55000]\t Training Loss 0.8557\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [50900][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [50950][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [51000][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [51050][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [51100][55000]\t Training Loss 0.8551\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [51150][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [51200][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [51250][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [51300][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [51350][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [51400][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [51450][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [51500][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [51550][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [51600][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [51650][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [51700][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [51750][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [51800][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [51850][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [51900][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [51950][55000]\t Training Loss 0.8545\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [52000][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [52050][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [52100][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [52150][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [52200][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [52250][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [52300][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [52350][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [52400][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [52450][55000]\t Training Loss 0.8552\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [52500][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [52550][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [11][20]\t Batch [52600][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [52650][55000]\t Training Loss 0.8546\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [52700][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [52750][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [52800][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [52850][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [52900][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [52950][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [53000][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [53050][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53100][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53150][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [53200][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53250][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53300][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53350][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53400][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [53450][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [53500][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [53550][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53600][55000]\t Training Loss 0.8553\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [53650][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53700][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53750][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53800][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [53850][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53900][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [53950][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [54000][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [54050][55000]\t Training Loss 0.8554\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [54100][55000]\t Training Loss 0.8556\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [54150][55000]\t Training Loss 0.8555\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [54200][55000]\t Training Loss 0.8555\t Accuracy 0.8446\n",
      "Epoch [11][20]\t Batch [54250][55000]\t Training Loss 0.8554\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [54300][55000]\t Training Loss 0.8554\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [54350][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [54400][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [54450][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [54500][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [54550][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [54600][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [54650][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [54700][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "Epoch [11][20]\t Batch [54750][55000]\t Training Loss 0.8546\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [54800][55000]\t Training Loss 0.8546\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [54850][55000]\t Training Loss 0.8544\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [54900][55000]\t Training Loss 0.8546\t Accuracy 0.8450\n",
      "Epoch [11][20]\t Batch [54950][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8551\t Average training accuracy 0.8447\n",
      "Epoch [11]\t Average validation loss 0.7925\t Average validation accuracy 0.8698\n",
      "\n",
      "Epoch [12][20]\t Batch [0][55000]\t Training Loss 1.4725\t Accuracy 0.0000\n",
      "Epoch [12][20]\t Batch [50][55000]\t Training Loss 0.9391\t Accuracy 0.7255\n",
      "Epoch [12][20]\t Batch [100][55000]\t Training Loss 0.8493\t Accuracy 0.8020\n",
      "Epoch [12][20]\t Batch [150][55000]\t Training Loss 0.8506\t Accuracy 0.8146\n",
      "Epoch [12][20]\t Batch [200][55000]\t Training Loss 0.8614\t Accuracy 0.8159\n",
      "Epoch [12][20]\t Batch [250][55000]\t Training Loss 0.8436\t Accuracy 0.8247\n",
      "Epoch [12][20]\t Batch [300][55000]\t Training Loss 0.8452\t Accuracy 0.8206\n",
      "Epoch [12][20]\t Batch [350][55000]\t Training Loss 0.8254\t Accuracy 0.8234\n",
      "Epoch [12][20]\t Batch [400][55000]\t Training Loss 0.8116\t Accuracy 0.8279\n",
      "Epoch [12][20]\t Batch [450][55000]\t Training Loss 0.8174\t Accuracy 0.8293\n",
      "Epoch [12][20]\t Batch [500][55000]\t Training Loss 0.8237\t Accuracy 0.8303\n",
      "Epoch [12][20]\t Batch [550][55000]\t Training Loss 0.8410\t Accuracy 0.8276\n",
      "Epoch [12][20]\t Batch [600][55000]\t Training Loss 0.8455\t Accuracy 0.8303\n",
      "Epoch [12][20]\t Batch [650][55000]\t Training Loss 0.8653\t Accuracy 0.8203\n",
      "Epoch [12][20]\t Batch [700][55000]\t Training Loss 0.8630\t Accuracy 0.8260\n",
      "Epoch [12][20]\t Batch [750][55000]\t Training Loss 0.8592\t Accuracy 0.8269\n",
      "Epoch [12][20]\t Batch [800][55000]\t Training Loss 0.8532\t Accuracy 0.8290\n",
      "Epoch [12][20]\t Batch [850][55000]\t Training Loss 0.8517\t Accuracy 0.8308\n",
      "Epoch [12][20]\t Batch [900][55000]\t Training Loss 0.8581\t Accuracy 0.8302\n",
      "Epoch [12][20]\t Batch [950][55000]\t Training Loss 0.8649\t Accuracy 0.8254\n",
      "Epoch [12][20]\t Batch [1000][55000]\t Training Loss 0.8643\t Accuracy 0.8282\n",
      "Epoch [12][20]\t Batch [1050][55000]\t Training Loss 0.8720\t Accuracy 0.8268\n",
      "Epoch [12][20]\t Batch [1100][55000]\t Training Loss 0.8804\t Accuracy 0.8265\n",
      "Epoch [12][20]\t Batch [1150][55000]\t Training Loss 0.8897\t Accuracy 0.8236\n",
      "Epoch [12][20]\t Batch [1200][55000]\t Training Loss 0.8839\t Accuracy 0.8268\n",
      "Epoch [12][20]\t Batch [1250][55000]\t Training Loss 0.8852\t Accuracy 0.8265\n",
      "Epoch [12][20]\t Batch [1300][55000]\t Training Loss 0.8855\t Accuracy 0.8263\n",
      "Epoch [12][20]\t Batch [1350][55000]\t Training Loss 0.8823\t Accuracy 0.8275\n",
      "Epoch [12][20]\t Batch [1400][55000]\t Training Loss 0.8829\t Accuracy 0.8266\n",
      "Epoch [12][20]\t Batch [1450][55000]\t Training Loss 0.8838\t Accuracy 0.8256\n",
      "Epoch [12][20]\t Batch [1500][55000]\t Training Loss 0.8818\t Accuracy 0.8281\n",
      "Epoch [12][20]\t Batch [1550][55000]\t Training Loss 0.8813\t Accuracy 0.8285\n",
      "Epoch [12][20]\t Batch [1600][55000]\t Training Loss 0.8840\t Accuracy 0.8276\n",
      "Epoch [12][20]\t Batch [1650][55000]\t Training Loss 0.8808\t Accuracy 0.8292\n",
      "Epoch [12][20]\t Batch [1700][55000]\t Training Loss 0.8792\t Accuracy 0.8307\n",
      "Epoch [12][20]\t Batch [1750][55000]\t Training Loss 0.8735\t Accuracy 0.8321\n",
      "Epoch [12][20]\t Batch [1800][55000]\t Training Loss 0.8713\t Accuracy 0.8340\n",
      "Epoch [12][20]\t Batch [1850][55000]\t Training Loss 0.8697\t Accuracy 0.8347\n",
      "Epoch [12][20]\t Batch [1900][55000]\t Training Loss 0.8675\t Accuracy 0.8348\n",
      "Epoch [12][20]\t Batch [1950][55000]\t Training Loss 0.8660\t Accuracy 0.8360\n",
      "Epoch [12][20]\t Batch [2000][55000]\t Training Loss 0.8635\t Accuracy 0.8371\n",
      "Epoch [12][20]\t Batch [2050][55000]\t Training Loss 0.8626\t Accuracy 0.8376\n",
      "Epoch [12][20]\t Batch [2100][55000]\t Training Loss 0.8598\t Accuracy 0.8382\n",
      "Epoch [12][20]\t Batch [2150][55000]\t Training Loss 0.8565\t Accuracy 0.8401\n",
      "Epoch [12][20]\t Batch [2200][55000]\t Training Loss 0.8517\t Accuracy 0.8419\n",
      "Epoch [12][20]\t Batch [2250][55000]\t Training Loss 0.8509\t Accuracy 0.8423\n",
      "Epoch [12][20]\t Batch [2300][55000]\t Training Loss 0.8480\t Accuracy 0.8418\n",
      "Epoch [12][20]\t Batch [2350][55000]\t Training Loss 0.8464\t Accuracy 0.8422\n",
      "Epoch [12][20]\t Batch [2400][55000]\t Training Loss 0.8474\t Accuracy 0.8405\n",
      "Epoch [12][20]\t Batch [2450][55000]\t Training Loss 0.8503\t Accuracy 0.8392\n",
      "Epoch [12][20]\t Batch [2500][55000]\t Training Loss 0.8481\t Accuracy 0.8413\n",
      "Epoch [12][20]\t Batch [2550][55000]\t Training Loss 0.8468\t Accuracy 0.8412\n",
      "Epoch [12][20]\t Batch [2600][55000]\t Training Loss 0.8463\t Accuracy 0.8401\n",
      "Epoch [12][20]\t Batch [2650][55000]\t Training Loss 0.8451\t Accuracy 0.8404\n",
      "Epoch [12][20]\t Batch [2700][55000]\t Training Loss 0.8448\t Accuracy 0.8408\n",
      "Epoch [12][20]\t Batch [2750][55000]\t Training Loss 0.8447\t Accuracy 0.8408\n",
      "Epoch [12][20]\t Batch [2800][55000]\t Training Loss 0.8450\t Accuracy 0.8390\n",
      "Epoch [12][20]\t Batch [2850][55000]\t Training Loss 0.8439\t Accuracy 0.8390\n",
      "Epoch [12][20]\t Batch [2900][55000]\t Training Loss 0.8406\t Accuracy 0.8401\n",
      "Epoch [12][20]\t Batch [2950][55000]\t Training Loss 0.8407\t Accuracy 0.8397\n",
      "Epoch [12][20]\t Batch [3000][55000]\t Training Loss 0.8406\t Accuracy 0.8404\n",
      "Epoch [12][20]\t Batch [3050][55000]\t Training Loss 0.8416\t Accuracy 0.8401\n",
      "Epoch [12][20]\t Batch [3100][55000]\t Training Loss 0.8439\t Accuracy 0.8397\n",
      "Epoch [12][20]\t Batch [3150][55000]\t Training Loss 0.8438\t Accuracy 0.8401\n",
      "Epoch [12][20]\t Batch [3200][55000]\t Training Loss 0.8436\t Accuracy 0.8413\n",
      "Epoch [12][20]\t Batch [3250][55000]\t Training Loss 0.8430\t Accuracy 0.8407\n",
      "Epoch [12][20]\t Batch [3300][55000]\t Training Loss 0.8443\t Accuracy 0.8407\n",
      "Epoch [12][20]\t Batch [3350][55000]\t Training Loss 0.8425\t Accuracy 0.8421\n",
      "Epoch [12][20]\t Batch [3400][55000]\t Training Loss 0.8448\t Accuracy 0.8409\n",
      "Epoch [12][20]\t Batch [3450][55000]\t Training Loss 0.8445\t Accuracy 0.8418\n",
      "Epoch [12][20]\t Batch [3500][55000]\t Training Loss 0.8445\t Accuracy 0.8415\n",
      "Epoch [12][20]\t Batch [3550][55000]\t Training Loss 0.8461\t Accuracy 0.8409\n",
      "Epoch [12][20]\t Batch [3600][55000]\t Training Loss 0.8459\t Accuracy 0.8403\n",
      "Epoch [12][20]\t Batch [3650][55000]\t Training Loss 0.8445\t Accuracy 0.8411\n",
      "Epoch [12][20]\t Batch [3700][55000]\t Training Loss 0.8453\t Accuracy 0.8406\n",
      "Epoch [12][20]\t Batch [3750][55000]\t Training Loss 0.8452\t Accuracy 0.8408\n",
      "Epoch [12][20]\t Batch [3800][55000]\t Training Loss 0.8455\t Accuracy 0.8408\n",
      "Epoch [12][20]\t Batch [3850][55000]\t Training Loss 0.8451\t Accuracy 0.8413\n",
      "Epoch [12][20]\t Batch [3900][55000]\t Training Loss 0.8440\t Accuracy 0.8426\n",
      "Epoch [12][20]\t Batch [3950][55000]\t Training Loss 0.8428\t Accuracy 0.8438\n",
      "Epoch [12][20]\t Batch [4000][55000]\t Training Loss 0.8425\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [4050][55000]\t Training Loss 0.8409\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [4100][55000]\t Training Loss 0.8414\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [4150][55000]\t Training Loss 0.8419\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [4200][55000]\t Training Loss 0.8424\t Accuracy 0.8436\n",
      "Epoch [12][20]\t Batch [4250][55000]\t Training Loss 0.8409\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [4300][55000]\t Training Loss 0.8410\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [4350][55000]\t Training Loss 0.8415\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [4400][55000]\t Training Loss 0.8414\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [4450][55000]\t Training Loss 0.8416\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [4500][55000]\t Training Loss 0.8418\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [4550][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [4600][55000]\t Training Loss 0.8384\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [4650][55000]\t Training Loss 0.8383\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [4700][55000]\t Training Loss 0.8379\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [4750][55000]\t Training Loss 0.8367\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [4800][55000]\t Training Loss 0.8374\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [4850][55000]\t Training Loss 0.8386\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [4900][55000]\t Training Loss 0.8371\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [4950][55000]\t Training Loss 0.8374\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [5000][55000]\t Training Loss 0.8375\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [5050][55000]\t Training Loss 0.8370\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [5100][55000]\t Training Loss 0.8370\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [5150][55000]\t Training Loss 0.8374\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [5200][55000]\t Training Loss 0.8388\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [5250][55000]\t Training Loss 0.8379\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [5300][55000]\t Training Loss 0.8379\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [5350][55000]\t Training Loss 0.8385\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [5400][55000]\t Training Loss 0.8377\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [5450][55000]\t Training Loss 0.8370\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [5500][55000]\t Training Loss 0.8352\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [5550][55000]\t Training Loss 0.8352\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [5600][55000]\t Training Loss 0.8343\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [5650][55000]\t Training Loss 0.8349\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [5700][55000]\t Training Loss 0.8346\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [5750][55000]\t Training Loss 0.8350\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [5800][55000]\t Training Loss 0.8354\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [5850][55000]\t Training Loss 0.8355\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [5900][55000]\t Training Loss 0.8360\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [5950][55000]\t Training Loss 0.8359\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [6000][55000]\t Training Loss 0.8349\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [6050][55000]\t Training Loss 0.8335\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [6100][55000]\t Training Loss 0.8321\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [6150][55000]\t Training Loss 0.8300\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [6200][55000]\t Training Loss 0.8300\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [6250][55000]\t Training Loss 0.8289\t Accuracy 0.8485\n",
      "Epoch [12][20]\t Batch [6300][55000]\t Training Loss 0.8292\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [6350][55000]\t Training Loss 0.8289\t Accuracy 0.8484\n",
      "Epoch [12][20]\t Batch [6400][55000]\t Training Loss 0.8283\t Accuracy 0.8488\n",
      "Epoch [12][20]\t Batch [6450][55000]\t Training Loss 0.8278\t Accuracy 0.8490\n",
      "Epoch [12][20]\t Batch [6500][55000]\t Training Loss 0.8285\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [6550][55000]\t Training Loss 0.8276\t Accuracy 0.8489\n",
      "Epoch [12][20]\t Batch [6600][55000]\t Training Loss 0.8263\t Accuracy 0.8494\n",
      "Epoch [12][20]\t Batch [6650][55000]\t Training Loss 0.8250\t Accuracy 0.8495\n",
      "Epoch [12][20]\t Batch [6700][55000]\t Training Loss 0.8252\t Accuracy 0.8494\n",
      "Epoch [12][20]\t Batch [6750][55000]\t Training Loss 0.8255\t Accuracy 0.8502\n",
      "Epoch [12][20]\t Batch [6800][55000]\t Training Loss 0.8259\t Accuracy 0.8502\n",
      "Epoch [12][20]\t Batch [6850][55000]\t Training Loss 0.8280\t Accuracy 0.8498\n",
      "Epoch [12][20]\t Batch [6900][55000]\t Training Loss 0.8280\t Accuracy 0.8499\n",
      "Epoch [12][20]\t Batch [6950][55000]\t Training Loss 0.8286\t Accuracy 0.8488\n",
      "Epoch [12][20]\t Batch [7000][55000]\t Training Loss 0.8285\t Accuracy 0.8489\n",
      "Epoch [12][20]\t Batch [7050][55000]\t Training Loss 0.8291\t Accuracy 0.8482\n",
      "Epoch [12][20]\t Batch [7100][55000]\t Training Loss 0.8293\t Accuracy 0.8482\n",
      "Epoch [12][20]\t Batch [7150][55000]\t Training Loss 0.8295\t Accuracy 0.8487\n",
      "Epoch [12][20]\t Batch [7200][55000]\t Training Loss 0.8303\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [7250][55000]\t Training Loss 0.8326\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [7300][55000]\t Training Loss 0.8345\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [7350][55000]\t Training Loss 0.8360\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [7400][55000]\t Training Loss 0.8370\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [7450][55000]\t Training Loss 0.8370\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [7500][55000]\t Training Loss 0.8370\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [7550][55000]\t Training Loss 0.8375\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [7600][55000]\t Training Loss 0.8370\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [7650][55000]\t Training Loss 0.8375\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [7700][55000]\t Training Loss 0.8380\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [7750][55000]\t Training Loss 0.8381\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [7800][55000]\t Training Loss 0.8389\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [7850][55000]\t Training Loss 0.8390\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [7900][55000]\t Training Loss 0.8392\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [7950][55000]\t Training Loss 0.8392\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [8000][55000]\t Training Loss 0.8393\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [8050][55000]\t Training Loss 0.8398\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [8100][55000]\t Training Loss 0.8384\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [8150][55000]\t Training Loss 0.8392\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [8200][55000]\t Training Loss 0.8392\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [8250][55000]\t Training Loss 0.8404\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [8300][55000]\t Training Loss 0.8407\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [8350][55000]\t Training Loss 0.8413\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [8400][55000]\t Training Loss 0.8407\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [8450][55000]\t Training Loss 0.8422\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [8500][55000]\t Training Loss 0.8414\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [8550][55000]\t Training Loss 0.8404\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [8600][55000]\t Training Loss 0.8396\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [8650][55000]\t Training Loss 0.8397\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [8700][55000]\t Training Loss 0.8403\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [8750][55000]\t Training Loss 0.8418\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [8800][55000]\t Training Loss 0.8427\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [8850][55000]\t Training Loss 0.8426\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [8900][55000]\t Training Loss 0.8444\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [8950][55000]\t Training Loss 0.8440\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [9000][55000]\t Training Loss 0.8432\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [9050][55000]\t Training Loss 0.8424\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [9100][55000]\t Training Loss 0.8422\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [9150][55000]\t Training Loss 0.8427\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [9200][55000]\t Training Loss 0.8422\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [9250][55000]\t Training Loss 0.8426\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [9300][55000]\t Training Loss 0.8428\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [9350][55000]\t Training Loss 0.8430\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [9400][55000]\t Training Loss 0.8432\t Accuracy 0.8455\n",
      "Epoch [12][20]\t Batch [9450][55000]\t Training Loss 0.8434\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [9500][55000]\t Training Loss 0.8426\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [9550][55000]\t Training Loss 0.8423\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [9600][55000]\t Training Loss 0.8428\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [9650][55000]\t Training Loss 0.8426\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [9700][55000]\t Training Loss 0.8421\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [9750][55000]\t Training Loss 0.8413\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [9800][55000]\t Training Loss 0.8418\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [9850][55000]\t Training Loss 0.8414\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [9900][55000]\t Training Loss 0.8409\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [9950][55000]\t Training Loss 0.8403\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [10000][55000]\t Training Loss 0.8400\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [10050][55000]\t Training Loss 0.8402\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [10100][55000]\t Training Loss 0.8400\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [10150][55000]\t Training Loss 0.8398\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [10200][55000]\t Training Loss 0.8400\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [10250][55000]\t Training Loss 0.8401\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [10300][55000]\t Training Loss 0.8400\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [10350][55000]\t Training Loss 0.8391\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [10400][55000]\t Training Loss 0.8382\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [10450][55000]\t Training Loss 0.8381\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [10500][55000]\t Training Loss 0.8371\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [10550][55000]\t Training Loss 0.8366\t Accuracy 0.8484\n",
      "Epoch [12][20]\t Batch [10600][55000]\t Training Loss 0.8362\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [10650][55000]\t Training Loss 0.8359\t Accuracy 0.8489\n",
      "Epoch [12][20]\t Batch [10700][55000]\t Training Loss 0.8357\t Accuracy 0.8493\n",
      "Epoch [12][20]\t Batch [10750][55000]\t Training Loss 0.8362\t Accuracy 0.8489\n",
      "Epoch [12][20]\t Batch [10800][55000]\t Training Loss 0.8367\t Accuracy 0.8487\n",
      "Epoch [12][20]\t Batch [10850][55000]\t Training Loss 0.8360\t Accuracy 0.8490\n",
      "Epoch [12][20]\t Batch [10900][55000]\t Training Loss 0.8355\t Accuracy 0.8492\n",
      "Epoch [12][20]\t Batch [10950][55000]\t Training Loss 0.8352\t Accuracy 0.8490\n",
      "Epoch [12][20]\t Batch [11000][55000]\t Training Loss 0.8351\t Accuracy 0.8492\n",
      "Epoch [12][20]\t Batch [11050][55000]\t Training Loss 0.8344\t Accuracy 0.8494\n",
      "Epoch [12][20]\t Batch [11100][55000]\t Training Loss 0.8337\t Accuracy 0.8496\n",
      "Epoch [12][20]\t Batch [11150][55000]\t Training Loss 0.8337\t Accuracy 0.8498\n",
      "Epoch [12][20]\t Batch [11200][55000]\t Training Loss 0.8333\t Accuracy 0.8498\n",
      "Epoch [12][20]\t Batch [11250][55000]\t Training Loss 0.8337\t Accuracy 0.8497\n",
      "Epoch [12][20]\t Batch [11300][55000]\t Training Loss 0.8333\t Accuracy 0.8497\n",
      "Epoch [12][20]\t Batch [11350][55000]\t Training Loss 0.8328\t Accuracy 0.8498\n",
      "Epoch [12][20]\t Batch [11400][55000]\t Training Loss 0.8329\t Accuracy 0.8497\n",
      "Epoch [12][20]\t Batch [11450][55000]\t Training Loss 0.8326\t Accuracy 0.8496\n",
      "Epoch [12][20]\t Batch [11500][55000]\t Training Loss 0.8323\t Accuracy 0.8495\n",
      "Epoch [12][20]\t Batch [11550][55000]\t Training Loss 0.8324\t Accuracy 0.8494\n",
      "Epoch [12][20]\t Batch [11600][55000]\t Training Loss 0.8336\t Accuracy 0.8487\n",
      "Epoch [12][20]\t Batch [11650][55000]\t Training Loss 0.8341\t Accuracy 0.8484\n",
      "Epoch [12][20]\t Batch [11700][55000]\t Training Loss 0.8341\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [11750][55000]\t Training Loss 0.8350\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [11800][55000]\t Training Loss 0.8353\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [11850][55000]\t Training Loss 0.8353\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [11900][55000]\t Training Loss 0.8356\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [11950][55000]\t Training Loss 0.8357\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [12000][55000]\t Training Loss 0.8357\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [12050][55000]\t Training Loss 0.8355\t Accuracy 0.8489\n",
      "Epoch [12][20]\t Batch [12100][55000]\t Training Loss 0.8354\t Accuracy 0.8488\n",
      "Epoch [12][20]\t Batch [12150][55000]\t Training Loss 0.8348\t Accuracy 0.8492\n",
      "Epoch [12][20]\t Batch [12200][55000]\t Training Loss 0.8351\t Accuracy 0.8492\n",
      "Epoch [12][20]\t Batch [12250][55000]\t Training Loss 0.8350\t Accuracy 0.8492\n",
      "Epoch [12][20]\t Batch [12300][55000]\t Training Loss 0.8350\t Accuracy 0.8492\n",
      "Epoch [12][20]\t Batch [12350][55000]\t Training Loss 0.8352\t Accuracy 0.8493\n",
      "Epoch [12][20]\t Batch [12400][55000]\t Training Loss 0.8355\t Accuracy 0.8494\n",
      "Epoch [12][20]\t Batch [12450][55000]\t Training Loss 0.8357\t Accuracy 0.8490\n",
      "Epoch [12][20]\t Batch [12500][55000]\t Training Loss 0.8359\t Accuracy 0.8487\n",
      "Epoch [12][20]\t Batch [12550][55000]\t Training Loss 0.8359\t Accuracy 0.8488\n",
      "Epoch [12][20]\t Batch [12600][55000]\t Training Loss 0.8367\t Accuracy 0.8485\n",
      "Epoch [12][20]\t Batch [12650][55000]\t Training Loss 0.8372\t Accuracy 0.8482\n",
      "Epoch [12][20]\t Batch [12700][55000]\t Training Loss 0.8379\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [12750][55000]\t Training Loss 0.8374\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [12800][55000]\t Training Loss 0.8380\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [12850][55000]\t Training Loss 0.8382\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [12900][55000]\t Training Loss 0.8382\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [12950][55000]\t Training Loss 0.8387\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [13000][55000]\t Training Loss 0.8387\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [13050][55000]\t Training Loss 0.8393\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [13100][55000]\t Training Loss 0.8399\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [13150][55000]\t Training Loss 0.8403\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [13200][55000]\t Training Loss 0.8405\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [13250][55000]\t Training Loss 0.8400\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [13300][55000]\t Training Loss 0.8399\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [13350][55000]\t Training Loss 0.8403\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [13400][55000]\t Training Loss 0.8406\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [13450][55000]\t Training Loss 0.8401\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [13500][55000]\t Training Loss 0.8397\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [13550][55000]\t Training Loss 0.8393\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [13600][55000]\t Training Loss 0.8384\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [13650][55000]\t Training Loss 0.8383\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [13700][55000]\t Training Loss 0.8391\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [13750][55000]\t Training Loss 0.8397\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [13800][55000]\t Training Loss 0.8398\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [13850][55000]\t Training Loss 0.8398\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [13900][55000]\t Training Loss 0.8400\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [13950][55000]\t Training Loss 0.8404\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [14000][55000]\t Training Loss 0.8411\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [14050][55000]\t Training Loss 0.8413\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [14100][55000]\t Training Loss 0.8414\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [14150][55000]\t Training Loss 0.8416\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [14200][55000]\t Training Loss 0.8415\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [14250][55000]\t Training Loss 0.8418\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [14300][55000]\t Training Loss 0.8421\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [14350][55000]\t Training Loss 0.8425\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [14400][55000]\t Training Loss 0.8433\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [14450][55000]\t Training Loss 0.8435\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [14500][55000]\t Training Loss 0.8436\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [14550][55000]\t Training Loss 0.8443\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [14600][55000]\t Training Loss 0.8443\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [14650][55000]\t Training Loss 0.8453\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [14700][55000]\t Training Loss 0.8461\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [14750][55000]\t Training Loss 0.8467\t Accuracy 0.8443\n",
      "Epoch [12][20]\t Batch [14800][55000]\t Training Loss 0.8476\t Accuracy 0.8441\n",
      "Epoch [12][20]\t Batch [14850][55000]\t Training Loss 0.8482\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [14900][55000]\t Training Loss 0.8481\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [14950][55000]\t Training Loss 0.8482\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [15000][55000]\t Training Loss 0.8482\t Accuracy 0.8436\n",
      "Epoch [12][20]\t Batch [15050][55000]\t Training Loss 0.8478\t Accuracy 0.8439\n",
      "Epoch [12][20]\t Batch [15100][55000]\t Training Loss 0.8476\t Accuracy 0.8441\n",
      "Epoch [12][20]\t Batch [15150][55000]\t Training Loss 0.8480\t Accuracy 0.8440\n",
      "Epoch [12][20]\t Batch [15200][55000]\t Training Loss 0.8482\t Accuracy 0.8440\n",
      "Epoch [12][20]\t Batch [15250][55000]\t Training Loss 0.8481\t Accuracy 0.8441\n",
      "Epoch [12][20]\t Batch [15300][55000]\t Training Loss 0.8481\t Accuracy 0.8441\n",
      "Epoch [12][20]\t Batch [15350][55000]\t Training Loss 0.8478\t Accuracy 0.8444\n",
      "Epoch [12][20]\t Batch [15400][55000]\t Training Loss 0.8482\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [15450][55000]\t Training Loss 0.8483\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [15500][55000]\t Training Loss 0.8480\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [15550][55000]\t Training Loss 0.8479\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [15600][55000]\t Training Loss 0.8478\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [15650][55000]\t Training Loss 0.8477\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [15700][55000]\t Training Loss 0.8476\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [15750][55000]\t Training Loss 0.8484\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [15800][55000]\t Training Loss 0.8489\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [15850][55000]\t Training Loss 0.8492\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [15900][55000]\t Training Loss 0.8497\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [15950][55000]\t Training Loss 0.8498\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [16000][55000]\t Training Loss 0.8498\t Accuracy 0.8443\n",
      "Epoch [12][20]\t Batch [16050][55000]\t Training Loss 0.8508\t Accuracy 0.8439\n",
      "Epoch [12][20]\t Batch [16100][55000]\t Training Loss 0.8508\t Accuracy 0.8439\n",
      "Epoch [12][20]\t Batch [16150][55000]\t Training Loss 0.8507\t Accuracy 0.8440\n",
      "Epoch [12][20]\t Batch [16200][55000]\t Training Loss 0.8508\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [16250][55000]\t Training Loss 0.8506\t Accuracy 0.8436\n",
      "Epoch [12][20]\t Batch [16300][55000]\t Training Loss 0.8503\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [16350][55000]\t Training Loss 0.8500\t Accuracy 0.8440\n",
      "Epoch [12][20]\t Batch [16400][55000]\t Training Loss 0.8501\t Accuracy 0.8440\n",
      "Epoch [12][20]\t Batch [16450][55000]\t Training Loss 0.8498\t Accuracy 0.8442\n",
      "Epoch [12][20]\t Batch [16500][55000]\t Training Loss 0.8496\t Accuracy 0.8444\n",
      "Epoch [12][20]\t Batch [16550][55000]\t Training Loss 0.8492\t Accuracy 0.8444\n",
      "Epoch [12][20]\t Batch [16600][55000]\t Training Loss 0.8493\t Accuracy 0.8444\n",
      "Epoch [12][20]\t Batch [16650][55000]\t Training Loss 0.8493\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [16700][55000]\t Training Loss 0.8494\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [16750][55000]\t Training Loss 0.8492\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [16800][55000]\t Training Loss 0.8499\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [16850][55000]\t Training Loss 0.8504\t Accuracy 0.8442\n",
      "Epoch [12][20]\t Batch [16900][55000]\t Training Loss 0.8506\t Accuracy 0.8443\n",
      "Epoch [12][20]\t Batch [16950][55000]\t Training Loss 0.8506\t Accuracy 0.8441\n",
      "Epoch [12][20]\t Batch [17000][55000]\t Training Loss 0.8514\t Accuracy 0.8438\n",
      "Epoch [12][20]\t Batch [17050][55000]\t Training Loss 0.8513\t Accuracy 0.8438\n",
      "Epoch [12][20]\t Batch [17100][55000]\t Training Loss 0.8518\t Accuracy 0.8436\n",
      "Epoch [12][20]\t Batch [17150][55000]\t Training Loss 0.8515\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [17200][55000]\t Training Loss 0.8516\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [17250][55000]\t Training Loss 0.8521\t Accuracy 0.8435\n",
      "Epoch [12][20]\t Batch [17300][55000]\t Training Loss 0.8519\t Accuracy 0.8436\n",
      "Epoch [12][20]\t Batch [17350][55000]\t Training Loss 0.8514\t Accuracy 0.8439\n",
      "Epoch [12][20]\t Batch [17400][55000]\t Training Loss 0.8515\t Accuracy 0.8439\n",
      "Epoch [12][20]\t Batch [17450][55000]\t Training Loss 0.8516\t Accuracy 0.8438\n",
      "Epoch [12][20]\t Batch [17500][55000]\t Training Loss 0.8517\t Accuracy 0.8439\n",
      "Epoch [12][20]\t Batch [17550][55000]\t Training Loss 0.8523\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [17600][55000]\t Training Loss 0.8530\t Accuracy 0.8434\n",
      "Epoch [12][20]\t Batch [17650][55000]\t Training Loss 0.8532\t Accuracy 0.8436\n",
      "Epoch [12][20]\t Batch [17700][55000]\t Training Loss 0.8539\t Accuracy 0.8434\n",
      "Epoch [12][20]\t Batch [17750][55000]\t Training Loss 0.8542\t Accuracy 0.8434\n",
      "Epoch [12][20]\t Batch [17800][55000]\t Training Loss 0.8546\t Accuracy 0.8432\n",
      "Epoch [12][20]\t Batch [17850][55000]\t Training Loss 0.8549\t Accuracy 0.8430\n",
      "Epoch [12][20]\t Batch [17900][55000]\t Training Loss 0.8552\t Accuracy 0.8429\n",
      "Epoch [12][20]\t Batch [17950][55000]\t Training Loss 0.8550\t Accuracy 0.8427\n",
      "Epoch [12][20]\t Batch [18000][55000]\t Training Loss 0.8547\t Accuracy 0.8428\n",
      "Epoch [12][20]\t Batch [18050][55000]\t Training Loss 0.8549\t Accuracy 0.8428\n",
      "Epoch [12][20]\t Batch [18100][55000]\t Training Loss 0.8548\t Accuracy 0.8430\n",
      "Epoch [12][20]\t Batch [18150][55000]\t Training Loss 0.8544\t Accuracy 0.8430\n",
      "Epoch [12][20]\t Batch [18200][55000]\t Training Loss 0.8539\t Accuracy 0.8433\n",
      "Epoch [12][20]\t Batch [18250][55000]\t Training Loss 0.8539\t Accuracy 0.8434\n",
      "Epoch [12][20]\t Batch [18300][55000]\t Training Loss 0.8535\t Accuracy 0.8435\n",
      "Epoch [12][20]\t Batch [18350][55000]\t Training Loss 0.8533\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [18400][55000]\t Training Loss 0.8534\t Accuracy 0.8437\n",
      "Epoch [12][20]\t Batch [18450][55000]\t Training Loss 0.8538\t Accuracy 0.8435\n",
      "Epoch [12][20]\t Batch [18500][55000]\t Training Loss 0.8538\t Accuracy 0.8435\n",
      "Epoch [12][20]\t Batch [18550][55000]\t Training Loss 0.8536\t Accuracy 0.8438\n",
      "Epoch [12][20]\t Batch [18600][55000]\t Training Loss 0.8536\t Accuracy 0.8440\n",
      "Epoch [12][20]\t Batch [18650][55000]\t Training Loss 0.8534\t Accuracy 0.8442\n",
      "Epoch [12][20]\t Batch [18700][55000]\t Training Loss 0.8537\t Accuracy 0.8440\n",
      "Epoch [12][20]\t Batch [18750][55000]\t Training Loss 0.8539\t Accuracy 0.8441\n",
      "Epoch [12][20]\t Batch [18800][55000]\t Training Loss 0.8535\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [18850][55000]\t Training Loss 0.8538\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [18900][55000]\t Training Loss 0.8532\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [18950][55000]\t Training Loss 0.8530\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [19000][55000]\t Training Loss 0.8529\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [19050][55000]\t Training Loss 0.8533\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [19100][55000]\t Training Loss 0.8536\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [19150][55000]\t Training Loss 0.8537\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [19200][55000]\t Training Loss 0.8538\t Accuracy 0.8444\n",
      "Epoch [12][20]\t Batch [19250][55000]\t Training Loss 0.8537\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [19300][55000]\t Training Loss 0.8535\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [19350][55000]\t Training Loss 0.8536\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [19400][55000]\t Training Loss 0.8533\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [19450][55000]\t Training Loss 0.8531\t Accuracy 0.8445\n",
      "Epoch [12][20]\t Batch [19500][55000]\t Training Loss 0.8527\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [19550][55000]\t Training Loss 0.8528\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [19600][55000]\t Training Loss 0.8527\t Accuracy 0.8443\n",
      "Epoch [12][20]\t Batch [19650][55000]\t Training Loss 0.8523\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [19700][55000]\t Training Loss 0.8518\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [19750][55000]\t Training Loss 0.8513\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [19800][55000]\t Training Loss 0.8506\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [19850][55000]\t Training Loss 0.8507\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [19900][55000]\t Training Loss 0.8504\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [19950][55000]\t Training Loss 0.8506\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [20000][55000]\t Training Loss 0.8505\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [20050][55000]\t Training Loss 0.8511\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [20100][55000]\t Training Loss 0.8512\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [20150][55000]\t Training Loss 0.8510\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [20200][55000]\t Training Loss 0.8514\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [20250][55000]\t Training Loss 0.8515\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [20300][55000]\t Training Loss 0.8516\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [20350][55000]\t Training Loss 0.8515\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [20400][55000]\t Training Loss 0.8512\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [20450][55000]\t Training Loss 0.8509\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [20500][55000]\t Training Loss 0.8506\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [20550][55000]\t Training Loss 0.8505\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [20600][55000]\t Training Loss 0.8505\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [20650][55000]\t Training Loss 0.8502\t Accuracy 0.8455\n",
      "Epoch [12][20]\t Batch [20700][55000]\t Training Loss 0.8500\t Accuracy 0.8455\n",
      "Epoch [12][20]\t Batch [20750][55000]\t Training Loss 0.8500\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [20800][55000]\t Training Loss 0.8500\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [20850][55000]\t Training Loss 0.8500\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [20900][55000]\t Training Loss 0.8502\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [20950][55000]\t Training Loss 0.8508\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [21000][55000]\t Training Loss 0.8509\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [21050][55000]\t Training Loss 0.8512\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [21100][55000]\t Training Loss 0.8509\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [21150][55000]\t Training Loss 0.8509\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [21200][55000]\t Training Loss 0.8506\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [21250][55000]\t Training Loss 0.8505\t Accuracy 0.8454\n",
      "Epoch [12][20]\t Batch [21300][55000]\t Training Loss 0.8500\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [21350][55000]\t Training Loss 0.8501\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [21400][55000]\t Training Loss 0.8502\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [21450][55000]\t Training Loss 0.8502\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [21500][55000]\t Training Loss 0.8498\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [21550][55000]\t Training Loss 0.8496\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [21600][55000]\t Training Loss 0.8498\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [21650][55000]\t Training Loss 0.8498\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [21700][55000]\t Training Loss 0.8497\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [21750][55000]\t Training Loss 0.8497\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [21800][55000]\t Training Loss 0.8492\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [21850][55000]\t Training Loss 0.8488\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [21900][55000]\t Training Loss 0.8484\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [21950][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [22000][55000]\t Training Loss 0.8475\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [22050][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [22100][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [22150][55000]\t Training Loss 0.8477\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [22200][55000]\t Training Loss 0.8479\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [22250][55000]\t Training Loss 0.8480\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [22300][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [22350][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [22400][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [22450][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [22500][55000]\t Training Loss 0.8482\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [22550][55000]\t Training Loss 0.8488\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [22600][55000]\t Training Loss 0.8493\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [22650][55000]\t Training Loss 0.8494\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [22700][55000]\t Training Loss 0.8493\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [22750][55000]\t Training Loss 0.8492\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [22800][55000]\t Training Loss 0.8491\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [22850][55000]\t Training Loss 0.8491\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [22900][55000]\t Training Loss 0.8489\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [22950][55000]\t Training Loss 0.8487\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [23000][55000]\t Training Loss 0.8484\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [23050][55000]\t Training Loss 0.8482\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [23100][55000]\t Training Loss 0.8484\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [23150][55000]\t Training Loss 0.8483\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [23200][55000]\t Training Loss 0.8483\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [23250][55000]\t Training Loss 0.8481\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [23300][55000]\t Training Loss 0.8477\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [23350][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [23400][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [23450][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [23500][55000]\t Training Loss 0.8470\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [23550][55000]\t Training Loss 0.8469\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [23600][55000]\t Training Loss 0.8470\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [23650][55000]\t Training Loss 0.8471\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [23700][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [23750][55000]\t Training Loss 0.8479\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [23800][55000]\t Training Loss 0.8475\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [23850][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [23900][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [23950][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [24000][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [24050][55000]\t Training Loss 0.8476\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [24100][55000]\t Training Loss 0.8474\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [24150][55000]\t Training Loss 0.8473\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [24200][55000]\t Training Loss 0.8471\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [24250][55000]\t Training Loss 0.8473\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [24300][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [24350][55000]\t Training Loss 0.8474\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [24400][55000]\t Training Loss 0.8474\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [24450][55000]\t Training Loss 0.8474\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [24500][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [24550][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [24600][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [24650][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [24700][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [24750][55000]\t Training Loss 0.8479\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [24800][55000]\t Training Loss 0.8482\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [24850][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [24900][55000]\t Training Loss 0.8482\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [24950][55000]\t Training Loss 0.8485\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [25000][55000]\t Training Loss 0.8487\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [25050][55000]\t Training Loss 0.8484\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [25100][55000]\t Training Loss 0.8483\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [25150][55000]\t Training Loss 0.8482\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [25200][55000]\t Training Loss 0.8480\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [25250][55000]\t Training Loss 0.8480\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [25300][55000]\t Training Loss 0.8479\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [25350][55000]\t Training Loss 0.8480\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [25400][55000]\t Training Loss 0.8476\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [25450][55000]\t Training Loss 0.8471\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [25500][55000]\t Training Loss 0.8469\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [25550][55000]\t Training Loss 0.8466\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [25600][55000]\t Training Loss 0.8465\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [25650][55000]\t Training Loss 0.8464\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [25700][55000]\t Training Loss 0.8462\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [25750][55000]\t Training Loss 0.8459\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [25800][55000]\t Training Loss 0.8459\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [25850][55000]\t Training Loss 0.8460\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [25900][55000]\t Training Loss 0.8461\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [25950][55000]\t Training Loss 0.8461\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [26000][55000]\t Training Loss 0.8460\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [26050][55000]\t Training Loss 0.8458\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [26100][55000]\t Training Loss 0.8456\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [26150][55000]\t Training Loss 0.8454\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [26200][55000]\t Training Loss 0.8452\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [26250][55000]\t Training Loss 0.8451\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [26300][55000]\t Training Loss 0.8453\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [26350][55000]\t Training Loss 0.8452\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [26400][55000]\t Training Loss 0.8456\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [26450][55000]\t Training Loss 0.8458\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [26500][55000]\t Training Loss 0.8460\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [26550][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [26600][55000]\t Training Loss 0.8464\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [26650][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [26700][55000]\t Training Loss 0.8466\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [26750][55000]\t Training Loss 0.8468\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [26800][55000]\t Training Loss 0.8467\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [26850][55000]\t Training Loss 0.8466\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [26900][55000]\t Training Loss 0.8469\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [26950][55000]\t Training Loss 0.8468\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [27000][55000]\t Training Loss 0.8466\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [27050][55000]\t Training Loss 0.8464\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [27100][55000]\t Training Loss 0.8463\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [27150][55000]\t Training Loss 0.8462\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [27200][55000]\t Training Loss 0.8466\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [27250][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [27300][55000]\t Training Loss 0.8465\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [27350][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [27400][55000]\t Training Loss 0.8464\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [27450][55000]\t Training Loss 0.8464\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [27500][55000]\t Training Loss 0.8464\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [27550][55000]\t Training Loss 0.8464\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [27600][55000]\t Training Loss 0.8462\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [27650][55000]\t Training Loss 0.8463\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [27700][55000]\t Training Loss 0.8463\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [27750][55000]\t Training Loss 0.8464\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [27800][55000]\t Training Loss 0.8465\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [27850][55000]\t Training Loss 0.8465\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [27900][55000]\t Training Loss 0.8464\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [27950][55000]\t Training Loss 0.8462\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [28000][55000]\t Training Loss 0.8459\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [28050][55000]\t Training Loss 0.8456\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [28100][55000]\t Training Loss 0.8453\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [28150][55000]\t Training Loss 0.8451\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [28200][55000]\t Training Loss 0.8452\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [28250][55000]\t Training Loss 0.8448\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [28300][55000]\t Training Loss 0.8447\t Accuracy 0.8481\n",
      "Epoch [12][20]\t Batch [28350][55000]\t Training Loss 0.8444\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [28400][55000]\t Training Loss 0.8449\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [28450][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [12][20]\t Batch [28500][55000]\t Training Loss 0.8445\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [28550][55000]\t Training Loss 0.8444\t Accuracy 0.8484\n",
      "Epoch [12][20]\t Batch [28600][55000]\t Training Loss 0.8442\t Accuracy 0.8485\n",
      "Epoch [12][20]\t Batch [28650][55000]\t Training Loss 0.8445\t Accuracy 0.8484\n",
      "Epoch [12][20]\t Batch [28700][55000]\t Training Loss 0.8448\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [28750][55000]\t Training Loss 0.8449\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [28800][55000]\t Training Loss 0.8448\t Accuracy 0.8482\n",
      "Epoch [12][20]\t Batch [28850][55000]\t Training Loss 0.8447\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [28900][55000]\t Training Loss 0.8446\t Accuracy 0.8484\n",
      "Epoch [12][20]\t Batch [28950][55000]\t Training Loss 0.8446\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [29000][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [12][20]\t Batch [29050][55000]\t Training Loss 0.8445\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [29100][55000]\t Training Loss 0.8447\t Accuracy 0.8482\n",
      "Epoch [12][20]\t Batch [29150][55000]\t Training Loss 0.8451\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [29200][55000]\t Training Loss 0.8454\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [29250][55000]\t Training Loss 0.8456\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [29300][55000]\t Training Loss 0.8454\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [29350][55000]\t Training Loss 0.8456\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [29400][55000]\t Training Loss 0.8455\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [29450][55000]\t Training Loss 0.8451\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [29500][55000]\t Training Loss 0.8449\t Accuracy 0.8480\n",
      "Epoch [12][20]\t Batch [29550][55000]\t Training Loss 0.8448\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [29600][55000]\t Training Loss 0.8448\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [29650][55000]\t Training Loss 0.8447\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [29700][55000]\t Training Loss 0.8448\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [29750][55000]\t Training Loss 0.8452\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [29800][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [29850][55000]\t Training Loss 0.8457\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [29900][55000]\t Training Loss 0.8460\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [29950][55000]\t Training Loss 0.8463\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [30000][55000]\t Training Loss 0.8466\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [30050][55000]\t Training Loss 0.8467\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [30100][55000]\t Training Loss 0.8471\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [30150][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [30200][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [30250][55000]\t Training Loss 0.8479\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [30300][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [30350][55000]\t Training Loss 0.8477\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [30400][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [30450][55000]\t Training Loss 0.8476\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [30500][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [30550][55000]\t Training Loss 0.8482\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [30600][55000]\t Training Loss 0.8482\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [30650][55000]\t Training Loss 0.8486\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [30700][55000]\t Training Loss 0.8489\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [30750][55000]\t Training Loss 0.8491\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [30800][55000]\t Training Loss 0.8493\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [30850][55000]\t Training Loss 0.8492\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [30900][55000]\t Training Loss 0.8495\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [30950][55000]\t Training Loss 0.8494\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [31000][55000]\t Training Loss 0.8494\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [31050][55000]\t Training Loss 0.8495\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [31100][55000]\t Training Loss 0.8495\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [31150][55000]\t Training Loss 0.8496\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [31200][55000]\t Training Loss 0.8496\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [31250][55000]\t Training Loss 0.8496\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [31300][55000]\t Training Loss 0.8499\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [31350][55000]\t Training Loss 0.8505\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [31400][55000]\t Training Loss 0.8507\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [31450][55000]\t Training Loss 0.8510\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [31500][55000]\t Training Loss 0.8510\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [31550][55000]\t Training Loss 0.8510\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [31600][55000]\t Training Loss 0.8512\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [31650][55000]\t Training Loss 0.8514\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [31700][55000]\t Training Loss 0.8516\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [31750][55000]\t Training Loss 0.8520\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [31800][55000]\t Training Loss 0.8522\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [31850][55000]\t Training Loss 0.8521\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [31900][55000]\t Training Loss 0.8521\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [31950][55000]\t Training Loss 0.8520\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [32000][55000]\t Training Loss 0.8521\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [32050][55000]\t Training Loss 0.8520\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [32100][55000]\t Training Loss 0.8519\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [32150][55000]\t Training Loss 0.8522\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [32200][55000]\t Training Loss 0.8524\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [32250][55000]\t Training Loss 0.8527\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [32300][55000]\t Training Loss 0.8530\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [32350][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [32400][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [32450][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [32500][55000]\t Training Loss 0.8538\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [32550][55000]\t Training Loss 0.8541\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [32600][55000]\t Training Loss 0.8541\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [32650][55000]\t Training Loss 0.8539\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [32700][55000]\t Training Loss 0.8539\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [32750][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [32800][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [32850][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [32900][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [32950][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33000][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33050][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33100][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33150][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [33200][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [33250][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33300][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33350][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [33400][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33450][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [33500][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [33550][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [33600][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33650][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [33700][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [33750][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [33800][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [33850][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [33900][55000]\t Training Loss 0.8532\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [33950][55000]\t Training Loss 0.8529\t Accuracy 0.8466\n",
      "Epoch [12][20]\t Batch [34000][55000]\t Training Loss 0.8528\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [34050][55000]\t Training Loss 0.8530\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [34100][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [34150][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [34200][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [34250][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [34300][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [34350][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [34400][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [34450][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [34500][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [34550][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [34600][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [34650][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [34700][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [34750][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [34800][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [34850][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [34900][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [34950][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [35000][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [35050][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [35100][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [35150][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [35200][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [35250][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [35300][55000]\t Training Loss 0.8534\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [35350][55000]\t Training Loss 0.8533\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [35400][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [35450][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [35500][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [35550][55000]\t Training Loss 0.8530\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [35600][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [35650][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [35700][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [35750][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [35800][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [35850][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [35900][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [35950][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [36000][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [36050][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [36100][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [36150][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [36200][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [36250][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [36300][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [36350][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [36400][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [36450][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [36500][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [36550][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [36600][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [36650][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [36700][55000]\t Training Loss 0.8520\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [36750][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [36800][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [36850][55000]\t Training Loss 0.8517\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [36900][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [36950][55000]\t Training Loss 0.8516\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [37000][55000]\t Training Loss 0.8514\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [37050][55000]\t Training Loss 0.8513\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [37100][55000]\t Training Loss 0.8514\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [37150][55000]\t Training Loss 0.8514\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [37200][55000]\t Training Loss 0.8513\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [37250][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [37300][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [12][20]\t Batch [37350][55000]\t Training Loss 0.8513\t Accuracy 0.8478\n",
      "Epoch [12][20]\t Batch [37400][55000]\t Training Loss 0.8516\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [37450][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [37500][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [37550][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [37600][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [37650][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [37700][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [37750][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [37800][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [37850][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [37900][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [37950][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [38000][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [38050][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [38100][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [38150][55000]\t Training Loss 0.8524\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [38200][55000]\t Training Loss 0.8523\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [38250][55000]\t Training Loss 0.8523\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [38300][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [38350][55000]\t Training Loss 0.8526\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [38400][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [38450][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [38500][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [38550][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [38600][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [38650][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [38700][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [38750][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [38800][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [38850][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [38900][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [38950][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39000][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39050][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39100][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [39150][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [39200][55000]\t Training Loss 0.8515\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [39250][55000]\t Training Loss 0.8513\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [39300][55000]\t Training Loss 0.8513\t Accuracy 0.8477\n",
      "Epoch [12][20]\t Batch [39350][55000]\t Training Loss 0.8516\t Accuracy 0.8475\n",
      "Epoch [12][20]\t Batch [39400][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [12][20]\t Batch [39450][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [39500][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [12][20]\t Batch [39550][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39600][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39650][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39700][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39750][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39800][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39850][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [39900][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [39950][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40000][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40050][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [40100][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [40150][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [40200][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40250][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [40300][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40350][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40400][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40450][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40500][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [40550][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40600][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [40650][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [40700][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [40750][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40800][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40850][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [40900][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [40950][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [41000][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [41050][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [41100][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [41150][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [41200][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [41250][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [41300][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [41350][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [41400][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [41450][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [41500][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [41550][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [41600][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [41650][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [41700][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [12][20]\t Batch [41750][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [41800][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [12][20]\t Batch [41850][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [41900][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [41950][55000]\t Training Loss 0.8536\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [42000][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [42050][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [42100][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [12][20]\t Batch [42150][55000]\t Training Loss 0.8535\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [42200][55000]\t Training Loss 0.8535\t Accuracy 0.8468\n",
      "Epoch [12][20]\t Batch [42250][55000]\t Training Loss 0.8535\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [42300][55000]\t Training Loss 0.8536\t Accuracy 0.8467\n",
      "Epoch [12][20]\t Batch [42350][55000]\t Training Loss 0.8538\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [42400][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [42450][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [42500][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [42550][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [42600][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [42650][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [42700][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [42750][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [42800][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [42850][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [42900][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [42950][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [43000][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [43050][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [43100][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [43150][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [43200][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [43250][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [43300][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [43350][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [43400][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [43450][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [43500][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [43550][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [43600][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [43650][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [43700][55000]\t Training Loss 0.8535\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [43750][55000]\t Training Loss 0.8535\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [43800][55000]\t Training Loss 0.8534\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [43850][55000]\t Training Loss 0.8535\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [43900][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [43950][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [44000][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [44050][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [44100][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [44150][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [44200][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [44250][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [44300][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [44350][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [44400][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [44450][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [44500][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [44550][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [44600][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [44650][55000]\t Training Loss 0.8540\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [44700][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [44750][55000]\t Training Loss 0.8539\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [44800][55000]\t Training Loss 0.8539\t Accuracy 0.8465\n",
      "Epoch [12][20]\t Batch [44850][55000]\t Training Loss 0.8540\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [44900][55000]\t Training Loss 0.8541\t Accuracy 0.8464\n",
      "Epoch [12][20]\t Batch [44950][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [45000][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45050][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45100][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45150][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [45200][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [45250][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45300][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45350][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [45400][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [45450][55000]\t Training Loss 0.8543\t Accuracy 0.8463\n",
      "Epoch [12][20]\t Batch [45500][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45550][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45600][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45650][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45700][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45750][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45800][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45850][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [45900][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [45950][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [46000][55000]\t Training Loss 0.8548\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [46050][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [46100][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [46150][55000]\t Training Loss 0.8551\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [46200][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [46250][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [46300][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [46350][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [46400][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [46450][55000]\t Training Loss 0.8555\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [46500][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [46550][55000]\t Training Loss 0.8553\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [46600][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [46650][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [46700][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [46750][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [46800][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [46850][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [46900][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [46950][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47000][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47050][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47100][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [47150][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [47200][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [47250][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [47300][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47350][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47400][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47450][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47500][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [47550][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47600][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47650][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [47700][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [47750][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [47800][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [47850][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47900][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [47950][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [48000][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [48050][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [48100][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [48150][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [48200][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [48250][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [48300][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [48350][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [48400][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [48450][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [48500][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [48550][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [48600][55000]\t Training Loss 0.8534\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [48650][55000]\t Training Loss 0.8533\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [48700][55000]\t Training Loss 0.8532\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [48750][55000]\t Training Loss 0.8530\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [48800][55000]\t Training Loss 0.8529\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [48850][55000]\t Training Loss 0.8528\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [48900][55000]\t Training Loss 0.8527\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [48950][55000]\t Training Loss 0.8529\t Accuracy 0.8461\n",
      "Epoch [12][20]\t Batch [49000][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [12][20]\t Batch [49050][55000]\t Training Loss 0.8534\t Accuracy 0.8459\n",
      "Epoch [12][20]\t Batch [49100][55000]\t Training Loss 0.8535\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [49150][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [12][20]\t Batch [49200][55000]\t Training Loss 0.8536\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [49250][55000]\t Training Loss 0.8538\t Accuracy 0.8455\n",
      "Epoch [12][20]\t Batch [49300][55000]\t Training Loss 0.8537\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [49350][55000]\t Training Loss 0.8535\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [49400][55000]\t Training Loss 0.8534\t Accuracy 0.8457\n",
      "Epoch [12][20]\t Batch [49450][55000]\t Training Loss 0.8533\t Accuracy 0.8456\n",
      "Epoch [12][20]\t Batch [49500][55000]\t Training Loss 0.8536\t Accuracy 0.8455\n",
      "Epoch [12][20]\t Batch [49550][55000]\t Training Loss 0.8539\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [49600][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [49650][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [49700][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [49750][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [49800][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [49850][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [49900][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [49950][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [50000][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [50050][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [50100][55000]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [50150][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [50200][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [50250][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [50300][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [50350][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [50400][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [50450][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [50500][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [50550][55000]\t Training Loss 0.8555\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [50600][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [50650][55000]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [50700][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [50750][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [50800][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [50850][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [50900][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [50950][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [51000][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [51050][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [51100][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [51150][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [51200][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [51250][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [51300][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [51350][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [51400][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [51450][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [51500][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [51550][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [51600][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [51650][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [51700][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [51750][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [51800][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [51850][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [51900][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [51950][55000]\t Training Loss 0.8544\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [52000][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [52050][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [52100][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [52150][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [52200][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [52250][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [52300][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [52350][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [52400][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [52450][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [52500][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [52550][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [12][20]\t Batch [52600][55000]\t Training Loss 0.8546\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [52650][55000]\t Training Loss 0.8545\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [52700][55000]\t Training Loss 0.8546\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [52750][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [52800][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [52850][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [52900][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [52950][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [53000][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [53050][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53100][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53150][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [53200][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53250][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53300][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53350][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53400][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [53450][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [53500][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [53550][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53600][55000]\t Training Loss 0.8553\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [53650][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53700][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53750][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53800][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [53850][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53900][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [53950][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [54000][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [54050][55000]\t Training Loss 0.8553\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [54100][55000]\t Training Loss 0.8555\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [54150][55000]\t Training Loss 0.8554\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [54200][55000]\t Training Loss 0.8554\t Accuracy 0.8446\n",
      "Epoch [12][20]\t Batch [54250][55000]\t Training Loss 0.8553\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [54300][55000]\t Training Loss 0.8553\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [54350][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [54400][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [54450][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [54500][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [54550][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [54600][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [54650][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [54700][55000]\t Training Loss 0.8546\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [54750][55000]\t Training Loss 0.8545\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [54800][55000]\t Training Loss 0.8545\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [54850][55000]\t Training Loss 0.8543\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [54900][55000]\t Training Loss 0.8545\t Accuracy 0.8450\n",
      "Epoch [12][20]\t Batch [54950][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8550\t Average training accuracy 0.8447\n",
      "Epoch [12]\t Average validation loss 0.7924\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [13][20]\t Batch [0][55000]\t Training Loss 1.4721\t Accuracy 0.0000\n",
      "Epoch [13][20]\t Batch [50][55000]\t Training Loss 0.9389\t Accuracy 0.7255\n",
      "Epoch [13][20]\t Batch [100][55000]\t Training Loss 0.8492\t Accuracy 0.8020\n",
      "Epoch [13][20]\t Batch [150][55000]\t Training Loss 0.8505\t Accuracy 0.8146\n",
      "Epoch [13][20]\t Batch [200][55000]\t Training Loss 0.8614\t Accuracy 0.8159\n",
      "Epoch [13][20]\t Batch [250][55000]\t Training Loss 0.8435\t Accuracy 0.8247\n",
      "Epoch [13][20]\t Batch [300][55000]\t Training Loss 0.8451\t Accuracy 0.8206\n",
      "Epoch [13][20]\t Batch [350][55000]\t Training Loss 0.8253\t Accuracy 0.8234\n",
      "Epoch [13][20]\t Batch [400][55000]\t Training Loss 0.8114\t Accuracy 0.8279\n",
      "Epoch [13][20]\t Batch [450][55000]\t Training Loss 0.8173\t Accuracy 0.8293\n",
      "Epoch [13][20]\t Batch [500][55000]\t Training Loss 0.8236\t Accuracy 0.8303\n",
      "Epoch [13][20]\t Batch [550][55000]\t Training Loss 0.8409\t Accuracy 0.8276\n",
      "Epoch [13][20]\t Batch [600][55000]\t Training Loss 0.8454\t Accuracy 0.8303\n",
      "Epoch [13][20]\t Batch [650][55000]\t Training Loss 0.8652\t Accuracy 0.8203\n",
      "Epoch [13][20]\t Batch [700][55000]\t Training Loss 0.8629\t Accuracy 0.8260\n",
      "Epoch [13][20]\t Batch [750][55000]\t Training Loss 0.8591\t Accuracy 0.8269\n",
      "Epoch [13][20]\t Batch [800][55000]\t Training Loss 0.8531\t Accuracy 0.8290\n",
      "Epoch [13][20]\t Batch [850][55000]\t Training Loss 0.8516\t Accuracy 0.8308\n",
      "Epoch [13][20]\t Batch [900][55000]\t Training Loss 0.8580\t Accuracy 0.8302\n",
      "Epoch [13][20]\t Batch [950][55000]\t Training Loss 0.8648\t Accuracy 0.8254\n",
      "Epoch [13][20]\t Batch [1000][55000]\t Training Loss 0.8643\t Accuracy 0.8282\n",
      "Epoch [13][20]\t Batch [1050][55000]\t Training Loss 0.8719\t Accuracy 0.8268\n",
      "Epoch [13][20]\t Batch [1100][55000]\t Training Loss 0.8804\t Accuracy 0.8265\n",
      "Epoch [13][20]\t Batch [1150][55000]\t Training Loss 0.8896\t Accuracy 0.8236\n",
      "Epoch [13][20]\t Batch [1200][55000]\t Training Loss 0.8838\t Accuracy 0.8268\n",
      "Epoch [13][20]\t Batch [1250][55000]\t Training Loss 0.8852\t Accuracy 0.8265\n",
      "Epoch [13][20]\t Batch [1300][55000]\t Training Loss 0.8854\t Accuracy 0.8263\n",
      "Epoch [13][20]\t Batch [1350][55000]\t Training Loss 0.8822\t Accuracy 0.8275\n",
      "Epoch [13][20]\t Batch [1400][55000]\t Training Loss 0.8828\t Accuracy 0.8266\n",
      "Epoch [13][20]\t Batch [1450][55000]\t Training Loss 0.8837\t Accuracy 0.8256\n",
      "Epoch [13][20]\t Batch [1500][55000]\t Training Loss 0.8818\t Accuracy 0.8281\n",
      "Epoch [13][20]\t Batch [1550][55000]\t Training Loss 0.8812\t Accuracy 0.8285\n",
      "Epoch [13][20]\t Batch [1600][55000]\t Training Loss 0.8839\t Accuracy 0.8276\n",
      "Epoch [13][20]\t Batch [1650][55000]\t Training Loss 0.8807\t Accuracy 0.8292\n",
      "Epoch [13][20]\t Batch [1700][55000]\t Training Loss 0.8791\t Accuracy 0.8307\n",
      "Epoch [13][20]\t Batch [1750][55000]\t Training Loss 0.8734\t Accuracy 0.8321\n",
      "Epoch [13][20]\t Batch [1800][55000]\t Training Loss 0.8712\t Accuracy 0.8340\n",
      "Epoch [13][20]\t Batch [1850][55000]\t Training Loss 0.8696\t Accuracy 0.8347\n",
      "Epoch [13][20]\t Batch [1900][55000]\t Training Loss 0.8674\t Accuracy 0.8348\n",
      "Epoch [13][20]\t Batch [1950][55000]\t Training Loss 0.8659\t Accuracy 0.8360\n",
      "Epoch [13][20]\t Batch [2000][55000]\t Training Loss 0.8634\t Accuracy 0.8371\n",
      "Epoch [13][20]\t Batch [2050][55000]\t Training Loss 0.8625\t Accuracy 0.8376\n",
      "Epoch [13][20]\t Batch [2100][55000]\t Training Loss 0.8597\t Accuracy 0.8382\n",
      "Epoch [13][20]\t Batch [2150][55000]\t Training Loss 0.8564\t Accuracy 0.8401\n",
      "Epoch [13][20]\t Batch [2200][55000]\t Training Loss 0.8516\t Accuracy 0.8419\n",
      "Epoch [13][20]\t Batch [2250][55000]\t Training Loss 0.8508\t Accuracy 0.8423\n",
      "Epoch [13][20]\t Batch [2300][55000]\t Training Loss 0.8479\t Accuracy 0.8418\n",
      "Epoch [13][20]\t Batch [2350][55000]\t Training Loss 0.8463\t Accuracy 0.8422\n",
      "Epoch [13][20]\t Batch [2400][55000]\t Training Loss 0.8473\t Accuracy 0.8405\n",
      "Epoch [13][20]\t Batch [2450][55000]\t Training Loss 0.8502\t Accuracy 0.8392\n",
      "Epoch [13][20]\t Batch [2500][55000]\t Training Loss 0.8480\t Accuracy 0.8413\n",
      "Epoch [13][20]\t Batch [2550][55000]\t Training Loss 0.8468\t Accuracy 0.8412\n",
      "Epoch [13][20]\t Batch [2600][55000]\t Training Loss 0.8462\t Accuracy 0.8401\n",
      "Epoch [13][20]\t Batch [2650][55000]\t Training Loss 0.8450\t Accuracy 0.8404\n",
      "Epoch [13][20]\t Batch [2700][55000]\t Training Loss 0.8448\t Accuracy 0.8408\n",
      "Epoch [13][20]\t Batch [2750][55000]\t Training Loss 0.8446\t Accuracy 0.8408\n",
      "Epoch [13][20]\t Batch [2800][55000]\t Training Loss 0.8449\t Accuracy 0.8390\n",
      "Epoch [13][20]\t Batch [2850][55000]\t Training Loss 0.8438\t Accuracy 0.8390\n",
      "Epoch [13][20]\t Batch [2900][55000]\t Training Loss 0.8405\t Accuracy 0.8401\n",
      "Epoch [13][20]\t Batch [2950][55000]\t Training Loss 0.8406\t Accuracy 0.8397\n",
      "Epoch [13][20]\t Batch [3000][55000]\t Training Loss 0.8405\t Accuracy 0.8404\n",
      "Epoch [13][20]\t Batch [3050][55000]\t Training Loss 0.8415\t Accuracy 0.8401\n",
      "Epoch [13][20]\t Batch [3100][55000]\t Training Loss 0.8438\t Accuracy 0.8397\n",
      "Epoch [13][20]\t Batch [3150][55000]\t Training Loss 0.8437\t Accuracy 0.8401\n",
      "Epoch [13][20]\t Batch [3200][55000]\t Training Loss 0.8435\t Accuracy 0.8413\n",
      "Epoch [13][20]\t Batch [3250][55000]\t Training Loss 0.8429\t Accuracy 0.8407\n",
      "Epoch [13][20]\t Batch [3300][55000]\t Training Loss 0.8442\t Accuracy 0.8407\n",
      "Epoch [13][20]\t Batch [3350][55000]\t Training Loss 0.8424\t Accuracy 0.8421\n",
      "Epoch [13][20]\t Batch [3400][55000]\t Training Loss 0.8448\t Accuracy 0.8409\n",
      "Epoch [13][20]\t Batch [3450][55000]\t Training Loss 0.8444\t Accuracy 0.8418\n",
      "Epoch [13][20]\t Batch [3500][55000]\t Training Loss 0.8444\t Accuracy 0.8415\n",
      "Epoch [13][20]\t Batch [3550][55000]\t Training Loss 0.8460\t Accuracy 0.8409\n",
      "Epoch [13][20]\t Batch [3600][55000]\t Training Loss 0.8458\t Accuracy 0.8403\n",
      "Epoch [13][20]\t Batch [3650][55000]\t Training Loss 0.8444\t Accuracy 0.8411\n",
      "Epoch [13][20]\t Batch [3700][55000]\t Training Loss 0.8452\t Accuracy 0.8406\n",
      "Epoch [13][20]\t Batch [3750][55000]\t Training Loss 0.8451\t Accuracy 0.8408\n",
      "Epoch [13][20]\t Batch [3800][55000]\t Training Loss 0.8454\t Accuracy 0.8408\n",
      "Epoch [13][20]\t Batch [3850][55000]\t Training Loss 0.8450\t Accuracy 0.8413\n",
      "Epoch [13][20]\t Batch [3900][55000]\t Training Loss 0.8439\t Accuracy 0.8426\n",
      "Epoch [13][20]\t Batch [3950][55000]\t Training Loss 0.8428\t Accuracy 0.8438\n",
      "Epoch [13][20]\t Batch [4000][55000]\t Training Loss 0.8424\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [4050][55000]\t Training Loss 0.8409\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [4100][55000]\t Training Loss 0.8413\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [4150][55000]\t Training Loss 0.8418\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [4200][55000]\t Training Loss 0.8423\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [4250][55000]\t Training Loss 0.8409\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [4300][55000]\t Training Loss 0.8409\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [4350][55000]\t Training Loss 0.8414\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [4400][55000]\t Training Loss 0.8413\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [4450][55000]\t Training Loss 0.8415\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [4500][55000]\t Training Loss 0.8417\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [4550][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [4600][55000]\t Training Loss 0.8383\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [4650][55000]\t Training Loss 0.8382\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [4700][55000]\t Training Loss 0.8378\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [4750][55000]\t Training Loss 0.8366\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [4800][55000]\t Training Loss 0.8373\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [4850][55000]\t Training Loss 0.8385\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [4900][55000]\t Training Loss 0.8371\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [4950][55000]\t Training Loss 0.8373\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [5000][55000]\t Training Loss 0.8375\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [5050][55000]\t Training Loss 0.8370\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [5100][55000]\t Training Loss 0.8370\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [5150][55000]\t Training Loss 0.8373\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [5200][55000]\t Training Loss 0.8388\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [5250][55000]\t Training Loss 0.8379\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [5300][55000]\t Training Loss 0.8378\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [5350][55000]\t Training Loss 0.8384\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [5400][55000]\t Training Loss 0.8377\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [5450][55000]\t Training Loss 0.8369\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [5500][55000]\t Training Loss 0.8352\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [5550][55000]\t Training Loss 0.8351\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [5600][55000]\t Training Loss 0.8342\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [5650][55000]\t Training Loss 0.8348\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [5700][55000]\t Training Loss 0.8345\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [5750][55000]\t Training Loss 0.8349\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [5800][55000]\t Training Loss 0.8353\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [5850][55000]\t Training Loss 0.8354\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [5900][55000]\t Training Loss 0.8360\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [5950][55000]\t Training Loss 0.8358\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [6000][55000]\t Training Loss 0.8348\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [6050][55000]\t Training Loss 0.8334\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [6100][55000]\t Training Loss 0.8320\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [6150][55000]\t Training Loss 0.8299\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [6200][55000]\t Training Loss 0.8300\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [6250][55000]\t Training Loss 0.8288\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [6300][55000]\t Training Loss 0.8292\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [6350][55000]\t Training Loss 0.8288\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [6400][55000]\t Training Loss 0.8282\t Accuracy 0.8488\n",
      "Epoch [13][20]\t Batch [6450][55000]\t Training Loss 0.8277\t Accuracy 0.8490\n",
      "Epoch [13][20]\t Batch [6500][55000]\t Training Loss 0.8285\t Accuracy 0.8486\n",
      "Epoch [13][20]\t Batch [6550][55000]\t Training Loss 0.8275\t Accuracy 0.8489\n",
      "Epoch [13][20]\t Batch [6600][55000]\t Training Loss 0.8262\t Accuracy 0.8494\n",
      "Epoch [13][20]\t Batch [6650][55000]\t Training Loss 0.8250\t Accuracy 0.8495\n",
      "Epoch [13][20]\t Batch [6700][55000]\t Training Loss 0.8251\t Accuracy 0.8494\n",
      "Epoch [13][20]\t Batch [6750][55000]\t Training Loss 0.8254\t Accuracy 0.8502\n",
      "Epoch [13][20]\t Batch [6800][55000]\t Training Loss 0.8258\t Accuracy 0.8502\n",
      "Epoch [13][20]\t Batch [6850][55000]\t Training Loss 0.8279\t Accuracy 0.8497\n",
      "Epoch [13][20]\t Batch [6900][55000]\t Training Loss 0.8279\t Accuracy 0.8497\n",
      "Epoch [13][20]\t Batch [6950][55000]\t Training Loss 0.8286\t Accuracy 0.8487\n",
      "Epoch [13][20]\t Batch [7000][55000]\t Training Loss 0.8285\t Accuracy 0.8487\n",
      "Epoch [13][20]\t Batch [7050][55000]\t Training Loss 0.8290\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [7100][55000]\t Training Loss 0.8293\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [7150][55000]\t Training Loss 0.8295\t Accuracy 0.8486\n",
      "Epoch [13][20]\t Batch [7200][55000]\t Training Loss 0.8302\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [7250][55000]\t Training Loss 0.8326\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [7300][55000]\t Training Loss 0.8344\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [7350][55000]\t Training Loss 0.8359\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [7400][55000]\t Training Loss 0.8369\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [7450][55000]\t Training Loss 0.8369\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [7500][55000]\t Training Loss 0.8369\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [7550][55000]\t Training Loss 0.8374\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [7600][55000]\t Training Loss 0.8369\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [7650][55000]\t Training Loss 0.8374\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [7700][55000]\t Training Loss 0.8379\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [7750][55000]\t Training Loss 0.8380\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [7800][55000]\t Training Loss 0.8388\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [7850][55000]\t Training Loss 0.8389\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [7900][55000]\t Training Loss 0.8391\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [7950][55000]\t Training Loss 0.8392\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [8000][55000]\t Training Loss 0.8392\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [8050][55000]\t Training Loss 0.8397\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [8100][55000]\t Training Loss 0.8383\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [8150][55000]\t Training Loss 0.8391\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [8200][55000]\t Training Loss 0.8391\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [8250][55000]\t Training Loss 0.8403\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [8300][55000]\t Training Loss 0.8406\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [8350][55000]\t Training Loss 0.8412\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [8400][55000]\t Training Loss 0.8406\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [8450][55000]\t Training Loss 0.8421\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [8500][55000]\t Training Loss 0.8413\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [8550][55000]\t Training Loss 0.8403\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [8600][55000]\t Training Loss 0.8395\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [8650][55000]\t Training Loss 0.8396\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [8700][55000]\t Training Loss 0.8402\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [8750][55000]\t Training Loss 0.8418\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [8800][55000]\t Training Loss 0.8426\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [8850][55000]\t Training Loss 0.8425\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [8900][55000]\t Training Loss 0.8443\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [8950][55000]\t Training Loss 0.8439\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [9000][55000]\t Training Loss 0.8432\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [9050][55000]\t Training Loss 0.8423\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [9100][55000]\t Training Loss 0.8421\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [9150][55000]\t Training Loss 0.8426\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [9200][55000]\t Training Loss 0.8422\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [9250][55000]\t Training Loss 0.8425\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [9300][55000]\t Training Loss 0.8427\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [9350][55000]\t Training Loss 0.8429\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [9400][55000]\t Training Loss 0.8432\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [9450][55000]\t Training Loss 0.8434\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [9500][55000]\t Training Loss 0.8425\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [9550][55000]\t Training Loss 0.8423\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [9600][55000]\t Training Loss 0.8427\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [9650][55000]\t Training Loss 0.8426\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [9700][55000]\t Training Loss 0.8420\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [9750][55000]\t Training Loss 0.8412\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [9800][55000]\t Training Loss 0.8417\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [9850][55000]\t Training Loss 0.8413\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [9900][55000]\t Training Loss 0.8408\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [9950][55000]\t Training Loss 0.8402\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [10000][55000]\t Training Loss 0.8399\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [10050][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [10100][55000]\t Training Loss 0.8399\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [10150][55000]\t Training Loss 0.8398\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [10200][55000]\t Training Loss 0.8399\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [10250][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [10300][55000]\t Training Loss 0.8400\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [10350][55000]\t Training Loss 0.8390\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [10400][55000]\t Training Loss 0.8382\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [10450][55000]\t Training Loss 0.8380\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [10500][55000]\t Training Loss 0.8370\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [10550][55000]\t Training Loss 0.8366\t Accuracy 0.8482\n",
      "Epoch [13][20]\t Batch [10600][55000]\t Training Loss 0.8361\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [10650][55000]\t Training Loss 0.8359\t Accuracy 0.8487\n",
      "Epoch [13][20]\t Batch [10700][55000]\t Training Loss 0.8356\t Accuracy 0.8491\n",
      "Epoch [13][20]\t Batch [10750][55000]\t Training Loss 0.8362\t Accuracy 0.8488\n",
      "Epoch [13][20]\t Batch [10800][55000]\t Training Loss 0.8366\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [10850][55000]\t Training Loss 0.8359\t Accuracy 0.8488\n",
      "Epoch [13][20]\t Batch [10900][55000]\t Training Loss 0.8355\t Accuracy 0.8490\n",
      "Epoch [13][20]\t Batch [10950][55000]\t Training Loss 0.8352\t Accuracy 0.8488\n",
      "Epoch [13][20]\t Batch [11000][55000]\t Training Loss 0.8350\t Accuracy 0.8490\n",
      "Epoch [13][20]\t Batch [11050][55000]\t Training Loss 0.8343\t Accuracy 0.8492\n",
      "Epoch [13][20]\t Batch [11100][55000]\t Training Loss 0.8336\t Accuracy 0.8494\n",
      "Epoch [13][20]\t Batch [11150][55000]\t Training Loss 0.8336\t Accuracy 0.8496\n",
      "Epoch [13][20]\t Batch [11200][55000]\t Training Loss 0.8333\t Accuracy 0.8497\n",
      "Epoch [13][20]\t Batch [11250][55000]\t Training Loss 0.8336\t Accuracy 0.8495\n",
      "Epoch [13][20]\t Batch [11300][55000]\t Training Loss 0.8332\t Accuracy 0.8496\n",
      "Epoch [13][20]\t Batch [11350][55000]\t Training Loss 0.8327\t Accuracy 0.8496\n",
      "Epoch [13][20]\t Batch [11400][55000]\t Training Loss 0.8329\t Accuracy 0.8496\n",
      "Epoch [13][20]\t Batch [11450][55000]\t Training Loss 0.8325\t Accuracy 0.8495\n",
      "Epoch [13][20]\t Batch [11500][55000]\t Training Loss 0.8323\t Accuracy 0.8494\n",
      "Epoch [13][20]\t Batch [11550][55000]\t Training Loss 0.8323\t Accuracy 0.8493\n",
      "Epoch [13][20]\t Batch [11600][55000]\t Training Loss 0.8335\t Accuracy 0.8486\n",
      "Epoch [13][20]\t Batch [11650][55000]\t Training Loss 0.8341\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [11700][55000]\t Training Loss 0.8340\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [11750][55000]\t Training Loss 0.8350\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [11800][55000]\t Training Loss 0.8352\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [11850][55000]\t Training Loss 0.8352\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [11900][55000]\t Training Loss 0.8355\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [11950][55000]\t Training Loss 0.8356\t Accuracy 0.8482\n",
      "Epoch [13][20]\t Batch [12000][55000]\t Training Loss 0.8356\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [12050][55000]\t Training Loss 0.8354\t Accuracy 0.8488\n",
      "Epoch [13][20]\t Batch [12100][55000]\t Training Loss 0.8354\t Accuracy 0.8487\n",
      "Epoch [13][20]\t Batch [12150][55000]\t Training Loss 0.8347\t Accuracy 0.8491\n",
      "Epoch [13][20]\t Batch [12200][55000]\t Training Loss 0.8351\t Accuracy 0.8491\n",
      "Epoch [13][20]\t Batch [12250][55000]\t Training Loss 0.8349\t Accuracy 0.8492\n",
      "Epoch [13][20]\t Batch [12300][55000]\t Training Loss 0.8349\t Accuracy 0.8491\n",
      "Epoch [13][20]\t Batch [12350][55000]\t Training Loss 0.8351\t Accuracy 0.8492\n",
      "Epoch [13][20]\t Batch [12400][55000]\t Training Loss 0.8355\t Accuracy 0.8493\n",
      "Epoch [13][20]\t Batch [12450][55000]\t Training Loss 0.8357\t Accuracy 0.8489\n",
      "Epoch [13][20]\t Batch [12500][55000]\t Training Loss 0.8359\t Accuracy 0.8486\n",
      "Epoch [13][20]\t Batch [12550][55000]\t Training Loss 0.8359\t Accuracy 0.8487\n",
      "Epoch [13][20]\t Batch [12600][55000]\t Training Loss 0.8366\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [12650][55000]\t Training Loss 0.8371\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [12700][55000]\t Training Loss 0.8378\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [12750][55000]\t Training Loss 0.8374\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [12800][55000]\t Training Loss 0.8379\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [12850][55000]\t Training Loss 0.8381\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [12900][55000]\t Training Loss 0.8381\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [12950][55000]\t Training Loss 0.8386\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [13000][55000]\t Training Loss 0.8386\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [13050][55000]\t Training Loss 0.8393\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [13100][55000]\t Training Loss 0.8399\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [13150][55000]\t Training Loss 0.8402\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [13200][55000]\t Training Loss 0.8404\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [13250][55000]\t Training Loss 0.8399\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [13300][55000]\t Training Loss 0.8398\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [13350][55000]\t Training Loss 0.8402\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [13400][55000]\t Training Loss 0.8405\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [13450][55000]\t Training Loss 0.8400\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [13500][55000]\t Training Loss 0.8396\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [13550][55000]\t Training Loss 0.8392\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [13600][55000]\t Training Loss 0.8384\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [13650][55000]\t Training Loss 0.8382\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [13700][55000]\t Training Loss 0.8390\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [13750][55000]\t Training Loss 0.8397\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [13800][55000]\t Training Loss 0.8398\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [13850][55000]\t Training Loss 0.8397\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [13900][55000]\t Training Loss 0.8400\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [13950][55000]\t Training Loss 0.8403\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [14000][55000]\t Training Loss 0.8411\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [14050][55000]\t Training Loss 0.8412\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [14100][55000]\t Training Loss 0.8413\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [14150][55000]\t Training Loss 0.8415\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [14200][55000]\t Training Loss 0.8414\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [14250][55000]\t Training Loss 0.8417\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [14300][55000]\t Training Loss 0.8420\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [14350][55000]\t Training Loss 0.8424\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [14400][55000]\t Training Loss 0.8433\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [14450][55000]\t Training Loss 0.8434\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [14500][55000]\t Training Loss 0.8435\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [14550][55000]\t Training Loss 0.8442\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [14600][55000]\t Training Loss 0.8443\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [14650][55000]\t Training Loss 0.8452\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [14700][55000]\t Training Loss 0.8460\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [14750][55000]\t Training Loss 0.8466\t Accuracy 0.8442\n",
      "Epoch [13][20]\t Batch [14800][55000]\t Training Loss 0.8475\t Accuracy 0.8440\n",
      "Epoch [13][20]\t Batch [14850][55000]\t Training Loss 0.8481\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [14900][55000]\t Training Loss 0.8480\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [14950][55000]\t Training Loss 0.8482\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [15000][55000]\t Training Loss 0.8481\t Accuracy 0.8435\n",
      "Epoch [13][20]\t Batch [15050][55000]\t Training Loss 0.8477\t Accuracy 0.8439\n",
      "Epoch [13][20]\t Batch [15100][55000]\t Training Loss 0.8475\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [15150][55000]\t Training Loss 0.8479\t Accuracy 0.8440\n",
      "Epoch [13][20]\t Batch [15200][55000]\t Training Loss 0.8481\t Accuracy 0.8440\n",
      "Epoch [13][20]\t Batch [15250][55000]\t Training Loss 0.8480\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [15300][55000]\t Training Loss 0.8480\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [15350][55000]\t Training Loss 0.8478\t Accuracy 0.8443\n",
      "Epoch [13][20]\t Batch [15400][55000]\t Training Loss 0.8481\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [15450][55000]\t Training Loss 0.8482\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [15500][55000]\t Training Loss 0.8479\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [15550][55000]\t Training Loss 0.8478\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [15600][55000]\t Training Loss 0.8477\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [15650][55000]\t Training Loss 0.8476\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [15700][55000]\t Training Loss 0.8475\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [15750][55000]\t Training Loss 0.8484\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [15800][55000]\t Training Loss 0.8488\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [15850][55000]\t Training Loss 0.8491\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [15900][55000]\t Training Loss 0.8496\t Accuracy 0.8444\n",
      "Epoch [13][20]\t Batch [15950][55000]\t Training Loss 0.8497\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [16000][55000]\t Training Loss 0.8497\t Accuracy 0.8442\n",
      "Epoch [13][20]\t Batch [16050][55000]\t Training Loss 0.8507\t Accuracy 0.8438\n",
      "Epoch [13][20]\t Batch [16100][55000]\t Training Loss 0.8507\t Accuracy 0.8438\n",
      "Epoch [13][20]\t Batch [16150][55000]\t Training Loss 0.8506\t Accuracy 0.8439\n",
      "Epoch [13][20]\t Batch [16200][55000]\t Training Loss 0.8507\t Accuracy 0.8437\n",
      "Epoch [13][20]\t Batch [16250][55000]\t Training Loss 0.8506\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [16300][55000]\t Training Loss 0.8503\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [16350][55000]\t Training Loss 0.8499\t Accuracy 0.8439\n",
      "Epoch [13][20]\t Batch [16400][55000]\t Training Loss 0.8500\t Accuracy 0.8439\n",
      "Epoch [13][20]\t Batch [16450][55000]\t Training Loss 0.8497\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [16500][55000]\t Training Loss 0.8495\t Accuracy 0.8443\n",
      "Epoch [13][20]\t Batch [16550][55000]\t Training Loss 0.8491\t Accuracy 0.8444\n",
      "Epoch [13][20]\t Batch [16600][55000]\t Training Loss 0.8492\t Accuracy 0.8444\n",
      "Epoch [13][20]\t Batch [16650][55000]\t Training Loss 0.8492\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [16700][55000]\t Training Loss 0.8493\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [16750][55000]\t Training Loss 0.8492\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [16800][55000]\t Training Loss 0.8498\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [16850][55000]\t Training Loss 0.8503\t Accuracy 0.8442\n",
      "Epoch [13][20]\t Batch [16900][55000]\t Training Loss 0.8506\t Accuracy 0.8443\n",
      "Epoch [13][20]\t Batch [16950][55000]\t Training Loss 0.8505\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [17000][55000]\t Training Loss 0.8513\t Accuracy 0.8438\n",
      "Epoch [13][20]\t Batch [17050][55000]\t Training Loss 0.8512\t Accuracy 0.8438\n",
      "Epoch [13][20]\t Batch [17100][55000]\t Training Loss 0.8517\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [17150][55000]\t Training Loss 0.8514\t Accuracy 0.8437\n",
      "Epoch [13][20]\t Batch [17200][55000]\t Training Loss 0.8515\t Accuracy 0.8437\n",
      "Epoch [13][20]\t Batch [17250][55000]\t Training Loss 0.8520\t Accuracy 0.8435\n",
      "Epoch [13][20]\t Batch [17300][55000]\t Training Loss 0.8518\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [17350][55000]\t Training Loss 0.8513\t Accuracy 0.8439\n",
      "Epoch [13][20]\t Batch [17400][55000]\t Training Loss 0.8514\t Accuracy 0.8439\n",
      "Epoch [13][20]\t Batch [17450][55000]\t Training Loss 0.8515\t Accuracy 0.8438\n",
      "Epoch [13][20]\t Batch [17500][55000]\t Training Loss 0.8516\t Accuracy 0.8439\n",
      "Epoch [13][20]\t Batch [17550][55000]\t Training Loss 0.8523\t Accuracy 0.8437\n",
      "Epoch [13][20]\t Batch [17600][55000]\t Training Loss 0.8529\t Accuracy 0.8434\n",
      "Epoch [13][20]\t Batch [17650][55000]\t Training Loss 0.8531\t Accuracy 0.8436\n",
      "Epoch [13][20]\t Batch [17700][55000]\t Training Loss 0.8538\t Accuracy 0.8434\n",
      "Epoch [13][20]\t Batch [17750][55000]\t Training Loss 0.8541\t Accuracy 0.8434\n",
      "Epoch [13][20]\t Batch [17800][55000]\t Training Loss 0.8545\t Accuracy 0.8432\n",
      "Epoch [13][20]\t Batch [17850][55000]\t Training Loss 0.8548\t Accuracy 0.8430\n",
      "Epoch [13][20]\t Batch [17900][55000]\t Training Loss 0.8552\t Accuracy 0.8429\n",
      "Epoch [13][20]\t Batch [17950][55000]\t Training Loss 0.8549\t Accuracy 0.8427\n",
      "Epoch [13][20]\t Batch [18000][55000]\t Training Loss 0.8546\t Accuracy 0.8428\n",
      "Epoch [13][20]\t Batch [18050][55000]\t Training Loss 0.8548\t Accuracy 0.8428\n",
      "Epoch [13][20]\t Batch [18100][55000]\t Training Loss 0.8547\t Accuracy 0.8430\n",
      "Epoch [13][20]\t Batch [18150][55000]\t Training Loss 0.8543\t Accuracy 0.8430\n",
      "Epoch [13][20]\t Batch [18200][55000]\t Training Loss 0.8538\t Accuracy 0.8433\n",
      "Epoch [13][20]\t Batch [18250][55000]\t Training Loss 0.8538\t Accuracy 0.8434\n",
      "Epoch [13][20]\t Batch [18300][55000]\t Training Loss 0.8534\t Accuracy 0.8435\n",
      "Epoch [13][20]\t Batch [18350][55000]\t Training Loss 0.8533\t Accuracy 0.8437\n",
      "Epoch [13][20]\t Batch [18400][55000]\t Training Loss 0.8533\t Accuracy 0.8437\n",
      "Epoch [13][20]\t Batch [18450][55000]\t Training Loss 0.8537\t Accuracy 0.8435\n",
      "Epoch [13][20]\t Batch [18500][55000]\t Training Loss 0.8537\t Accuracy 0.8435\n",
      "Epoch [13][20]\t Batch [18550][55000]\t Training Loss 0.8535\t Accuracy 0.8438\n",
      "Epoch [13][20]\t Batch [18600][55000]\t Training Loss 0.8535\t Accuracy 0.8440\n",
      "Epoch [13][20]\t Batch [18650][55000]\t Training Loss 0.8534\t Accuracy 0.8442\n",
      "Epoch [13][20]\t Batch [18700][55000]\t Training Loss 0.8536\t Accuracy 0.8440\n",
      "Epoch [13][20]\t Batch [18750][55000]\t Training Loss 0.8538\t Accuracy 0.8441\n",
      "Epoch [13][20]\t Batch [18800][55000]\t Training Loss 0.8534\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [18850][55000]\t Training Loss 0.8537\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [18900][55000]\t Training Loss 0.8531\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [18950][55000]\t Training Loss 0.8529\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [19000][55000]\t Training Loss 0.8528\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [19050][55000]\t Training Loss 0.8532\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [19100][55000]\t Training Loss 0.8535\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [19150][55000]\t Training Loss 0.8536\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [19200][55000]\t Training Loss 0.8537\t Accuracy 0.8444\n",
      "Epoch [13][20]\t Batch [19250][55000]\t Training Loss 0.8536\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [19300][55000]\t Training Loss 0.8534\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [19350][55000]\t Training Loss 0.8535\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [19400][55000]\t Training Loss 0.8533\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [19450][55000]\t Training Loss 0.8530\t Accuracy 0.8445\n",
      "Epoch [13][20]\t Batch [19500][55000]\t Training Loss 0.8526\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [19550][55000]\t Training Loss 0.8527\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [19600][55000]\t Training Loss 0.8527\t Accuracy 0.8443\n",
      "Epoch [13][20]\t Batch [19650][55000]\t Training Loss 0.8523\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [19700][55000]\t Training Loss 0.8517\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [19750][55000]\t Training Loss 0.8512\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [19800][55000]\t Training Loss 0.8505\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [19850][55000]\t Training Loss 0.8506\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [19900][55000]\t Training Loss 0.8503\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [19950][55000]\t Training Loss 0.8505\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [20000][55000]\t Training Loss 0.8504\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [20050][55000]\t Training Loss 0.8510\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [20100][55000]\t Training Loss 0.8511\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [20150][55000]\t Training Loss 0.8509\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [20200][55000]\t Training Loss 0.8513\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [20250][55000]\t Training Loss 0.8514\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [20300][55000]\t Training Loss 0.8515\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [20350][55000]\t Training Loss 0.8515\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [20400][55000]\t Training Loss 0.8511\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [20450][55000]\t Training Loss 0.8508\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [20500][55000]\t Training Loss 0.8505\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [20550][55000]\t Training Loss 0.8504\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [20600][55000]\t Training Loss 0.8504\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [20650][55000]\t Training Loss 0.8501\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [20700][55000]\t Training Loss 0.8499\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [20750][55000]\t Training Loss 0.8499\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [20800][55000]\t Training Loss 0.8499\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [20850][55000]\t Training Loss 0.8499\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [20900][55000]\t Training Loss 0.8501\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [20950][55000]\t Training Loss 0.8507\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [21000][55000]\t Training Loss 0.8509\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [21050][55000]\t Training Loss 0.8511\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [21100][55000]\t Training Loss 0.8509\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [21150][55000]\t Training Loss 0.8508\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [21200][55000]\t Training Loss 0.8506\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [21250][55000]\t Training Loss 0.8504\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [21300][55000]\t Training Loss 0.8499\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [21350][55000]\t Training Loss 0.8500\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [21400][55000]\t Training Loss 0.8501\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [21450][55000]\t Training Loss 0.8502\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [21500][55000]\t Training Loss 0.8497\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [21550][55000]\t Training Loss 0.8495\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [21600][55000]\t Training Loss 0.8497\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [21650][55000]\t Training Loss 0.8497\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [21700][55000]\t Training Loss 0.8497\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [21750][55000]\t Training Loss 0.8496\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [21800][55000]\t Training Loss 0.8491\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [21850][55000]\t Training Loss 0.8487\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [21900][55000]\t Training Loss 0.8483\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [21950][55000]\t Training Loss 0.8478\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [22000][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [22050][55000]\t Training Loss 0.8470\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [22100][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [22150][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [22200][55000]\t Training Loss 0.8478\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [22250][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [22300][55000]\t Training Loss 0.8480\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [22350][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [22400][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [22450][55000]\t Training Loss 0.8477\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [22500][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [22550][55000]\t Training Loss 0.8487\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [22600][55000]\t Training Loss 0.8492\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [22650][55000]\t Training Loss 0.8493\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [22700][55000]\t Training Loss 0.8492\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [22750][55000]\t Training Loss 0.8491\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [22800][55000]\t Training Loss 0.8490\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [22850][55000]\t Training Loss 0.8490\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [22900][55000]\t Training Loss 0.8488\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [22950][55000]\t Training Loss 0.8486\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [23000][55000]\t Training Loss 0.8483\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [23050][55000]\t Training Loss 0.8482\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [23100][55000]\t Training Loss 0.8483\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [23150][55000]\t Training Loss 0.8482\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [23200][55000]\t Training Loss 0.8482\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [23250][55000]\t Training Loss 0.8480\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [23300][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [23350][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [23400][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [23450][55000]\t Training Loss 0.8472\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [23500][55000]\t Training Loss 0.8470\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [23550][55000]\t Training Loss 0.8468\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [23600][55000]\t Training Loss 0.8469\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [23650][55000]\t Training Loss 0.8470\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [23700][55000]\t Training Loss 0.8472\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [23750][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [23800][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [23850][55000]\t Training Loss 0.8474\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [23900][55000]\t Training Loss 0.8475\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [23950][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [24000][55000]\t Training Loss 0.8476\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [24050][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [24100][55000]\t Training Loss 0.8474\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [24150][55000]\t Training Loss 0.8472\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [24200][55000]\t Training Loss 0.8471\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [24250][55000]\t Training Loss 0.8472\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [24300][55000]\t Training Loss 0.8474\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [24350][55000]\t Training Loss 0.8473\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [24400][55000]\t Training Loss 0.8473\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [24450][55000]\t Training Loss 0.8473\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [24500][55000]\t Training Loss 0.8474\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [24550][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [24600][55000]\t Training Loss 0.8475\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [24650][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [24700][55000]\t Training Loss 0.8475\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [24750][55000]\t Training Loss 0.8478\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [24800][55000]\t Training Loss 0.8481\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [24850][55000]\t Training Loss 0.8480\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [24900][55000]\t Training Loss 0.8481\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [24950][55000]\t Training Loss 0.8484\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [25000][55000]\t Training Loss 0.8486\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [25050][55000]\t Training Loss 0.8483\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [25100][55000]\t Training Loss 0.8483\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [25150][55000]\t Training Loss 0.8481\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [25200][55000]\t Training Loss 0.8480\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [25250][55000]\t Training Loss 0.8479\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [25300][55000]\t Training Loss 0.8478\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [25350][55000]\t Training Loss 0.8480\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [25400][55000]\t Training Loss 0.8475\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [25450][55000]\t Training Loss 0.8470\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [25500][55000]\t Training Loss 0.8469\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [25550][55000]\t Training Loss 0.8465\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [25600][55000]\t Training Loss 0.8464\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [25650][55000]\t Training Loss 0.8463\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [25700][55000]\t Training Loss 0.8461\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [25750][55000]\t Training Loss 0.8459\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [25800][55000]\t Training Loss 0.8458\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [25850][55000]\t Training Loss 0.8459\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [25900][55000]\t Training Loss 0.8460\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [25950][55000]\t Training Loss 0.8460\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [26000][55000]\t Training Loss 0.8460\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [26050][55000]\t Training Loss 0.8458\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [26100][55000]\t Training Loss 0.8455\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [26150][55000]\t Training Loss 0.8453\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [26200][55000]\t Training Loss 0.8451\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [26250][55000]\t Training Loss 0.8451\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [26300][55000]\t Training Loss 0.8453\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [26350][55000]\t Training Loss 0.8451\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [26400][55000]\t Training Loss 0.8455\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [26450][55000]\t Training Loss 0.8457\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [26500][55000]\t Training Loss 0.8459\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [26550][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [26600][55000]\t Training Loss 0.8463\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [26650][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [26700][55000]\t Training Loss 0.8465\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [26750][55000]\t Training Loss 0.8468\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [26800][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [26850][55000]\t Training Loss 0.8465\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [26900][55000]\t Training Loss 0.8468\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [26950][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [27000][55000]\t Training Loss 0.8465\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [27050][55000]\t Training Loss 0.8463\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [27100][55000]\t Training Loss 0.8462\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [27150][55000]\t Training Loss 0.8462\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [27200][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [27250][55000]\t Training Loss 0.8464\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [27300][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [27350][55000]\t Training Loss 0.8464\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [27400][55000]\t Training Loss 0.8463\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [27450][55000]\t Training Loss 0.8463\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [27500][55000]\t Training Loss 0.8463\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [27550][55000]\t Training Loss 0.8464\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [27600][55000]\t Training Loss 0.8462\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [27650][55000]\t Training Loss 0.8462\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [27700][55000]\t Training Loss 0.8462\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [27750][55000]\t Training Loss 0.8463\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [27800][55000]\t Training Loss 0.8464\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [27850][55000]\t Training Loss 0.8465\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [27900][55000]\t Training Loss 0.8463\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [27950][55000]\t Training Loss 0.8461\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [28000][55000]\t Training Loss 0.8459\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [28050][55000]\t Training Loss 0.8456\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [28100][55000]\t Training Loss 0.8452\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [28150][55000]\t Training Loss 0.8450\t Accuracy 0.8482\n",
      "Epoch [13][20]\t Batch [28200][55000]\t Training Loss 0.8451\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [28250][55000]\t Training Loss 0.8447\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [28300][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [13][20]\t Batch [28350][55000]\t Training Loss 0.8444\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [28400][55000]\t Training Loss 0.8448\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [28450][55000]\t Training Loss 0.8445\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [28500][55000]\t Training Loss 0.8444\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [28550][55000]\t Training Loss 0.8443\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [28600][55000]\t Training Loss 0.8442\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [28650][55000]\t Training Loss 0.8444\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [28700][55000]\t Training Loss 0.8447\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [28750][55000]\t Training Loss 0.8448\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [28800][55000]\t Training Loss 0.8447\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [28850][55000]\t Training Loss 0.8447\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [28900][55000]\t Training Loss 0.8445\t Accuracy 0.8484\n",
      "Epoch [13][20]\t Batch [28950][55000]\t Training Loss 0.8445\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [29000][55000]\t Training Loss 0.8444\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [29050][55000]\t Training Loss 0.8445\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [29100][55000]\t Training Loss 0.8446\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [29150][55000]\t Training Loss 0.8450\t Accuracy 0.8481\n",
      "Epoch [13][20]\t Batch [29200][55000]\t Training Loss 0.8453\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [29250][55000]\t Training Loss 0.8455\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [29300][55000]\t Training Loss 0.8454\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [29350][55000]\t Training Loss 0.8455\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [29400][55000]\t Training Loss 0.8454\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [29450][55000]\t Training Loss 0.8450\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [29500][55000]\t Training Loss 0.8448\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [29550][55000]\t Training Loss 0.8447\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [29600][55000]\t Training Loss 0.8447\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [29650][55000]\t Training Loss 0.8447\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [29700][55000]\t Training Loss 0.8448\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [29750][55000]\t Training Loss 0.8451\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [29800][55000]\t Training Loss 0.8453\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [29850][55000]\t Training Loss 0.8456\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [29900][55000]\t Training Loss 0.8459\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [29950][55000]\t Training Loss 0.8462\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [30000][55000]\t Training Loss 0.8466\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [30050][55000]\t Training Loss 0.8467\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [30100][55000]\t Training Loss 0.8471\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [30150][55000]\t Training Loss 0.8475\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [30200][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [30250][55000]\t Training Loss 0.8478\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [30300][55000]\t Training Loss 0.8476\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [30350][55000]\t Training Loss 0.8476\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [30400][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [30450][55000]\t Training Loss 0.8475\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [30500][55000]\t Training Loss 0.8477\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [30550][55000]\t Training Loss 0.8481\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [30600][55000]\t Training Loss 0.8481\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [30650][55000]\t Training Loss 0.8486\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [30700][55000]\t Training Loss 0.8488\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [30750][55000]\t Training Loss 0.8490\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [30800][55000]\t Training Loss 0.8492\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [30850][55000]\t Training Loss 0.8491\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [30900][55000]\t Training Loss 0.8495\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [30950][55000]\t Training Loss 0.8493\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [31000][55000]\t Training Loss 0.8493\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [31050][55000]\t Training Loss 0.8494\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [31100][55000]\t Training Loss 0.8494\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [31150][55000]\t Training Loss 0.8495\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [31200][55000]\t Training Loss 0.8495\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [31250][55000]\t Training Loss 0.8495\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [31300][55000]\t Training Loss 0.8498\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [31350][55000]\t Training Loss 0.8505\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [31400][55000]\t Training Loss 0.8506\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [31450][55000]\t Training Loss 0.8509\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [31500][55000]\t Training Loss 0.8509\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [31550][55000]\t Training Loss 0.8509\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [31600][55000]\t Training Loss 0.8511\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [31650][55000]\t Training Loss 0.8513\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [31700][55000]\t Training Loss 0.8515\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [31750][55000]\t Training Loss 0.8520\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [31800][55000]\t Training Loss 0.8521\t Accuracy 0.8466\n",
      "Epoch [13][20]\t Batch [31850][55000]\t Training Loss 0.8520\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [31900][55000]\t Training Loss 0.8520\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [31950][55000]\t Training Loss 0.8519\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [32000][55000]\t Training Loss 0.8520\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [32050][55000]\t Training Loss 0.8520\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [32100][55000]\t Training Loss 0.8519\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [32150][55000]\t Training Loss 0.8521\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [32200][55000]\t Training Loss 0.8523\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [32250][55000]\t Training Loss 0.8526\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [32300][55000]\t Training Loss 0.8530\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [32350][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [32400][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [32450][55000]\t Training Loss 0.8535\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [32500][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [32550][55000]\t Training Loss 0.8540\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [32600][55000]\t Training Loss 0.8540\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [32650][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [32700][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [32750][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [32800][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [32850][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [32900][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [32950][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [33000][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [33050][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [33100][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [33150][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [33200][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [33250][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [33300][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [33350][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [33400][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [33450][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [33500][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [33550][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [33600][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [33650][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [33700][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [33750][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [33800][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [33850][55000]\t Training Loss 0.8535\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [33900][55000]\t Training Loss 0.8531\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [33950][55000]\t Training Loss 0.8528\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [34000][55000]\t Training Loss 0.8528\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [34050][55000]\t Training Loss 0.8529\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [34100][55000]\t Training Loss 0.8530\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [34150][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [34200][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [34250][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [34300][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [34350][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [34400][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [34450][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [34500][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [34550][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [34600][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [34650][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [34700][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [34750][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [34800][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [34850][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [34900][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [34950][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [35000][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [35050][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [35100][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [35150][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [35200][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [35250][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [35300][55000]\t Training Loss 0.8534\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [35350][55000]\t Training Loss 0.8532\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [35400][55000]\t Training Loss 0.8532\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [35450][55000]\t Training Loss 0.8531\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [35500][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [35550][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [35600][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [35650][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [35700][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [35750][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [35800][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [35850][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [35900][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [35950][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [36000][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [36050][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [36100][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [36150][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [36200][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [36250][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [36300][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [36350][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [36400][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [36450][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [36500][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [36550][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [36600][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [36650][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [36700][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [36750][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [36800][55000]\t Training Loss 0.8516\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [36850][55000]\t Training Loss 0.8516\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [36900][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [36950][55000]\t Training Loss 0.8515\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [37000][55000]\t Training Loss 0.8514\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [37050][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [37100][55000]\t Training Loss 0.8513\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [37150][55000]\t Training Loss 0.8513\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [37200][55000]\t Training Loss 0.8512\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [37250][55000]\t Training Loss 0.8511\t Accuracy 0.8480\n",
      "Epoch [13][20]\t Batch [37300][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [13][20]\t Batch [37350][55000]\t Training Loss 0.8513\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [37400][55000]\t Training Loss 0.8515\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [37450][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [37500][55000]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [37550][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [37600][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [37650][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [37700][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [37750][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [37800][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [37850][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [37900][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [37950][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [38000][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [38050][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [38100][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [38150][55000]\t Training Loss 0.8523\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [38200][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [38250][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [38300][55000]\t Training Loss 0.8523\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [38350][55000]\t Training Loss 0.8525\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [38400][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [38450][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [38500][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [38550][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [38600][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [38650][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [38700][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [38750][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [38800][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [38850][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [38900][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [38950][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [39000][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [39050][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [39100][55000]\t Training Loss 0.8517\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [39150][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [39200][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [39250][55000]\t Training Loss 0.8512\t Accuracy 0.8477\n",
      "Epoch [13][20]\t Batch [39300][55000]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [13][20]\t Batch [39350][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [39400][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [13][20]\t Batch [39450][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [13][20]\t Batch [39500][55000]\t Training Loss 0.8518\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [39550][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [39600][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [39650][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [39700][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [39750][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [39800][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [13][20]\t Batch [39850][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [39900][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [39950][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40000][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40050][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [40100][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [40150][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40200][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40250][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40300][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40350][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40400][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40450][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40500][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40550][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40600][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40650][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [40700][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [40750][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40800][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [40850][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [40900][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [40950][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [41000][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [41050][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [41100][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [41150][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [41200][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [41250][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [41300][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [41350][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [41400][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [41450][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [41500][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [41550][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [41600][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [41650][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [41700][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [13][20]\t Batch [41750][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [41800][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [41850][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [41900][55000]\t Training Loss 0.8534\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [41950][55000]\t Training Loss 0.8535\t Accuracy 0.8470\n",
      "Epoch [13][20]\t Batch [42000][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [42050][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [42100][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [13][20]\t Batch [42150][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [42200][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [13][20]\t Batch [42250][55000]\t Training Loss 0.8535\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [42300][55000]\t Training Loss 0.8535\t Accuracy 0.8467\n",
      "Epoch [13][20]\t Batch [42350][55000]\t Training Loss 0.8537\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [42400][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [42450][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [42500][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [42550][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [42600][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [42650][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [42700][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [42750][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [42800][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [42850][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [42900][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [42950][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [43000][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [43050][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [43100][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [43150][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [43200][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [43250][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [43300][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [43350][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [43400][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [43450][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [43500][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [43550][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [43600][55000]\t Training Loss 0.8536\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [43650][55000]\t Training Loss 0.8534\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [43700][55000]\t Training Loss 0.8534\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [43750][55000]\t Training Loss 0.8534\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [43800][55000]\t Training Loss 0.8534\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [43850][55000]\t Training Loss 0.8534\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [43900][55000]\t Training Loss 0.8535\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [43950][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [44000][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [44050][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [44100][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [44150][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [44200][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [44250][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [44300][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [44350][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [44400][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [44450][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [44500][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [44550][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [44600][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [44650][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [44700][55000]\t Training Loss 0.8538\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [44750][55000]\t Training Loss 0.8538\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [44800][55000]\t Training Loss 0.8539\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [44850][55000]\t Training Loss 0.8539\t Accuracy 0.8465\n",
      "Epoch [13][20]\t Batch [44900][55000]\t Training Loss 0.8540\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [44950][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [45000][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [45050][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45100][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45150][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [45200][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45250][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45300][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45350][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [45400][55000]\t Training Loss 0.8541\t Accuracy 0.8464\n",
      "Epoch [13][20]\t Batch [45450][55000]\t Training Loss 0.8542\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [45500][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45550][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45600][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45650][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45700][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45750][55000]\t Training Loss 0.8543\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [45800][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45850][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [45900][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [45950][55000]\t Training Loss 0.8546\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [46000][55000]\t Training Loss 0.8547\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [46050][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [46100][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [46150][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [46200][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [46250][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [46300][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [46350][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [46400][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [46450][55000]\t Training Loss 0.8554\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [46500][55000]\t Training Loss 0.8553\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [46550][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [46600][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [46650][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [46700][55000]\t Training Loss 0.8550\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [46750][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [46800][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [46850][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [46900][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [46950][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47000][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47050][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47100][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47150][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47200][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [47250][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [47300][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47350][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47400][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47450][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47500][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [47550][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47600][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47650][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [47700][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [47750][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [47800][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [47850][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47900][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [47950][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [48000][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [48050][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [48100][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [48150][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [48200][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48250][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48300][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48350][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48400][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [48450][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48500][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48550][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48600][55000]\t Training Loss 0.8533\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [48650][55000]\t Training Loss 0.8532\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48700][55000]\t Training Loss 0.8531\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [48750][55000]\t Training Loss 0.8530\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [48800][55000]\t Training Loss 0.8529\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [48850][55000]\t Training Loss 0.8528\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [48900][55000]\t Training Loss 0.8527\t Accuracy 0.8463\n",
      "Epoch [13][20]\t Batch [48950][55000]\t Training Loss 0.8529\t Accuracy 0.8461\n",
      "Epoch [13][20]\t Batch [49000][55000]\t Training Loss 0.8530\t Accuracy 0.8460\n",
      "Epoch [13][20]\t Batch [49050][55000]\t Training Loss 0.8534\t Accuracy 0.8459\n",
      "Epoch [13][20]\t Batch [49100][55000]\t Training Loss 0.8535\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [49150][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [13][20]\t Batch [49200][55000]\t Training Loss 0.8536\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [49250][55000]\t Training Loss 0.8537\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [49300][55000]\t Training Loss 0.8536\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [49350][55000]\t Training Loss 0.8535\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [49400][55000]\t Training Loss 0.8533\t Accuracy 0.8457\n",
      "Epoch [13][20]\t Batch [49450][55000]\t Training Loss 0.8533\t Accuracy 0.8456\n",
      "Epoch [13][20]\t Batch [49500][55000]\t Training Loss 0.8535\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [49550][55000]\t Training Loss 0.8539\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [49600][55000]\t Training Loss 0.8541\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [49650][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [49700][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [49750][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [49800][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [49850][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [49900][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [49950][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [50000][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [50050][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [50100][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [50150][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [50200][55000]\t Training Loss 0.8547\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [50250][55000]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [50300][55000]\t Training Loss 0.8548\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [50350][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [50400][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [50450][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [50500][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [50550][55000]\t Training Loss 0.8554\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [50600][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [50650][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [50700][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [50750][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [50800][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [50850][55000]\t Training Loss 0.8556\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [50900][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [50950][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [51000][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [51050][55000]\t Training Loss 0.8551\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [51100][55000]\t Training Loss 0.8550\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [51150][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [51200][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [51250][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [51300][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [51350][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [51400][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [51450][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [51500][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [51550][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [51600][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [51650][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [51700][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [51750][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [51800][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [51850][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [51900][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [51950][55000]\t Training Loss 0.8544\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [52000][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [52050][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [52100][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [52150][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [52200][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [52250][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [52300][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [52350][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [52400][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [52450][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [52500][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [52550][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [13][20]\t Batch [52600][55000]\t Training Loss 0.8546\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [52650][55000]\t Training Loss 0.8544\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [52700][55000]\t Training Loss 0.8545\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [52750][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [52800][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [52850][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [52900][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [52950][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [53000][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53050][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [53100][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [53150][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53200][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53250][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [53300][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [53350][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53400][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53450][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53500][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53550][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53600][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [53650][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [53700][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [53750][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [53800][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53850][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [53900][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [53950][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54000][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54050][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54100][55000]\t Training Loss 0.8554\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [54150][55000]\t Training Loss 0.8553\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [54200][55000]\t Training Loss 0.8554\t Accuracy 0.8446\n",
      "Epoch [13][20]\t Batch [54250][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [54300][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [54350][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54400][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54450][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54500][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [54550][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54600][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54650][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [54700][55000]\t Training Loss 0.8546\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [54750][55000]\t Training Loss 0.8545\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [54800][55000]\t Training Loss 0.8544\t Accuracy 0.8449\n",
      "Epoch [13][20]\t Batch [54850][55000]\t Training Loss 0.8543\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [54900][55000]\t Training Loss 0.8544\t Accuracy 0.8450\n",
      "Epoch [13][20]\t Batch [54950][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8549\t Average training accuracy 0.8447\n",
      "Epoch [13]\t Average validation loss 0.7923\t Average validation accuracy 0.8702\n",
      "\n",
      "Epoch [14][20]\t Batch [0][55000]\t Training Loss 1.4716\t Accuracy 0.0000\n",
      "Epoch [14][20]\t Batch [50][55000]\t Training Loss 0.9386\t Accuracy 0.7255\n",
      "Epoch [14][20]\t Batch [100][55000]\t Training Loss 0.8490\t Accuracy 0.8020\n",
      "Epoch [14][20]\t Batch [150][55000]\t Training Loss 0.8504\t Accuracy 0.8146\n",
      "Epoch [14][20]\t Batch [200][55000]\t Training Loss 0.8612\t Accuracy 0.8159\n",
      "Epoch [14][20]\t Batch [250][55000]\t Training Loss 0.8434\t Accuracy 0.8247\n",
      "Epoch [14][20]\t Batch [300][55000]\t Training Loss 0.8450\t Accuracy 0.8206\n",
      "Epoch [14][20]\t Batch [350][55000]\t Training Loss 0.8253\t Accuracy 0.8234\n",
      "Epoch [14][20]\t Batch [400][55000]\t Training Loss 0.8114\t Accuracy 0.8279\n",
      "Epoch [14][20]\t Batch [450][55000]\t Training Loss 0.8172\t Accuracy 0.8293\n",
      "Epoch [14][20]\t Batch [500][55000]\t Training Loss 0.8236\t Accuracy 0.8303\n",
      "Epoch [14][20]\t Batch [550][55000]\t Training Loss 0.8409\t Accuracy 0.8276\n",
      "Epoch [14][20]\t Batch [600][55000]\t Training Loss 0.8453\t Accuracy 0.8303\n",
      "Epoch [14][20]\t Batch [650][55000]\t Training Loss 0.8651\t Accuracy 0.8203\n",
      "Epoch [14][20]\t Batch [700][55000]\t Training Loss 0.8628\t Accuracy 0.8260\n",
      "Epoch [14][20]\t Batch [750][55000]\t Training Loss 0.8590\t Accuracy 0.8269\n",
      "Epoch [14][20]\t Batch [800][55000]\t Training Loss 0.8530\t Accuracy 0.8290\n",
      "Epoch [14][20]\t Batch [850][55000]\t Training Loss 0.8516\t Accuracy 0.8308\n",
      "Epoch [14][20]\t Batch [900][55000]\t Training Loss 0.8580\t Accuracy 0.8302\n",
      "Epoch [14][20]\t Batch [950][55000]\t Training Loss 0.8648\t Accuracy 0.8254\n",
      "Epoch [14][20]\t Batch [1000][55000]\t Training Loss 0.8642\t Accuracy 0.8282\n",
      "Epoch [14][20]\t Batch [1050][55000]\t Training Loss 0.8719\t Accuracy 0.8268\n",
      "Epoch [14][20]\t Batch [1100][55000]\t Training Loss 0.8803\t Accuracy 0.8265\n",
      "Epoch [14][20]\t Batch [1150][55000]\t Training Loss 0.8895\t Accuracy 0.8236\n",
      "Epoch [14][20]\t Batch [1200][55000]\t Training Loss 0.8838\t Accuracy 0.8268\n",
      "Epoch [14][20]\t Batch [1250][55000]\t Training Loss 0.8851\t Accuracy 0.8265\n",
      "Epoch [14][20]\t Batch [1300][55000]\t Training Loss 0.8854\t Accuracy 0.8263\n",
      "Epoch [14][20]\t Batch [1350][55000]\t Training Loss 0.8822\t Accuracy 0.8275\n",
      "Epoch [14][20]\t Batch [1400][55000]\t Training Loss 0.8828\t Accuracy 0.8266\n",
      "Epoch [14][20]\t Batch [1450][55000]\t Training Loss 0.8836\t Accuracy 0.8256\n",
      "Epoch [14][20]\t Batch [1500][55000]\t Training Loss 0.8817\t Accuracy 0.8281\n",
      "Epoch [14][20]\t Batch [1550][55000]\t Training Loss 0.8812\t Accuracy 0.8285\n",
      "Epoch [14][20]\t Batch [1600][55000]\t Training Loss 0.8838\t Accuracy 0.8270\n",
      "Epoch [14][20]\t Batch [1650][55000]\t Training Loss 0.8807\t Accuracy 0.8286\n",
      "Epoch [14][20]\t Batch [1700][55000]\t Training Loss 0.8790\t Accuracy 0.8301\n",
      "Epoch [14][20]\t Batch [1750][55000]\t Training Loss 0.8733\t Accuracy 0.8315\n",
      "Epoch [14][20]\t Batch [1800][55000]\t Training Loss 0.8712\t Accuracy 0.8334\n",
      "Epoch [14][20]\t Batch [1850][55000]\t Training Loss 0.8695\t Accuracy 0.8341\n",
      "Epoch [14][20]\t Batch [1900][55000]\t Training Loss 0.8673\t Accuracy 0.8343\n",
      "Epoch [14][20]\t Batch [1950][55000]\t Training Loss 0.8659\t Accuracy 0.8355\n",
      "Epoch [14][20]\t Batch [2000][55000]\t Training Loss 0.8634\t Accuracy 0.8366\n",
      "Epoch [14][20]\t Batch [2050][55000]\t Training Loss 0.8625\t Accuracy 0.8372\n",
      "Epoch [14][20]\t Batch [2100][55000]\t Training Loss 0.8596\t Accuracy 0.8377\n",
      "Epoch [14][20]\t Batch [2150][55000]\t Training Loss 0.8563\t Accuracy 0.8396\n",
      "Epoch [14][20]\t Batch [2200][55000]\t Training Loss 0.8515\t Accuracy 0.8414\n",
      "Epoch [14][20]\t Batch [2250][55000]\t Training Loss 0.8507\t Accuracy 0.8418\n",
      "Epoch [14][20]\t Batch [2300][55000]\t Training Loss 0.8478\t Accuracy 0.8414\n",
      "Epoch [14][20]\t Batch [2350][55000]\t Training Loss 0.8463\t Accuracy 0.8418\n",
      "Epoch [14][20]\t Batch [2400][55000]\t Training Loss 0.8473\t Accuracy 0.8401\n",
      "Epoch [14][20]\t Batch [2450][55000]\t Training Loss 0.8501\t Accuracy 0.8388\n",
      "Epoch [14][20]\t Batch [2500][55000]\t Training Loss 0.8479\t Accuracy 0.8409\n",
      "Epoch [14][20]\t Batch [2550][55000]\t Training Loss 0.8467\t Accuracy 0.8408\n",
      "Epoch [14][20]\t Batch [2600][55000]\t Training Loss 0.8461\t Accuracy 0.8401\n",
      "Epoch [14][20]\t Batch [2650][55000]\t Training Loss 0.8449\t Accuracy 0.8404\n",
      "Epoch [14][20]\t Batch [2700][55000]\t Training Loss 0.8447\t Accuracy 0.8408\n",
      "Epoch [14][20]\t Batch [2750][55000]\t Training Loss 0.8446\t Accuracy 0.8408\n",
      "Epoch [14][20]\t Batch [2800][55000]\t Training Loss 0.8448\t Accuracy 0.8390\n",
      "Epoch [14][20]\t Batch [2850][55000]\t Training Loss 0.8437\t Accuracy 0.8390\n",
      "Epoch [14][20]\t Batch [2900][55000]\t Training Loss 0.8404\t Accuracy 0.8401\n",
      "Epoch [14][20]\t Batch [2950][55000]\t Training Loss 0.8405\t Accuracy 0.8397\n",
      "Epoch [14][20]\t Batch [3000][55000]\t Training Loss 0.8404\t Accuracy 0.8404\n",
      "Epoch [14][20]\t Batch [3050][55000]\t Training Loss 0.8415\t Accuracy 0.8401\n",
      "Epoch [14][20]\t Batch [3100][55000]\t Training Loss 0.8438\t Accuracy 0.8397\n",
      "Epoch [14][20]\t Batch [3150][55000]\t Training Loss 0.8436\t Accuracy 0.8401\n",
      "Epoch [14][20]\t Batch [3200][55000]\t Training Loss 0.8434\t Accuracy 0.8413\n",
      "Epoch [14][20]\t Batch [3250][55000]\t Training Loss 0.8428\t Accuracy 0.8407\n",
      "Epoch [14][20]\t Batch [3300][55000]\t Training Loss 0.8441\t Accuracy 0.8407\n",
      "Epoch [14][20]\t Batch [3350][55000]\t Training Loss 0.8424\t Accuracy 0.8421\n",
      "Epoch [14][20]\t Batch [3400][55000]\t Training Loss 0.8447\t Accuracy 0.8409\n",
      "Epoch [14][20]\t Batch [3450][55000]\t Training Loss 0.8443\t Accuracy 0.8418\n",
      "Epoch [14][20]\t Batch [3500][55000]\t Training Loss 0.8443\t Accuracy 0.8415\n",
      "Epoch [14][20]\t Batch [3550][55000]\t Training Loss 0.8459\t Accuracy 0.8409\n",
      "Epoch [14][20]\t Batch [3600][55000]\t Training Loss 0.8458\t Accuracy 0.8403\n",
      "Epoch [14][20]\t Batch [3650][55000]\t Training Loss 0.8444\t Accuracy 0.8411\n",
      "Epoch [14][20]\t Batch [3700][55000]\t Training Loss 0.8451\t Accuracy 0.8406\n",
      "Epoch [14][20]\t Batch [3750][55000]\t Training Loss 0.8451\t Accuracy 0.8408\n",
      "Epoch [14][20]\t Batch [3800][55000]\t Training Loss 0.8454\t Accuracy 0.8408\n",
      "Epoch [14][20]\t Batch [3850][55000]\t Training Loss 0.8450\t Accuracy 0.8411\n",
      "Epoch [14][20]\t Batch [3900][55000]\t Training Loss 0.8438\t Accuracy 0.8423\n",
      "Epoch [14][20]\t Batch [3950][55000]\t Training Loss 0.8427\t Accuracy 0.8436\n",
      "Epoch [14][20]\t Batch [4000][55000]\t Training Loss 0.8423\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [4050][55000]\t Training Loss 0.8408\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [4100][55000]\t Training Loss 0.8413\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [4150][55000]\t Training Loss 0.8417\t Accuracy 0.8444\n",
      "Epoch [14][20]\t Batch [4200][55000]\t Training Loss 0.8422\t Accuracy 0.8434\n",
      "Epoch [14][20]\t Batch [4250][55000]\t Training Loss 0.8408\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [4300][55000]\t Training Loss 0.8408\t Accuracy 0.8445\n",
      "Epoch [14][20]\t Batch [4350][55000]\t Training Loss 0.8413\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [4400][55000]\t Training Loss 0.8412\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [4450][55000]\t Training Loss 0.8414\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [4500][55000]\t Training Loss 0.8417\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [4550][55000]\t Training Loss 0.8400\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [4600][55000]\t Training Loss 0.8383\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [4650][55000]\t Training Loss 0.8381\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [4700][55000]\t Training Loss 0.8378\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [4750][55000]\t Training Loss 0.8366\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [4800][55000]\t Training Loss 0.8373\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [4850][55000]\t Training Loss 0.8384\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [4900][55000]\t Training Loss 0.8370\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [4950][55000]\t Training Loss 0.8373\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [5000][55000]\t Training Loss 0.8374\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [5050][55000]\t Training Loss 0.8369\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [5100][55000]\t Training Loss 0.8369\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [5150][55000]\t Training Loss 0.8372\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [5200][55000]\t Training Loss 0.8387\t Accuracy 0.8454\n",
      "Epoch [14][20]\t Batch [5250][55000]\t Training Loss 0.8378\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [5300][55000]\t Training Loss 0.8377\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [5350][55000]\t Training Loss 0.8384\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [5400][55000]\t Training Loss 0.8376\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [5450][55000]\t Training Loss 0.8369\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [5500][55000]\t Training Loss 0.8351\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [5550][55000]\t Training Loss 0.8350\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [5600][55000]\t Training Loss 0.8342\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [5650][55000]\t Training Loss 0.8347\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [5700][55000]\t Training Loss 0.8344\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [5750][55000]\t Training Loss 0.8349\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [5800][55000]\t Training Loss 0.8352\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [5850][55000]\t Training Loss 0.8354\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [5900][55000]\t Training Loss 0.8359\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [5950][55000]\t Training Loss 0.8358\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [6000][55000]\t Training Loss 0.8347\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [6050][55000]\t Training Loss 0.8334\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [6100][55000]\t Training Loss 0.8319\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [6150][55000]\t Training Loss 0.8298\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [6200][55000]\t Training Loss 0.8299\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [6250][55000]\t Training Loss 0.8287\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [6300][55000]\t Training Loss 0.8291\t Accuracy 0.8481\n",
      "Epoch [14][20]\t Batch [6350][55000]\t Training Loss 0.8288\t Accuracy 0.8484\n",
      "Epoch [14][20]\t Batch [6400][55000]\t Training Loss 0.8282\t Accuracy 0.8488\n",
      "Epoch [14][20]\t Batch [6450][55000]\t Training Loss 0.8276\t Accuracy 0.8490\n",
      "Epoch [14][20]\t Batch [6500][55000]\t Training Loss 0.8284\t Accuracy 0.8486\n",
      "Epoch [14][20]\t Batch [6550][55000]\t Training Loss 0.8274\t Accuracy 0.8489\n",
      "Epoch [14][20]\t Batch [6600][55000]\t Training Loss 0.8261\t Accuracy 0.8494\n",
      "Epoch [14][20]\t Batch [6650][55000]\t Training Loss 0.8249\t Accuracy 0.8495\n",
      "Epoch [14][20]\t Batch [6700][55000]\t Training Loss 0.8251\t Accuracy 0.8494\n",
      "Epoch [14][20]\t Batch [6750][55000]\t Training Loss 0.8253\t Accuracy 0.8502\n",
      "Epoch [14][20]\t Batch [6800][55000]\t Training Loss 0.8257\t Accuracy 0.8502\n",
      "Epoch [14][20]\t Batch [6850][55000]\t Training Loss 0.8279\t Accuracy 0.8497\n",
      "Epoch [14][20]\t Batch [6900][55000]\t Training Loss 0.8278\t Accuracy 0.8497\n",
      "Epoch [14][20]\t Batch [6950][55000]\t Training Loss 0.8285\t Accuracy 0.8487\n",
      "Epoch [14][20]\t Batch [7000][55000]\t Training Loss 0.8284\t Accuracy 0.8487\n",
      "Epoch [14][20]\t Batch [7050][55000]\t Training Loss 0.8289\t Accuracy 0.8481\n",
      "Epoch [14][20]\t Batch [7100][55000]\t Training Loss 0.8292\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [7150][55000]\t Training Loss 0.8294\t Accuracy 0.8486\n",
      "Epoch [14][20]\t Batch [7200][55000]\t Training Loss 0.8302\t Accuracy 0.8485\n",
      "Epoch [14][20]\t Batch [7250][55000]\t Training Loss 0.8325\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [7300][55000]\t Training Loss 0.8343\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [7350][55000]\t Training Loss 0.8358\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [7400][55000]\t Training Loss 0.8369\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [7450][55000]\t Training Loss 0.8368\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [7500][55000]\t Training Loss 0.8368\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [7550][55000]\t Training Loss 0.8373\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [7600][55000]\t Training Loss 0.8369\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [7650][55000]\t Training Loss 0.8373\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [7700][55000]\t Training Loss 0.8378\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [7750][55000]\t Training Loss 0.8379\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [7800][55000]\t Training Loss 0.8387\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [7850][55000]\t Training Loss 0.8389\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [7900][55000]\t Training Loss 0.8390\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [7950][55000]\t Training Loss 0.8391\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [8000][55000]\t Training Loss 0.8391\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [8050][55000]\t Training Loss 0.8396\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [8100][55000]\t Training Loss 0.8382\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [8150][55000]\t Training Loss 0.8390\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [8200][55000]\t Training Loss 0.8390\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [8250][55000]\t Training Loss 0.8402\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [8300][55000]\t Training Loss 0.8406\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [8350][55000]\t Training Loss 0.8412\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [8400][55000]\t Training Loss 0.8405\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [8450][55000]\t Training Loss 0.8420\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [8500][55000]\t Training Loss 0.8412\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [8550][55000]\t Training Loss 0.8403\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [8600][55000]\t Training Loss 0.8395\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [8650][55000]\t Training Loss 0.8396\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [8700][55000]\t Training Loss 0.8402\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [8750][55000]\t Training Loss 0.8417\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [8800][55000]\t Training Loss 0.8425\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [8850][55000]\t Training Loss 0.8425\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [8900][55000]\t Training Loss 0.8443\t Accuracy 0.8453\n",
      "Epoch [14][20]\t Batch [8950][55000]\t Training Loss 0.8438\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [9000][55000]\t Training Loss 0.8431\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [9050][55000]\t Training Loss 0.8423\t Accuracy 0.8453\n",
      "Epoch [14][20]\t Batch [9100][55000]\t Training Loss 0.8420\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [9150][55000]\t Training Loss 0.8425\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [9200][55000]\t Training Loss 0.8421\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [9250][55000]\t Training Loss 0.8424\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [9300][55000]\t Training Loss 0.8427\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [9350][55000]\t Training Loss 0.8428\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [9400][55000]\t Training Loss 0.8431\t Accuracy 0.8454\n",
      "Epoch [14][20]\t Batch [9450][55000]\t Training Loss 0.8433\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [9500][55000]\t Training Loss 0.8424\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [9550][55000]\t Training Loss 0.8422\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [9600][55000]\t Training Loss 0.8427\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [9650][55000]\t Training Loss 0.8425\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [9700][55000]\t Training Loss 0.8419\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [9750][55000]\t Training Loss 0.8411\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [9800][55000]\t Training Loss 0.8417\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [9850][55000]\t Training Loss 0.8413\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [9900][55000]\t Training Loss 0.8407\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [9950][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [10000][55000]\t Training Loss 0.8399\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [10050][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [10100][55000]\t Training Loss 0.8398\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [10150][55000]\t Training Loss 0.8397\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [10200][55000]\t Training Loss 0.8398\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [10250][55000]\t Training Loss 0.8400\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [10300][55000]\t Training Loss 0.8399\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [10350][55000]\t Training Loss 0.8389\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [10400][55000]\t Training Loss 0.8381\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [10450][55000]\t Training Loss 0.8379\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [10500][55000]\t Training Loss 0.8370\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [10550][55000]\t Training Loss 0.8365\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [10600][55000]\t Training Loss 0.8361\t Accuracy 0.8484\n",
      "Epoch [14][20]\t Batch [10650][55000]\t Training Loss 0.8358\t Accuracy 0.8487\n",
      "Epoch [14][20]\t Batch [10700][55000]\t Training Loss 0.8355\t Accuracy 0.8491\n",
      "Epoch [14][20]\t Batch [10750][55000]\t Training Loss 0.8361\t Accuracy 0.8488\n",
      "Epoch [14][20]\t Batch [10800][55000]\t Training Loss 0.8366\t Accuracy 0.8485\n",
      "Epoch [14][20]\t Batch [10850][55000]\t Training Loss 0.8359\t Accuracy 0.8488\n",
      "Epoch [14][20]\t Batch [10900][55000]\t Training Loss 0.8354\t Accuracy 0.8490\n",
      "Epoch [14][20]\t Batch [10950][55000]\t Training Loss 0.8351\t Accuracy 0.8488\n",
      "Epoch [14][20]\t Batch [11000][55000]\t Training Loss 0.8350\t Accuracy 0.8490\n",
      "Epoch [14][20]\t Batch [11050][55000]\t Training Loss 0.8342\t Accuracy 0.8492\n",
      "Epoch [14][20]\t Batch [11100][55000]\t Training Loss 0.8336\t Accuracy 0.8494\n",
      "Epoch [14][20]\t Batch [11150][55000]\t Training Loss 0.8335\t Accuracy 0.8496\n",
      "Epoch [14][20]\t Batch [11200][55000]\t Training Loss 0.8332\t Accuracy 0.8497\n",
      "Epoch [14][20]\t Batch [11250][55000]\t Training Loss 0.8336\t Accuracy 0.8495\n",
      "Epoch [14][20]\t Batch [11300][55000]\t Training Loss 0.8332\t Accuracy 0.8496\n",
      "Epoch [14][20]\t Batch [11350][55000]\t Training Loss 0.8326\t Accuracy 0.8496\n",
      "Epoch [14][20]\t Batch [11400][55000]\t Training Loss 0.8328\t Accuracy 0.8496\n",
      "Epoch [14][20]\t Batch [11450][55000]\t Training Loss 0.8324\t Accuracy 0.8495\n",
      "Epoch [14][20]\t Batch [11500][55000]\t Training Loss 0.8322\t Accuracy 0.8494\n",
      "Epoch [14][20]\t Batch [11550][55000]\t Training Loss 0.8323\t Accuracy 0.8493\n",
      "Epoch [14][20]\t Batch [11600][55000]\t Training Loss 0.8335\t Accuracy 0.8486\n",
      "Epoch [14][20]\t Batch [11650][55000]\t Training Loss 0.8340\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [11700][55000]\t Training Loss 0.8340\t Accuracy 0.8485\n",
      "Epoch [14][20]\t Batch [11750][55000]\t Training Loss 0.8349\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [11800][55000]\t Training Loss 0.8351\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [11850][55000]\t Training Loss 0.8352\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [11900][55000]\t Training Loss 0.8355\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [11950][55000]\t Training Loss 0.8356\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [12000][55000]\t Training Loss 0.8356\t Accuracy 0.8485\n",
      "Epoch [14][20]\t Batch [12050][55000]\t Training Loss 0.8354\t Accuracy 0.8488\n",
      "Epoch [14][20]\t Batch [12100][55000]\t Training Loss 0.8353\t Accuracy 0.8487\n",
      "Epoch [14][20]\t Batch [12150][55000]\t Training Loss 0.8346\t Accuracy 0.8491\n",
      "Epoch [14][20]\t Batch [12200][55000]\t Training Loss 0.8350\t Accuracy 0.8491\n",
      "Epoch [14][20]\t Batch [12250][55000]\t Training Loss 0.8349\t Accuracy 0.8492\n",
      "Epoch [14][20]\t Batch [12300][55000]\t Training Loss 0.8349\t Accuracy 0.8491\n",
      "Epoch [14][20]\t Batch [12350][55000]\t Training Loss 0.8351\t Accuracy 0.8492\n",
      "Epoch [14][20]\t Batch [12400][55000]\t Training Loss 0.8354\t Accuracy 0.8493\n",
      "Epoch [14][20]\t Batch [12450][55000]\t Training Loss 0.8356\t Accuracy 0.8489\n",
      "Epoch [14][20]\t Batch [12500][55000]\t Training Loss 0.8358\t Accuracy 0.8486\n",
      "Epoch [14][20]\t Batch [12550][55000]\t Training Loss 0.8358\t Accuracy 0.8487\n",
      "Epoch [14][20]\t Batch [12600][55000]\t Training Loss 0.8365\t Accuracy 0.8484\n",
      "Epoch [14][20]\t Batch [12650][55000]\t Training Loss 0.8370\t Accuracy 0.8481\n",
      "Epoch [14][20]\t Batch [12700][55000]\t Training Loss 0.8377\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [12750][55000]\t Training Loss 0.8373\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [12800][55000]\t Training Loss 0.8379\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [12850][55000]\t Training Loss 0.8381\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [12900][55000]\t Training Loss 0.8380\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [12950][55000]\t Training Loss 0.8385\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [13000][55000]\t Training Loss 0.8385\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [13050][55000]\t Training Loss 0.8392\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [13100][55000]\t Training Loss 0.8398\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [13150][55000]\t Training Loss 0.8402\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [13200][55000]\t Training Loss 0.8404\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [13250][55000]\t Training Loss 0.8398\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [13300][55000]\t Training Loss 0.8398\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [13350][55000]\t Training Loss 0.8402\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [13400][55000]\t Training Loss 0.8404\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [13450][55000]\t Training Loss 0.8399\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [13500][55000]\t Training Loss 0.8396\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [13550][55000]\t Training Loss 0.8392\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [13600][55000]\t Training Loss 0.8383\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [13650][55000]\t Training Loss 0.8381\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [13700][55000]\t Training Loss 0.8390\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [13750][55000]\t Training Loss 0.8396\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [13800][55000]\t Training Loss 0.8397\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [13850][55000]\t Training Loss 0.8397\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [13900][55000]\t Training Loss 0.8399\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [13950][55000]\t Training Loss 0.8402\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [14000][55000]\t Training Loss 0.8410\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [14050][55000]\t Training Loss 0.8411\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [14100][55000]\t Training Loss 0.8412\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [14150][55000]\t Training Loss 0.8415\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [14200][55000]\t Training Loss 0.8414\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [14250][55000]\t Training Loss 0.8416\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [14300][55000]\t Training Loss 0.8420\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [14350][55000]\t Training Loss 0.8423\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [14400][55000]\t Training Loss 0.8432\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [14450][55000]\t Training Loss 0.8434\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [14500][55000]\t Training Loss 0.8435\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [14550][55000]\t Training Loss 0.8442\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [14600][55000]\t Training Loss 0.8442\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [14650][55000]\t Training Loss 0.8452\t Accuracy 0.8446\n",
      "Epoch [14][20]\t Batch [14700][55000]\t Training Loss 0.8459\t Accuracy 0.8444\n",
      "Epoch [14][20]\t Batch [14750][55000]\t Training Loss 0.8465\t Accuracy 0.8441\n",
      "Epoch [14][20]\t Batch [14800][55000]\t Training Loss 0.8475\t Accuracy 0.8439\n",
      "Epoch [14][20]\t Batch [14850][55000]\t Training Loss 0.8481\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [14900][55000]\t Training Loss 0.8480\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [14950][55000]\t Training Loss 0.8481\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [15000][55000]\t Training Loss 0.8480\t Accuracy 0.8434\n",
      "Epoch [14][20]\t Batch [15050][55000]\t Training Loss 0.8477\t Accuracy 0.8437\n",
      "Epoch [14][20]\t Batch [15100][55000]\t Training Loss 0.8474\t Accuracy 0.8439\n",
      "Epoch [14][20]\t Batch [15150][55000]\t Training Loss 0.8478\t Accuracy 0.8438\n",
      "Epoch [14][20]\t Batch [15200][55000]\t Training Loss 0.8480\t Accuracy 0.8438\n",
      "Epoch [14][20]\t Batch [15250][55000]\t Training Loss 0.8480\t Accuracy 0.8439\n",
      "Epoch [14][20]\t Batch [15300][55000]\t Training Loss 0.8480\t Accuracy 0.8439\n",
      "Epoch [14][20]\t Batch [15350][55000]\t Training Loss 0.8477\t Accuracy 0.8442\n",
      "Epoch [14][20]\t Batch [15400][55000]\t Training Loss 0.8480\t Accuracy 0.8444\n",
      "Epoch [14][20]\t Batch [15450][55000]\t Training Loss 0.8481\t Accuracy 0.8445\n",
      "Epoch [14][20]\t Batch [15500][55000]\t Training Loss 0.8479\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [15550][55000]\t Training Loss 0.8478\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [15600][55000]\t Training Loss 0.8476\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [15650][55000]\t Training Loss 0.8476\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [15700][55000]\t Training Loss 0.8474\t Accuracy 0.8454\n",
      "Epoch [14][20]\t Batch [15750][55000]\t Training Loss 0.8483\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [15800][55000]\t Training Loss 0.8487\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [15850][55000]\t Training Loss 0.8491\t Accuracy 0.8446\n",
      "Epoch [14][20]\t Batch [15900][55000]\t Training Loss 0.8496\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [15950][55000]\t Training Loss 0.8497\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [16000][55000]\t Training Loss 0.8497\t Accuracy 0.8441\n",
      "Epoch [14][20]\t Batch [16050][55000]\t Training Loss 0.8507\t Accuracy 0.8437\n",
      "Epoch [14][20]\t Batch [16100][55000]\t Training Loss 0.8506\t Accuracy 0.8437\n",
      "Epoch [14][20]\t Batch [16150][55000]\t Training Loss 0.8506\t Accuracy 0.8438\n",
      "Epoch [14][20]\t Batch [16200][55000]\t Training Loss 0.8507\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [16250][55000]\t Training Loss 0.8505\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [16300][55000]\t Training Loss 0.8502\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [16350][55000]\t Training Loss 0.8499\t Accuracy 0.8438\n",
      "Epoch [14][20]\t Batch [16400][55000]\t Training Loss 0.8499\t Accuracy 0.8438\n",
      "Epoch [14][20]\t Batch [16450][55000]\t Training Loss 0.8496\t Accuracy 0.8440\n",
      "Epoch [14][20]\t Batch [16500][55000]\t Training Loss 0.8495\t Accuracy 0.8442\n",
      "Epoch [14][20]\t Batch [16550][55000]\t Training Loss 0.8490\t Accuracy 0.8442\n",
      "Epoch [14][20]\t Batch [16600][55000]\t Training Loss 0.8491\t Accuracy 0.8442\n",
      "Epoch [14][20]\t Batch [16650][55000]\t Training Loss 0.8491\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [16700][55000]\t Training Loss 0.8492\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [16750][55000]\t Training Loss 0.8491\t Accuracy 0.8445\n",
      "Epoch [14][20]\t Batch [16800][55000]\t Training Loss 0.8497\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [16850][55000]\t Training Loss 0.8503\t Accuracy 0.8440\n",
      "Epoch [14][20]\t Batch [16900][55000]\t Training Loss 0.8505\t Accuracy 0.8441\n",
      "Epoch [14][20]\t Batch [16950][55000]\t Training Loss 0.8505\t Accuracy 0.8440\n",
      "Epoch [14][20]\t Batch [17000][55000]\t Training Loss 0.8512\t Accuracy 0.8436\n",
      "Epoch [14][20]\t Batch [17050][55000]\t Training Loss 0.8511\t Accuracy 0.8436\n",
      "Epoch [14][20]\t Batch [17100][55000]\t Training Loss 0.8516\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [17150][55000]\t Training Loss 0.8514\t Accuracy 0.8436\n",
      "Epoch [14][20]\t Batch [17200][55000]\t Training Loss 0.8514\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [17250][55000]\t Training Loss 0.8519\t Accuracy 0.8434\n",
      "Epoch [14][20]\t Batch [17300][55000]\t Training Loss 0.8518\t Accuracy 0.8434\n",
      "Epoch [14][20]\t Batch [17350][55000]\t Training Loss 0.8512\t Accuracy 0.8437\n",
      "Epoch [14][20]\t Batch [17400][55000]\t Training Loss 0.8514\t Accuracy 0.8437\n",
      "Epoch [14][20]\t Batch [17450][55000]\t Training Loss 0.8515\t Accuracy 0.8437\n",
      "Epoch [14][20]\t Batch [17500][55000]\t Training Loss 0.8515\t Accuracy 0.8437\n",
      "Epoch [14][20]\t Batch [17550][55000]\t Training Loss 0.8522\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [17600][55000]\t Training Loss 0.8528\t Accuracy 0.8432\n",
      "Epoch [14][20]\t Batch [17650][55000]\t Training Loss 0.8530\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [17700][55000]\t Training Loss 0.8537\t Accuracy 0.8432\n",
      "Epoch [14][20]\t Batch [17750][55000]\t Training Loss 0.8541\t Accuracy 0.8433\n",
      "Epoch [14][20]\t Batch [17800][55000]\t Training Loss 0.8545\t Accuracy 0.8430\n",
      "Epoch [14][20]\t Batch [17850][55000]\t Training Loss 0.8547\t Accuracy 0.8428\n",
      "Epoch [14][20]\t Batch [17900][55000]\t Training Loss 0.8551\t Accuracy 0.8427\n",
      "Epoch [14][20]\t Batch [17950][55000]\t Training Loss 0.8548\t Accuracy 0.8425\n",
      "Epoch [14][20]\t Batch [18000][55000]\t Training Loss 0.8545\t Accuracy 0.8427\n",
      "Epoch [14][20]\t Batch [18050][55000]\t Training Loss 0.8548\t Accuracy 0.8427\n",
      "Epoch [14][20]\t Batch [18100][55000]\t Training Loss 0.8546\t Accuracy 0.8428\n",
      "Epoch [14][20]\t Batch [18150][55000]\t Training Loss 0.8542\t Accuracy 0.8429\n",
      "Epoch [14][20]\t Batch [18200][55000]\t Training Loss 0.8538\t Accuracy 0.8431\n",
      "Epoch [14][20]\t Batch [18250][55000]\t Training Loss 0.8538\t Accuracy 0.8432\n",
      "Epoch [14][20]\t Batch [18300][55000]\t Training Loss 0.8533\t Accuracy 0.8433\n",
      "Epoch [14][20]\t Batch [18350][55000]\t Training Loss 0.8532\t Accuracy 0.8436\n",
      "Epoch [14][20]\t Batch [18400][55000]\t Training Loss 0.8532\t Accuracy 0.8435\n",
      "Epoch [14][20]\t Batch [18450][55000]\t Training Loss 0.8536\t Accuracy 0.8434\n",
      "Epoch [14][20]\t Batch [18500][55000]\t Training Loss 0.8537\t Accuracy 0.8434\n",
      "Epoch [14][20]\t Batch [18550][55000]\t Training Loss 0.8534\t Accuracy 0.8436\n",
      "Epoch [14][20]\t Batch [18600][55000]\t Training Loss 0.8534\t Accuracy 0.8439\n",
      "Epoch [14][20]\t Batch [18650][55000]\t Training Loss 0.8533\t Accuracy 0.8440\n",
      "Epoch [14][20]\t Batch [18700][55000]\t Training Loss 0.8535\t Accuracy 0.8439\n",
      "Epoch [14][20]\t Batch [18750][55000]\t Training Loss 0.8537\t Accuracy 0.8439\n",
      "Epoch [14][20]\t Batch [18800][55000]\t Training Loss 0.8533\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [18850][55000]\t Training Loss 0.8536\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [18900][55000]\t Training Loss 0.8531\t Accuracy 0.8446\n",
      "Epoch [14][20]\t Batch [18950][55000]\t Training Loss 0.8529\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [19000][55000]\t Training Loss 0.8528\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [19050][55000]\t Training Loss 0.8532\t Accuracy 0.8446\n",
      "Epoch [14][20]\t Batch [19100][55000]\t Training Loss 0.8535\t Accuracy 0.8445\n",
      "Epoch [14][20]\t Batch [19150][55000]\t Training Loss 0.8536\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [19200][55000]\t Training Loss 0.8537\t Accuracy 0.8442\n",
      "Epoch [14][20]\t Batch [19250][55000]\t Training Loss 0.8535\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [19300][55000]\t Training Loss 0.8534\t Accuracy 0.8444\n",
      "Epoch [14][20]\t Batch [19350][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [19400][55000]\t Training Loss 0.8532\t Accuracy 0.8444\n",
      "Epoch [14][20]\t Batch [19450][55000]\t Training Loss 0.8529\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [19500][55000]\t Training Loss 0.8526\t Accuracy 0.8445\n",
      "Epoch [14][20]\t Batch [19550][55000]\t Training Loss 0.8526\t Accuracy 0.8444\n",
      "Epoch [14][20]\t Batch [19600][55000]\t Training Loss 0.8526\t Accuracy 0.8441\n",
      "Epoch [14][20]\t Batch [19650][55000]\t Training Loss 0.8522\t Accuracy 0.8444\n",
      "Epoch [14][20]\t Batch [19700][55000]\t Training Loss 0.8516\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [19750][55000]\t Training Loss 0.8511\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [19800][55000]\t Training Loss 0.8505\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [19850][55000]\t Training Loss 0.8505\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [19900][55000]\t Training Loss 0.8503\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [19950][55000]\t Training Loss 0.8505\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [20000][55000]\t Training Loss 0.8504\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [20050][55000]\t Training Loss 0.8509\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [20100][55000]\t Training Loss 0.8510\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [20150][55000]\t Training Loss 0.8508\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [20200][55000]\t Training Loss 0.8512\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [20250][55000]\t Training Loss 0.8513\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [20300][55000]\t Training Loss 0.8514\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [20350][55000]\t Training Loss 0.8514\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [20400][55000]\t Training Loss 0.8511\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [20450][55000]\t Training Loss 0.8507\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [20500][55000]\t Training Loss 0.8504\t Accuracy 0.8454\n",
      "Epoch [14][20]\t Batch [20550][55000]\t Training Loss 0.8504\t Accuracy 0.8454\n",
      "Epoch [14][20]\t Batch [20600][55000]\t Training Loss 0.8504\t Accuracy 0.8454\n",
      "Epoch [14][20]\t Batch [20650][55000]\t Training Loss 0.8500\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [20700][55000]\t Training Loss 0.8498\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [20750][55000]\t Training Loss 0.8498\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [20800][55000]\t Training Loss 0.8498\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [20850][55000]\t Training Loss 0.8498\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [20900][55000]\t Training Loss 0.8501\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [20950][55000]\t Training Loss 0.8506\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [21000][55000]\t Training Loss 0.8508\t Accuracy 0.8445\n",
      "Epoch [14][20]\t Batch [21050][55000]\t Training Loss 0.8510\t Accuracy 0.8445\n",
      "Epoch [14][20]\t Batch [21100][55000]\t Training Loss 0.8508\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [21150][55000]\t Training Loss 0.8508\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [21200][55000]\t Training Loss 0.8505\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [21250][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [21300][55000]\t Training Loss 0.8499\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [21350][55000]\t Training Loss 0.8500\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [21400][55000]\t Training Loss 0.8500\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [21450][55000]\t Training Loss 0.8501\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [21500][55000]\t Training Loss 0.8497\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [21550][55000]\t Training Loss 0.8495\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [21600][55000]\t Training Loss 0.8496\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [21650][55000]\t Training Loss 0.8496\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [21700][55000]\t Training Loss 0.8496\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [21750][55000]\t Training Loss 0.8496\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [21800][55000]\t Training Loss 0.8490\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [21850][55000]\t Training Loss 0.8486\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [21900][55000]\t Training Loss 0.8482\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [21950][55000]\t Training Loss 0.8477\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [22000][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [22050][55000]\t Training Loss 0.8470\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [22100][55000]\t Training Loss 0.8471\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [22150][55000]\t Training Loss 0.8475\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [22200][55000]\t Training Loss 0.8478\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [22250][55000]\t Training Loss 0.8478\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [22300][55000]\t Training Loss 0.8480\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [22350][55000]\t Training Loss 0.8477\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [22400][55000]\t Training Loss 0.8477\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [22450][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [22500][55000]\t Training Loss 0.8481\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [22550][55000]\t Training Loss 0.8487\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [22600][55000]\t Training Loss 0.8491\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [22650][55000]\t Training Loss 0.8493\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [22700][55000]\t Training Loss 0.8492\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [22750][55000]\t Training Loss 0.8490\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [22800][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [22850][55000]\t Training Loss 0.8490\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [22900][55000]\t Training Loss 0.8487\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [22950][55000]\t Training Loss 0.8485\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [23000][55000]\t Training Loss 0.8483\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [23050][55000]\t Training Loss 0.8481\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [23100][55000]\t Training Loss 0.8482\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [23150][55000]\t Training Loss 0.8481\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [23200][55000]\t Training Loss 0.8482\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [23250][55000]\t Training Loss 0.8479\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [23300][55000]\t Training Loss 0.8476\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [23350][55000]\t Training Loss 0.8474\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [23400][55000]\t Training Loss 0.8471\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [23450][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [23500][55000]\t Training Loss 0.8469\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [23550][55000]\t Training Loss 0.8468\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [23600][55000]\t Training Loss 0.8468\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [23650][55000]\t Training Loss 0.8470\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [23700][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [23750][55000]\t Training Loss 0.8477\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [23800][55000]\t Training Loss 0.8474\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [23850][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [23900][55000]\t Training Loss 0.8475\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [23950][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [24000][55000]\t Training Loss 0.8475\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [24050][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [24100][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [24150][55000]\t Training Loss 0.8472\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [24200][55000]\t Training Loss 0.8470\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [24250][55000]\t Training Loss 0.8471\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [24300][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [24350][55000]\t Training Loss 0.8473\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [24400][55000]\t Training Loss 0.8473\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [24450][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [24500][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [24550][55000]\t Training Loss 0.8474\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [24600][55000]\t Training Loss 0.8474\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [24650][55000]\t Training Loss 0.8475\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [24700][55000]\t Training Loss 0.8475\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [24750][55000]\t Training Loss 0.8477\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [24800][55000]\t Training Loss 0.8481\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [24850][55000]\t Training Loss 0.8479\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [24900][55000]\t Training Loss 0.8480\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [24950][55000]\t Training Loss 0.8483\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [25000][55000]\t Training Loss 0.8485\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [25050][55000]\t Training Loss 0.8483\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [25100][55000]\t Training Loss 0.8482\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [25150][55000]\t Training Loss 0.8480\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [25200][55000]\t Training Loss 0.8479\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [25250][55000]\t Training Loss 0.8478\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [25300][55000]\t Training Loss 0.8478\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [25350][55000]\t Training Loss 0.8479\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [25400][55000]\t Training Loss 0.8474\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [25450][55000]\t Training Loss 0.8470\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [25500][55000]\t Training Loss 0.8468\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [25550][55000]\t Training Loss 0.8464\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [25600][55000]\t Training Loss 0.8464\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [25650][55000]\t Training Loss 0.8462\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [25700][55000]\t Training Loss 0.8461\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [25750][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [25800][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [25850][55000]\t Training Loss 0.8459\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [25900][55000]\t Training Loss 0.8459\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [25950][55000]\t Training Loss 0.8459\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [26000][55000]\t Training Loss 0.8459\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [26050][55000]\t Training Loss 0.8457\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [26100][55000]\t Training Loss 0.8454\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [26150][55000]\t Training Loss 0.8452\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [26200][55000]\t Training Loss 0.8450\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [26250][55000]\t Training Loss 0.8450\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [26300][55000]\t Training Loss 0.8452\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [26350][55000]\t Training Loss 0.8451\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [26400][55000]\t Training Loss 0.8455\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [26450][55000]\t Training Loss 0.8457\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [26500][55000]\t Training Loss 0.8458\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [26550][55000]\t Training Loss 0.8460\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [26600][55000]\t Training Loss 0.8462\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [26650][55000]\t Training Loss 0.8466\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [26700][55000]\t Training Loss 0.8465\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [26750][55000]\t Training Loss 0.8467\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [26800][55000]\t Training Loss 0.8466\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [26850][55000]\t Training Loss 0.8465\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [26900][55000]\t Training Loss 0.8468\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [26950][55000]\t Training Loss 0.8467\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [27000][55000]\t Training Loss 0.8465\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [27050][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [27100][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [27150][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [27200][55000]\t Training Loss 0.8464\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [27250][55000]\t Training Loss 0.8464\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [27300][55000]\t Training Loss 0.8464\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [27350][55000]\t Training Loss 0.8463\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [27400][55000]\t Training Loss 0.8463\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [27450][55000]\t Training Loss 0.8462\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [27500][55000]\t Training Loss 0.8462\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [27550][55000]\t Training Loss 0.8463\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [27600][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [27650][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [27700][55000]\t Training Loss 0.8462\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [27750][55000]\t Training Loss 0.8463\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [27800][55000]\t Training Loss 0.8464\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [27850][55000]\t Training Loss 0.8464\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [27900][55000]\t Training Loss 0.8463\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [27950][55000]\t Training Loss 0.8460\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [28000][55000]\t Training Loss 0.8458\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [28050][55000]\t Training Loss 0.8455\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [28100][55000]\t Training Loss 0.8451\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [28150][55000]\t Training Loss 0.8449\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [28200][55000]\t Training Loss 0.8450\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [28250][55000]\t Training Loss 0.8446\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [28300][55000]\t Training Loss 0.8445\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [28350][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [28400][55000]\t Training Loss 0.8448\t Accuracy 0.8480\n",
      "Epoch [14][20]\t Batch [28450][55000]\t Training Loss 0.8445\t Accuracy 0.8481\n",
      "Epoch [14][20]\t Batch [28500][55000]\t Training Loss 0.8443\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [28550][55000]\t Training Loss 0.8442\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [28600][55000]\t Training Loss 0.8441\t Accuracy 0.8484\n",
      "Epoch [14][20]\t Batch [28650][55000]\t Training Loss 0.8444\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [28700][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [28750][55000]\t Training Loss 0.8447\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [28800][55000]\t Training Loss 0.8447\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [28850][55000]\t Training Loss 0.8446\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [28900][55000]\t Training Loss 0.8445\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [28950][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [29000][55000]\t Training Loss 0.8444\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [29050][55000]\t Training Loss 0.8444\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [29100][55000]\t Training Loss 0.8445\t Accuracy 0.8481\n",
      "Epoch [14][20]\t Batch [29150][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [29200][55000]\t Training Loss 0.8453\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [29250][55000]\t Training Loss 0.8455\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [29300][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [29350][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [29400][55000]\t Training Loss 0.8454\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [29450][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [29500][55000]\t Training Loss 0.8447\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [29550][55000]\t Training Loss 0.8447\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [29600][55000]\t Training Loss 0.8447\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [29650][55000]\t Training Loss 0.8446\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [29700][55000]\t Training Loss 0.8447\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [29750][55000]\t Training Loss 0.8451\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [29800][55000]\t Training Loss 0.8452\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [29850][55000]\t Training Loss 0.8455\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [29900][55000]\t Training Loss 0.8459\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [29950][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [30000][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [30050][55000]\t Training Loss 0.8466\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [30100][55000]\t Training Loss 0.8470\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [30150][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [30200][55000]\t Training Loss 0.8477\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [30250][55000]\t Training Loss 0.8478\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [30300][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [30350][55000]\t Training Loss 0.8476\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [30400][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [30450][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [30500][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [30550][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [30600][55000]\t Training Loss 0.8481\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [30650][55000]\t Training Loss 0.8485\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [30700][55000]\t Training Loss 0.8488\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [30750][55000]\t Training Loss 0.8489\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [30800][55000]\t Training Loss 0.8491\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [30850][55000]\t Training Loss 0.8490\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [30900][55000]\t Training Loss 0.8494\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [30950][55000]\t Training Loss 0.8493\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [31000][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [31050][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [31100][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [31150][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [31200][55000]\t Training Loss 0.8495\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [31250][55000]\t Training Loss 0.8494\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [31300][55000]\t Training Loss 0.8497\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [31350][55000]\t Training Loss 0.8504\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [31400][55000]\t Training Loss 0.8505\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [31450][55000]\t Training Loss 0.8508\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [31500][55000]\t Training Loss 0.8509\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [31550][55000]\t Training Loss 0.8509\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [31600][55000]\t Training Loss 0.8510\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [31650][55000]\t Training Loss 0.8512\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [31700][55000]\t Training Loss 0.8515\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [31750][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [31800][55000]\t Training Loss 0.8520\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [31850][55000]\t Training Loss 0.8520\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [31900][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [31950][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [32000][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [32050][55000]\t Training Loss 0.8519\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [32100][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [32150][55000]\t Training Loss 0.8520\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [32200][55000]\t Training Loss 0.8522\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [32250][55000]\t Training Loss 0.8525\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [32300][55000]\t Training Loss 0.8529\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [32350][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [32400][55000]\t Training Loss 0.8532\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [32450][55000]\t Training Loss 0.8535\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [32500][55000]\t Training Loss 0.8537\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [32550][55000]\t Training Loss 0.8540\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [32600][55000]\t Training Loss 0.8539\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [32650][55000]\t Training Loss 0.8538\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [32700][55000]\t Training Loss 0.8538\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [32750][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [32800][55000]\t Training Loss 0.8539\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [32850][55000]\t Training Loss 0.8539\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [32900][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [32950][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33000][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33050][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33100][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33150][55000]\t Training Loss 0.8537\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [33200][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [33250][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33300][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33350][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33400][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [33450][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [33500][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [33550][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [33600][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33650][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [33700][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [33750][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [33800][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [33850][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [33900][55000]\t Training Loss 0.8530\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [33950][55000]\t Training Loss 0.8527\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [34000][55000]\t Training Loss 0.8527\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [34050][55000]\t Training Loss 0.8529\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [34100][55000]\t Training Loss 0.8530\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [34150][55000]\t Training Loss 0.8530\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [34200][55000]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [34250][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [34300][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [34350][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [34400][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [34450][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [34500][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [34550][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [34600][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [34650][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [34700][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [34750][55000]\t Training Loss 0.8527\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [34800][55000]\t Training Loss 0.8527\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [34850][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [34900][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [34950][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [35000][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [35050][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [35100][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [35150][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [35200][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [35250][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [35300][55000]\t Training Loss 0.8533\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [35350][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [35400][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [35450][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [35500][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [35550][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [35600][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [35650][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [35700][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [35750][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [35800][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [35850][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [35900][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [35950][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [36000][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [36050][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [36100][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [36150][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [36200][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [36250][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [36300][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [36350][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [36400][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [36450][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [36500][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [36550][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [36600][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [36650][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [36700][55000]\t Training Loss 0.8519\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [36750][55000]\t Training Loss 0.8517\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [36800][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [36850][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [36900][55000]\t Training Loss 0.8516\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [36950][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [37000][55000]\t Training Loss 0.8513\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [37050][55000]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [37100][55000]\t Training Loss 0.8513\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [37150][55000]\t Training Loss 0.8513\t Accuracy 0.8478\n",
      "Epoch [14][20]\t Batch [37200][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [37250][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [37300][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [14][20]\t Batch [37350][55000]\t Training Loss 0.8512\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [37400][55000]\t Training Loss 0.8515\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [37450][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [37500][55000]\t Training Loss 0.8519\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [37550][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [37600][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [37650][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [37700][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [37750][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [37800][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [37850][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [37900][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [37950][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [38000][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [38050][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [38100][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [38150][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [38200][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [38250][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [38300][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [38350][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [38400][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [38450][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [38500][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [38550][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [38600][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [38650][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [38700][55000]\t Training Loss 0.8527\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [38750][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [38800][55000]\t Training Loss 0.8526\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [38850][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [38900][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [38950][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [39000][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [39050][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [39100][55000]\t Training Loss 0.8516\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [39150][55000]\t Training Loss 0.8515\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [39200][55000]\t Training Loss 0.8513\t Accuracy 0.8476\n",
      "Epoch [14][20]\t Batch [39250][55000]\t Training Loss 0.8512\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [39300][55000]\t Training Loss 0.8512\t Accuracy 0.8477\n",
      "Epoch [14][20]\t Batch [39350][55000]\t Training Loss 0.8515\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [39400][55000]\t Training Loss 0.8515\t Accuracy 0.8475\n",
      "Epoch [14][20]\t Batch [39450][55000]\t Training Loss 0.8518\t Accuracy 0.8474\n",
      "Epoch [14][20]\t Batch [39500][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [39550][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [39600][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [39650][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [39700][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [39750][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [39800][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [39850][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [39900][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [39950][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40000][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40050][55000]\t Training Loss 0.8526\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [40100][55000]\t Training Loss 0.8525\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [40150][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40200][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40250][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40300][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40350][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [40400][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [40450][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [40500][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40550][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40600][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40650][55000]\t Training Loss 0.8522\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [40700][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40750][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40800][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [40850][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [40900][55000]\t Training Loss 0.8520\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [40950][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [41000][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [41050][55000]\t Training Loss 0.8519\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [41100][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [41150][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [41200][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [41250][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [41300][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [14][20]\t Batch [41350][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [41400][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [41450][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [41500][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [41550][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [41600][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [41650][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [41700][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [14][20]\t Batch [41750][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [41800][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [41850][55000]\t Training Loss 0.8535\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [41900][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [41950][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [14][20]\t Batch [42000][55000]\t Training Loss 0.8534\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [42050][55000]\t Training Loss 0.8533\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [42100][55000]\t Training Loss 0.8532\t Accuracy 0.8468\n",
      "Epoch [14][20]\t Batch [42150][55000]\t Training Loss 0.8533\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [42200][55000]\t Training Loss 0.8533\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [42250][55000]\t Training Loss 0.8534\t Accuracy 0.8467\n",
      "Epoch [14][20]\t Batch [42300][55000]\t Training Loss 0.8534\t Accuracy 0.8466\n",
      "Epoch [14][20]\t Batch [42350][55000]\t Training Loss 0.8537\t Accuracy 0.8465\n",
      "Epoch [14][20]\t Batch [42400][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [42450][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [42500][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [42550][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [42600][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [42650][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [42700][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [42750][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [42800][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [42850][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [42900][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [42950][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [43000][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [43050][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [43100][55000]\t Training Loss 0.8545\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [43150][55000]\t Training Loss 0.8545\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [43200][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [43250][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [43300][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [43350][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [43400][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [43450][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [43500][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [43550][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [43600][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [43650][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [43700][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [43750][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [43800][55000]\t Training Loss 0.8533\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [43850][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [43900][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [43950][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [44000][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [44050][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [44100][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [44150][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [44200][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [44250][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [44300][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [44350][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [44400][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [44450][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [44500][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [44550][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [44600][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [44650][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [44700][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [44750][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [44800][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [44850][55000]\t Training Loss 0.8539\t Accuracy 0.8464\n",
      "Epoch [14][20]\t Batch [44900][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [44950][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [45000][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [45050][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45100][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45150][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [45200][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45250][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45300][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45350][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [45400][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [14][20]\t Batch [45450][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [45500][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45550][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45600][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45650][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45700][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45750][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [45800][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45850][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [45900][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [45950][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [46000][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [46050][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46100][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [46150][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46200][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46250][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46300][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [46350][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [46400][55000]\t Training Loss 0.8553\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [46450][55000]\t Training Loss 0.8554\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [46500][55000]\t Training Loss 0.8553\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [46550][55000]\t Training Loss 0.8552\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46600][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46650][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [46700][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46750][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46800][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46850][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46900][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [46950][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47000][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47050][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47100][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47150][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47200][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47250][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47300][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47350][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47400][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47450][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47500][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47550][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47600][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47650][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47700][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [47750][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [47800][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [47850][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47900][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [47950][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [48000][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [48050][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [48100][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [48150][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48200][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48250][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [48300][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48350][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48400][55000]\t Training Loss 0.8537\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [48450][55000]\t Training Loss 0.8534\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48500][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [48550][55000]\t Training Loss 0.8533\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48600][55000]\t Training Loss 0.8533\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48650][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48700][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [48750][55000]\t Training Loss 0.8529\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [48800][55000]\t Training Loss 0.8528\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [48850][55000]\t Training Loss 0.8527\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [48900][55000]\t Training Loss 0.8526\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [48950][55000]\t Training Loss 0.8528\t Accuracy 0.8461\n",
      "Epoch [14][20]\t Batch [49000][55000]\t Training Loss 0.8530\t Accuracy 0.8460\n",
      "Epoch [14][20]\t Batch [49050][55000]\t Training Loss 0.8533\t Accuracy 0.8459\n",
      "Epoch [14][20]\t Batch [49100][55000]\t Training Loss 0.8534\t Accuracy 0.8458\n",
      "Epoch [14][20]\t Batch [49150][55000]\t Training Loss 0.8535\t Accuracy 0.8457\n",
      "Epoch [14][20]\t Batch [49200][55000]\t Training Loss 0.8535\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [49250][55000]\t Training Loss 0.8537\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [49300][55000]\t Training Loss 0.8536\t Accuracy 0.8455\n",
      "Epoch [14][20]\t Batch [49350][55000]\t Training Loss 0.8534\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [49400][55000]\t Training Loss 0.8533\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [49450][55000]\t Training Loss 0.8532\t Accuracy 0.8456\n",
      "Epoch [14][20]\t Batch [49500][55000]\t Training Loss 0.8535\t Accuracy 0.8454\n",
      "Epoch [14][20]\t Batch [49550][55000]\t Training Loss 0.8538\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [49600][55000]\t Training Loss 0.8541\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [49650][55000]\t Training Loss 0.8541\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [49700][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [49750][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [49800][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [49850][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [49900][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [49950][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [50000][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [50050][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [50100][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [50150][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [50200][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [50250][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [50300][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [50350][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [50400][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [50450][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [50500][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [50550][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [50600][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [50650][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [50700][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [50750][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [50800][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [50850][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [50900][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [50950][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [51000][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [51050][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [51100][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [51150][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [51200][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [51250][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [51300][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [51350][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [51400][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [51450][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [51500][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [51550][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [51600][55000]\t Training Loss 0.8546\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [51650][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [51700][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [51750][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [51800][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [51850][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [51900][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [51950][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [52000][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [52050][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [52100][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [52150][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [52200][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [52250][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [52300][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [52350][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [52400][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [52450][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [52500][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [52550][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [52600][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [52650][55000]\t Training Loss 0.8543\t Accuracy 0.8453\n",
      "Epoch [14][20]\t Batch [52700][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [14][20]\t Batch [52750][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [52800][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [52850][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [52900][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [14][20]\t Batch [52950][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [53000][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53050][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [53100][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53150][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [53200][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53250][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53300][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53350][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53400][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [53450][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [53500][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53550][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53600][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [53650][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [53700][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [53750][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53800][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [53850][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53900][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [53950][55000]\t Training Loss 0.8549\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54000][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54050][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54100][55000]\t Training Loss 0.8554\t Accuracy 0.8446\n",
      "Epoch [14][20]\t Batch [54150][55000]\t Training Loss 0.8553\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54200][55000]\t Training Loss 0.8553\t Accuracy 0.8446\n",
      "Epoch [14][20]\t Batch [54250][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54300][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54350][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54400][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [54450][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [54500][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [54550][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [54600][55000]\t Training Loss 0.8548\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54650][55000]\t Training Loss 0.8547\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [54700][55000]\t Training Loss 0.8545\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [54750][55000]\t Training Loss 0.8544\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [54800][55000]\t Training Loss 0.8544\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [54850][55000]\t Training Loss 0.8542\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [54900][55000]\t Training Loss 0.8544\t Accuracy 0.8449\n",
      "Epoch [14][20]\t Batch [54950][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8548\t Average training accuracy 0.8447\n",
      "Epoch [14]\t Average validation loss 0.7922\t Average validation accuracy 0.8698\n",
      "\n",
      "Epoch [15][20]\t Batch [0][55000]\t Training Loss 1.4708\t Accuracy 0.0000\n",
      "Epoch [15][20]\t Batch [50][55000]\t Training Loss 0.9384\t Accuracy 0.7255\n",
      "Epoch [15][20]\t Batch [100][55000]\t Training Loss 0.8489\t Accuracy 0.8020\n",
      "Epoch [15][20]\t Batch [150][55000]\t Training Loss 0.8503\t Accuracy 0.8146\n",
      "Epoch [15][20]\t Batch [200][55000]\t Training Loss 0.8612\t Accuracy 0.8159\n",
      "Epoch [15][20]\t Batch [250][55000]\t Training Loss 0.8434\t Accuracy 0.8247\n",
      "Epoch [15][20]\t Batch [300][55000]\t Training Loss 0.8450\t Accuracy 0.8206\n",
      "Epoch [15][20]\t Batch [350][55000]\t Training Loss 0.8252\t Accuracy 0.8234\n",
      "Epoch [15][20]\t Batch [400][55000]\t Training Loss 0.8113\t Accuracy 0.8279\n",
      "Epoch [15][20]\t Batch [450][55000]\t Training Loss 0.8171\t Accuracy 0.8293\n",
      "Epoch [15][20]\t Batch [500][55000]\t Training Loss 0.8235\t Accuracy 0.8303\n",
      "Epoch [15][20]\t Batch [550][55000]\t Training Loss 0.8408\t Accuracy 0.8276\n",
      "Epoch [15][20]\t Batch [600][55000]\t Training Loss 0.8452\t Accuracy 0.8303\n",
      "Epoch [15][20]\t Batch [650][55000]\t Training Loss 0.8650\t Accuracy 0.8203\n",
      "Epoch [15][20]\t Batch [700][55000]\t Training Loss 0.8627\t Accuracy 0.8260\n",
      "Epoch [15][20]\t Batch [750][55000]\t Training Loss 0.8590\t Accuracy 0.8269\n",
      "Epoch [15][20]\t Batch [800][55000]\t Training Loss 0.8530\t Accuracy 0.8290\n",
      "Epoch [15][20]\t Batch [850][55000]\t Training Loss 0.8515\t Accuracy 0.8308\n",
      "Epoch [15][20]\t Batch [900][55000]\t Training Loss 0.8579\t Accuracy 0.8302\n",
      "Epoch [15][20]\t Batch [950][55000]\t Training Loss 0.8647\t Accuracy 0.8254\n",
      "Epoch [15][20]\t Batch [1000][55000]\t Training Loss 0.8641\t Accuracy 0.8272\n",
      "Epoch [15][20]\t Batch [1050][55000]\t Training Loss 0.8718\t Accuracy 0.8259\n",
      "Epoch [15][20]\t Batch [1100][55000]\t Training Loss 0.8802\t Accuracy 0.8256\n",
      "Epoch [15][20]\t Batch [1150][55000]\t Training Loss 0.8895\t Accuracy 0.8228\n",
      "Epoch [15][20]\t Batch [1200][55000]\t Training Loss 0.8837\t Accuracy 0.8260\n",
      "Epoch [15][20]\t Batch [1250][55000]\t Training Loss 0.8850\t Accuracy 0.8257\n",
      "Epoch [15][20]\t Batch [1300][55000]\t Training Loss 0.8853\t Accuracy 0.8255\n",
      "Epoch [15][20]\t Batch [1350][55000]\t Training Loss 0.8821\t Accuracy 0.8268\n",
      "Epoch [15][20]\t Batch [1400][55000]\t Training Loss 0.8827\t Accuracy 0.8258\n",
      "Epoch [15][20]\t Batch [1450][55000]\t Training Loss 0.8836\t Accuracy 0.8249\n",
      "Epoch [15][20]\t Batch [1500][55000]\t Training Loss 0.8816\t Accuracy 0.8274\n",
      "Epoch [15][20]\t Batch [1550][55000]\t Training Loss 0.8811\t Accuracy 0.8279\n",
      "Epoch [15][20]\t Batch [1600][55000]\t Training Loss 0.8837\t Accuracy 0.8264\n",
      "Epoch [15][20]\t Batch [1650][55000]\t Training Loss 0.8806\t Accuracy 0.8280\n",
      "Epoch [15][20]\t Batch [1700][55000]\t Training Loss 0.8789\t Accuracy 0.8295\n",
      "Epoch [15][20]\t Batch [1750][55000]\t Training Loss 0.8733\t Accuracy 0.8310\n",
      "Epoch [15][20]\t Batch [1800][55000]\t Training Loss 0.8711\t Accuracy 0.8329\n",
      "Epoch [15][20]\t Batch [1850][55000]\t Training Loss 0.8695\t Accuracy 0.8336\n",
      "Epoch [15][20]\t Batch [1900][55000]\t Training Loss 0.8672\t Accuracy 0.8338\n",
      "Epoch [15][20]\t Batch [1950][55000]\t Training Loss 0.8658\t Accuracy 0.8350\n",
      "Epoch [15][20]\t Batch [2000][55000]\t Training Loss 0.8633\t Accuracy 0.8361\n",
      "Epoch [15][20]\t Batch [2050][55000]\t Training Loss 0.8624\t Accuracy 0.8367\n",
      "Epoch [15][20]\t Batch [2100][55000]\t Training Loss 0.8595\t Accuracy 0.8372\n",
      "Epoch [15][20]\t Batch [2150][55000]\t Training Loss 0.8562\t Accuracy 0.8391\n",
      "Epoch [15][20]\t Batch [2200][55000]\t Training Loss 0.8514\t Accuracy 0.8410\n",
      "Epoch [15][20]\t Batch [2250][55000]\t Training Loss 0.8507\t Accuracy 0.8414\n",
      "Epoch [15][20]\t Batch [2300][55000]\t Training Loss 0.8477\t Accuracy 0.8409\n",
      "Epoch [15][20]\t Batch [2350][55000]\t Training Loss 0.8462\t Accuracy 0.8413\n",
      "Epoch [15][20]\t Batch [2400][55000]\t Training Loss 0.8472\t Accuracy 0.8397\n",
      "Epoch [15][20]\t Batch [2450][55000]\t Training Loss 0.8500\t Accuracy 0.8384\n",
      "Epoch [15][20]\t Batch [2500][55000]\t Training Loss 0.8479\t Accuracy 0.8405\n",
      "Epoch [15][20]\t Batch [2550][55000]\t Training Loss 0.8466\t Accuracy 0.8405\n",
      "Epoch [15][20]\t Batch [2600][55000]\t Training Loss 0.8460\t Accuracy 0.8393\n",
      "Epoch [15][20]\t Batch [2650][55000]\t Training Loss 0.8448\t Accuracy 0.8397\n",
      "Epoch [15][20]\t Batch [2700][55000]\t Training Loss 0.8446\t Accuracy 0.8401\n",
      "Epoch [15][20]\t Batch [2750][55000]\t Training Loss 0.8445\t Accuracy 0.8401\n",
      "Epoch [15][20]\t Batch [2800][55000]\t Training Loss 0.8448\t Accuracy 0.8383\n",
      "Epoch [15][20]\t Batch [2850][55000]\t Training Loss 0.8437\t Accuracy 0.8383\n",
      "Epoch [15][20]\t Batch [2900][55000]\t Training Loss 0.8404\t Accuracy 0.8394\n",
      "Epoch [15][20]\t Batch [2950][55000]\t Training Loss 0.8405\t Accuracy 0.8390\n",
      "Epoch [15][20]\t Batch [3000][55000]\t Training Loss 0.8403\t Accuracy 0.8397\n",
      "Epoch [15][20]\t Batch [3050][55000]\t Training Loss 0.8414\t Accuracy 0.8394\n",
      "Epoch [15][20]\t Batch [3100][55000]\t Training Loss 0.8437\t Accuracy 0.8391\n",
      "Epoch [15][20]\t Batch [3150][55000]\t Training Loss 0.8436\t Accuracy 0.8394\n",
      "Epoch [15][20]\t Batch [3200][55000]\t Training Loss 0.8434\t Accuracy 0.8407\n",
      "Epoch [15][20]\t Batch [3250][55000]\t Training Loss 0.8427\t Accuracy 0.8400\n",
      "Epoch [15][20]\t Batch [3300][55000]\t Training Loss 0.8441\t Accuracy 0.8400\n",
      "Epoch [15][20]\t Batch [3350][55000]\t Training Loss 0.8423\t Accuracy 0.8415\n",
      "Epoch [15][20]\t Batch [3400][55000]\t Training Loss 0.8446\t Accuracy 0.8403\n",
      "Epoch [15][20]\t Batch [3450][55000]\t Training Loss 0.8443\t Accuracy 0.8412\n",
      "Epoch [15][20]\t Batch [3500][55000]\t Training Loss 0.8443\t Accuracy 0.8409\n",
      "Epoch [15][20]\t Batch [3550][55000]\t Training Loss 0.8459\t Accuracy 0.8403\n",
      "Epoch [15][20]\t Batch [3600][55000]\t Training Loss 0.8457\t Accuracy 0.8398\n",
      "Epoch [15][20]\t Batch [3650][55000]\t Training Loss 0.8443\t Accuracy 0.8406\n",
      "Epoch [15][20]\t Batch [3700][55000]\t Training Loss 0.8451\t Accuracy 0.8400\n",
      "Epoch [15][20]\t Batch [3750][55000]\t Training Loss 0.8450\t Accuracy 0.8403\n",
      "Epoch [15][20]\t Batch [3800][55000]\t Training Loss 0.8453\t Accuracy 0.8403\n",
      "Epoch [15][20]\t Batch [3850][55000]\t Training Loss 0.8449\t Accuracy 0.8406\n",
      "Epoch [15][20]\t Batch [3900][55000]\t Training Loss 0.8437\t Accuracy 0.8418\n",
      "Epoch [15][20]\t Batch [3950][55000]\t Training Loss 0.8426\t Accuracy 0.8431\n",
      "Epoch [15][20]\t Batch [4000][55000]\t Training Loss 0.8423\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [4050][55000]\t Training Loss 0.8407\t Accuracy 0.8445\n",
      "Epoch [15][20]\t Batch [4100][55000]\t Training Loss 0.8412\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [4150][55000]\t Training Loss 0.8417\t Accuracy 0.8439\n",
      "Epoch [15][20]\t Batch [4200][55000]\t Training Loss 0.8422\t Accuracy 0.8429\n",
      "Epoch [15][20]\t Batch [4250][55000]\t Training Loss 0.8407\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [4300][55000]\t Training Loss 0.8408\t Accuracy 0.8440\n",
      "Epoch [15][20]\t Batch [4350][55000]\t Training Loss 0.8413\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [4400][55000]\t Training Loss 0.8412\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [4450][55000]\t Training Loss 0.8414\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [4500][55000]\t Training Loss 0.8416\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [4550][55000]\t Training Loss 0.8399\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [4600][55000]\t Training Loss 0.8382\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [4650][55000]\t Training Loss 0.8381\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [4700][55000]\t Training Loss 0.8377\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [4750][55000]\t Training Loss 0.8365\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [4800][55000]\t Training Loss 0.8372\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [4850][55000]\t Training Loss 0.8384\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [4900][55000]\t Training Loss 0.8369\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [4950][55000]\t Training Loss 0.8372\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [5000][55000]\t Training Loss 0.8373\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [5050][55000]\t Training Loss 0.8368\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [5100][55000]\t Training Loss 0.8368\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [5150][55000]\t Training Loss 0.8372\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [5200][55000]\t Training Loss 0.8387\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [5250][55000]\t Training Loss 0.8377\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [5300][55000]\t Training Loss 0.8377\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [5350][55000]\t Training Loss 0.8383\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [5400][55000]\t Training Loss 0.8375\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [5450][55000]\t Training Loss 0.8368\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [5500][55000]\t Training Loss 0.8350\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [5550][55000]\t Training Loss 0.8350\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [5600][55000]\t Training Loss 0.8341\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [5650][55000]\t Training Loss 0.8347\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [5700][55000]\t Training Loss 0.8344\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [5750][55000]\t Training Loss 0.8348\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [5800][55000]\t Training Loss 0.8352\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [5850][55000]\t Training Loss 0.8353\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [5900][55000]\t Training Loss 0.8359\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [5950][55000]\t Training Loss 0.8357\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [6000][55000]\t Training Loss 0.8347\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [6050][55000]\t Training Loss 0.8333\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [6100][55000]\t Training Loss 0.8319\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [6150][55000]\t Training Loss 0.8298\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [6200][55000]\t Training Loss 0.8298\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [6250][55000]\t Training Loss 0.8287\t Accuracy 0.8480\n",
      "Epoch [15][20]\t Batch [6300][55000]\t Training Loss 0.8290\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [6350][55000]\t Training Loss 0.8287\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [6400][55000]\t Training Loss 0.8281\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [6450][55000]\t Training Loss 0.8276\t Accuracy 0.8486\n",
      "Epoch [15][20]\t Batch [6500][55000]\t Training Loss 0.8283\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [6550][55000]\t Training Loss 0.8274\t Accuracy 0.8484\n",
      "Epoch [15][20]\t Batch [6600][55000]\t Training Loss 0.8261\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [6650][55000]\t Training Loss 0.8248\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [6700][55000]\t Training Loss 0.8250\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [6750][55000]\t Training Loss 0.8252\t Accuracy 0.8498\n",
      "Epoch [15][20]\t Batch [6800][55000]\t Training Loss 0.8257\t Accuracy 0.8497\n",
      "Epoch [15][20]\t Batch [6850][55000]\t Training Loss 0.8278\t Accuracy 0.8492\n",
      "Epoch [15][20]\t Batch [6900][55000]\t Training Loss 0.8278\t Accuracy 0.8493\n",
      "Epoch [15][20]\t Batch [6950][55000]\t Training Loss 0.8284\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [7000][55000]\t Training Loss 0.8283\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [7050][55000]\t Training Loss 0.8289\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [7100][55000]\t Training Loss 0.8291\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [7150][55000]\t Training Loss 0.8293\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [7200][55000]\t Training Loss 0.8301\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [7250][55000]\t Training Loss 0.8324\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [7300][55000]\t Training Loss 0.8343\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [7350][55000]\t Training Loss 0.8358\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [7400][55000]\t Training Loss 0.8368\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [7450][55000]\t Training Loss 0.8368\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [7500][55000]\t Training Loss 0.8368\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [7550][55000]\t Training Loss 0.8373\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [7600][55000]\t Training Loss 0.8368\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [7650][55000]\t Training Loss 0.8373\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [7700][55000]\t Training Loss 0.8378\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [7750][55000]\t Training Loss 0.8379\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [7800][55000]\t Training Loss 0.8387\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [7850][55000]\t Training Loss 0.8388\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [7900][55000]\t Training Loss 0.8390\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [7950][55000]\t Training Loss 0.8390\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [8000][55000]\t Training Loss 0.8391\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [8050][55000]\t Training Loss 0.8396\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [8100][55000]\t Training Loss 0.8382\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [8150][55000]\t Training Loss 0.8390\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [8200][55000]\t Training Loss 0.8390\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [8250][55000]\t Training Loss 0.8402\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [8300][55000]\t Training Loss 0.8405\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [8350][55000]\t Training Loss 0.8411\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [8400][55000]\t Training Loss 0.8405\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [8450][55000]\t Training Loss 0.8420\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [8500][55000]\t Training Loss 0.8412\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [8550][55000]\t Training Loss 0.8402\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [8600][55000]\t Training Loss 0.8394\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [8650][55000]\t Training Loss 0.8395\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [8700][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [8750][55000]\t Training Loss 0.8417\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [8800][55000]\t Training Loss 0.8425\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [8850][55000]\t Training Loss 0.8424\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [8900][55000]\t Training Loss 0.8442\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [8950][55000]\t Training Loss 0.8438\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [9000][55000]\t Training Loss 0.8430\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [9050][55000]\t Training Loss 0.8422\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [9100][55000]\t Training Loss 0.8420\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [9150][55000]\t Training Loss 0.8425\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [9200][55000]\t Training Loss 0.8420\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [9250][55000]\t Training Loss 0.8424\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [9300][55000]\t Training Loss 0.8426\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [9350][55000]\t Training Loss 0.8428\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [9400][55000]\t Training Loss 0.8430\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [9450][55000]\t Training Loss 0.8433\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [9500][55000]\t Training Loss 0.8424\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [9550][55000]\t Training Loss 0.8422\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [9600][55000]\t Training Loss 0.8426\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [9650][55000]\t Training Loss 0.8424\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [9700][55000]\t Training Loss 0.8419\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [9750][55000]\t Training Loss 0.8411\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [9800][55000]\t Training Loss 0.8416\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [9850][55000]\t Training Loss 0.8412\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [9900][55000]\t Training Loss 0.8407\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [9950][55000]\t Training Loss 0.8401\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [10000][55000]\t Training Loss 0.8398\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [10050][55000]\t Training Loss 0.8400\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [10100][55000]\t Training Loss 0.8398\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [10150][55000]\t Training Loss 0.8396\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [10200][55000]\t Training Loss 0.8398\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [10250][55000]\t Training Loss 0.8399\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [10300][55000]\t Training Loss 0.8399\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [10350][55000]\t Training Loss 0.8389\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [10400][55000]\t Training Loss 0.8381\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [10450][55000]\t Training Loss 0.8379\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [10500][55000]\t Training Loss 0.8369\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [10550][55000]\t Training Loss 0.8365\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [10600][55000]\t Training Loss 0.8360\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [10650][55000]\t Training Loss 0.8358\t Accuracy 0.8485\n",
      "Epoch [15][20]\t Batch [10700][55000]\t Training Loss 0.8355\t Accuracy 0.8488\n",
      "Epoch [15][20]\t Batch [10750][55000]\t Training Loss 0.8360\t Accuracy 0.8485\n",
      "Epoch [15][20]\t Batch [10800][55000]\t Training Loss 0.8365\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [10850][55000]\t Training Loss 0.8358\t Accuracy 0.8485\n",
      "Epoch [15][20]\t Batch [10900][55000]\t Training Loss 0.8353\t Accuracy 0.8487\n",
      "Epoch [15][20]\t Batch [10950][55000]\t Training Loss 0.8350\t Accuracy 0.8485\n",
      "Epoch [15][20]\t Batch [11000][55000]\t Training Loss 0.8349\t Accuracy 0.8487\n",
      "Epoch [15][20]\t Batch [11050][55000]\t Training Loss 0.8342\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [11100][55000]\t Training Loss 0.8335\t Accuracy 0.8491\n",
      "Epoch [15][20]\t Batch [11150][55000]\t Training Loss 0.8335\t Accuracy 0.8493\n",
      "Epoch [15][20]\t Batch [11200][55000]\t Training Loss 0.8331\t Accuracy 0.8494\n",
      "Epoch [15][20]\t Batch [11250][55000]\t Training Loss 0.8335\t Accuracy 0.8493\n",
      "Epoch [15][20]\t Batch [11300][55000]\t Training Loss 0.8331\t Accuracy 0.8493\n",
      "Epoch [15][20]\t Batch [11350][55000]\t Training Loss 0.8326\t Accuracy 0.8494\n",
      "Epoch [15][20]\t Batch [11400][55000]\t Training Loss 0.8327\t Accuracy 0.8493\n",
      "Epoch [15][20]\t Batch [11450][55000]\t Training Loss 0.8324\t Accuracy 0.8493\n",
      "Epoch [15][20]\t Batch [11500][55000]\t Training Loss 0.8322\t Accuracy 0.8491\n",
      "Epoch [15][20]\t Batch [11550][55000]\t Training Loss 0.8322\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [11600][55000]\t Training Loss 0.8334\t Accuracy 0.8484\n",
      "Epoch [15][20]\t Batch [11650][55000]\t Training Loss 0.8340\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [11700][55000]\t Training Loss 0.8339\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [11750][55000]\t Training Loss 0.8349\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [11800][55000]\t Training Loss 0.8351\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [11850][55000]\t Training Loss 0.8351\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [11900][55000]\t Training Loss 0.8354\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [11950][55000]\t Training Loss 0.8355\t Accuracy 0.8480\n",
      "Epoch [15][20]\t Batch [12000][55000]\t Training Loss 0.8355\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [12050][55000]\t Training Loss 0.8353\t Accuracy 0.8486\n",
      "Epoch [15][20]\t Batch [12100][55000]\t Training Loss 0.8353\t Accuracy 0.8484\n",
      "Epoch [15][20]\t Batch [12150][55000]\t Training Loss 0.8346\t Accuracy 0.8489\n",
      "Epoch [15][20]\t Batch [12200][55000]\t Training Loss 0.8349\t Accuracy 0.8489\n",
      "Epoch [15][20]\t Batch [12250][55000]\t Training Loss 0.8348\t Accuracy 0.8489\n",
      "Epoch [15][20]\t Batch [12300][55000]\t Training Loss 0.8348\t Accuracy 0.8489\n",
      "Epoch [15][20]\t Batch [12350][55000]\t Training Loss 0.8350\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [12400][55000]\t Training Loss 0.8354\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [12450][55000]\t Training Loss 0.8355\t Accuracy 0.8487\n",
      "Epoch [15][20]\t Batch [12500][55000]\t Training Loss 0.8358\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [12550][55000]\t Training Loss 0.8358\t Accuracy 0.8485\n",
      "Epoch [15][20]\t Batch [12600][55000]\t Training Loss 0.8365\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [12650][55000]\t Training Loss 0.8370\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [12700][55000]\t Training Loss 0.8377\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [12750][55000]\t Training Loss 0.8373\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [12800][55000]\t Training Loss 0.8378\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [12850][55000]\t Training Loss 0.8380\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [12900][55000]\t Training Loss 0.8380\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [12950][55000]\t Training Loss 0.8385\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [13000][55000]\t Training Loss 0.8385\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [13050][55000]\t Training Loss 0.8391\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [13100][55000]\t Training Loss 0.8398\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [13150][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [13200][55000]\t Training Loss 0.8403\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [13250][55000]\t Training Loss 0.8398\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [13300][55000]\t Training Loss 0.8397\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [13350][55000]\t Training Loss 0.8401\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [13400][55000]\t Training Loss 0.8404\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [13450][55000]\t Training Loss 0.8399\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [13500][55000]\t Training Loss 0.8395\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [13550][55000]\t Training Loss 0.8391\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [13600][55000]\t Training Loss 0.8383\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [13650][55000]\t Training Loss 0.8381\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [13700][55000]\t Training Loss 0.8389\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [13750][55000]\t Training Loss 0.8396\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [13800][55000]\t Training Loss 0.8397\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [13850][55000]\t Training Loss 0.8396\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [13900][55000]\t Training Loss 0.8398\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [13950][55000]\t Training Loss 0.8402\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [14000][55000]\t Training Loss 0.8409\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [14050][55000]\t Training Loss 0.8411\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [14100][55000]\t Training Loss 0.8412\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [14150][55000]\t Training Loss 0.8414\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [14200][55000]\t Training Loss 0.8413\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [14250][55000]\t Training Loss 0.8416\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [14300][55000]\t Training Loss 0.8419\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [14350][55000]\t Training Loss 0.8423\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [14400][55000]\t Training Loss 0.8431\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [14450][55000]\t Training Loss 0.8433\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [14500][55000]\t Training Loss 0.8434\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [14550][55000]\t Training Loss 0.8441\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [14600][55000]\t Training Loss 0.8441\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [14650][55000]\t Training Loss 0.8451\t Accuracy 0.8444\n",
      "Epoch [15][20]\t Batch [14700][55000]\t Training Loss 0.8459\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [14750][55000]\t Training Loss 0.8465\t Accuracy 0.8439\n",
      "Epoch [15][20]\t Batch [14800][55000]\t Training Loss 0.8474\t Accuracy 0.8437\n",
      "Epoch [15][20]\t Batch [14850][55000]\t Training Loss 0.8480\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [14900][55000]\t Training Loss 0.8479\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [14950][55000]\t Training Loss 0.8480\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [15000][55000]\t Training Loss 0.8480\t Accuracy 0.8433\n",
      "Epoch [15][20]\t Batch [15050][55000]\t Training Loss 0.8476\t Accuracy 0.8436\n",
      "Epoch [15][20]\t Batch [15100][55000]\t Training Loss 0.8474\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [15150][55000]\t Training Loss 0.8478\t Accuracy 0.8437\n",
      "Epoch [15][20]\t Batch [15200][55000]\t Training Loss 0.8480\t Accuracy 0.8437\n",
      "Epoch [15][20]\t Batch [15250][55000]\t Training Loss 0.8479\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [15300][55000]\t Training Loss 0.8479\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [15350][55000]\t Training Loss 0.8477\t Accuracy 0.8440\n",
      "Epoch [15][20]\t Batch [15400][55000]\t Training Loss 0.8480\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [15450][55000]\t Training Loss 0.8481\t Accuracy 0.8443\n",
      "Epoch [15][20]\t Batch [15500][55000]\t Training Loss 0.8478\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [15550][55000]\t Training Loss 0.8477\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [15600][55000]\t Training Loss 0.8476\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [15650][55000]\t Training Loss 0.8475\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [15700][55000]\t Training Loss 0.8474\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [15750][55000]\t Training Loss 0.8482\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [15800][55000]\t Training Loss 0.8487\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [15850][55000]\t Training Loss 0.8490\t Accuracy 0.8445\n",
      "Epoch [15][20]\t Batch [15900][55000]\t Training Loss 0.8495\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [15950][55000]\t Training Loss 0.8496\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [16000][55000]\t Training Loss 0.8496\t Accuracy 0.8439\n",
      "Epoch [15][20]\t Batch [16050][55000]\t Training Loss 0.8506\t Accuracy 0.8436\n",
      "Epoch [15][20]\t Batch [16100][55000]\t Training Loss 0.8506\t Accuracy 0.8436\n",
      "Epoch [15][20]\t Batch [16150][55000]\t Training Loss 0.8505\t Accuracy 0.8437\n",
      "Epoch [15][20]\t Batch [16200][55000]\t Training Loss 0.8506\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [16250][55000]\t Training Loss 0.8505\t Accuracy 0.8433\n",
      "Epoch [15][20]\t Batch [16300][55000]\t Training Loss 0.8502\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [16350][55000]\t Training Loss 0.8498\t Accuracy 0.8437\n",
      "Epoch [15][20]\t Batch [16400][55000]\t Training Loss 0.8499\t Accuracy 0.8437\n",
      "Epoch [15][20]\t Batch [16450][55000]\t Training Loss 0.8496\t Accuracy 0.8439\n",
      "Epoch [15][20]\t Batch [16500][55000]\t Training Loss 0.8494\t Accuracy 0.8441\n",
      "Epoch [15][20]\t Batch [16550][55000]\t Training Loss 0.8490\t Accuracy 0.8441\n",
      "Epoch [15][20]\t Batch [16600][55000]\t Training Loss 0.8491\t Accuracy 0.8441\n",
      "Epoch [15][20]\t Batch [16650][55000]\t Training Loss 0.8491\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [16700][55000]\t Training Loss 0.8492\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [16750][55000]\t Training Loss 0.8490\t Accuracy 0.8444\n",
      "Epoch [15][20]\t Batch [16800][55000]\t Training Loss 0.8497\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [16850][55000]\t Training Loss 0.8502\t Accuracy 0.8439\n",
      "Epoch [15][20]\t Batch [16900][55000]\t Training Loss 0.8504\t Accuracy 0.8440\n",
      "Epoch [15][20]\t Batch [16950][55000]\t Training Loss 0.8504\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [17000][55000]\t Training Loss 0.8512\t Accuracy 0.8435\n",
      "Epoch [15][20]\t Batch [17050][55000]\t Training Loss 0.8511\t Accuracy 0.8435\n",
      "Epoch [15][20]\t Batch [17100][55000]\t Training Loss 0.8516\t Accuracy 0.8433\n",
      "Epoch [15][20]\t Batch [17150][55000]\t Training Loss 0.8513\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [17200][55000]\t Training Loss 0.8514\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [17250][55000]\t Training Loss 0.8519\t Accuracy 0.8433\n",
      "Epoch [15][20]\t Batch [17300][55000]\t Training Loss 0.8517\t Accuracy 0.8433\n",
      "Epoch [15][20]\t Batch [17350][55000]\t Training Loss 0.8512\t Accuracy 0.8436\n",
      "Epoch [15][20]\t Batch [17400][55000]\t Training Loss 0.8513\t Accuracy 0.8436\n",
      "Epoch [15][20]\t Batch [17450][55000]\t Training Loss 0.8514\t Accuracy 0.8436\n",
      "Epoch [15][20]\t Batch [17500][55000]\t Training Loss 0.8515\t Accuracy 0.8436\n",
      "Epoch [15][20]\t Batch [17550][55000]\t Training Loss 0.8521\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [17600][55000]\t Training Loss 0.8528\t Accuracy 0.8431\n",
      "Epoch [15][20]\t Batch [17650][55000]\t Training Loss 0.8530\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [17700][55000]\t Training Loss 0.8537\t Accuracy 0.8431\n",
      "Epoch [15][20]\t Batch [17750][55000]\t Training Loss 0.8540\t Accuracy 0.8432\n",
      "Epoch [15][20]\t Batch [17800][55000]\t Training Loss 0.8544\t Accuracy 0.8429\n",
      "Epoch [15][20]\t Batch [17850][55000]\t Training Loss 0.8547\t Accuracy 0.8427\n",
      "Epoch [15][20]\t Batch [17900][55000]\t Training Loss 0.8550\t Accuracy 0.8426\n",
      "Epoch [15][20]\t Batch [17950][55000]\t Training Loss 0.8548\t Accuracy 0.8424\n",
      "Epoch [15][20]\t Batch [18000][55000]\t Training Loss 0.8545\t Accuracy 0.8426\n",
      "Epoch [15][20]\t Batch [18050][55000]\t Training Loss 0.8547\t Accuracy 0.8426\n",
      "Epoch [15][20]\t Batch [18100][55000]\t Training Loss 0.8546\t Accuracy 0.8427\n",
      "Epoch [15][20]\t Batch [18150][55000]\t Training Loss 0.8542\t Accuracy 0.8428\n",
      "Epoch [15][20]\t Batch [18200][55000]\t Training Loss 0.8537\t Accuracy 0.8430\n",
      "Epoch [15][20]\t Batch [18250][55000]\t Training Loss 0.8537\t Accuracy 0.8431\n",
      "Epoch [15][20]\t Batch [18300][55000]\t Training Loss 0.8533\t Accuracy 0.8432\n",
      "Epoch [15][20]\t Batch [18350][55000]\t Training Loss 0.8531\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [18400][55000]\t Training Loss 0.8532\t Accuracy 0.8434\n",
      "Epoch [15][20]\t Batch [18450][55000]\t Training Loss 0.8536\t Accuracy 0.8433\n",
      "Epoch [15][20]\t Batch [18500][55000]\t Training Loss 0.8536\t Accuracy 0.8433\n",
      "Epoch [15][20]\t Batch [18550][55000]\t Training Loss 0.8534\t Accuracy 0.8435\n",
      "Epoch [15][20]\t Batch [18600][55000]\t Training Loss 0.8534\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [18650][55000]\t Training Loss 0.8532\t Accuracy 0.8439\n",
      "Epoch [15][20]\t Batch [18700][55000]\t Training Loss 0.8535\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [18750][55000]\t Training Loss 0.8537\t Accuracy 0.8438\n",
      "Epoch [15][20]\t Batch [18800][55000]\t Training Loss 0.8533\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [18850][55000]\t Training Loss 0.8536\t Accuracy 0.8441\n",
      "Epoch [15][20]\t Batch [18900][55000]\t Training Loss 0.8530\t Accuracy 0.8445\n",
      "Epoch [15][20]\t Batch [18950][55000]\t Training Loss 0.8528\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [19000][55000]\t Training Loss 0.8527\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [19050][55000]\t Training Loss 0.8531\t Accuracy 0.8445\n",
      "Epoch [15][20]\t Batch [19100][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [15][20]\t Batch [19150][55000]\t Training Loss 0.8535\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [19200][55000]\t Training Loss 0.8536\t Accuracy 0.8441\n",
      "Epoch [15][20]\t Batch [19250][55000]\t Training Loss 0.8535\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [19300][55000]\t Training Loss 0.8533\t Accuracy 0.8443\n",
      "Epoch [15][20]\t Batch [19350][55000]\t Training Loss 0.8534\t Accuracy 0.8441\n",
      "Epoch [15][20]\t Batch [19400][55000]\t Training Loss 0.8532\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [19450][55000]\t Training Loss 0.8529\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [19500][55000]\t Training Loss 0.8525\t Accuracy 0.8443\n",
      "Epoch [15][20]\t Batch [19550][55000]\t Training Loss 0.8526\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [19600][55000]\t Training Loss 0.8525\t Accuracy 0.8440\n",
      "Epoch [15][20]\t Batch [19650][55000]\t Training Loss 0.8521\t Accuracy 0.8442\n",
      "Epoch [15][20]\t Batch [19700][55000]\t Training Loss 0.8516\t Accuracy 0.8445\n",
      "Epoch [15][20]\t Batch [19750][55000]\t Training Loss 0.8511\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [19800][55000]\t Training Loss 0.8504\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [19850][55000]\t Training Loss 0.8505\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [19900][55000]\t Training Loss 0.8502\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [19950][55000]\t Training Loss 0.8504\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [20000][55000]\t Training Loss 0.8503\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [20050][55000]\t Training Loss 0.8509\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [20100][55000]\t Training Loss 0.8510\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [20150][55000]\t Training Loss 0.8508\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [20200][55000]\t Training Loss 0.8512\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [20250][55000]\t Training Loss 0.8513\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [20300][55000]\t Training Loss 0.8514\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [20350][55000]\t Training Loss 0.8514\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [20400][55000]\t Training Loss 0.8510\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [20450][55000]\t Training Loss 0.8507\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [20500][55000]\t Training Loss 0.8504\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [20550][55000]\t Training Loss 0.8503\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [20600][55000]\t Training Loss 0.8503\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [20650][55000]\t Training Loss 0.8500\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [20700][55000]\t Training Loss 0.8498\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [20750][55000]\t Training Loss 0.8498\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [20800][55000]\t Training Loss 0.8498\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [20850][55000]\t Training Loss 0.8498\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [20900][55000]\t Training Loss 0.8500\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [20950][55000]\t Training Loss 0.8506\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [21000][55000]\t Training Loss 0.8507\t Accuracy 0.8444\n",
      "Epoch [15][20]\t Batch [21050][55000]\t Training Loss 0.8510\t Accuracy 0.8443\n",
      "Epoch [15][20]\t Batch [21100][55000]\t Training Loss 0.8507\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [21150][55000]\t Training Loss 0.8507\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [21200][55000]\t Training Loss 0.8504\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [21250][55000]\t Training Loss 0.8503\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [21300][55000]\t Training Loss 0.8498\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [21350][55000]\t Training Loss 0.8499\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [21400][55000]\t Training Loss 0.8500\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [21450][55000]\t Training Loss 0.8500\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [21500][55000]\t Training Loss 0.8496\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [21550][55000]\t Training Loss 0.8494\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [21600][55000]\t Training Loss 0.8496\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [21650][55000]\t Training Loss 0.8496\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [21700][55000]\t Training Loss 0.8496\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [21750][55000]\t Training Loss 0.8495\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [21800][55000]\t Training Loss 0.8490\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [21850][55000]\t Training Loss 0.8486\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [21900][55000]\t Training Loss 0.8482\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [21950][55000]\t Training Loss 0.8477\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [22000][55000]\t Training Loss 0.8473\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [22050][55000]\t Training Loss 0.8469\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [22100][55000]\t Training Loss 0.8471\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [22150][55000]\t Training Loss 0.8475\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [22200][55000]\t Training Loss 0.8477\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [22250][55000]\t Training Loss 0.8478\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [22300][55000]\t Training Loss 0.8479\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [22350][55000]\t Training Loss 0.8477\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [22400][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [22450][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [22500][55000]\t Training Loss 0.8480\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [22550][55000]\t Training Loss 0.8486\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [22600][55000]\t Training Loss 0.8491\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [22650][55000]\t Training Loss 0.8492\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [22700][55000]\t Training Loss 0.8491\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [22750][55000]\t Training Loss 0.8490\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [22800][55000]\t Training Loss 0.8489\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [22850][55000]\t Training Loss 0.8489\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [22900][55000]\t Training Loss 0.8487\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [22950][55000]\t Training Loss 0.8485\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [23000][55000]\t Training Loss 0.8482\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [23050][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [23100][55000]\t Training Loss 0.8482\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [23150][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [23200][55000]\t Training Loss 0.8481\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [23250][55000]\t Training Loss 0.8479\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [23300][55000]\t Training Loss 0.8475\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [23350][55000]\t Training Loss 0.8474\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [23400][55000]\t Training Loss 0.8471\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [23450][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [23500][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [23550][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [23600][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [23650][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [23700][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [23750][55000]\t Training Loss 0.8477\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [23800][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [23850][55000]\t Training Loss 0.8473\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [23900][55000]\t Training Loss 0.8474\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [23950][55000]\t Training Loss 0.8474\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [24000][55000]\t Training Loss 0.8475\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [24050][55000]\t Training Loss 0.8474\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [24100][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [24150][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [24200][55000]\t Training Loss 0.8469\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [24250][55000]\t Training Loss 0.8471\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [24300][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [24350][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [24400][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [24450][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [24500][55000]\t Training Loss 0.8472\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [24550][55000]\t Training Loss 0.8473\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [24600][55000]\t Training Loss 0.8474\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [24650][55000]\t Training Loss 0.8474\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [24700][55000]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [24750][55000]\t Training Loss 0.8477\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [24800][55000]\t Training Loss 0.8480\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [24850][55000]\t Training Loss 0.8479\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [24900][55000]\t Training Loss 0.8480\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [24950][55000]\t Training Loss 0.8483\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [25000][55000]\t Training Loss 0.8485\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [25050][55000]\t Training Loss 0.8482\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [25100][55000]\t Training Loss 0.8482\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [25150][55000]\t Training Loss 0.8480\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [25200][55000]\t Training Loss 0.8479\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [25250][55000]\t Training Loss 0.8478\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [25300][55000]\t Training Loss 0.8477\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [25350][55000]\t Training Loss 0.8478\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [25400][55000]\t Training Loss 0.8474\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [25450][55000]\t Training Loss 0.8469\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [25500][55000]\t Training Loss 0.8468\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [25550][55000]\t Training Loss 0.8464\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [25600][55000]\t Training Loss 0.8463\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [25650][55000]\t Training Loss 0.8462\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [25700][55000]\t Training Loss 0.8460\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [25750][55000]\t Training Loss 0.8458\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [25800][55000]\t Training Loss 0.8457\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [25850][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [25900][55000]\t Training Loss 0.8459\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [25950][55000]\t Training Loss 0.8459\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [26000][55000]\t Training Loss 0.8459\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [26050][55000]\t Training Loss 0.8457\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [26100][55000]\t Training Loss 0.8454\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [26150][55000]\t Training Loss 0.8452\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [26200][55000]\t Training Loss 0.8450\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [26250][55000]\t Training Loss 0.8450\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [26300][55000]\t Training Loss 0.8452\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [26350][55000]\t Training Loss 0.8450\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [26400][55000]\t Training Loss 0.8454\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [26450][55000]\t Training Loss 0.8456\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [26500][55000]\t Training Loss 0.8458\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [26550][55000]\t Training Loss 0.8460\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [26600][55000]\t Training Loss 0.8462\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [26650][55000]\t Training Loss 0.8466\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [26700][55000]\t Training Loss 0.8464\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [26750][55000]\t Training Loss 0.8467\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [26800][55000]\t Training Loss 0.8465\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [26850][55000]\t Training Loss 0.8464\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [26900][55000]\t Training Loss 0.8467\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [26950][55000]\t Training Loss 0.8466\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [27000][55000]\t Training Loss 0.8464\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [27050][55000]\t Training Loss 0.8462\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [27100][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [27150][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [27200][55000]\t Training Loss 0.8464\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [27250][55000]\t Training Loss 0.8463\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [27300][55000]\t Training Loss 0.8463\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [27350][55000]\t Training Loss 0.8463\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [27400][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [27450][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [27500][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [27550][55000]\t Training Loss 0.8463\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [27600][55000]\t Training Loss 0.8460\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [27650][55000]\t Training Loss 0.8461\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [27700][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [27750][55000]\t Training Loss 0.8462\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [27800][55000]\t Training Loss 0.8463\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [27850][55000]\t Training Loss 0.8464\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [27900][55000]\t Training Loss 0.8462\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [27950][55000]\t Training Loss 0.8460\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [28000][55000]\t Training Loss 0.8458\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [28050][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [28100][55000]\t Training Loss 0.8451\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [28150][55000]\t Training Loss 0.8449\t Accuracy 0.8480\n",
      "Epoch [15][20]\t Batch [28200][55000]\t Training Loss 0.8450\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [28250][55000]\t Training Loss 0.8446\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [28300][55000]\t Training Loss 0.8445\t Accuracy 0.8480\n",
      "Epoch [15][20]\t Batch [28350][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [28400][55000]\t Training Loss 0.8447\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [28450][55000]\t Training Loss 0.8444\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [28500][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [28550][55000]\t Training Loss 0.8442\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [28600][55000]\t Training Loss 0.8440\t Accuracy 0.8484\n",
      "Epoch [15][20]\t Batch [28650][55000]\t Training Loss 0.8443\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [28700][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [28750][55000]\t Training Loss 0.8447\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [28800][55000]\t Training Loss 0.8446\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [28850][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [28900][55000]\t Training Loss 0.8444\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [28950][55000]\t Training Loss 0.8444\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [29000][55000]\t Training Loss 0.8443\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [29050][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [29100][55000]\t Training Loss 0.8445\t Accuracy 0.8481\n",
      "Epoch [15][20]\t Batch [29150][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [29200][55000]\t Training Loss 0.8452\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [29250][55000]\t Training Loss 0.8454\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [29300][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [29350][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [29400][55000]\t Training Loss 0.8453\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [29450][55000]\t Training Loss 0.8449\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [29500][55000]\t Training Loss 0.8447\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [29550][55000]\t Training Loss 0.8446\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [29600][55000]\t Training Loss 0.8446\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [29650][55000]\t Training Loss 0.8446\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [29700][55000]\t Training Loss 0.8446\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [29750][55000]\t Training Loss 0.8450\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [29800][55000]\t Training Loss 0.8452\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [29850][55000]\t Training Loss 0.8455\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [29900][55000]\t Training Loss 0.8458\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [29950][55000]\t Training Loss 0.8461\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [30000][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [30050][55000]\t Training Loss 0.8466\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [30100][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [30150][55000]\t Training Loss 0.8474\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [30200][55000]\t Training Loss 0.8477\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [30250][55000]\t Training Loss 0.8477\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [30300][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [30350][55000]\t Training Loss 0.8475\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [30400][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [30450][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [30500][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [30550][55000]\t Training Loss 0.8480\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [30600][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [30650][55000]\t Training Loss 0.8485\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [30700][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [30750][55000]\t Training Loss 0.8489\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [30800][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [30850][55000]\t Training Loss 0.8490\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [30900][55000]\t Training Loss 0.8493\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [30950][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [31000][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [31050][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [31100][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [31150][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [31200][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [31250][55000]\t Training Loss 0.8494\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [31300][55000]\t Training Loss 0.8497\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [31350][55000]\t Training Loss 0.8504\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [31400][55000]\t Training Loss 0.8505\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [31450][55000]\t Training Loss 0.8508\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [31500][55000]\t Training Loss 0.8508\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [31550][55000]\t Training Loss 0.8508\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [31600][55000]\t Training Loss 0.8510\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [31650][55000]\t Training Loss 0.8512\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [31700][55000]\t Training Loss 0.8514\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [31750][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [31800][55000]\t Training Loss 0.8520\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [31850][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [31900][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [31950][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [32000][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [32050][55000]\t Training Loss 0.8519\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [32100][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [32150][55000]\t Training Loss 0.8520\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [32200][55000]\t Training Loss 0.8522\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [32250][55000]\t Training Loss 0.8525\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [32300][55000]\t Training Loss 0.8528\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [32350][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [32400][55000]\t Training Loss 0.8532\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [32450][55000]\t Training Loss 0.8534\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [32500][55000]\t Training Loss 0.8536\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [32550][55000]\t Training Loss 0.8539\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [32600][55000]\t Training Loss 0.8539\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [32650][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [32700][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [32750][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [32800][55000]\t Training Loss 0.8539\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [32850][55000]\t Training Loss 0.8539\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [32900][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [32950][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33000][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33050][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33100][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33150][55000]\t Training Loss 0.8537\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [33200][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [33250][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33300][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33350][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33400][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [33450][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [33500][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [33550][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [33600][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33650][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [33700][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [33750][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [33800][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [33850][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [33900][55000]\t Training Loss 0.8530\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [33950][55000]\t Training Loss 0.8527\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [34000][55000]\t Training Loss 0.8526\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [34050][55000]\t Training Loss 0.8528\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [34100][55000]\t Training Loss 0.8529\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [34150][55000]\t Training Loss 0.8530\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [34200][55000]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [34250][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [34300][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [34350][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [34400][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [34450][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [34500][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [34550][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [34600][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [34650][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [34700][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [34750][55000]\t Training Loss 0.8527\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [34800][55000]\t Training Loss 0.8526\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [34850][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [34900][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [34950][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [35000][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [35050][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [35100][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [35150][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [35200][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [35250][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [35300][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [35350][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [35400][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [35450][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [35500][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [35550][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [35600][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [35650][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [35700][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [35750][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [35800][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [35850][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [35900][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [35950][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [36000][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [36050][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [36100][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [36150][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [36200][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [36250][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [36300][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [36350][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [36400][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [36450][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [36500][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [36550][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [36600][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [36650][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [36700][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [36750][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [36800][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [36850][55000]\t Training Loss 0.8515\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [36900][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [36950][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [37000][55000]\t Training Loss 0.8513\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [37050][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [37100][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [37150][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [37200][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [37250][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [37300][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [15][20]\t Batch [37350][55000]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [15][20]\t Batch [37400][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [37450][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [37500][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [37550][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [37600][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [37650][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [37700][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [37750][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [37800][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [37850][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [37900][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [37950][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [38000][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [38050][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [38100][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [38150][55000]\t Training Loss 0.8522\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [38200][55000]\t Training Loss 0.8521\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [38250][55000]\t Training Loss 0.8521\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [38300][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [38350][55000]\t Training Loss 0.8524\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [38400][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [38450][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [38500][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [38550][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [38600][55000]\t Training Loss 0.8525\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [38650][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [38700][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [38750][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [38800][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [38850][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [38900][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [38950][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39000][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39050][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39100][55000]\t Training Loss 0.8516\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [39150][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [39200][55000]\t Training Loss 0.8513\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [39250][55000]\t Training Loss 0.8511\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [39300][55000]\t Training Loss 0.8511\t Accuracy 0.8477\n",
      "Epoch [15][20]\t Batch [39350][55000]\t Training Loss 0.8514\t Accuracy 0.8475\n",
      "Epoch [15][20]\t Batch [39400][55000]\t Training Loss 0.8514\t Accuracy 0.8476\n",
      "Epoch [15][20]\t Batch [39450][55000]\t Training Loss 0.8517\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [39500][55000]\t Training Loss 0.8517\t Accuracy 0.8474\n",
      "Epoch [15][20]\t Batch [39550][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39600][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39650][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39700][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39750][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39800][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39850][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [39900][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [39950][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40000][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40050][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [40100][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [40150][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [40200][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40250][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [40300][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40350][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40400][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40450][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40500][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [40550][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40600][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [40650][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [40700][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [40750][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40800][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40850][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [40900][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [40950][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [41000][55000]\t Training Loss 0.8517\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [41050][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [41100][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [41150][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [41200][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [41250][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [41300][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [41350][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [41400][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [41450][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [41500][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [41550][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [41600][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [41650][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [41700][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [15][20]\t Batch [41750][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [41800][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [15][20]\t Batch [41850][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [41900][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [41950][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [42000][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [42050][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [15][20]\t Batch [42100][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [42150][55000]\t Training Loss 0.8533\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [42200][55000]\t Training Loss 0.8533\t Accuracy 0.8468\n",
      "Epoch [15][20]\t Batch [42250][55000]\t Training Loss 0.8534\t Accuracy 0.8467\n",
      "Epoch [15][20]\t Batch [42300][55000]\t Training Loss 0.8534\t Accuracy 0.8466\n",
      "Epoch [15][20]\t Batch [42350][55000]\t Training Loss 0.8536\t Accuracy 0.8465\n",
      "Epoch [15][20]\t Batch [42400][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [42450][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [42500][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [42550][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [42600][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [42650][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [42700][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [42750][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [42800][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [42850][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [42900][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [42950][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [43000][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [43050][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [43100][55000]\t Training Loss 0.8544\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [43150][55000]\t Training Loss 0.8545\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [43200][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [43250][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [43300][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [43350][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [43400][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [43450][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [43500][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [43550][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [43600][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [43650][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [43700][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [43750][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [43800][55000]\t Training Loss 0.8533\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [43850][55000]\t Training Loss 0.8533\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [43900][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [43950][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [44000][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [44050][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [44100][55000]\t Training Loss 0.8536\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [44150][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [44200][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [44250][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [44300][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [44350][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [44400][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [44450][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [44500][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [44550][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [44600][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [44650][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [44700][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [44750][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [44800][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [44850][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [15][20]\t Batch [44900][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [44950][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [45000][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [45050][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45100][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45150][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [45200][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45250][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45300][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [45350][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [45400][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [45450][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [15][20]\t Batch [45500][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45550][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45600][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45650][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45700][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45750][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [45800][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45850][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [45900][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [45950][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [46000][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [46050][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46100][55000]\t Training Loss 0.8549\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [46150][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46200][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46250][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46300][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [46350][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [46400][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [46450][55000]\t Training Loss 0.8553\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [46500][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [46550][55000]\t Training Loss 0.8551\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46600][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46650][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [46700][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46750][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46800][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46850][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46900][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [46950][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47000][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47050][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47100][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47150][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47200][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47250][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47300][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47350][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47400][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47450][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47500][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47550][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47600][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47650][55000]\t Training Loss 0.8547\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47700][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [47750][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [47800][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [47850][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47900][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [47950][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [48000][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [48050][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [48100][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [48150][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48200][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48250][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [48300][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48350][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48400][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [48450][55000]\t Training Loss 0.8534\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48500][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [48550][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48600][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48650][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48700][55000]\t Training Loss 0.8530\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [48750][55000]\t Training Loss 0.8529\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [48800][55000]\t Training Loss 0.8528\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [48850][55000]\t Training Loss 0.8527\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [48900][55000]\t Training Loss 0.8526\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [48950][55000]\t Training Loss 0.8528\t Accuracy 0.8461\n",
      "Epoch [15][20]\t Batch [49000][55000]\t Training Loss 0.8529\t Accuracy 0.8460\n",
      "Epoch [15][20]\t Batch [49050][55000]\t Training Loss 0.8533\t Accuracy 0.8459\n",
      "Epoch [15][20]\t Batch [49100][55000]\t Training Loss 0.8534\t Accuracy 0.8458\n",
      "Epoch [15][20]\t Batch [49150][55000]\t Training Loss 0.8535\t Accuracy 0.8457\n",
      "Epoch [15][20]\t Batch [49200][55000]\t Training Loss 0.8535\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [49250][55000]\t Training Loss 0.8536\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [49300][55000]\t Training Loss 0.8535\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [49350][55000]\t Training Loss 0.8534\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [49400][55000]\t Training Loss 0.8532\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [49450][55000]\t Training Loss 0.8532\t Accuracy 0.8456\n",
      "Epoch [15][20]\t Batch [49500][55000]\t Training Loss 0.8534\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [49550][55000]\t Training Loss 0.8538\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [49600][55000]\t Training Loss 0.8540\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [49650][55000]\t Training Loss 0.8541\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [49700][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [49750][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [49800][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [49850][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [49900][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [49950][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [50000][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [50050][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [50100][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [50150][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [50200][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [50250][55000]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [50300][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [50350][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [50400][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [50450][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [50500][55000]\t Training Loss 0.8553\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [50550][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [50600][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [50650][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [50700][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [50750][55000]\t Training Loss 0.8555\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [50800][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [50850][55000]\t Training Loss 0.8555\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [50900][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [50950][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [51000][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [51050][55000]\t Training Loss 0.8550\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [51100][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [51150][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [51200][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [51250][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [51300][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [51350][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [51400][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [51450][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [51500][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [51550][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [51600][55000]\t Training Loss 0.8545\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [51650][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [51700][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [51750][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [51800][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [51850][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [51900][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [51950][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [52000][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [52050][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [52100][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [52150][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [52200][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [52250][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [52300][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [52350][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [52400][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [52450][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [52500][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [52550][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [52600][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [52650][55000]\t Training Loss 0.8543\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [52700][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [15][20]\t Batch [52750][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [52800][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [52850][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [52900][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [15][20]\t Batch [52950][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [53000][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53050][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [53100][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53150][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [53200][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53250][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53300][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53350][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53400][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [53450][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [53500][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53550][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53600][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [53650][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [53700][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [53750][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53800][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53850][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53900][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [53950][55000]\t Training Loss 0.8549\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [54000][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [54050][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [54100][55000]\t Training Loss 0.8553\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [54150][55000]\t Training Loss 0.8552\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [54200][55000]\t Training Loss 0.8553\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [54250][55000]\t Training Loss 0.8551\t Accuracy 0.8446\n",
      "Epoch [15][20]\t Batch [54300][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [54350][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [54400][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [54450][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [54500][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [54550][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [54600][55000]\t Training Loss 0.8548\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [54650][55000]\t Training Loss 0.8546\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [54700][55000]\t Training Loss 0.8545\t Accuracy 0.8448\n",
      "Epoch [15][20]\t Batch [54750][55000]\t Training Loss 0.8544\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [54800][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [54850][55000]\t Training Loss 0.8542\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [54900][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [54950][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8548\t Average training accuracy 0.8447\n",
      "Epoch [15]\t Average validation loss 0.7922\t Average validation accuracy 0.8698\n",
      "\n",
      "Epoch [16][20]\t Batch [0][55000]\t Training Loss 1.4700\t Accuracy 0.0000\n",
      "Epoch [16][20]\t Batch [50][55000]\t Training Loss 0.9382\t Accuracy 0.7255\n",
      "Epoch [16][20]\t Batch [100][55000]\t Training Loss 0.8488\t Accuracy 0.8020\n",
      "Epoch [16][20]\t Batch [150][55000]\t Training Loss 0.8502\t Accuracy 0.8146\n",
      "Epoch [16][20]\t Batch [200][55000]\t Training Loss 0.8611\t Accuracy 0.8159\n",
      "Epoch [16][20]\t Batch [250][55000]\t Training Loss 0.8433\t Accuracy 0.8247\n",
      "Epoch [16][20]\t Batch [300][55000]\t Training Loss 0.8449\t Accuracy 0.8206\n",
      "Epoch [16][20]\t Batch [350][55000]\t Training Loss 0.8251\t Accuracy 0.8234\n",
      "Epoch [16][20]\t Batch [400][55000]\t Training Loss 0.8112\t Accuracy 0.8279\n",
      "Epoch [16][20]\t Batch [450][55000]\t Training Loss 0.8171\t Accuracy 0.8293\n",
      "Epoch [16][20]\t Batch [500][55000]\t Training Loss 0.8234\t Accuracy 0.8303\n",
      "Epoch [16][20]\t Batch [550][55000]\t Training Loss 0.8407\t Accuracy 0.8276\n",
      "Epoch [16][20]\t Batch [600][55000]\t Training Loss 0.8452\t Accuracy 0.8303\n",
      "Epoch [16][20]\t Batch [650][55000]\t Training Loss 0.8650\t Accuracy 0.8203\n",
      "Epoch [16][20]\t Batch [700][55000]\t Training Loss 0.8627\t Accuracy 0.8260\n",
      "Epoch [16][20]\t Batch [750][55000]\t Training Loss 0.8589\t Accuracy 0.8269\n",
      "Epoch [16][20]\t Batch [800][55000]\t Training Loss 0.8529\t Accuracy 0.8290\n",
      "Epoch [16][20]\t Batch [850][55000]\t Training Loss 0.8514\t Accuracy 0.8308\n",
      "Epoch [16][20]\t Batch [900][55000]\t Training Loss 0.8579\t Accuracy 0.8302\n",
      "Epoch [16][20]\t Batch [950][55000]\t Training Loss 0.8647\t Accuracy 0.8254\n",
      "Epoch [16][20]\t Batch [1000][55000]\t Training Loss 0.8641\t Accuracy 0.8272\n",
      "Epoch [16][20]\t Batch [1050][55000]\t Training Loss 0.8718\t Accuracy 0.8259\n",
      "Epoch [16][20]\t Batch [1100][55000]\t Training Loss 0.8802\t Accuracy 0.8256\n",
      "Epoch [16][20]\t Batch [1150][55000]\t Training Loss 0.8894\t Accuracy 0.8228\n",
      "Epoch [16][20]\t Batch [1200][55000]\t Training Loss 0.8836\t Accuracy 0.8260\n",
      "Epoch [16][20]\t Batch [1250][55000]\t Training Loss 0.8850\t Accuracy 0.8257\n",
      "Epoch [16][20]\t Batch [1300][55000]\t Training Loss 0.8853\t Accuracy 0.8255\n",
      "Epoch [16][20]\t Batch [1350][55000]\t Training Loss 0.8821\t Accuracy 0.8268\n",
      "Epoch [16][20]\t Batch [1400][55000]\t Training Loss 0.8827\t Accuracy 0.8258\n",
      "Epoch [16][20]\t Batch [1450][55000]\t Training Loss 0.8835\t Accuracy 0.8249\n",
      "Epoch [16][20]\t Batch [1500][55000]\t Training Loss 0.8816\t Accuracy 0.8274\n",
      "Epoch [16][20]\t Batch [1550][55000]\t Training Loss 0.8810\t Accuracy 0.8279\n",
      "Epoch [16][20]\t Batch [1600][55000]\t Training Loss 0.8837\t Accuracy 0.8264\n",
      "Epoch [16][20]\t Batch [1650][55000]\t Training Loss 0.8805\t Accuracy 0.8280\n",
      "Epoch [16][20]\t Batch [1700][55000]\t Training Loss 0.8789\t Accuracy 0.8295\n",
      "Epoch [16][20]\t Batch [1750][55000]\t Training Loss 0.8732\t Accuracy 0.8310\n",
      "Epoch [16][20]\t Batch [1800][55000]\t Training Loss 0.8710\t Accuracy 0.8329\n",
      "Epoch [16][20]\t Batch [1850][55000]\t Training Loss 0.8694\t Accuracy 0.8336\n",
      "Epoch [16][20]\t Batch [1900][55000]\t Training Loss 0.8672\t Accuracy 0.8338\n",
      "Epoch [16][20]\t Batch [1950][55000]\t Training Loss 0.8657\t Accuracy 0.8350\n",
      "Epoch [16][20]\t Batch [2000][55000]\t Training Loss 0.8632\t Accuracy 0.8361\n",
      "Epoch [16][20]\t Batch [2050][55000]\t Training Loss 0.8623\t Accuracy 0.8367\n",
      "Epoch [16][20]\t Batch [2100][55000]\t Training Loss 0.8595\t Accuracy 0.8372\n",
      "Epoch [16][20]\t Batch [2150][55000]\t Training Loss 0.8562\t Accuracy 0.8391\n",
      "Epoch [16][20]\t Batch [2200][55000]\t Training Loss 0.8514\t Accuracy 0.8410\n",
      "Epoch [16][20]\t Batch [2250][55000]\t Training Loss 0.8506\t Accuracy 0.8414\n",
      "Epoch [16][20]\t Batch [2300][55000]\t Training Loss 0.8477\t Accuracy 0.8409\n",
      "Epoch [16][20]\t Batch [2350][55000]\t Training Loss 0.8461\t Accuracy 0.8413\n",
      "Epoch [16][20]\t Batch [2400][55000]\t Training Loss 0.8471\t Accuracy 0.8397\n",
      "Epoch [16][20]\t Batch [2450][55000]\t Training Loss 0.8500\t Accuracy 0.8384\n",
      "Epoch [16][20]\t Batch [2500][55000]\t Training Loss 0.8478\t Accuracy 0.8405\n",
      "Epoch [16][20]\t Batch [2550][55000]\t Training Loss 0.8466\t Accuracy 0.8405\n",
      "Epoch [16][20]\t Batch [2600][55000]\t Training Loss 0.8460\t Accuracy 0.8393\n",
      "Epoch [16][20]\t Batch [2650][55000]\t Training Loss 0.8448\t Accuracy 0.8397\n",
      "Epoch [16][20]\t Batch [2700][55000]\t Training Loss 0.8445\t Accuracy 0.8401\n",
      "Epoch [16][20]\t Batch [2750][55000]\t Training Loss 0.8444\t Accuracy 0.8401\n",
      "Epoch [16][20]\t Batch [2800][55000]\t Training Loss 0.8447\t Accuracy 0.8383\n",
      "Epoch [16][20]\t Batch [2850][55000]\t Training Loss 0.8436\t Accuracy 0.8383\n",
      "Epoch [16][20]\t Batch [2900][55000]\t Training Loss 0.8403\t Accuracy 0.8394\n",
      "Epoch [16][20]\t Batch [2950][55000]\t Training Loss 0.8404\t Accuracy 0.8390\n",
      "Epoch [16][20]\t Batch [3000][55000]\t Training Loss 0.8403\t Accuracy 0.8397\n",
      "Epoch [16][20]\t Batch [3050][55000]\t Training Loss 0.8413\t Accuracy 0.8394\n",
      "Epoch [16][20]\t Batch [3100][55000]\t Training Loss 0.8436\t Accuracy 0.8391\n",
      "Epoch [16][20]\t Batch [3150][55000]\t Training Loss 0.8435\t Accuracy 0.8394\n",
      "Epoch [16][20]\t Batch [3200][55000]\t Training Loss 0.8433\t Accuracy 0.8407\n",
      "Epoch [16][20]\t Batch [3250][55000]\t Training Loss 0.8427\t Accuracy 0.8400\n",
      "Epoch [16][20]\t Batch [3300][55000]\t Training Loss 0.8440\t Accuracy 0.8400\n",
      "Epoch [16][20]\t Batch [3350][55000]\t Training Loss 0.8422\t Accuracy 0.8415\n",
      "Epoch [16][20]\t Batch [3400][55000]\t Training Loss 0.8446\t Accuracy 0.8403\n",
      "Epoch [16][20]\t Batch [3450][55000]\t Training Loss 0.8442\t Accuracy 0.8412\n",
      "Epoch [16][20]\t Batch [3500][55000]\t Training Loss 0.8442\t Accuracy 0.8409\n",
      "Epoch [16][20]\t Batch [3550][55000]\t Training Loss 0.8458\t Accuracy 0.8403\n",
      "Epoch [16][20]\t Batch [3600][55000]\t Training Loss 0.8457\t Accuracy 0.8398\n",
      "Epoch [16][20]\t Batch [3650][55000]\t Training Loss 0.8443\t Accuracy 0.8406\n",
      "Epoch [16][20]\t Batch [3700][55000]\t Training Loss 0.8450\t Accuracy 0.8400\n",
      "Epoch [16][20]\t Batch [3750][55000]\t Training Loss 0.8449\t Accuracy 0.8403\n",
      "Epoch [16][20]\t Batch [3800][55000]\t Training Loss 0.8453\t Accuracy 0.8403\n",
      "Epoch [16][20]\t Batch [3850][55000]\t Training Loss 0.8449\t Accuracy 0.8406\n",
      "Epoch [16][20]\t Batch [3900][55000]\t Training Loss 0.8437\t Accuracy 0.8418\n",
      "Epoch [16][20]\t Batch [3950][55000]\t Training Loss 0.8426\t Accuracy 0.8431\n",
      "Epoch [16][20]\t Batch [4000][55000]\t Training Loss 0.8422\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [4050][55000]\t Training Loss 0.8407\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [4100][55000]\t Training Loss 0.8411\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [4150][55000]\t Training Loss 0.8416\t Accuracy 0.8439\n",
      "Epoch [16][20]\t Batch [4200][55000]\t Training Loss 0.8421\t Accuracy 0.8429\n",
      "Epoch [16][20]\t Batch [4250][55000]\t Training Loss 0.8407\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [4300][55000]\t Training Loss 0.8407\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [4350][55000]\t Training Loss 0.8412\t Accuracy 0.8444\n",
      "Epoch [16][20]\t Batch [4400][55000]\t Training Loss 0.8411\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [4450][55000]\t Training Loss 0.8413\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [4500][55000]\t Training Loss 0.8416\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [4550][55000]\t Training Loss 0.8399\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [4600][55000]\t Training Loss 0.8382\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [4650][55000]\t Training Loss 0.8380\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [4700][55000]\t Training Loss 0.8377\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [4750][55000]\t Training Loss 0.8365\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [4800][55000]\t Training Loss 0.8371\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [4850][55000]\t Training Loss 0.8383\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [4900][55000]\t Training Loss 0.8369\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [4950][55000]\t Training Loss 0.8372\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [5000][55000]\t Training Loss 0.8373\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [5050][55000]\t Training Loss 0.8368\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [5100][55000]\t Training Loss 0.8368\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [5150][55000]\t Training Loss 0.8371\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [5200][55000]\t Training Loss 0.8386\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [5250][55000]\t Training Loss 0.8377\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [5300][55000]\t Training Loss 0.8376\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [5350][55000]\t Training Loss 0.8383\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [5400][55000]\t Training Loss 0.8375\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [5450][55000]\t Training Loss 0.8368\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [5500][55000]\t Training Loss 0.8350\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [5550][55000]\t Training Loss 0.8349\t Accuracy 0.8456\n",
      "Epoch [16][20]\t Batch [5600][55000]\t Training Loss 0.8340\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [5650][55000]\t Training Loss 0.8346\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [5700][55000]\t Training Loss 0.8343\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [5750][55000]\t Training Loss 0.8348\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [5800][55000]\t Training Loss 0.8351\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [5850][55000]\t Training Loss 0.8353\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [5900][55000]\t Training Loss 0.8358\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [5950][55000]\t Training Loss 0.8357\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [6000][55000]\t Training Loss 0.8346\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [6050][55000]\t Training Loss 0.8333\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [6100][55000]\t Training Loss 0.8318\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [6150][55000]\t Training Loss 0.8297\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [6200][55000]\t Training Loss 0.8298\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [6250][55000]\t Training Loss 0.8286\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [6300][55000]\t Training Loss 0.8290\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [6350][55000]\t Training Loss 0.8286\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [6400][55000]\t Training Loss 0.8281\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [6450][55000]\t Training Loss 0.8275\t Accuracy 0.8486\n",
      "Epoch [16][20]\t Batch [6500][55000]\t Training Loss 0.8283\t Accuracy 0.8482\n",
      "Epoch [16][20]\t Batch [6550][55000]\t Training Loss 0.8273\t Accuracy 0.8484\n",
      "Epoch [16][20]\t Batch [6600][55000]\t Training Loss 0.8260\t Accuracy 0.8490\n",
      "Epoch [16][20]\t Batch [6650][55000]\t Training Loss 0.8248\t Accuracy 0.8490\n",
      "Epoch [16][20]\t Batch [6700][55000]\t Training Loss 0.8250\t Accuracy 0.8490\n",
      "Epoch [16][20]\t Batch [6750][55000]\t Training Loss 0.8252\t Accuracy 0.8498\n",
      "Epoch [16][20]\t Batch [6800][55000]\t Training Loss 0.8256\t Accuracy 0.8497\n",
      "Epoch [16][20]\t Batch [6850][55000]\t Training Loss 0.8278\t Accuracy 0.8492\n",
      "Epoch [16][20]\t Batch [6900][55000]\t Training Loss 0.8277\t Accuracy 0.8493\n",
      "Epoch [16][20]\t Batch [6950][55000]\t Training Loss 0.8284\t Accuracy 0.8482\n",
      "Epoch [16][20]\t Batch [7000][55000]\t Training Loss 0.8283\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [7050][55000]\t Training Loss 0.8288\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [7100][55000]\t Training Loss 0.8291\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [7150][55000]\t Training Loss 0.8293\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [7200][55000]\t Training Loss 0.8301\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [7250][55000]\t Training Loss 0.8324\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [7300][55000]\t Training Loss 0.8342\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [7350][55000]\t Training Loss 0.8357\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [7400][55000]\t Training Loss 0.8368\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [7450][55000]\t Training Loss 0.8367\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [7500][55000]\t Training Loss 0.8367\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [7550][55000]\t Training Loss 0.8372\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [7600][55000]\t Training Loss 0.8368\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [7650][55000]\t Training Loss 0.8372\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [7700][55000]\t Training Loss 0.8377\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [7750][55000]\t Training Loss 0.8378\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [7800][55000]\t Training Loss 0.8386\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [7850][55000]\t Training Loss 0.8387\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [7900][55000]\t Training Loss 0.8389\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [7950][55000]\t Training Loss 0.8390\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [8000][55000]\t Training Loss 0.8390\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [8050][55000]\t Training Loss 0.8395\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [8100][55000]\t Training Loss 0.8381\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [8150][55000]\t Training Loss 0.8389\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [8200][55000]\t Training Loss 0.8389\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [8250][55000]\t Training Loss 0.8401\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [8300][55000]\t Training Loss 0.8405\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [8350][55000]\t Training Loss 0.8411\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [8400][55000]\t Training Loss 0.8404\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [8450][55000]\t Training Loss 0.8419\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [8500][55000]\t Training Loss 0.8411\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [8550][55000]\t Training Loss 0.8402\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [8600][55000]\t Training Loss 0.8394\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [8650][55000]\t Training Loss 0.8395\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [8700][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [8750][55000]\t Training Loss 0.8416\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [8800][55000]\t Training Loss 0.8424\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [8850][55000]\t Training Loss 0.8424\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [8900][55000]\t Training Loss 0.8442\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [8950][55000]\t Training Loss 0.8437\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [9000][55000]\t Training Loss 0.8430\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [9050][55000]\t Training Loss 0.8422\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [9100][55000]\t Training Loss 0.8419\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [9150][55000]\t Training Loss 0.8424\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [9200][55000]\t Training Loss 0.8420\t Accuracy 0.8456\n",
      "Epoch [16][20]\t Batch [9250][55000]\t Training Loss 0.8423\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [9300][55000]\t Training Loss 0.8426\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [9350][55000]\t Training Loss 0.8428\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [9400][55000]\t Training Loss 0.8430\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [9450][55000]\t Training Loss 0.8432\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [9500][55000]\t Training Loss 0.8423\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [9550][55000]\t Training Loss 0.8421\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [9600][55000]\t Training Loss 0.8426\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [9650][55000]\t Training Loss 0.8424\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [9700][55000]\t Training Loss 0.8419\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [9750][55000]\t Training Loss 0.8410\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [9800][55000]\t Training Loss 0.8416\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [9850][55000]\t Training Loss 0.8412\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [9900][55000]\t Training Loss 0.8407\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [9950][55000]\t Training Loss 0.8400\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [10000][55000]\t Training Loss 0.8398\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [10050][55000]\t Training Loss 0.8400\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [10100][55000]\t Training Loss 0.8398\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [10150][55000]\t Training Loss 0.8396\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [10200][55000]\t Training Loss 0.8398\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [10250][55000]\t Training Loss 0.8399\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [10300][55000]\t Training Loss 0.8398\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [10350][55000]\t Training Loss 0.8388\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [10400][55000]\t Training Loss 0.8380\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [10450][55000]\t Training Loss 0.8378\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [10500][55000]\t Training Loss 0.8369\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [10550][55000]\t Training Loss 0.8364\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [10600][55000]\t Training Loss 0.8360\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [10650][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [10700][55000]\t Training Loss 0.8355\t Accuracy 0.8488\n",
      "Epoch [16][20]\t Batch [10750][55000]\t Training Loss 0.8360\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [10800][55000]\t Training Loss 0.8365\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [10850][55000]\t Training Loss 0.8358\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [10900][55000]\t Training Loss 0.8353\t Accuracy 0.8487\n",
      "Epoch [16][20]\t Batch [10950][55000]\t Training Loss 0.8350\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [11000][55000]\t Training Loss 0.8349\t Accuracy 0.8487\n",
      "Epoch [16][20]\t Batch [11050][55000]\t Training Loss 0.8341\t Accuracy 0.8491\n",
      "Epoch [16][20]\t Batch [11100][55000]\t Training Loss 0.8335\t Accuracy 0.8492\n",
      "Epoch [16][20]\t Batch [11150][55000]\t Training Loss 0.8335\t Accuracy 0.8494\n",
      "Epoch [16][20]\t Batch [11200][55000]\t Training Loss 0.8331\t Accuracy 0.8495\n",
      "Epoch [16][20]\t Batch [11250][55000]\t Training Loss 0.8335\t Accuracy 0.8493\n",
      "Epoch [16][20]\t Batch [11300][55000]\t Training Loss 0.8331\t Accuracy 0.8494\n",
      "Epoch [16][20]\t Batch [11350][55000]\t Training Loss 0.8325\t Accuracy 0.8494\n",
      "Epoch [16][20]\t Batch [11400][55000]\t Training Loss 0.8327\t Accuracy 0.8494\n",
      "Epoch [16][20]\t Batch [11450][55000]\t Training Loss 0.8323\t Accuracy 0.8494\n",
      "Epoch [16][20]\t Batch [11500][55000]\t Training Loss 0.8321\t Accuracy 0.8492\n",
      "Epoch [16][20]\t Batch [11550][55000]\t Training Loss 0.8322\t Accuracy 0.8491\n",
      "Epoch [16][20]\t Batch [11600][55000]\t Training Loss 0.8334\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [11650][55000]\t Training Loss 0.8339\t Accuracy 0.8482\n",
      "Epoch [16][20]\t Batch [11700][55000]\t Training Loss 0.8339\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [11750][55000]\t Training Loss 0.8348\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [11800][55000]\t Training Loss 0.8350\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [11850][55000]\t Training Loss 0.8351\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [11900][55000]\t Training Loss 0.8354\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [11950][55000]\t Training Loss 0.8355\t Accuracy 0.8480\n",
      "Epoch [16][20]\t Batch [12000][55000]\t Training Loss 0.8355\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [12050][55000]\t Training Loss 0.8353\t Accuracy 0.8486\n",
      "Epoch [16][20]\t Batch [12100][55000]\t Training Loss 0.8352\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [12150][55000]\t Training Loss 0.8345\t Accuracy 0.8490\n",
      "Epoch [16][20]\t Batch [12200][55000]\t Training Loss 0.8349\t Accuracy 0.8489\n",
      "Epoch [16][20]\t Batch [12250][55000]\t Training Loss 0.8348\t Accuracy 0.8490\n",
      "Epoch [16][20]\t Batch [12300][55000]\t Training Loss 0.8348\t Accuracy 0.8490\n",
      "Epoch [16][20]\t Batch [12350][55000]\t Training Loss 0.8350\t Accuracy 0.8491\n",
      "Epoch [16][20]\t Batch [12400][55000]\t Training Loss 0.8353\t Accuracy 0.8491\n",
      "Epoch [16][20]\t Batch [12450][55000]\t Training Loss 0.8355\t Accuracy 0.8488\n",
      "Epoch [16][20]\t Batch [12500][55000]\t Training Loss 0.8357\t Accuracy 0.8484\n",
      "Epoch [16][20]\t Batch [12550][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [12600][55000]\t Training Loss 0.8364\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [12650][55000]\t Training Loss 0.8369\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [12700][55000]\t Training Loss 0.8376\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [12750][55000]\t Training Loss 0.8372\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [12800][55000]\t Training Loss 0.8378\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [12850][55000]\t Training Loss 0.8380\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [12900][55000]\t Training Loss 0.8379\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [12950][55000]\t Training Loss 0.8384\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [13000][55000]\t Training Loss 0.8384\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [13050][55000]\t Training Loss 0.8391\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [13100][55000]\t Training Loss 0.8397\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [13150][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [13200][55000]\t Training Loss 0.8403\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [13250][55000]\t Training Loss 0.8398\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [13300][55000]\t Training Loss 0.8397\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [13350][55000]\t Training Loss 0.8401\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [13400][55000]\t Training Loss 0.8403\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [13450][55000]\t Training Loss 0.8398\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [13500][55000]\t Training Loss 0.8395\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [13550][55000]\t Training Loss 0.8391\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [13600][55000]\t Training Loss 0.8382\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [13650][55000]\t Training Loss 0.8381\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [13700][55000]\t Training Loss 0.8389\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [13750][55000]\t Training Loss 0.8395\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [13800][55000]\t Training Loss 0.8396\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [13850][55000]\t Training Loss 0.8396\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [13900][55000]\t Training Loss 0.8398\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [13950][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [14000][55000]\t Training Loss 0.8409\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [14050][55000]\t Training Loss 0.8410\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [14100][55000]\t Training Loss 0.8411\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [14150][55000]\t Training Loss 0.8414\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [14200][55000]\t Training Loss 0.8413\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [14250][55000]\t Training Loss 0.8415\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [14300][55000]\t Training Loss 0.8419\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [14350][55000]\t Training Loss 0.8423\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [14400][55000]\t Training Loss 0.8431\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [14450][55000]\t Training Loss 0.8433\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [14500][55000]\t Training Loss 0.8434\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [14550][55000]\t Training Loss 0.8441\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [14600][55000]\t Training Loss 0.8441\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [14650][55000]\t Training Loss 0.8451\t Accuracy 0.8444\n",
      "Epoch [16][20]\t Batch [14700][55000]\t Training Loss 0.8458\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [14750][55000]\t Training Loss 0.8465\t Accuracy 0.8439\n",
      "Epoch [16][20]\t Batch [14800][55000]\t Training Loss 0.8474\t Accuracy 0.8437\n",
      "Epoch [16][20]\t Batch [14850][55000]\t Training Loss 0.8480\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [14900][55000]\t Training Loss 0.8479\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [14950][55000]\t Training Loss 0.8480\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [15000][55000]\t Training Loss 0.8479\t Accuracy 0.8433\n",
      "Epoch [16][20]\t Batch [15050][55000]\t Training Loss 0.8476\t Accuracy 0.8436\n",
      "Epoch [16][20]\t Batch [15100][55000]\t Training Loss 0.8474\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [15150][55000]\t Training Loss 0.8477\t Accuracy 0.8437\n",
      "Epoch [16][20]\t Batch [15200][55000]\t Training Loss 0.8479\t Accuracy 0.8437\n",
      "Epoch [16][20]\t Batch [15250][55000]\t Training Loss 0.8479\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [15300][55000]\t Training Loss 0.8479\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [15350][55000]\t Training Loss 0.8476\t Accuracy 0.8440\n",
      "Epoch [16][20]\t Batch [15400][55000]\t Training Loss 0.8479\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [15450][55000]\t Training Loss 0.8480\t Accuracy 0.8443\n",
      "Epoch [16][20]\t Batch [15500][55000]\t Training Loss 0.8478\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [15550][55000]\t Training Loss 0.8477\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [15600][55000]\t Training Loss 0.8475\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [15650][55000]\t Training Loss 0.8475\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [15700][55000]\t Training Loss 0.8473\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [15750][55000]\t Training Loss 0.8482\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [15800][55000]\t Training Loss 0.8486\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [15850][55000]\t Training Loss 0.8490\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [15900][55000]\t Training Loss 0.8495\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [15950][55000]\t Training Loss 0.8496\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [16000][55000]\t Training Loss 0.8496\t Accuracy 0.8439\n",
      "Epoch [16][20]\t Batch [16050][55000]\t Training Loss 0.8506\t Accuracy 0.8436\n",
      "Epoch [16][20]\t Batch [16100][55000]\t Training Loss 0.8505\t Accuracy 0.8436\n",
      "Epoch [16][20]\t Batch [16150][55000]\t Training Loss 0.8505\t Accuracy 0.8437\n",
      "Epoch [16][20]\t Batch [16200][55000]\t Training Loss 0.8506\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [16250][55000]\t Training Loss 0.8504\t Accuracy 0.8433\n",
      "Epoch [16][20]\t Batch [16300][55000]\t Training Loss 0.8501\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [16350][55000]\t Training Loss 0.8498\t Accuracy 0.8437\n",
      "Epoch [16][20]\t Batch [16400][55000]\t Training Loss 0.8498\t Accuracy 0.8437\n",
      "Epoch [16][20]\t Batch [16450][55000]\t Training Loss 0.8495\t Accuracy 0.8439\n",
      "Epoch [16][20]\t Batch [16500][55000]\t Training Loss 0.8494\t Accuracy 0.8441\n",
      "Epoch [16][20]\t Batch [16550][55000]\t Training Loss 0.8489\t Accuracy 0.8441\n",
      "Epoch [16][20]\t Batch [16600][55000]\t Training Loss 0.8490\t Accuracy 0.8441\n",
      "Epoch [16][20]\t Batch [16650][55000]\t Training Loss 0.8490\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [16700][55000]\t Training Loss 0.8491\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [16750][55000]\t Training Loss 0.8490\t Accuracy 0.8444\n",
      "Epoch [16][20]\t Batch [16800][55000]\t Training Loss 0.8496\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [16850][55000]\t Training Loss 0.8502\t Accuracy 0.8439\n",
      "Epoch [16][20]\t Batch [16900][55000]\t Training Loss 0.8504\t Accuracy 0.8440\n",
      "Epoch [16][20]\t Batch [16950][55000]\t Training Loss 0.8504\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [17000][55000]\t Training Loss 0.8512\t Accuracy 0.8435\n",
      "Epoch [16][20]\t Batch [17050][55000]\t Training Loss 0.8511\t Accuracy 0.8435\n",
      "Epoch [16][20]\t Batch [17100][55000]\t Training Loss 0.8515\t Accuracy 0.8433\n",
      "Epoch [16][20]\t Batch [17150][55000]\t Training Loss 0.8513\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [17200][55000]\t Training Loss 0.8514\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [17250][55000]\t Training Loss 0.8518\t Accuracy 0.8433\n",
      "Epoch [16][20]\t Batch [17300][55000]\t Training Loss 0.8517\t Accuracy 0.8433\n",
      "Epoch [16][20]\t Batch [17350][55000]\t Training Loss 0.8511\t Accuracy 0.8436\n",
      "Epoch [16][20]\t Batch [17400][55000]\t Training Loss 0.8513\t Accuracy 0.8436\n",
      "Epoch [16][20]\t Batch [17450][55000]\t Training Loss 0.8514\t Accuracy 0.8436\n",
      "Epoch [16][20]\t Batch [17500][55000]\t Training Loss 0.8514\t Accuracy 0.8436\n",
      "Epoch [16][20]\t Batch [17550][55000]\t Training Loss 0.8521\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [17600][55000]\t Training Loss 0.8527\t Accuracy 0.8431\n",
      "Epoch [16][20]\t Batch [17650][55000]\t Training Loss 0.8529\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [17700][55000]\t Training Loss 0.8536\t Accuracy 0.8431\n",
      "Epoch [16][20]\t Batch [17750][55000]\t Training Loss 0.8540\t Accuracy 0.8432\n",
      "Epoch [16][20]\t Batch [17800][55000]\t Training Loss 0.8544\t Accuracy 0.8429\n",
      "Epoch [16][20]\t Batch [17850][55000]\t Training Loss 0.8546\t Accuracy 0.8427\n",
      "Epoch [16][20]\t Batch [17900][55000]\t Training Loss 0.8550\t Accuracy 0.8426\n",
      "Epoch [16][20]\t Batch [17950][55000]\t Training Loss 0.8548\t Accuracy 0.8424\n",
      "Epoch [16][20]\t Batch [18000][55000]\t Training Loss 0.8544\t Accuracy 0.8426\n",
      "Epoch [16][20]\t Batch [18050][55000]\t Training Loss 0.8547\t Accuracy 0.8426\n",
      "Epoch [16][20]\t Batch [18100][55000]\t Training Loss 0.8545\t Accuracy 0.8427\n",
      "Epoch [16][20]\t Batch [18150][55000]\t Training Loss 0.8541\t Accuracy 0.8428\n",
      "Epoch [16][20]\t Batch [18200][55000]\t Training Loss 0.8537\t Accuracy 0.8430\n",
      "Epoch [16][20]\t Batch [18250][55000]\t Training Loss 0.8537\t Accuracy 0.8431\n",
      "Epoch [16][20]\t Batch [18300][55000]\t Training Loss 0.8532\t Accuracy 0.8432\n",
      "Epoch [16][20]\t Batch [18350][55000]\t Training Loss 0.8531\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [18400][55000]\t Training Loss 0.8531\t Accuracy 0.8434\n",
      "Epoch [16][20]\t Batch [18450][55000]\t Training Loss 0.8535\t Accuracy 0.8433\n",
      "Epoch [16][20]\t Batch [18500][55000]\t Training Loss 0.8536\t Accuracy 0.8433\n",
      "Epoch [16][20]\t Batch [18550][55000]\t Training Loss 0.8533\t Accuracy 0.8435\n",
      "Epoch [16][20]\t Batch [18600][55000]\t Training Loss 0.8533\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [18650][55000]\t Training Loss 0.8532\t Accuracy 0.8439\n",
      "Epoch [16][20]\t Batch [18700][55000]\t Training Loss 0.8534\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [18750][55000]\t Training Loss 0.8536\t Accuracy 0.8438\n",
      "Epoch [16][20]\t Batch [18800][55000]\t Training Loss 0.8532\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [18850][55000]\t Training Loss 0.8535\t Accuracy 0.8441\n",
      "Epoch [16][20]\t Batch [18900][55000]\t Training Loss 0.8530\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [18950][55000]\t Training Loss 0.8528\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [19000][55000]\t Training Loss 0.8527\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [19050][55000]\t Training Loss 0.8531\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [19100][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [16][20]\t Batch [19150][55000]\t Training Loss 0.8535\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [19200][55000]\t Training Loss 0.8536\t Accuracy 0.8441\n",
      "Epoch [16][20]\t Batch [19250][55000]\t Training Loss 0.8534\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [19300][55000]\t Training Loss 0.8533\t Accuracy 0.8443\n",
      "Epoch [16][20]\t Batch [19350][55000]\t Training Loss 0.8533\t Accuracy 0.8441\n",
      "Epoch [16][20]\t Batch [19400][55000]\t Training Loss 0.8531\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [19450][55000]\t Training Loss 0.8528\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [19500][55000]\t Training Loss 0.8525\t Accuracy 0.8443\n",
      "Epoch [16][20]\t Batch [19550][55000]\t Training Loss 0.8525\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [19600][55000]\t Training Loss 0.8525\t Accuracy 0.8440\n",
      "Epoch [16][20]\t Batch [19650][55000]\t Training Loss 0.8521\t Accuracy 0.8442\n",
      "Epoch [16][20]\t Batch [19700][55000]\t Training Loss 0.8515\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [19750][55000]\t Training Loss 0.8510\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [19800][55000]\t Training Loss 0.8504\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [19850][55000]\t Training Loss 0.8504\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [19900][55000]\t Training Loss 0.8502\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [19950][55000]\t Training Loss 0.8504\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [20000][55000]\t Training Loss 0.8503\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [20050][55000]\t Training Loss 0.8508\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [20100][55000]\t Training Loss 0.8509\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [20150][55000]\t Training Loss 0.8507\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [20200][55000]\t Training Loss 0.8511\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [20250][55000]\t Training Loss 0.8512\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [20300][55000]\t Training Loss 0.8513\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [20350][55000]\t Training Loss 0.8513\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [20400][55000]\t Training Loss 0.8510\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [20450][55000]\t Training Loss 0.8506\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [20500][55000]\t Training Loss 0.8504\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [20550][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [20600][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [20650][55000]\t Training Loss 0.8499\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [20700][55000]\t Training Loss 0.8497\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [20750][55000]\t Training Loss 0.8497\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [20800][55000]\t Training Loss 0.8498\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [20850][55000]\t Training Loss 0.8497\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [20900][55000]\t Training Loss 0.8500\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [20950][55000]\t Training Loss 0.8505\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [21000][55000]\t Training Loss 0.8507\t Accuracy 0.8443\n",
      "Epoch [16][20]\t Batch [21050][55000]\t Training Loss 0.8509\t Accuracy 0.8443\n",
      "Epoch [16][20]\t Batch [21100][55000]\t Training Loss 0.8507\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [21150][55000]\t Training Loss 0.8507\t Accuracy 0.8445\n",
      "Epoch [16][20]\t Batch [21200][55000]\t Training Loss 0.8504\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [21250][55000]\t Training Loss 0.8502\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [21300][55000]\t Training Loss 0.8498\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [21350][55000]\t Training Loss 0.8499\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [21400][55000]\t Training Loss 0.8499\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [21450][55000]\t Training Loss 0.8500\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [21500][55000]\t Training Loss 0.8496\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [21550][55000]\t Training Loss 0.8494\t Accuracy 0.8456\n",
      "Epoch [16][20]\t Batch [21600][55000]\t Training Loss 0.8495\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [21650][55000]\t Training Loss 0.8496\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [21700][55000]\t Training Loss 0.8495\t Accuracy 0.8456\n",
      "Epoch [16][20]\t Batch [21750][55000]\t Training Loss 0.8495\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [21800][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [21850][55000]\t Training Loss 0.8485\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [21900][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [21950][55000]\t Training Loss 0.8476\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [22000][55000]\t Training Loss 0.8472\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [22050][55000]\t Training Loss 0.8469\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [22100][55000]\t Training Loss 0.8470\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [22150][55000]\t Training Loss 0.8474\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [22200][55000]\t Training Loss 0.8477\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [22250][55000]\t Training Loss 0.8477\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [22300][55000]\t Training Loss 0.8479\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [22350][55000]\t Training Loss 0.8476\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [22400][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [22450][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [22500][55000]\t Training Loss 0.8480\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [22550][55000]\t Training Loss 0.8486\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [22600][55000]\t Training Loss 0.8490\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [22650][55000]\t Training Loss 0.8492\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [22700][55000]\t Training Loss 0.8491\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [22750][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [22800][55000]\t Training Loss 0.8489\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [22850][55000]\t Training Loss 0.8489\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [22900][55000]\t Training Loss 0.8486\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [22950][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [23000][55000]\t Training Loss 0.8482\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [23050][55000]\t Training Loss 0.8480\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [23100][55000]\t Training Loss 0.8481\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [23150][55000]\t Training Loss 0.8480\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [23200][55000]\t Training Loss 0.8481\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [23250][55000]\t Training Loss 0.8478\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [23300][55000]\t Training Loss 0.8475\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [23350][55000]\t Training Loss 0.8473\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [23400][55000]\t Training Loss 0.8470\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [23450][55000]\t Training Loss 0.8470\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [23500][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [23550][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [23600][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [23650][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [23700][55000]\t Training Loss 0.8470\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [23750][55000]\t Training Loss 0.8476\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [23800][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [23850][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [23900][55000]\t Training Loss 0.8474\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [23950][55000]\t Training Loss 0.8474\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [24000][55000]\t Training Loss 0.8474\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [24050][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [24100][55000]\t Training Loss 0.8472\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [24150][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [24200][55000]\t Training Loss 0.8469\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [24250][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [24300][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [24350][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [24400][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [24450][55000]\t Training Loss 0.8472\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [24500][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [24550][55000]\t Training Loss 0.8473\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [24600][55000]\t Training Loss 0.8473\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [24650][55000]\t Training Loss 0.8474\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [24700][55000]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [24750][55000]\t Training Loss 0.8476\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [24800][55000]\t Training Loss 0.8480\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [24850][55000]\t Training Loss 0.8478\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [24900][55000]\t Training Loss 0.8479\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [24950][55000]\t Training Loss 0.8482\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [25000][55000]\t Training Loss 0.8485\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [25050][55000]\t Training Loss 0.8482\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [25100][55000]\t Training Loss 0.8481\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [25150][55000]\t Training Loss 0.8479\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [25200][55000]\t Training Loss 0.8478\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [25250][55000]\t Training Loss 0.8477\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [25300][55000]\t Training Loss 0.8477\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [25350][55000]\t Training Loss 0.8478\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [25400][55000]\t Training Loss 0.8474\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [25450][55000]\t Training Loss 0.8469\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [25500][55000]\t Training Loss 0.8467\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [25550][55000]\t Training Loss 0.8463\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [25600][55000]\t Training Loss 0.8463\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [25650][55000]\t Training Loss 0.8461\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [25700][55000]\t Training Loss 0.8460\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [25750][55000]\t Training Loss 0.8457\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [25800][55000]\t Training Loss 0.8457\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [25850][55000]\t Training Loss 0.8458\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [25900][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [25950][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [26000][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [26050][55000]\t Training Loss 0.8456\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [26100][55000]\t Training Loss 0.8454\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [26150][55000]\t Training Loss 0.8452\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [26200][55000]\t Training Loss 0.8450\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [26250][55000]\t Training Loss 0.8449\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [26300][55000]\t Training Loss 0.8451\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [26350][55000]\t Training Loss 0.8450\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [26400][55000]\t Training Loss 0.8454\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [26450][55000]\t Training Loss 0.8456\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [26500][55000]\t Training Loss 0.8457\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [26550][55000]\t Training Loss 0.8459\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [26600][55000]\t Training Loss 0.8461\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [26650][55000]\t Training Loss 0.8465\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [26700][55000]\t Training Loss 0.8464\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [26750][55000]\t Training Loss 0.8466\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [26800][55000]\t Training Loss 0.8465\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [26850][55000]\t Training Loss 0.8464\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [26900][55000]\t Training Loss 0.8467\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [26950][55000]\t Training Loss 0.8466\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [27000][55000]\t Training Loss 0.8464\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [27050][55000]\t Training Loss 0.8461\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [27100][55000]\t Training Loss 0.8460\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [27150][55000]\t Training Loss 0.8460\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [27200][55000]\t Training Loss 0.8463\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [27250][55000]\t Training Loss 0.8463\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [27300][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [27350][55000]\t Training Loss 0.8462\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [27400][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [27450][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [27500][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [27550][55000]\t Training Loss 0.8462\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [27600][55000]\t Training Loss 0.8460\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [27650][55000]\t Training Loss 0.8461\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [27700][55000]\t Training Loss 0.8461\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [27750][55000]\t Training Loss 0.8462\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [27800][55000]\t Training Loss 0.8463\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [27850][55000]\t Training Loss 0.8463\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [27900][55000]\t Training Loss 0.8462\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [27950][55000]\t Training Loss 0.8459\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [28000][55000]\t Training Loss 0.8457\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [28050][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [28100][55000]\t Training Loss 0.8450\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [28150][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [28200][55000]\t Training Loss 0.8450\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [28250][55000]\t Training Loss 0.8445\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [28300][55000]\t Training Loss 0.8445\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [28350][55000]\t Training Loss 0.8442\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [28400][55000]\t Training Loss 0.8447\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [28450][55000]\t Training Loss 0.8444\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [28500][55000]\t Training Loss 0.8442\t Accuracy 0.8482\n",
      "Epoch [16][20]\t Batch [28550][55000]\t Training Loss 0.8441\t Accuracy 0.8482\n",
      "Epoch [16][20]\t Batch [28600][55000]\t Training Loss 0.8440\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [28650][55000]\t Training Loss 0.8443\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [28700][55000]\t Training Loss 0.8445\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [28750][55000]\t Training Loss 0.8446\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [28800][55000]\t Training Loss 0.8446\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [28850][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [16][20]\t Batch [28900][55000]\t Training Loss 0.8444\t Accuracy 0.8482\n",
      "Epoch [16][20]\t Batch [28950][55000]\t Training Loss 0.8444\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [29000][55000]\t Training Loss 0.8443\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [29050][55000]\t Training Loss 0.8443\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [29100][55000]\t Training Loss 0.8444\t Accuracy 0.8481\n",
      "Epoch [16][20]\t Batch [29150][55000]\t Training Loss 0.8448\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [29200][55000]\t Training Loss 0.8452\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [29250][55000]\t Training Loss 0.8454\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [29300][55000]\t Training Loss 0.8452\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [29350][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [29400][55000]\t Training Loss 0.8453\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [29450][55000]\t Training Loss 0.8449\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [29500][55000]\t Training Loss 0.8447\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [29550][55000]\t Training Loss 0.8446\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [29600][55000]\t Training Loss 0.8446\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [29650][55000]\t Training Loss 0.8445\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [29700][55000]\t Training Loss 0.8446\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [29750][55000]\t Training Loss 0.8450\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [29800][55000]\t Training Loss 0.8451\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [29850][55000]\t Training Loss 0.8454\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [29900][55000]\t Training Loss 0.8458\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [29950][55000]\t Training Loss 0.8460\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [30000][55000]\t Training Loss 0.8464\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [30050][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [30100][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [30150][55000]\t Training Loss 0.8474\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [30200][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [30250][55000]\t Training Loss 0.8477\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [30300][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [30350][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [30400][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [30450][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [30500][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [30550][55000]\t Training Loss 0.8480\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [30600][55000]\t Training Loss 0.8480\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [30650][55000]\t Training Loss 0.8484\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [30700][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [30750][55000]\t Training Loss 0.8488\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [30800][55000]\t Training Loss 0.8490\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [30850][55000]\t Training Loss 0.8489\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [30900][55000]\t Training Loss 0.8493\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [30950][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [31000][55000]\t Training Loss 0.8492\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [31050][55000]\t Training Loss 0.8493\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [31100][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [31150][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [31200][55000]\t Training Loss 0.8494\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [31250][55000]\t Training Loss 0.8493\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [31300][55000]\t Training Loss 0.8496\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [31350][55000]\t Training Loss 0.8503\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [31400][55000]\t Training Loss 0.8504\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [31450][55000]\t Training Loss 0.8507\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [31500][55000]\t Training Loss 0.8508\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [31550][55000]\t Training Loss 0.8508\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [31600][55000]\t Training Loss 0.8510\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [31650][55000]\t Training Loss 0.8511\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [31700][55000]\t Training Loss 0.8514\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [31750][55000]\t Training Loss 0.8518\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [31800][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [31850][55000]\t Training Loss 0.8519\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [31900][55000]\t Training Loss 0.8519\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [31950][55000]\t Training Loss 0.8518\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [32000][55000]\t Training Loss 0.8518\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [32050][55000]\t Training Loss 0.8518\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [32100][55000]\t Training Loss 0.8517\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [32150][55000]\t Training Loss 0.8520\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [32200][55000]\t Training Loss 0.8521\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [32250][55000]\t Training Loss 0.8524\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [32300][55000]\t Training Loss 0.8528\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [32350][55000]\t Training Loss 0.8531\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [32400][55000]\t Training Loss 0.8532\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [32450][55000]\t Training Loss 0.8534\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [32500][55000]\t Training Loss 0.8536\t Accuracy 0.8456\n",
      "Epoch [16][20]\t Batch [32550][55000]\t Training Loss 0.8539\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [32600][55000]\t Training Loss 0.8538\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [32650][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [32700][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [32750][55000]\t Training Loss 0.8537\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [32800][55000]\t Training Loss 0.8538\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [32850][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [32900][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [32950][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [33000][55000]\t Training Loss 0.8535\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [33050][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [33100][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [33150][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [33200][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [33250][55000]\t Training Loss 0.8537\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [33300][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [33350][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [33400][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [33450][55000]\t Training Loss 0.8539\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [33500][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [33550][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [33600][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [33650][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [33700][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [33750][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [33800][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [33850][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [33900][55000]\t Training Loss 0.8530\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [33950][55000]\t Training Loss 0.8526\t Accuracy 0.8465\n",
      "Epoch [16][20]\t Batch [34000][55000]\t Training Loss 0.8526\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [34050][55000]\t Training Loss 0.8528\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [34100][55000]\t Training Loss 0.8529\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [34150][55000]\t Training Loss 0.8529\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [34200][55000]\t Training Loss 0.8527\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [34250][55000]\t Training Loss 0.8525\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [34300][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [34350][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [34400][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [34450][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [34500][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [34550][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [34600][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [34650][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [34700][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [34750][55000]\t Training Loss 0.8526\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [34800][55000]\t Training Loss 0.8526\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [34850][55000]\t Training Loss 0.8530\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [34900][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [34950][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [35000][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [35050][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [35100][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [35150][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [35200][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [35250][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [35300][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [35350][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [35400][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [35450][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [35500][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [35550][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [35600][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [35650][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [35700][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [35750][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [35800][55000]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [35850][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [35900][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [35950][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [36000][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [36050][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [36100][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [36150][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [36200][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [36250][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [36300][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [36350][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [36400][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [36450][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [36500][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [36550][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [36600][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [36650][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [36700][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [36750][55000]\t Training Loss 0.8516\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [36800][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [36850][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [36900][55000]\t Training Loss 0.8515\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [36950][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [37000][55000]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [37050][55000]\t Training Loss 0.8511\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [37100][55000]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [37150][55000]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [16][20]\t Batch [37200][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [37250][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [37300][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [16][20]\t Batch [37350][55000]\t Training Loss 0.8511\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [37400][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [37450][55000]\t Training Loss 0.8517\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [37500][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [37550][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [37600][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [37650][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [37700][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [37750][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [37800][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [37850][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [37900][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [37950][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [38000][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [38050][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [38100][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [38150][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [38200][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [38250][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [38300][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [38350][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [38400][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [38450][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [38500][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [38550][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [38600][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [38650][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [38700][55000]\t Training Loss 0.8526\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [38750][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [38800][55000]\t Training Loss 0.8525\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [38850][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [38900][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [38950][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [39000][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [39050][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [39100][55000]\t Training Loss 0.8516\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [39150][55000]\t Training Loss 0.8514\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [39200][55000]\t Training Loss 0.8512\t Accuracy 0.8476\n",
      "Epoch [16][20]\t Batch [39250][55000]\t Training Loss 0.8511\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [39300][55000]\t Training Loss 0.8511\t Accuracy 0.8477\n",
      "Epoch [16][20]\t Batch [39350][55000]\t Training Loss 0.8514\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [39400][55000]\t Training Loss 0.8514\t Accuracy 0.8475\n",
      "Epoch [16][20]\t Batch [39450][55000]\t Training Loss 0.8517\t Accuracy 0.8474\n",
      "Epoch [16][20]\t Batch [39500][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [39550][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [39600][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [39650][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [39700][55000]\t Training Loss 0.8517\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [39750][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [39800][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [39850][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [39900][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [39950][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40000][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40050][55000]\t Training Loss 0.8525\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [40100][55000]\t Training Loss 0.8525\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [40150][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40200][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40250][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40300][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40350][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40400][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40450][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40500][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40550][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40600][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40650][55000]\t Training Loss 0.8522\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [40700][55000]\t Training Loss 0.8522\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [40750][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40800][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [40850][55000]\t Training Loss 0.8520\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [40900][55000]\t Training Loss 0.8519\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [40950][55000]\t Training Loss 0.8517\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [41000][55000]\t Training Loss 0.8517\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [41050][55000]\t Training Loss 0.8518\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [41100][55000]\t Training Loss 0.8519\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [41150][55000]\t Training Loss 0.8520\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [41200][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [41250][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [41300][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [16][20]\t Batch [41350][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [41400][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [41450][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [41500][55000]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [41550][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [41600][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [41650][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [41700][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [16][20]\t Batch [41750][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [41800][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [41850][55000]\t Training Loss 0.8534\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [41900][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [41950][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [16][20]\t Batch [42000][55000]\t Training Loss 0.8533\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [42050][55000]\t Training Loss 0.8532\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [42100][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [16][20]\t Batch [42150][55000]\t Training Loss 0.8532\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [42200][55000]\t Training Loss 0.8532\t Accuracy 0.8467\n",
      "Epoch [16][20]\t Batch [42250][55000]\t Training Loss 0.8533\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [42300][55000]\t Training Loss 0.8533\t Accuracy 0.8466\n",
      "Epoch [16][20]\t Batch [42350][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [42400][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [42450][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [42500][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [42550][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [42600][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [42650][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [42700][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [42750][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [42800][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [42850][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [42900][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [42950][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [43000][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [43050][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [43100][55000]\t Training Loss 0.8544\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [43150][55000]\t Training Loss 0.8544\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [43200][55000]\t Training Loss 0.8543\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [43250][55000]\t Training Loss 0.8543\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [43300][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [43350][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [43400][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [43450][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [43500][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [43550][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [43600][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [43650][55000]\t Training Loss 0.8533\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [43700][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [43750][55000]\t Training Loss 0.8532\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [43800][55000]\t Training Loss 0.8532\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [43850][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [43900][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [43950][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [44000][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [44050][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [44100][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [44150][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [44200][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [44250][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [44300][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [44350][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [44400][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [44450][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [44500][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [44550][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [44600][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [44650][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [44700][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [44750][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [44800][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [16][20]\t Batch [44850][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [16][20]\t Batch [44900][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [44950][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45000][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45050][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [45100][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45150][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [45200][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [45250][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45300][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45350][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [45400][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [45450][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [45500][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [45550][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45600][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45650][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [45700][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [45750][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45800][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45850][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [45900][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [45950][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [46000][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [46050][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [46100][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [46150][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46200][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46250][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46300][55000]\t Training Loss 0.8549\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [46350][55000]\t Training Loss 0.8550\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [46400][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [46450][55000]\t Training Loss 0.8553\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [46500][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [46550][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46600][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46650][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46700][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46750][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46800][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [46850][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [46900][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [46950][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47000][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47050][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47100][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [47150][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [47200][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [47250][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [47300][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47350][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47400][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47450][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47500][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [47550][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [47600][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47650][55000]\t Training Loss 0.8547\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [47700][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [47750][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [47800][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [47850][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47900][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [47950][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [48000][55000]\t Training Loss 0.8544\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [48050][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [48100][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [48150][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [48200][55000]\t Training Loss 0.8538\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [48250][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [48300][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [48350][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [48400][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [48450][55000]\t Training Loss 0.8533\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [48500][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [48550][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [48600][55000]\t Training Loss 0.8532\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [48650][55000]\t Training Loss 0.8531\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [48700][55000]\t Training Loss 0.8530\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [48750][55000]\t Training Loss 0.8528\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [48800][55000]\t Training Loss 0.8527\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [48850][55000]\t Training Loss 0.8526\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [48900][55000]\t Training Loss 0.8525\t Accuracy 0.8461\n",
      "Epoch [16][20]\t Batch [48950][55000]\t Training Loss 0.8527\t Accuracy 0.8460\n",
      "Epoch [16][20]\t Batch [49000][55000]\t Training Loss 0.8529\t Accuracy 0.8459\n",
      "Epoch [16][20]\t Batch [49050][55000]\t Training Loss 0.8532\t Accuracy 0.8458\n",
      "Epoch [16][20]\t Batch [49100][55000]\t Training Loss 0.8533\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [49150][55000]\t Training Loss 0.8534\t Accuracy 0.8457\n",
      "Epoch [16][20]\t Batch [49200][55000]\t Training Loss 0.8534\t Accuracy 0.8456\n",
      "Epoch [16][20]\t Batch [49250][55000]\t Training Loss 0.8536\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [49300][55000]\t Training Loss 0.8535\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [49350][55000]\t Training Loss 0.8533\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [49400][55000]\t Training Loss 0.8532\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [49450][55000]\t Training Loss 0.8531\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [49500][55000]\t Training Loss 0.8534\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [49550][55000]\t Training Loss 0.8537\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [49600][55000]\t Training Loss 0.8540\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [49650][55000]\t Training Loss 0.8540\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [49700][55000]\t Training Loss 0.8543\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [49750][55000]\t Training Loss 0.8543\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [49800][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [49850][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [49900][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [49950][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [50000][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [50050][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [50100][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [50150][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [50200][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [50250][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [50300][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [50350][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [50400][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [50450][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [50500][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [50550][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [50600][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [50650][55000]\t Training Loss 0.8554\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [50700][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [50750][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [50800][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [50850][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [50900][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [50950][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [51000][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [51050][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [51100][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [51150][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [51200][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [51250][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [51300][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [51350][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [51400][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [51450][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [51500][55000]\t Training Loss 0.8547\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [51550][55000]\t Training Loss 0.8546\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [51600][55000]\t Training Loss 0.8545\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [51650][55000]\t Training Loss 0.8544\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [51700][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [51750][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [51800][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [51850][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [51900][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [51950][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [52000][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [52050][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [52100][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [52150][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [52200][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [52250][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [52300][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [52350][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [52400][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [52450][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [52500][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [52550][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [52600][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [52650][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [52700][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [16][20]\t Batch [52750][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [52800][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [16][20]\t Batch [52850][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [52900][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [52950][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [53000][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53050][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [53100][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53150][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53200][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53250][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53300][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [53350][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53400][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53450][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [53500][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53550][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53600][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [53650][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [53700][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [53750][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53800][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53850][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53900][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [53950][55000]\t Training Loss 0.8549\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [54000][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [54050][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [54100][55000]\t Training Loss 0.8553\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [54150][55000]\t Training Loss 0.8552\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [54200][55000]\t Training Loss 0.8552\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [54250][55000]\t Training Loss 0.8551\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [54300][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [54350][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [54400][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [54450][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [54500][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [54550][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [54600][55000]\t Training Loss 0.8547\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [54650][55000]\t Training Loss 0.8546\t Accuracy 0.8447\n",
      "Epoch [16][20]\t Batch [54700][55000]\t Training Loss 0.8544\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [54750][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [54800][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [54850][55000]\t Training Loss 0.8541\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [54900][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [54950][55000]\t Training Loss 0.8546\t Accuracy 0.8448\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8548\t Average training accuracy 0.8447\n",
      "Epoch [16]\t Average validation loss 0.7921\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [17][20]\t Batch [0][55000]\t Training Loss 1.4698\t Accuracy 0.0000\n",
      "Epoch [17][20]\t Batch [50][55000]\t Training Loss 0.9381\t Accuracy 0.7255\n",
      "Epoch [17][20]\t Batch [100][55000]\t Training Loss 0.8488\t Accuracy 0.8020\n",
      "Epoch [17][20]\t Batch [150][55000]\t Training Loss 0.8502\t Accuracy 0.8146\n",
      "Epoch [17][20]\t Batch [200][55000]\t Training Loss 0.8611\t Accuracy 0.8159\n",
      "Epoch [17][20]\t Batch [250][55000]\t Training Loss 0.8432\t Accuracy 0.8247\n",
      "Epoch [17][20]\t Batch [300][55000]\t Training Loss 0.8448\t Accuracy 0.8206\n",
      "Epoch [17][20]\t Batch [350][55000]\t Training Loss 0.8250\t Accuracy 0.8234\n",
      "Epoch [17][20]\t Batch [400][55000]\t Training Loss 0.8111\t Accuracy 0.8279\n",
      "Epoch [17][20]\t Batch [450][55000]\t Training Loss 0.8170\t Accuracy 0.8293\n",
      "Epoch [17][20]\t Batch [500][55000]\t Training Loss 0.8234\t Accuracy 0.8303\n",
      "Epoch [17][20]\t Batch [550][55000]\t Training Loss 0.8407\t Accuracy 0.8276\n",
      "Epoch [17][20]\t Batch [600][55000]\t Training Loss 0.8451\t Accuracy 0.8303\n",
      "Epoch [17][20]\t Batch [650][55000]\t Training Loss 0.8649\t Accuracy 0.8203\n",
      "Epoch [17][20]\t Batch [700][55000]\t Training Loss 0.8626\t Accuracy 0.8260\n",
      "Epoch [17][20]\t Batch [750][55000]\t Training Loss 0.8589\t Accuracy 0.8269\n",
      "Epoch [17][20]\t Batch [800][55000]\t Training Loss 0.8529\t Accuracy 0.8290\n",
      "Epoch [17][20]\t Batch [850][55000]\t Training Loss 0.8514\t Accuracy 0.8308\n",
      "Epoch [17][20]\t Batch [900][55000]\t Training Loss 0.8578\t Accuracy 0.8302\n",
      "Epoch [17][20]\t Batch [950][55000]\t Training Loss 0.8646\t Accuracy 0.8254\n",
      "Epoch [17][20]\t Batch [1000][55000]\t Training Loss 0.8640\t Accuracy 0.8272\n",
      "Epoch [17][20]\t Batch [1050][55000]\t Training Loss 0.8717\t Accuracy 0.8259\n",
      "Epoch [17][20]\t Batch [1100][55000]\t Training Loss 0.8802\t Accuracy 0.8256\n",
      "Epoch [17][20]\t Batch [1150][55000]\t Training Loss 0.8894\t Accuracy 0.8228\n",
      "Epoch [17][20]\t Batch [1200][55000]\t Training Loss 0.8836\t Accuracy 0.8260\n",
      "Epoch [17][20]\t Batch [1250][55000]\t Training Loss 0.8850\t Accuracy 0.8257\n",
      "Epoch [17][20]\t Batch [1300][55000]\t Training Loss 0.8852\t Accuracy 0.8255\n",
      "Epoch [17][20]\t Batch [1350][55000]\t Training Loss 0.8820\t Accuracy 0.8268\n",
      "Epoch [17][20]\t Batch [1400][55000]\t Training Loss 0.8826\t Accuracy 0.8258\n",
      "Epoch [17][20]\t Batch [1450][55000]\t Training Loss 0.8835\t Accuracy 0.8249\n",
      "Epoch [17][20]\t Batch [1500][55000]\t Training Loss 0.8815\t Accuracy 0.8274\n",
      "Epoch [17][20]\t Batch [1550][55000]\t Training Loss 0.8810\t Accuracy 0.8279\n",
      "Epoch [17][20]\t Batch [1600][55000]\t Training Loss 0.8836\t Accuracy 0.8264\n",
      "Epoch [17][20]\t Batch [1650][55000]\t Training Loss 0.8805\t Accuracy 0.8280\n",
      "Epoch [17][20]\t Batch [1700][55000]\t Training Loss 0.8788\t Accuracy 0.8295\n",
      "Epoch [17][20]\t Batch [1750][55000]\t Training Loss 0.8732\t Accuracy 0.8310\n",
      "Epoch [17][20]\t Batch [1800][55000]\t Training Loss 0.8710\t Accuracy 0.8329\n",
      "Epoch [17][20]\t Batch [1850][55000]\t Training Loss 0.8693\t Accuracy 0.8336\n",
      "Epoch [17][20]\t Batch [1900][55000]\t Training Loss 0.8671\t Accuracy 0.8338\n",
      "Epoch [17][20]\t Batch [1950][55000]\t Training Loss 0.8657\t Accuracy 0.8350\n",
      "Epoch [17][20]\t Batch [2000][55000]\t Training Loss 0.8632\t Accuracy 0.8361\n",
      "Epoch [17][20]\t Batch [2050][55000]\t Training Loss 0.8623\t Accuracy 0.8367\n",
      "Epoch [17][20]\t Batch [2100][55000]\t Training Loss 0.8594\t Accuracy 0.8372\n",
      "Epoch [17][20]\t Batch [2150][55000]\t Training Loss 0.8561\t Accuracy 0.8391\n",
      "Epoch [17][20]\t Batch [2200][55000]\t Training Loss 0.8513\t Accuracy 0.8410\n",
      "Epoch [17][20]\t Batch [2250][55000]\t Training Loss 0.8506\t Accuracy 0.8414\n",
      "Epoch [17][20]\t Batch [2300][55000]\t Training Loss 0.8476\t Accuracy 0.8409\n",
      "Epoch [17][20]\t Batch [2350][55000]\t Training Loss 0.8461\t Accuracy 0.8413\n",
      "Epoch [17][20]\t Batch [2400][55000]\t Training Loss 0.8471\t Accuracy 0.8397\n",
      "Epoch [17][20]\t Batch [2450][55000]\t Training Loss 0.8499\t Accuracy 0.8384\n",
      "Epoch [17][20]\t Batch [2500][55000]\t Training Loss 0.8478\t Accuracy 0.8405\n",
      "Epoch [17][20]\t Batch [2550][55000]\t Training Loss 0.8465\t Accuracy 0.8405\n",
      "Epoch [17][20]\t Batch [2600][55000]\t Training Loss 0.8459\t Accuracy 0.8393\n",
      "Epoch [17][20]\t Batch [2650][55000]\t Training Loss 0.8447\t Accuracy 0.8397\n",
      "Epoch [17][20]\t Batch [2700][55000]\t Training Loss 0.8445\t Accuracy 0.8401\n",
      "Epoch [17][20]\t Batch [2750][55000]\t Training Loss 0.8444\t Accuracy 0.8401\n",
      "Epoch [17][20]\t Batch [2800][55000]\t Training Loss 0.8447\t Accuracy 0.8383\n",
      "Epoch [17][20]\t Batch [2850][55000]\t Training Loss 0.8436\t Accuracy 0.8383\n",
      "Epoch [17][20]\t Batch [2900][55000]\t Training Loss 0.8403\t Accuracy 0.8394\n",
      "Epoch [17][20]\t Batch [2950][55000]\t Training Loss 0.8404\t Accuracy 0.8390\n",
      "Epoch [17][20]\t Batch [3000][55000]\t Training Loss 0.8402\t Accuracy 0.8397\n",
      "Epoch [17][20]\t Batch [3050][55000]\t Training Loss 0.8413\t Accuracy 0.8394\n",
      "Epoch [17][20]\t Batch [3100][55000]\t Training Loss 0.8436\t Accuracy 0.8391\n",
      "Epoch [17][20]\t Batch [3150][55000]\t Training Loss 0.8435\t Accuracy 0.8394\n",
      "Epoch [17][20]\t Batch [3200][55000]\t Training Loss 0.8433\t Accuracy 0.8407\n",
      "Epoch [17][20]\t Batch [3250][55000]\t Training Loss 0.8427\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [3300][55000]\t Training Loss 0.8440\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [3350][55000]\t Training Loss 0.8422\t Accuracy 0.8415\n",
      "Epoch [17][20]\t Batch [3400][55000]\t Training Loss 0.8445\t Accuracy 0.8403\n",
      "Epoch [17][20]\t Batch [3450][55000]\t Training Loss 0.8442\t Accuracy 0.8412\n",
      "Epoch [17][20]\t Batch [3500][55000]\t Training Loss 0.8442\t Accuracy 0.8409\n",
      "Epoch [17][20]\t Batch [3550][55000]\t Training Loss 0.8458\t Accuracy 0.8403\n",
      "Epoch [17][20]\t Batch [3600][55000]\t Training Loss 0.8456\t Accuracy 0.8398\n",
      "Epoch [17][20]\t Batch [3650][55000]\t Training Loss 0.8442\t Accuracy 0.8406\n",
      "Epoch [17][20]\t Batch [3700][55000]\t Training Loss 0.8450\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [3750][55000]\t Training Loss 0.8449\t Accuracy 0.8403\n",
      "Epoch [17][20]\t Batch [3800][55000]\t Training Loss 0.8452\t Accuracy 0.8403\n",
      "Epoch [17][20]\t Batch [3850][55000]\t Training Loss 0.8448\t Accuracy 0.8406\n",
      "Epoch [17][20]\t Batch [3900][55000]\t Training Loss 0.8437\t Accuracy 0.8418\n",
      "Epoch [17][20]\t Batch [3950][55000]\t Training Loss 0.8425\t Accuracy 0.8431\n",
      "Epoch [17][20]\t Batch [4000][55000]\t Training Loss 0.8422\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [4050][55000]\t Training Loss 0.8406\t Accuracy 0.8445\n",
      "Epoch [17][20]\t Batch [4100][55000]\t Training Loss 0.8411\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [4150][55000]\t Training Loss 0.8416\t Accuracy 0.8439\n",
      "Epoch [17][20]\t Batch [4200][55000]\t Training Loss 0.8421\t Accuracy 0.8429\n",
      "Epoch [17][20]\t Batch [4250][55000]\t Training Loss 0.8406\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [4300][55000]\t Training Loss 0.8407\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [4350][55000]\t Training Loss 0.8412\t Accuracy 0.8444\n",
      "Epoch [17][20]\t Batch [4400][55000]\t Training Loss 0.8411\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [4450][55000]\t Training Loss 0.8413\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [4500][55000]\t Training Loss 0.8415\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [4550][55000]\t Training Loss 0.8398\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [4600][55000]\t Training Loss 0.8381\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [4650][55000]\t Training Loss 0.8380\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [4700][55000]\t Training Loss 0.8376\t Accuracy 0.8445\n",
      "Epoch [17][20]\t Batch [4750][55000]\t Training Loss 0.8364\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [4800][55000]\t Training Loss 0.8371\t Accuracy 0.8454\n",
      "Epoch [17][20]\t Batch [4850][55000]\t Training Loss 0.8383\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [4900][55000]\t Training Loss 0.8368\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [4950][55000]\t Training Loss 0.8371\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [5000][55000]\t Training Loss 0.8372\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [5050][55000]\t Training Loss 0.8367\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [5100][55000]\t Training Loss 0.8368\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [5150][55000]\t Training Loss 0.8371\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [5200][55000]\t Training Loss 0.8386\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [5250][55000]\t Training Loss 0.8376\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [5300][55000]\t Training Loss 0.8376\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [5350][55000]\t Training Loss 0.8382\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [5400][55000]\t Training Loss 0.8374\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [5450][55000]\t Training Loss 0.8367\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [5500][55000]\t Training Loss 0.8349\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [5550][55000]\t Training Loss 0.8349\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [5600][55000]\t Training Loss 0.8340\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [5650][55000]\t Training Loss 0.8346\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [5700][55000]\t Training Loss 0.8343\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [5750][55000]\t Training Loss 0.8347\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [5800][55000]\t Training Loss 0.8351\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [5850][55000]\t Training Loss 0.8352\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [5900][55000]\t Training Loss 0.8358\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [5950][55000]\t Training Loss 0.8356\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [6000][55000]\t Training Loss 0.8346\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [6050][55000]\t Training Loss 0.8332\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [6100][55000]\t Training Loss 0.8318\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [6150][55000]\t Training Loss 0.8297\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [6200][55000]\t Training Loss 0.8297\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [6250][55000]\t Training Loss 0.8286\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [6300][55000]\t Training Loss 0.8289\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [6350][55000]\t Training Loss 0.8286\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [6400][55000]\t Training Loss 0.8280\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [6450][55000]\t Training Loss 0.8275\t Accuracy 0.8486\n",
      "Epoch [17][20]\t Batch [6500][55000]\t Training Loss 0.8282\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [6550][55000]\t Training Loss 0.8273\t Accuracy 0.8484\n",
      "Epoch [17][20]\t Batch [6600][55000]\t Training Loss 0.8260\t Accuracy 0.8490\n",
      "Epoch [17][20]\t Batch [6650][55000]\t Training Loss 0.8247\t Accuracy 0.8490\n",
      "Epoch [17][20]\t Batch [6700][55000]\t Training Loss 0.8249\t Accuracy 0.8490\n",
      "Epoch [17][20]\t Batch [6750][55000]\t Training Loss 0.8251\t Accuracy 0.8498\n",
      "Epoch [17][20]\t Batch [6800][55000]\t Training Loss 0.8256\t Accuracy 0.8497\n",
      "Epoch [17][20]\t Batch [6850][55000]\t Training Loss 0.8277\t Accuracy 0.8492\n",
      "Epoch [17][20]\t Batch [6900][55000]\t Training Loss 0.8277\t Accuracy 0.8493\n",
      "Epoch [17][20]\t Batch [6950][55000]\t Training Loss 0.8283\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [7000][55000]\t Training Loss 0.8282\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [7050][55000]\t Training Loss 0.8288\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [7100][55000]\t Training Loss 0.8290\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [7150][55000]\t Training Loss 0.8292\t Accuracy 0.8481\n",
      "Epoch [17][20]\t Batch [7200][55000]\t Training Loss 0.8300\t Accuracy 0.8481\n",
      "Epoch [17][20]\t Batch [7250][55000]\t Training Loss 0.8324\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [7300][55000]\t Training Loss 0.8342\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [7350][55000]\t Training Loss 0.8357\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [7400][55000]\t Training Loss 0.8367\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [7450][55000]\t Training Loss 0.8367\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [7500][55000]\t Training Loss 0.8367\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [7550][55000]\t Training Loss 0.8372\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [7600][55000]\t Training Loss 0.8367\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [7650][55000]\t Training Loss 0.8372\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [7700][55000]\t Training Loss 0.8377\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [7750][55000]\t Training Loss 0.8378\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [7800][55000]\t Training Loss 0.8386\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [7850][55000]\t Training Loss 0.8387\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [7900][55000]\t Training Loss 0.8389\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [7950][55000]\t Training Loss 0.8389\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [8000][55000]\t Training Loss 0.8390\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [8050][55000]\t Training Loss 0.8395\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [8100][55000]\t Training Loss 0.8381\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [8150][55000]\t Training Loss 0.8389\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [8200][55000]\t Training Loss 0.8389\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [8250][55000]\t Training Loss 0.8401\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [8300][55000]\t Training Loss 0.8404\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [8350][55000]\t Training Loss 0.8410\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [8400][55000]\t Training Loss 0.8404\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [8450][55000]\t Training Loss 0.8419\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [8500][55000]\t Training Loss 0.8411\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [8550][55000]\t Training Loss 0.8401\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [8600][55000]\t Training Loss 0.8393\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [8650][55000]\t Training Loss 0.8394\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [8700][55000]\t Training Loss 0.8400\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [8750][55000]\t Training Loss 0.8416\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [8800][55000]\t Training Loss 0.8424\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [8850][55000]\t Training Loss 0.8423\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [8900][55000]\t Training Loss 0.8441\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [8950][55000]\t Training Loss 0.8437\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [9000][55000]\t Training Loss 0.8430\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [9050][55000]\t Training Loss 0.8421\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [9100][55000]\t Training Loss 0.8419\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [9150][55000]\t Training Loss 0.8424\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [9200][55000]\t Training Loss 0.8420\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [9250][55000]\t Training Loss 0.8423\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [9300][55000]\t Training Loss 0.8425\t Accuracy 0.8454\n",
      "Epoch [17][20]\t Batch [9350][55000]\t Training Loss 0.8427\t Accuracy 0.8454\n",
      "Epoch [17][20]\t Batch [9400][55000]\t Training Loss 0.8430\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [9450][55000]\t Training Loss 0.8432\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [9500][55000]\t Training Loss 0.8423\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [9550][55000]\t Training Loss 0.8421\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [9600][55000]\t Training Loss 0.8425\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [9650][55000]\t Training Loss 0.8424\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [9700][55000]\t Training Loss 0.8418\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [9750][55000]\t Training Loss 0.8410\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [9800][55000]\t Training Loss 0.8415\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [9850][55000]\t Training Loss 0.8411\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [9900][55000]\t Training Loss 0.8406\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [9950][55000]\t Training Loss 0.8400\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [10000][55000]\t Training Loss 0.8397\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [10050][55000]\t Training Loss 0.8399\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [10100][55000]\t Training Loss 0.8397\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [10150][55000]\t Training Loss 0.8396\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [10200][55000]\t Training Loss 0.8397\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [10250][55000]\t Training Loss 0.8399\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [10300][55000]\t Training Loss 0.8398\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [10350][55000]\t Training Loss 0.8388\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [10400][55000]\t Training Loss 0.8380\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [10450][55000]\t Training Loss 0.8378\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [10500][55000]\t Training Loss 0.8368\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [10550][55000]\t Training Loss 0.8364\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [10600][55000]\t Training Loss 0.8360\t Accuracy 0.8481\n",
      "Epoch [17][20]\t Batch [10650][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [10700][55000]\t Training Loss 0.8354\t Accuracy 0.8488\n",
      "Epoch [17][20]\t Batch [10750][55000]\t Training Loss 0.8360\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [10800][55000]\t Training Loss 0.8364\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [10850][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [10900][55000]\t Training Loss 0.8353\t Accuracy 0.8487\n",
      "Epoch [17][20]\t Batch [10950][55000]\t Training Loss 0.8350\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [11000][55000]\t Training Loss 0.8348\t Accuracy 0.8487\n",
      "Epoch [17][20]\t Batch [11050][55000]\t Training Loss 0.8341\t Accuracy 0.8491\n",
      "Epoch [17][20]\t Batch [11100][55000]\t Training Loss 0.8334\t Accuracy 0.8492\n",
      "Epoch [17][20]\t Batch [11150][55000]\t Training Loss 0.8334\t Accuracy 0.8494\n",
      "Epoch [17][20]\t Batch [11200][55000]\t Training Loss 0.8331\t Accuracy 0.8495\n",
      "Epoch [17][20]\t Batch [11250][55000]\t Training Loss 0.8334\t Accuracy 0.8493\n",
      "Epoch [17][20]\t Batch [11300][55000]\t Training Loss 0.8330\t Accuracy 0.8494\n",
      "Epoch [17][20]\t Batch [11350][55000]\t Training Loss 0.8325\t Accuracy 0.8494\n",
      "Epoch [17][20]\t Batch [11400][55000]\t Training Loss 0.8327\t Accuracy 0.8494\n",
      "Epoch [17][20]\t Batch [11450][55000]\t Training Loss 0.8323\t Accuracy 0.8494\n",
      "Epoch [17][20]\t Batch [11500][55000]\t Training Loss 0.8321\t Accuracy 0.8492\n",
      "Epoch [17][20]\t Batch [11550][55000]\t Training Loss 0.8322\t Accuracy 0.8491\n",
      "Epoch [17][20]\t Batch [11600][55000]\t Training Loss 0.8333\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [11650][55000]\t Training Loss 0.8339\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [11700][55000]\t Training Loss 0.8338\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [11750][55000]\t Training Loss 0.8348\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [11800][55000]\t Training Loss 0.8350\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [11850][55000]\t Training Loss 0.8351\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [11900][55000]\t Training Loss 0.8354\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [11950][55000]\t Training Loss 0.8354\t Accuracy 0.8480\n",
      "Epoch [17][20]\t Batch [12000][55000]\t Training Loss 0.8354\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [12050][55000]\t Training Loss 0.8353\t Accuracy 0.8486\n",
      "Epoch [17][20]\t Batch [12100][55000]\t Training Loss 0.8352\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [12150][55000]\t Training Loss 0.8345\t Accuracy 0.8490\n",
      "Epoch [17][20]\t Batch [12200][55000]\t Training Loss 0.8349\t Accuracy 0.8489\n",
      "Epoch [17][20]\t Batch [12250][55000]\t Training Loss 0.8347\t Accuracy 0.8490\n",
      "Epoch [17][20]\t Batch [12300][55000]\t Training Loss 0.8348\t Accuracy 0.8490\n",
      "Epoch [17][20]\t Batch [12350][55000]\t Training Loss 0.8350\t Accuracy 0.8491\n",
      "Epoch [17][20]\t Batch [12400][55000]\t Training Loss 0.8353\t Accuracy 0.8491\n",
      "Epoch [17][20]\t Batch [12450][55000]\t Training Loss 0.8355\t Accuracy 0.8488\n",
      "Epoch [17][20]\t Batch [12500][55000]\t Training Loss 0.8357\t Accuracy 0.8484\n",
      "Epoch [17][20]\t Batch [12550][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [12600][55000]\t Training Loss 0.8364\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [12650][55000]\t Training Loss 0.8369\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [12700][55000]\t Training Loss 0.8376\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [12750][55000]\t Training Loss 0.8372\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [12800][55000]\t Training Loss 0.8377\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [12850][55000]\t Training Loss 0.8380\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [12900][55000]\t Training Loss 0.8379\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [12950][55000]\t Training Loss 0.8384\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [13000][55000]\t Training Loss 0.8384\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [13050][55000]\t Training Loss 0.8391\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [13100][55000]\t Training Loss 0.8397\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [13150][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [13200][55000]\t Training Loss 0.8402\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [13250][55000]\t Training Loss 0.8397\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [13300][55000]\t Training Loss 0.8397\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [13350][55000]\t Training Loss 0.8401\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [13400][55000]\t Training Loss 0.8403\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [13450][55000]\t Training Loss 0.8398\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [13500][55000]\t Training Loss 0.8395\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [13550][55000]\t Training Loss 0.8390\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [13600][55000]\t Training Loss 0.8382\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [13650][55000]\t Training Loss 0.8380\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [13700][55000]\t Training Loss 0.8389\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [13750][55000]\t Training Loss 0.8395\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [13800][55000]\t Training Loss 0.8396\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [13850][55000]\t Training Loss 0.8396\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [13900][55000]\t Training Loss 0.8398\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [13950][55000]\t Training Loss 0.8401\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [14000][55000]\t Training Loss 0.8409\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [14050][55000]\t Training Loss 0.8410\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [14100][55000]\t Training Loss 0.8411\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [14150][55000]\t Training Loss 0.8413\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [14200][55000]\t Training Loss 0.8412\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [14250][55000]\t Training Loss 0.8415\t Accuracy 0.8454\n",
      "Epoch [17][20]\t Batch [14300][55000]\t Training Loss 0.8418\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [14350][55000]\t Training Loss 0.8422\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [14400][55000]\t Training Loss 0.8431\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [14450][55000]\t Training Loss 0.8432\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [14500][55000]\t Training Loss 0.8433\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [14550][55000]\t Training Loss 0.8440\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [14600][55000]\t Training Loss 0.8441\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [14650][55000]\t Training Loss 0.8450\t Accuracy 0.8444\n",
      "Epoch [17][20]\t Batch [14700][55000]\t Training Loss 0.8458\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [14750][55000]\t Training Loss 0.8464\t Accuracy 0.8439\n",
      "Epoch [17][20]\t Batch [14800][55000]\t Training Loss 0.8474\t Accuracy 0.8437\n",
      "Epoch [17][20]\t Batch [14850][55000]\t Training Loss 0.8480\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [14900][55000]\t Training Loss 0.8479\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [14950][55000]\t Training Loss 0.8480\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [15000][55000]\t Training Loss 0.8479\t Accuracy 0.8433\n",
      "Epoch [17][20]\t Batch [15050][55000]\t Training Loss 0.8475\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [15100][55000]\t Training Loss 0.8473\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [15150][55000]\t Training Loss 0.8477\t Accuracy 0.8437\n",
      "Epoch [17][20]\t Batch [15200][55000]\t Training Loss 0.8479\t Accuracy 0.8437\n",
      "Epoch [17][20]\t Batch [15250][55000]\t Training Loss 0.8478\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [15300][55000]\t Training Loss 0.8478\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [15350][55000]\t Training Loss 0.8476\t Accuracy 0.8440\n",
      "Epoch [17][20]\t Batch [15400][55000]\t Training Loss 0.8479\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [15450][55000]\t Training Loss 0.8480\t Accuracy 0.8443\n",
      "Epoch [17][20]\t Batch [15500][55000]\t Training Loss 0.8478\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [15550][55000]\t Training Loss 0.8476\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [15600][55000]\t Training Loss 0.8475\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [15650][55000]\t Training Loss 0.8474\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [15700][55000]\t Training Loss 0.8473\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [15750][55000]\t Training Loss 0.8482\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [15800][55000]\t Training Loss 0.8486\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [15850][55000]\t Training Loss 0.8489\t Accuracy 0.8445\n",
      "Epoch [17][20]\t Batch [15900][55000]\t Training Loss 0.8494\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [15950][55000]\t Training Loss 0.8495\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [16000][55000]\t Training Loss 0.8496\t Accuracy 0.8439\n",
      "Epoch [17][20]\t Batch [16050][55000]\t Training Loss 0.8505\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [16100][55000]\t Training Loss 0.8505\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [16150][55000]\t Training Loss 0.8504\t Accuracy 0.8437\n",
      "Epoch [17][20]\t Batch [16200][55000]\t Training Loss 0.8505\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [16250][55000]\t Training Loss 0.8504\t Accuracy 0.8433\n",
      "Epoch [17][20]\t Batch [16300][55000]\t Training Loss 0.8501\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [16350][55000]\t Training Loss 0.8497\t Accuracy 0.8437\n",
      "Epoch [17][20]\t Batch [16400][55000]\t Training Loss 0.8498\t Accuracy 0.8437\n",
      "Epoch [17][20]\t Batch [16450][55000]\t Training Loss 0.8495\t Accuracy 0.8439\n",
      "Epoch [17][20]\t Batch [16500][55000]\t Training Loss 0.8493\t Accuracy 0.8441\n",
      "Epoch [17][20]\t Batch [16550][55000]\t Training Loss 0.8489\t Accuracy 0.8441\n",
      "Epoch [17][20]\t Batch [16600][55000]\t Training Loss 0.8490\t Accuracy 0.8441\n",
      "Epoch [17][20]\t Batch [16650][55000]\t Training Loss 0.8490\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [16700][55000]\t Training Loss 0.8491\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [16750][55000]\t Training Loss 0.8490\t Accuracy 0.8444\n",
      "Epoch [17][20]\t Batch [16800][55000]\t Training Loss 0.8496\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [16850][55000]\t Training Loss 0.8501\t Accuracy 0.8439\n",
      "Epoch [17][20]\t Batch [16900][55000]\t Training Loss 0.8504\t Accuracy 0.8440\n",
      "Epoch [17][20]\t Batch [16950][55000]\t Training Loss 0.8503\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [17000][55000]\t Training Loss 0.8511\t Accuracy 0.8435\n",
      "Epoch [17][20]\t Batch [17050][55000]\t Training Loss 0.8510\t Accuracy 0.8435\n",
      "Epoch [17][20]\t Batch [17100][55000]\t Training Loss 0.8515\t Accuracy 0.8433\n",
      "Epoch [17][20]\t Batch [17150][55000]\t Training Loss 0.8512\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [17200][55000]\t Training Loss 0.8513\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [17250][55000]\t Training Loss 0.8518\t Accuracy 0.8433\n",
      "Epoch [17][20]\t Batch [17300][55000]\t Training Loss 0.8516\t Accuracy 0.8433\n",
      "Epoch [17][20]\t Batch [17350][55000]\t Training Loss 0.8511\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [17400][55000]\t Training Loss 0.8512\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [17450][55000]\t Training Loss 0.8513\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [17500][55000]\t Training Loss 0.8514\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [17550][55000]\t Training Loss 0.8521\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [17600][55000]\t Training Loss 0.8527\t Accuracy 0.8432\n",
      "Epoch [17][20]\t Batch [17650][55000]\t Training Loss 0.8529\t Accuracy 0.8434\n",
      "Epoch [17][20]\t Batch [17700][55000]\t Training Loss 0.8536\t Accuracy 0.8432\n",
      "Epoch [17][20]\t Batch [17750][55000]\t Training Loss 0.8540\t Accuracy 0.8432\n",
      "Epoch [17][20]\t Batch [17800][55000]\t Training Loss 0.8543\t Accuracy 0.8430\n",
      "Epoch [17][20]\t Batch [17850][55000]\t Training Loss 0.8546\t Accuracy 0.8428\n",
      "Epoch [17][20]\t Batch [17900][55000]\t Training Loss 0.8550\t Accuracy 0.8426\n",
      "Epoch [17][20]\t Batch [17950][55000]\t Training Loss 0.8547\t Accuracy 0.8425\n",
      "Epoch [17][20]\t Batch [18000][55000]\t Training Loss 0.8544\t Accuracy 0.8426\n",
      "Epoch [17][20]\t Batch [18050][55000]\t Training Loss 0.8546\t Accuracy 0.8426\n",
      "Epoch [17][20]\t Batch [18100][55000]\t Training Loss 0.8545\t Accuracy 0.8428\n",
      "Epoch [17][20]\t Batch [18150][55000]\t Training Loss 0.8541\t Accuracy 0.8428\n",
      "Epoch [17][20]\t Batch [18200][55000]\t Training Loss 0.8536\t Accuracy 0.8431\n",
      "Epoch [17][20]\t Batch [18250][55000]\t Training Loss 0.8536\t Accuracy 0.8431\n",
      "Epoch [17][20]\t Batch [18300][55000]\t Training Loss 0.8532\t Accuracy 0.8432\n",
      "Epoch [17][20]\t Batch [18350][55000]\t Training Loss 0.8531\t Accuracy 0.8435\n",
      "Epoch [17][20]\t Batch [18400][55000]\t Training Loss 0.8531\t Accuracy 0.8435\n",
      "Epoch [17][20]\t Batch [18450][55000]\t Training Loss 0.8535\t Accuracy 0.8433\n",
      "Epoch [17][20]\t Batch [18500][55000]\t Training Loss 0.8535\t Accuracy 0.8433\n",
      "Epoch [17][20]\t Batch [18550][55000]\t Training Loss 0.8533\t Accuracy 0.8436\n",
      "Epoch [17][20]\t Batch [18600][55000]\t Training Loss 0.8533\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [18650][55000]\t Training Loss 0.8532\t Accuracy 0.8440\n",
      "Epoch [17][20]\t Batch [18700][55000]\t Training Loss 0.8534\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [18750][55000]\t Training Loss 0.8536\t Accuracy 0.8438\n",
      "Epoch [17][20]\t Batch [18800][55000]\t Training Loss 0.8532\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [18850][55000]\t Training Loss 0.8535\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [18900][55000]\t Training Loss 0.8530\t Accuracy 0.8445\n",
      "Epoch [17][20]\t Batch [18950][55000]\t Training Loss 0.8527\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [19000][55000]\t Training Loss 0.8526\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [19050][55000]\t Training Loss 0.8530\t Accuracy 0.8445\n",
      "Epoch [17][20]\t Batch [19100][55000]\t Training Loss 0.8533\t Accuracy 0.8444\n",
      "Epoch [17][20]\t Batch [19150][55000]\t Training Loss 0.8534\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [19200][55000]\t Training Loss 0.8535\t Accuracy 0.8441\n",
      "Epoch [17][20]\t Batch [19250][55000]\t Training Loss 0.8534\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [19300][55000]\t Training Loss 0.8532\t Accuracy 0.8443\n",
      "Epoch [17][20]\t Batch [19350][55000]\t Training Loss 0.8533\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [19400][55000]\t Training Loss 0.8531\t Accuracy 0.8443\n",
      "Epoch [17][20]\t Batch [19450][55000]\t Training Loss 0.8528\t Accuracy 0.8442\n",
      "Epoch [17][20]\t Batch [19500][55000]\t Training Loss 0.8525\t Accuracy 0.8444\n",
      "Epoch [17][20]\t Batch [19550][55000]\t Training Loss 0.8525\t Accuracy 0.8443\n",
      "Epoch [17][20]\t Batch [19600][55000]\t Training Loss 0.8525\t Accuracy 0.8440\n",
      "Epoch [17][20]\t Batch [19650][55000]\t Training Loss 0.8521\t Accuracy 0.8443\n",
      "Epoch [17][20]\t Batch [19700][55000]\t Training Loss 0.8515\t Accuracy 0.8445\n",
      "Epoch [17][20]\t Batch [19750][55000]\t Training Loss 0.8510\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [19800][55000]\t Training Loss 0.8503\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [19850][55000]\t Training Loss 0.8504\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [19900][55000]\t Training Loss 0.8501\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [19950][55000]\t Training Loss 0.8503\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [20000][55000]\t Training Loss 0.8502\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [20050][55000]\t Training Loss 0.8508\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [20100][55000]\t Training Loss 0.8509\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [20150][55000]\t Training Loss 0.8507\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [20200][55000]\t Training Loss 0.8511\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [20250][55000]\t Training Loss 0.8512\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [20300][55000]\t Training Loss 0.8513\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [20350][55000]\t Training Loss 0.8513\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [20400][55000]\t Training Loss 0.8510\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [20450][55000]\t Training Loss 0.8506\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [20500][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [20550][55000]\t Training Loss 0.8502\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [20600][55000]\t Training Loss 0.8502\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [20650][55000]\t Training Loss 0.8499\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [20700][55000]\t Training Loss 0.8497\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [20750][55000]\t Training Loss 0.8497\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [20800][55000]\t Training Loss 0.8497\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [20850][55000]\t Training Loss 0.8497\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [20900][55000]\t Training Loss 0.8500\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [20950][55000]\t Training Loss 0.8505\t Accuracy 0.8445\n",
      "Epoch [17][20]\t Batch [21000][55000]\t Training Loss 0.8507\t Accuracy 0.8443\n",
      "Epoch [17][20]\t Batch [21050][55000]\t Training Loss 0.8509\t Accuracy 0.8443\n",
      "Epoch [17][20]\t Batch [21100][55000]\t Training Loss 0.8507\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [21150][55000]\t Training Loss 0.8506\t Accuracy 0.8445\n",
      "Epoch [17][20]\t Batch [21200][55000]\t Training Loss 0.8504\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [21250][55000]\t Training Loss 0.8502\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [21300][55000]\t Training Loss 0.8497\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [21350][55000]\t Training Loss 0.8498\t Accuracy 0.8454\n",
      "Epoch [17][20]\t Batch [21400][55000]\t Training Loss 0.8499\t Accuracy 0.8454\n",
      "Epoch [17][20]\t Batch [21450][55000]\t Training Loss 0.8500\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [21500][55000]\t Training Loss 0.8495\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [21550][55000]\t Training Loss 0.8493\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [21600][55000]\t Training Loss 0.8495\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [21650][55000]\t Training Loss 0.8495\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [21700][55000]\t Training Loss 0.8495\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [21750][55000]\t Training Loss 0.8494\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [21800][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [21850][55000]\t Training Loss 0.8485\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [21900][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [21950][55000]\t Training Loss 0.8476\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [22000][55000]\t Training Loss 0.8472\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [22050][55000]\t Training Loss 0.8468\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [22100][55000]\t Training Loss 0.8470\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [22150][55000]\t Training Loss 0.8474\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [22200][55000]\t Training Loss 0.8476\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [22250][55000]\t Training Loss 0.8477\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [22300][55000]\t Training Loss 0.8478\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [22350][55000]\t Training Loss 0.8476\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [22400][55000]\t Training Loss 0.8475\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [22450][55000]\t Training Loss 0.8475\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [22500][55000]\t Training Loss 0.8479\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [22550][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [22600][55000]\t Training Loss 0.8490\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [22650][55000]\t Training Loss 0.8491\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [22700][55000]\t Training Loss 0.8490\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [22750][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [22800][55000]\t Training Loss 0.8488\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [22850][55000]\t Training Loss 0.8489\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [22900][55000]\t Training Loss 0.8486\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [22950][55000]\t Training Loss 0.8484\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [23000][55000]\t Training Loss 0.8482\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [23050][55000]\t Training Loss 0.8480\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [23100][55000]\t Training Loss 0.8481\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [23150][55000]\t Training Loss 0.8480\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [23200][55000]\t Training Loss 0.8481\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [23250][55000]\t Training Loss 0.8478\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [23300][55000]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [23350][55000]\t Training Loss 0.8473\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [23400][55000]\t Training Loss 0.8470\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [23450][55000]\t Training Loss 0.8470\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [23500][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [23550][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [23600][55000]\t Training Loss 0.8467\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [23650][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [23700][55000]\t Training Loss 0.8470\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [23750][55000]\t Training Loss 0.8476\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [23800][55000]\t Training Loss 0.8472\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [23850][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [23900][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [23950][55000]\t Training Loss 0.8474\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [24000][55000]\t Training Loss 0.8474\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [24050][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [24100][55000]\t Training Loss 0.8472\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [24150][55000]\t Training Loss 0.8470\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [24200][55000]\t Training Loss 0.8469\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [24250][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [24300][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [24350][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [24400][55000]\t Training Loss 0.8471\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [24450][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [24500][55000]\t Training Loss 0.8472\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [24550][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [24600][55000]\t Training Loss 0.8473\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [24650][55000]\t Training Loss 0.8474\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [24700][55000]\t Training Loss 0.8473\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [24750][55000]\t Training Loss 0.8476\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [24800][55000]\t Training Loss 0.8479\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [24850][55000]\t Training Loss 0.8478\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [24900][55000]\t Training Loss 0.8479\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [24950][55000]\t Training Loss 0.8482\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [25000][55000]\t Training Loss 0.8484\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [25050][55000]\t Training Loss 0.8481\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [25100][55000]\t Training Loss 0.8481\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [25150][55000]\t Training Loss 0.8479\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [25200][55000]\t Training Loss 0.8478\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [25250][55000]\t Training Loss 0.8477\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [25300][55000]\t Training Loss 0.8476\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [25350][55000]\t Training Loss 0.8478\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [25400][55000]\t Training Loss 0.8473\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [25450][55000]\t Training Loss 0.8469\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [25500][55000]\t Training Loss 0.8467\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [25550][55000]\t Training Loss 0.8463\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [25600][55000]\t Training Loss 0.8462\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [25650][55000]\t Training Loss 0.8461\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [25700][55000]\t Training Loss 0.8459\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [25750][55000]\t Training Loss 0.8457\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [25800][55000]\t Training Loss 0.8456\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [25850][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [25900][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [25950][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [26000][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [26050][55000]\t Training Loss 0.8456\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [26100][55000]\t Training Loss 0.8453\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [26150][55000]\t Training Loss 0.8451\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [26200][55000]\t Training Loss 0.8449\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [26250][55000]\t Training Loss 0.8449\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [26300][55000]\t Training Loss 0.8451\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [26350][55000]\t Training Loss 0.8450\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [26400][55000]\t Training Loss 0.8454\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [26450][55000]\t Training Loss 0.8455\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [26500][55000]\t Training Loss 0.8457\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [26550][55000]\t Training Loss 0.8459\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [26600][55000]\t Training Loss 0.8461\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [26650][55000]\t Training Loss 0.8465\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [26700][55000]\t Training Loss 0.8464\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [26750][55000]\t Training Loss 0.8466\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [26800][55000]\t Training Loss 0.8465\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [26850][55000]\t Training Loss 0.8464\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [26900][55000]\t Training Loss 0.8467\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [26950][55000]\t Training Loss 0.8465\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [27000][55000]\t Training Loss 0.8464\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [27050][55000]\t Training Loss 0.8461\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [27100][55000]\t Training Loss 0.8460\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [27150][55000]\t Training Loss 0.8460\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [27200][55000]\t Training Loss 0.8463\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [27250][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [27300][55000]\t Training Loss 0.8463\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [27350][55000]\t Training Loss 0.8462\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [27400][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [27450][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [27500][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [27550][55000]\t Training Loss 0.8462\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [27600][55000]\t Training Loss 0.8460\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [27650][55000]\t Training Loss 0.8460\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [27700][55000]\t Training Loss 0.8460\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [27750][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [27800][55000]\t Training Loss 0.8462\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [27850][55000]\t Training Loss 0.8463\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [27900][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [27950][55000]\t Training Loss 0.8459\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [28000][55000]\t Training Loss 0.8457\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [28050][55000]\t Training Loss 0.8454\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [28100][55000]\t Training Loss 0.8450\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [28150][55000]\t Training Loss 0.8448\t Accuracy 0.8480\n",
      "Epoch [17][20]\t Batch [28200][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [28250][55000]\t Training Loss 0.8445\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [28300][55000]\t Training Loss 0.8444\t Accuracy 0.8480\n",
      "Epoch [17][20]\t Batch [28350][55000]\t Training Loss 0.8442\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [28400][55000]\t Training Loss 0.8446\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [28450][55000]\t Training Loss 0.8443\t Accuracy 0.8481\n",
      "Epoch [17][20]\t Batch [28500][55000]\t Training Loss 0.8442\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [28550][55000]\t Training Loss 0.8441\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [28600][55000]\t Training Loss 0.8440\t Accuracy 0.8484\n",
      "Epoch [17][20]\t Batch [28650][55000]\t Training Loss 0.8442\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [28700][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [28750][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [28800][55000]\t Training Loss 0.8446\t Accuracy 0.8481\n",
      "Epoch [17][20]\t Batch [28850][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [28900][55000]\t Training Loss 0.8443\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [28950][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [29000][55000]\t Training Loss 0.8443\t Accuracy 0.8481\n",
      "Epoch [17][20]\t Batch [29050][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [29100][55000]\t Training Loss 0.8444\t Accuracy 0.8481\n",
      "Epoch [17][20]\t Batch [29150][55000]\t Training Loss 0.8448\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [29200][55000]\t Training Loss 0.8451\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [29250][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [29300][55000]\t Training Loss 0.8452\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [29350][55000]\t Training Loss 0.8453\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [29400][55000]\t Training Loss 0.8452\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [29450][55000]\t Training Loss 0.8448\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [29500][55000]\t Training Loss 0.8446\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [29550][55000]\t Training Loss 0.8446\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [29600][55000]\t Training Loss 0.8446\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [29650][55000]\t Training Loss 0.8445\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [29700][55000]\t Training Loss 0.8446\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [29750][55000]\t Training Loss 0.8450\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [29800][55000]\t Training Loss 0.8451\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [29850][55000]\t Training Loss 0.8454\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [29900][55000]\t Training Loss 0.8457\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [29950][55000]\t Training Loss 0.8460\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [30000][55000]\t Training Loss 0.8464\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [30050][55000]\t Training Loss 0.8465\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [30100][55000]\t Training Loss 0.8469\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [30150][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [30200][55000]\t Training Loss 0.8476\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [30250][55000]\t Training Loss 0.8477\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [30300][55000]\t Training Loss 0.8474\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [30350][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [30400][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [30450][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [30500][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [30550][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [30600][55000]\t Training Loss 0.8480\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [30650][55000]\t Training Loss 0.8484\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [30700][55000]\t Training Loss 0.8487\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [30750][55000]\t Training Loss 0.8488\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [30800][55000]\t Training Loss 0.8490\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [30850][55000]\t Training Loss 0.8489\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [30900][55000]\t Training Loss 0.8493\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [30950][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [31000][55000]\t Training Loss 0.8491\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [31050][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [31100][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [31150][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [31200][55000]\t Training Loss 0.8493\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [31250][55000]\t Training Loss 0.8493\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [31300][55000]\t Training Loss 0.8496\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [31350][55000]\t Training Loss 0.8503\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [31400][55000]\t Training Loss 0.8504\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [31450][55000]\t Training Loss 0.8507\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [31500][55000]\t Training Loss 0.8507\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [31550][55000]\t Training Loss 0.8507\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [31600][55000]\t Training Loss 0.8509\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [31650][55000]\t Training Loss 0.8511\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [31700][55000]\t Training Loss 0.8514\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [31750][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [31800][55000]\t Training Loss 0.8519\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [31850][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [31900][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [31950][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [32000][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [32050][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [32100][55000]\t Training Loss 0.8517\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [32150][55000]\t Training Loss 0.8519\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [32200][55000]\t Training Loss 0.8521\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [32250][55000]\t Training Loss 0.8524\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [32300][55000]\t Training Loss 0.8528\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [32350][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [32400][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [32450][55000]\t Training Loss 0.8533\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [32500][55000]\t Training Loss 0.8536\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [32550][55000]\t Training Loss 0.8539\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [32600][55000]\t Training Loss 0.8538\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [32650][55000]\t Training Loss 0.8537\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [32700][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [32750][55000]\t Training Loss 0.8537\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [32800][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [32850][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [32900][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [32950][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [33000][55000]\t Training Loss 0.8534\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [33050][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [33100][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [33150][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [33200][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [33250][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [33300][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [33350][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [33400][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [33450][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [33500][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [33550][55000]\t Training Loss 0.8538\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [33600][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [33650][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [33700][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [33750][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [33800][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [33850][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [33900][55000]\t Training Loss 0.8529\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [33950][55000]\t Training Loss 0.8526\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [34000][55000]\t Training Loss 0.8526\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [34050][55000]\t Training Loss 0.8528\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [34100][55000]\t Training Loss 0.8528\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [34150][55000]\t Training Loss 0.8529\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [34200][55000]\t Training Loss 0.8527\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [34250][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [34300][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [34350][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [34400][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [34450][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [34500][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [34550][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [34600][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [34650][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [34700][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [34750][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [34800][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [34850][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [34900][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [34950][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [35000][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [35050][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [35100][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [35150][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [35200][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [35250][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [35300][55000]\t Training Loss 0.8532\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [35350][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [35400][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [35450][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [35500][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [35550][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [35600][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [35650][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [35700][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [35750][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [35800][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [35850][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [35900][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [35950][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [36000][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [36050][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [36100][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [36150][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [36200][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [36250][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [36300][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [36350][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [36400][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [36450][55000]\t Training Loss 0.8524\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [36500][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [36550][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [36600][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [36650][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [36700][55000]\t Training Loss 0.8518\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [36750][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [36800][55000]\t Training Loss 0.8514\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [36850][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [36900][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [36950][55000]\t Training Loss 0.8513\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [37000][55000]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [37050][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [37100][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [37150][55000]\t Training Loss 0.8512\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [37200][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [37250][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [37300][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [17][20]\t Batch [37350][55000]\t Training Loss 0.8511\t Accuracy 0.8478\n",
      "Epoch [17][20]\t Batch [37400][55000]\t Training Loss 0.8513\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [37450][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [37500][55000]\t Training Loss 0.8518\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [37550][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [37600][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [37650][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [37700][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [37750][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [37800][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [37850][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [37900][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [37950][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [38000][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [38050][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [38100][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [38150][55000]\t Training Loss 0.8521\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [38200][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [38250][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [38300][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [38350][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [38400][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [38450][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [38500][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [38550][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [38600][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [38650][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [38700][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [38750][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [38800][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [38850][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [38900][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [38950][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39000][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39050][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39100][55000]\t Training Loss 0.8515\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [39150][55000]\t Training Loss 0.8514\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [39200][55000]\t Training Loss 0.8512\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [39250][55000]\t Training Loss 0.8510\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [39300][55000]\t Training Loss 0.8510\t Accuracy 0.8477\n",
      "Epoch [17][20]\t Batch [39350][55000]\t Training Loss 0.8513\t Accuracy 0.8475\n",
      "Epoch [17][20]\t Batch [39400][55000]\t Training Loss 0.8513\t Accuracy 0.8476\n",
      "Epoch [17][20]\t Batch [39450][55000]\t Training Loss 0.8516\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [39500][55000]\t Training Loss 0.8517\t Accuracy 0.8474\n",
      "Epoch [17][20]\t Batch [39550][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39600][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39650][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39700][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39750][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39800][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39850][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [39900][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [39950][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40000][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40050][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40100][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40150][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40200][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40250][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40300][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40350][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40400][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40450][55000]\t Training Loss 0.8520\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40500][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40550][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40600][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40650][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40700][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40750][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [40800][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40850][55000]\t Training Loss 0.8520\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40900][55000]\t Training Loss 0.8518\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [40950][55000]\t Training Loss 0.8517\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [41000][55000]\t Training Loss 0.8517\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [41050][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [41100][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [41150][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [41200][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [41250][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [41300][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [41350][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [41400][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [17][20]\t Batch [41450][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [41500][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [41550][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [41600][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [41650][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [41700][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [41750][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [41800][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [17][20]\t Batch [41850][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [41900][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [41950][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [42000][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [17][20]\t Batch [42050][55000]\t Training Loss 0.8532\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [42100][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [42150][55000]\t Training Loss 0.8532\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [42200][55000]\t Training Loss 0.8532\t Accuracy 0.8468\n",
      "Epoch [17][20]\t Batch [42250][55000]\t Training Loss 0.8533\t Accuracy 0.8467\n",
      "Epoch [17][20]\t Batch [42300][55000]\t Training Loss 0.8533\t Accuracy 0.8466\n",
      "Epoch [17][20]\t Batch [42350][55000]\t Training Loss 0.8535\t Accuracy 0.8465\n",
      "Epoch [17][20]\t Batch [42400][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [42450][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [42500][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [42550][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [42600][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [42650][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [42700][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [42750][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [42800][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [42850][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [42900][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [42950][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [43000][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [43050][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [43100][55000]\t Training Loss 0.8543\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [43150][55000]\t Training Loss 0.8544\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [43200][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [43250][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [43300][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [43350][55000]\t Training Loss 0.8540\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [43400][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [43450][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [43500][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [43550][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [43600][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [43650][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [43700][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [43750][55000]\t Training Loss 0.8532\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [43800][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [43850][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [43900][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [43950][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [44000][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [44050][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [44100][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [44150][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [44200][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [44250][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [44300][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [44350][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [44400][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [44450][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [44500][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [44550][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [44600][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [44650][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [44700][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [44750][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [44800][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [44850][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [17][20]\t Batch [44900][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [44950][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [45000][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [45050][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45100][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45150][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [45200][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45250][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45300][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [45350][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [45400][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [45450][55000]\t Training Loss 0.8541\t Accuracy 0.8463\n",
      "Epoch [17][20]\t Batch [45500][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45550][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45600][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45650][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45700][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45750][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [45800][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45850][55000]\t Training Loss 0.8543\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [45900][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [45950][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [46000][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [46050][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46100][55000]\t Training Loss 0.8548\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [46150][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46200][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46250][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46300][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [46350][55000]\t Training Loss 0.8550\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [46400][55000]\t Training Loss 0.8551\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [46450][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [46500][55000]\t Training Loss 0.8551\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [46550][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46600][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46650][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [46700][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46750][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46800][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46850][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46900][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [46950][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47000][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47050][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47100][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47150][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47200][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47250][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47300][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47350][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47400][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47450][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47500][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47550][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47600][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47650][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47700][55000]\t Training Loss 0.8546\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [47750][55000]\t Training Loss 0.8545\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [47800][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [47850][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47900][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [47950][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [48000][55000]\t Training Loss 0.8544\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [48050][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [48100][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [48150][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48200][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48250][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [48300][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48350][55000]\t Training Loss 0.8534\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48400][55000]\t Training Loss 0.8535\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [48450][55000]\t Training Loss 0.8533\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48500][55000]\t Training Loss 0.8532\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [48550][55000]\t Training Loss 0.8532\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48600][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48650][55000]\t Training Loss 0.8530\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48700][55000]\t Training Loss 0.8529\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [48750][55000]\t Training Loss 0.8528\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [48800][55000]\t Training Loss 0.8527\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [48850][55000]\t Training Loss 0.8526\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [48900][55000]\t Training Loss 0.8525\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [48950][55000]\t Training Loss 0.8527\t Accuracy 0.8461\n",
      "Epoch [17][20]\t Batch [49000][55000]\t Training Loss 0.8528\t Accuracy 0.8460\n",
      "Epoch [17][20]\t Batch [49050][55000]\t Training Loss 0.8532\t Accuracy 0.8459\n",
      "Epoch [17][20]\t Batch [49100][55000]\t Training Loss 0.8533\t Accuracy 0.8458\n",
      "Epoch [17][20]\t Batch [49150][55000]\t Training Loss 0.8534\t Accuracy 0.8457\n",
      "Epoch [17][20]\t Batch [49200][55000]\t Training Loss 0.8534\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [49250][55000]\t Training Loss 0.8536\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [49300][55000]\t Training Loss 0.8534\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [49350][55000]\t Training Loss 0.8533\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [49400][55000]\t Training Loss 0.8531\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [49450][55000]\t Training Loss 0.8531\t Accuracy 0.8456\n",
      "Epoch [17][20]\t Batch [49500][55000]\t Training Loss 0.8533\t Accuracy 0.8454\n",
      "Epoch [17][20]\t Batch [49550][55000]\t Training Loss 0.8537\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [49600][55000]\t Training Loss 0.8539\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [49650][55000]\t Training Loss 0.8540\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [49700][55000]\t Training Loss 0.8542\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [49750][55000]\t Training Loss 0.8542\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [49800][55000]\t Training Loss 0.8542\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [49850][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [49900][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [49950][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [50000][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [50050][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [50100][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [50150][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [50200][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [50250][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [50300][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [50350][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [50400][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [50450][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [50500][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [50550][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [50600][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [50650][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [50700][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [50750][55000]\t Training Loss 0.8554\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [50800][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [50850][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [50900][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [50950][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [51000][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [51050][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [51100][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [51150][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [51200][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [51250][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [51300][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [51350][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [51400][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [51450][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [51500][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [51550][55000]\t Training Loss 0.8546\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [51600][55000]\t Training Loss 0.8544\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [51650][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [51700][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [51750][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [51800][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [51850][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [51900][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [51950][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [52000][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [52050][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [52100][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [52150][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [52200][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [52250][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [52300][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [52350][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [52400][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [52450][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [52500][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [52550][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [52600][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [52650][55000]\t Training Loss 0.8542\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [52700][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [17][20]\t Batch [52750][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [52800][55000]\t Training Loss 0.8546\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [52850][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [52900][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [17][20]\t Batch [52950][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [53000][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53050][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [53100][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53150][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [53200][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53250][55000]\t Training Loss 0.8552\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53300][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53350][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53400][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [53450][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [53500][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53550][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53600][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [53650][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [53700][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [53750][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53800][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [53850][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53900][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [53950][55000]\t Training Loss 0.8548\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54000][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54050][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54100][55000]\t Training Loss 0.8552\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [54150][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54200][55000]\t Training Loss 0.8552\t Accuracy 0.8446\n",
      "Epoch [17][20]\t Batch [54250][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54300][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54350][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54400][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [54450][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [54500][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [54550][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [54600][55000]\t Training Loss 0.8547\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54650][55000]\t Training Loss 0.8546\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [54700][55000]\t Training Loss 0.8544\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [54750][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [54800][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [54850][55000]\t Training Loss 0.8541\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [54900][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [54950][55000]\t Training Loss 0.8546\t Accuracy 0.8448\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8547\t Average training accuracy 0.8447\n",
      "Epoch [17]\t Average validation loss 0.7921\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [18][20]\t Batch [0][55000]\t Training Loss 1.4694\t Accuracy 0.0000\n",
      "Epoch [18][20]\t Batch [50][55000]\t Training Loss 0.9380\t Accuracy 0.7255\n",
      "Epoch [18][20]\t Batch [100][55000]\t Training Loss 0.8487\t Accuracy 0.8020\n",
      "Epoch [18][20]\t Batch [150][55000]\t Training Loss 0.8501\t Accuracy 0.8146\n",
      "Epoch [18][20]\t Batch [200][55000]\t Training Loss 0.8610\t Accuracy 0.8159\n",
      "Epoch [18][20]\t Batch [250][55000]\t Training Loss 0.8431\t Accuracy 0.8247\n",
      "Epoch [18][20]\t Batch [300][55000]\t Training Loss 0.8447\t Accuracy 0.8206\n",
      "Epoch [18][20]\t Batch [350][55000]\t Training Loss 0.8249\t Accuracy 0.8234\n",
      "Epoch [18][20]\t Batch [400][55000]\t Training Loss 0.8110\t Accuracy 0.8279\n",
      "Epoch [18][20]\t Batch [450][55000]\t Training Loss 0.8169\t Accuracy 0.8293\n",
      "Epoch [18][20]\t Batch [500][55000]\t Training Loss 0.8233\t Accuracy 0.8303\n",
      "Epoch [18][20]\t Batch [550][55000]\t Training Loss 0.8406\t Accuracy 0.8276\n",
      "Epoch [18][20]\t Batch [600][55000]\t Training Loss 0.8450\t Accuracy 0.8303\n",
      "Epoch [18][20]\t Batch [650][55000]\t Training Loss 0.8649\t Accuracy 0.8203\n",
      "Epoch [18][20]\t Batch [700][55000]\t Training Loss 0.8626\t Accuracy 0.8260\n",
      "Epoch [18][20]\t Batch [750][55000]\t Training Loss 0.8588\t Accuracy 0.8269\n",
      "Epoch [18][20]\t Batch [800][55000]\t Training Loss 0.8528\t Accuracy 0.8290\n",
      "Epoch [18][20]\t Batch [850][55000]\t Training Loss 0.8513\t Accuracy 0.8308\n",
      "Epoch [18][20]\t Batch [900][55000]\t Training Loss 0.8578\t Accuracy 0.8302\n",
      "Epoch [18][20]\t Batch [950][55000]\t Training Loss 0.8645\t Accuracy 0.8254\n",
      "Epoch [18][20]\t Batch [1000][55000]\t Training Loss 0.8640\t Accuracy 0.8272\n",
      "Epoch [18][20]\t Batch [1050][55000]\t Training Loss 0.8717\t Accuracy 0.8259\n",
      "Epoch [18][20]\t Batch [1100][55000]\t Training Loss 0.8801\t Accuracy 0.8256\n",
      "Epoch [18][20]\t Batch [1150][55000]\t Training Loss 0.8893\t Accuracy 0.8228\n",
      "Epoch [18][20]\t Batch [1200][55000]\t Training Loss 0.8835\t Accuracy 0.8260\n",
      "Epoch [18][20]\t Batch [1250][55000]\t Training Loss 0.8849\t Accuracy 0.8257\n",
      "Epoch [18][20]\t Batch [1300][55000]\t Training Loss 0.8852\t Accuracy 0.8255\n",
      "Epoch [18][20]\t Batch [1350][55000]\t Training Loss 0.8820\t Accuracy 0.8268\n",
      "Epoch [18][20]\t Batch [1400][55000]\t Training Loss 0.8826\t Accuracy 0.8258\n",
      "Epoch [18][20]\t Batch [1450][55000]\t Training Loss 0.8834\t Accuracy 0.8249\n",
      "Epoch [18][20]\t Batch [1500][55000]\t Training Loss 0.8815\t Accuracy 0.8274\n",
      "Epoch [18][20]\t Batch [1550][55000]\t Training Loss 0.8809\t Accuracy 0.8279\n",
      "Epoch [18][20]\t Batch [1600][55000]\t Training Loss 0.8835\t Accuracy 0.8264\n",
      "Epoch [18][20]\t Batch [1650][55000]\t Training Loss 0.8804\t Accuracy 0.8280\n",
      "Epoch [18][20]\t Batch [1700][55000]\t Training Loss 0.8787\t Accuracy 0.8295\n",
      "Epoch [18][20]\t Batch [1750][55000]\t Training Loss 0.8731\t Accuracy 0.8310\n",
      "Epoch [18][20]\t Batch [1800][55000]\t Training Loss 0.8709\t Accuracy 0.8329\n",
      "Epoch [18][20]\t Batch [1850][55000]\t Training Loss 0.8693\t Accuracy 0.8336\n",
      "Epoch [18][20]\t Batch [1900][55000]\t Training Loss 0.8671\t Accuracy 0.8338\n",
      "Epoch [18][20]\t Batch [1950][55000]\t Training Loss 0.8656\t Accuracy 0.8350\n",
      "Epoch [18][20]\t Batch [2000][55000]\t Training Loss 0.8631\t Accuracy 0.8361\n",
      "Epoch [18][20]\t Batch [2050][55000]\t Training Loss 0.8622\t Accuracy 0.8367\n",
      "Epoch [18][20]\t Batch [2100][55000]\t Training Loss 0.8594\t Accuracy 0.8372\n",
      "Epoch [18][20]\t Batch [2150][55000]\t Training Loss 0.8561\t Accuracy 0.8391\n",
      "Epoch [18][20]\t Batch [2200][55000]\t Training Loss 0.8513\t Accuracy 0.8410\n",
      "Epoch [18][20]\t Batch [2250][55000]\t Training Loss 0.8505\t Accuracy 0.8414\n",
      "Epoch [18][20]\t Batch [2300][55000]\t Training Loss 0.8476\t Accuracy 0.8409\n",
      "Epoch [18][20]\t Batch [2350][55000]\t Training Loss 0.8460\t Accuracy 0.8413\n",
      "Epoch [18][20]\t Batch [2400][55000]\t Training Loss 0.8470\t Accuracy 0.8397\n",
      "Epoch [18][20]\t Batch [2450][55000]\t Training Loss 0.8499\t Accuracy 0.8384\n",
      "Epoch [18][20]\t Batch [2500][55000]\t Training Loss 0.8477\t Accuracy 0.8405\n",
      "Epoch [18][20]\t Batch [2550][55000]\t Training Loss 0.8465\t Accuracy 0.8405\n",
      "Epoch [18][20]\t Batch [2600][55000]\t Training Loss 0.8459\t Accuracy 0.8393\n",
      "Epoch [18][20]\t Batch [2650][55000]\t Training Loss 0.8447\t Accuracy 0.8397\n",
      "Epoch [18][20]\t Batch [2700][55000]\t Training Loss 0.8444\t Accuracy 0.8401\n",
      "Epoch [18][20]\t Batch [2750][55000]\t Training Loss 0.8443\t Accuracy 0.8401\n",
      "Epoch [18][20]\t Batch [2800][55000]\t Training Loss 0.8446\t Accuracy 0.8383\n",
      "Epoch [18][20]\t Batch [2850][55000]\t Training Loss 0.8435\t Accuracy 0.8383\n",
      "Epoch [18][20]\t Batch [2900][55000]\t Training Loss 0.8402\t Accuracy 0.8394\n",
      "Epoch [18][20]\t Batch [2950][55000]\t Training Loss 0.8403\t Accuracy 0.8390\n",
      "Epoch [18][20]\t Batch [3000][55000]\t Training Loss 0.8402\t Accuracy 0.8397\n",
      "Epoch [18][20]\t Batch [3050][55000]\t Training Loss 0.8412\t Accuracy 0.8394\n",
      "Epoch [18][20]\t Batch [3100][55000]\t Training Loss 0.8435\t Accuracy 0.8391\n",
      "Epoch [18][20]\t Batch [3150][55000]\t Training Loss 0.8434\t Accuracy 0.8394\n",
      "Epoch [18][20]\t Batch [3200][55000]\t Training Loss 0.8432\t Accuracy 0.8407\n",
      "Epoch [18][20]\t Batch [3250][55000]\t Training Loss 0.8426\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [3300][55000]\t Training Loss 0.8439\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [3350][55000]\t Training Loss 0.8421\t Accuracy 0.8415\n",
      "Epoch [18][20]\t Batch [3400][55000]\t Training Loss 0.8445\t Accuracy 0.8403\n",
      "Epoch [18][20]\t Batch [3450][55000]\t Training Loss 0.8441\t Accuracy 0.8412\n",
      "Epoch [18][20]\t Batch [3500][55000]\t Training Loss 0.8441\t Accuracy 0.8409\n",
      "Epoch [18][20]\t Batch [3550][55000]\t Training Loss 0.8457\t Accuracy 0.8403\n",
      "Epoch [18][20]\t Batch [3600][55000]\t Training Loss 0.8455\t Accuracy 0.8398\n",
      "Epoch [18][20]\t Batch [3650][55000]\t Training Loss 0.8442\t Accuracy 0.8406\n",
      "Epoch [18][20]\t Batch [3700][55000]\t Training Loss 0.8449\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [3750][55000]\t Training Loss 0.8448\t Accuracy 0.8403\n",
      "Epoch [18][20]\t Batch [3800][55000]\t Training Loss 0.8452\t Accuracy 0.8403\n",
      "Epoch [18][20]\t Batch [3850][55000]\t Training Loss 0.8448\t Accuracy 0.8406\n",
      "Epoch [18][20]\t Batch [3900][55000]\t Training Loss 0.8436\t Accuracy 0.8418\n",
      "Epoch [18][20]\t Batch [3950][55000]\t Training Loss 0.8425\t Accuracy 0.8431\n",
      "Epoch [18][20]\t Batch [4000][55000]\t Training Loss 0.8421\t Accuracy 0.8438\n",
      "Epoch [18][20]\t Batch [4050][55000]\t Training Loss 0.8406\t Accuracy 0.8445\n",
      "Epoch [18][20]\t Batch [4100][55000]\t Training Loss 0.8410\t Accuracy 0.8442\n",
      "Epoch [18][20]\t Batch [4150][55000]\t Training Loss 0.8415\t Accuracy 0.8439\n",
      "Epoch [18][20]\t Batch [4200][55000]\t Training Loss 0.8420\t Accuracy 0.8429\n",
      "Epoch [18][20]\t Batch [4250][55000]\t Training Loss 0.8406\t Accuracy 0.8438\n",
      "Epoch [18][20]\t Batch [4300][55000]\t Training Loss 0.8406\t Accuracy 0.8438\n",
      "Epoch [18][20]\t Batch [4350][55000]\t Training Loss 0.8411\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [4400][55000]\t Training Loss 0.8410\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [4450][55000]\t Training Loss 0.8412\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [4500][55000]\t Training Loss 0.8415\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [4550][55000]\t Training Loss 0.8398\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [4600][55000]\t Training Loss 0.8381\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [4650][55000]\t Training Loss 0.8379\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [4700][55000]\t Training Loss 0.8376\t Accuracy 0.8445\n",
      "Epoch [18][20]\t Batch [4750][55000]\t Training Loss 0.8364\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [4800][55000]\t Training Loss 0.8371\t Accuracy 0.8454\n",
      "Epoch [18][20]\t Batch [4850][55000]\t Training Loss 0.8382\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [4900][55000]\t Training Loss 0.8368\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [4950][55000]\t Training Loss 0.8371\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [5000][55000]\t Training Loss 0.8372\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [5050][55000]\t Training Loss 0.8367\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [5100][55000]\t Training Loss 0.8367\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [5150][55000]\t Training Loss 0.8370\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [5200][55000]\t Training Loss 0.8385\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [5250][55000]\t Training Loss 0.8376\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [5300][55000]\t Training Loss 0.8375\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [5350][55000]\t Training Loss 0.8382\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [5400][55000]\t Training Loss 0.8374\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [5450][55000]\t Training Loss 0.8367\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [5500][55000]\t Training Loss 0.8349\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [5550][55000]\t Training Loss 0.8348\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [5600][55000]\t Training Loss 0.8340\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [5650][55000]\t Training Loss 0.8345\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [5700][55000]\t Training Loss 0.8342\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [5750][55000]\t Training Loss 0.8347\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [5800][55000]\t Training Loss 0.8350\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [5850][55000]\t Training Loss 0.8352\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [5900][55000]\t Training Loss 0.8357\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [5950][55000]\t Training Loss 0.8356\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [6000][55000]\t Training Loss 0.8345\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [6050][55000]\t Training Loss 0.8332\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [6100][55000]\t Training Loss 0.8317\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [6150][55000]\t Training Loss 0.8296\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [6200][55000]\t Training Loss 0.8297\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [6250][55000]\t Training Loss 0.8285\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [6300][55000]\t Training Loss 0.8289\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [6350][55000]\t Training Loss 0.8286\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [6400][55000]\t Training Loss 0.8280\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [6450][55000]\t Training Loss 0.8274\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [6500][55000]\t Training Loss 0.8282\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [6550][55000]\t Training Loss 0.8272\t Accuracy 0.8484\n",
      "Epoch [18][20]\t Batch [6600][55000]\t Training Loss 0.8259\t Accuracy 0.8490\n",
      "Epoch [18][20]\t Batch [6650][55000]\t Training Loss 0.8247\t Accuracy 0.8490\n",
      "Epoch [18][20]\t Batch [6700][55000]\t Training Loss 0.8249\t Accuracy 0.8490\n",
      "Epoch [18][20]\t Batch [6750][55000]\t Training Loss 0.8251\t Accuracy 0.8498\n",
      "Epoch [18][20]\t Batch [6800][55000]\t Training Loss 0.8255\t Accuracy 0.8497\n",
      "Epoch [18][20]\t Batch [6850][55000]\t Training Loss 0.8277\t Accuracy 0.8492\n",
      "Epoch [18][20]\t Batch [6900][55000]\t Training Loss 0.8276\t Accuracy 0.8493\n",
      "Epoch [18][20]\t Batch [6950][55000]\t Training Loss 0.8283\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [7000][55000]\t Training Loss 0.8282\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [7050][55000]\t Training Loss 0.8287\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [7100][55000]\t Training Loss 0.8290\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [7150][55000]\t Training Loss 0.8292\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [7200][55000]\t Training Loss 0.8300\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [7250][55000]\t Training Loss 0.8323\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [7300][55000]\t Training Loss 0.8342\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [7350][55000]\t Training Loss 0.8356\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [7400][55000]\t Training Loss 0.8367\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [7450][55000]\t Training Loss 0.8366\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [7500][55000]\t Training Loss 0.8366\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [7550][55000]\t Training Loss 0.8371\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [7600][55000]\t Training Loss 0.8367\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [7650][55000]\t Training Loss 0.8371\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [7700][55000]\t Training Loss 0.8376\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [7750][55000]\t Training Loss 0.8377\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [7800][55000]\t Training Loss 0.8385\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [7850][55000]\t Training Loss 0.8387\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [7900][55000]\t Training Loss 0.8388\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [7950][55000]\t Training Loss 0.8389\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [8000][55000]\t Training Loss 0.8389\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [8050][55000]\t Training Loss 0.8395\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [8100][55000]\t Training Loss 0.8380\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [8150][55000]\t Training Loss 0.8388\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [8200][55000]\t Training Loss 0.8388\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [8250][55000]\t Training Loss 0.8401\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [8300][55000]\t Training Loss 0.8404\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [8350][55000]\t Training Loss 0.8410\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [8400][55000]\t Training Loss 0.8403\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [8450][55000]\t Training Loss 0.8419\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [8500][55000]\t Training Loss 0.8411\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [8550][55000]\t Training Loss 0.8401\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [8600][55000]\t Training Loss 0.8393\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [8650][55000]\t Training Loss 0.8394\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [8700][55000]\t Training Loss 0.8400\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [8750][55000]\t Training Loss 0.8415\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [8800][55000]\t Training Loss 0.8424\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [8850][55000]\t Training Loss 0.8423\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [8900][55000]\t Training Loss 0.8441\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [8950][55000]\t Training Loss 0.8437\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [9000][55000]\t Training Loss 0.8429\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [9050][55000]\t Training Loss 0.8421\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [9100][55000]\t Training Loss 0.8419\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [9150][55000]\t Training Loss 0.8424\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [9200][55000]\t Training Loss 0.8419\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [9250][55000]\t Training Loss 0.8423\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [9300][55000]\t Training Loss 0.8425\t Accuracy 0.8454\n",
      "Epoch [18][20]\t Batch [9350][55000]\t Training Loss 0.8427\t Accuracy 0.8454\n",
      "Epoch [18][20]\t Batch [9400][55000]\t Training Loss 0.8429\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [9450][55000]\t Training Loss 0.8431\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [9500][55000]\t Training Loss 0.8423\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [9550][55000]\t Training Loss 0.8420\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [9600][55000]\t Training Loss 0.8425\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [9650][55000]\t Training Loss 0.8423\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [9700][55000]\t Training Loss 0.8418\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [9750][55000]\t Training Loss 0.8410\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [9800][55000]\t Training Loss 0.8415\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [9850][55000]\t Training Loss 0.8411\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [9900][55000]\t Training Loss 0.8406\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [9950][55000]\t Training Loss 0.8400\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [10000][55000]\t Training Loss 0.8397\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [10050][55000]\t Training Loss 0.8399\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [10100][55000]\t Training Loss 0.8397\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [10150][55000]\t Training Loss 0.8395\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [10200][55000]\t Training Loss 0.8397\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [10250][55000]\t Training Loss 0.8398\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [10300][55000]\t Training Loss 0.8397\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [10350][55000]\t Training Loss 0.8388\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [10400][55000]\t Training Loss 0.8380\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [10450][55000]\t Training Loss 0.8378\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [10500][55000]\t Training Loss 0.8368\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [10550][55000]\t Training Loss 0.8363\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [10600][55000]\t Training Loss 0.8359\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [10650][55000]\t Training Loss 0.8356\t Accuracy 0.8485\n",
      "Epoch [18][20]\t Batch [10700][55000]\t Training Loss 0.8354\t Accuracy 0.8488\n",
      "Epoch [18][20]\t Batch [10750][55000]\t Training Loss 0.8359\t Accuracy 0.8485\n",
      "Epoch [18][20]\t Batch [10800][55000]\t Training Loss 0.8364\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [10850][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [18][20]\t Batch [10900][55000]\t Training Loss 0.8352\t Accuracy 0.8487\n",
      "Epoch [18][20]\t Batch [10950][55000]\t Training Loss 0.8349\t Accuracy 0.8485\n",
      "Epoch [18][20]\t Batch [11000][55000]\t Training Loss 0.8348\t Accuracy 0.8487\n",
      "Epoch [18][20]\t Batch [11050][55000]\t Training Loss 0.8341\t Accuracy 0.8491\n",
      "Epoch [18][20]\t Batch [11100][55000]\t Training Loss 0.8334\t Accuracy 0.8492\n",
      "Epoch [18][20]\t Batch [11150][55000]\t Training Loss 0.8334\t Accuracy 0.8494\n",
      "Epoch [18][20]\t Batch [11200][55000]\t Training Loss 0.8330\t Accuracy 0.8495\n",
      "Epoch [18][20]\t Batch [11250][55000]\t Training Loss 0.8334\t Accuracy 0.8493\n",
      "Epoch [18][20]\t Batch [11300][55000]\t Training Loss 0.8330\t Accuracy 0.8494\n",
      "Epoch [18][20]\t Batch [11350][55000]\t Training Loss 0.8325\t Accuracy 0.8494\n",
      "Epoch [18][20]\t Batch [11400][55000]\t Training Loss 0.8326\t Accuracy 0.8494\n",
      "Epoch [18][20]\t Batch [11450][55000]\t Training Loss 0.8323\t Accuracy 0.8494\n",
      "Epoch [18][20]\t Batch [11500][55000]\t Training Loss 0.8321\t Accuracy 0.8492\n",
      "Epoch [18][20]\t Batch [11550][55000]\t Training Loss 0.8321\t Accuracy 0.8491\n",
      "Epoch [18][20]\t Batch [11600][55000]\t Training Loss 0.8333\t Accuracy 0.8485\n",
      "Epoch [18][20]\t Batch [11650][55000]\t Training Loss 0.8339\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [11700][55000]\t Training Loss 0.8338\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [11750][55000]\t Training Loss 0.8348\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [11800][55000]\t Training Loss 0.8350\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [11850][55000]\t Training Loss 0.8350\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [11900][55000]\t Training Loss 0.8353\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [11950][55000]\t Training Loss 0.8354\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [12000][55000]\t Training Loss 0.8354\t Accuracy 0.8484\n",
      "Epoch [18][20]\t Batch [12050][55000]\t Training Loss 0.8352\t Accuracy 0.8487\n",
      "Epoch [18][20]\t Batch [12100][55000]\t Training Loss 0.8352\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [12150][55000]\t Training Loss 0.8345\t Accuracy 0.8491\n",
      "Epoch [18][20]\t Batch [12200][55000]\t Training Loss 0.8348\t Accuracy 0.8490\n",
      "Epoch [18][20]\t Batch [12250][55000]\t Training Loss 0.8347\t Accuracy 0.8491\n",
      "Epoch [18][20]\t Batch [12300][55000]\t Training Loss 0.8347\t Accuracy 0.8490\n",
      "Epoch [18][20]\t Batch [12350][55000]\t Training Loss 0.8349\t Accuracy 0.8492\n",
      "Epoch [18][20]\t Batch [12400][55000]\t Training Loss 0.8353\t Accuracy 0.8492\n",
      "Epoch [18][20]\t Batch [12450][55000]\t Training Loss 0.8354\t Accuracy 0.8488\n",
      "Epoch [18][20]\t Batch [12500][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [18][20]\t Batch [12550][55000]\t Training Loss 0.8357\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [12600][55000]\t Training Loss 0.8364\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [12650][55000]\t Training Loss 0.8369\t Accuracy 0.8480\n",
      "Epoch [18][20]\t Batch [12700][55000]\t Training Loss 0.8376\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [12750][55000]\t Training Loss 0.8372\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [12800][55000]\t Training Loss 0.8377\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [12850][55000]\t Training Loss 0.8379\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [12900][55000]\t Training Loss 0.8379\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [12950][55000]\t Training Loss 0.8384\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [13000][55000]\t Training Loss 0.8384\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [13050][55000]\t Training Loss 0.8390\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [13100][55000]\t Training Loss 0.8397\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [13150][55000]\t Training Loss 0.8400\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [13200][55000]\t Training Loss 0.8402\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [13250][55000]\t Training Loss 0.8397\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [13300][55000]\t Training Loss 0.8396\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [13350][55000]\t Training Loss 0.8400\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [13400][55000]\t Training Loss 0.8403\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [13450][55000]\t Training Loss 0.8398\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [13500][55000]\t Training Loss 0.8394\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [13550][55000]\t Training Loss 0.8390\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [13600][55000]\t Training Loss 0.8382\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [13650][55000]\t Training Loss 0.8380\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [13700][55000]\t Training Loss 0.8388\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [13750][55000]\t Training Loss 0.8395\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [13800][55000]\t Training Loss 0.8396\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [13850][55000]\t Training Loss 0.8395\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [13900][55000]\t Training Loss 0.8397\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [13950][55000]\t Training Loss 0.8401\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [14000][55000]\t Training Loss 0.8408\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [14050][55000]\t Training Loss 0.8410\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [14100][55000]\t Training Loss 0.8411\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [14150][55000]\t Training Loss 0.8413\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [14200][55000]\t Training Loss 0.8412\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [14250][55000]\t Training Loss 0.8415\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [14300][55000]\t Training Loss 0.8418\t Accuracy 0.8454\n",
      "Epoch [18][20]\t Batch [14350][55000]\t Training Loss 0.8422\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [14400][55000]\t Training Loss 0.8430\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [14450][55000]\t Training Loss 0.8432\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [14500][55000]\t Training Loss 0.8433\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [14550][55000]\t Training Loss 0.8440\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [14600][55000]\t Training Loss 0.8440\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [14650][55000]\t Training Loss 0.8450\t Accuracy 0.8445\n",
      "Epoch [18][20]\t Batch [14700][55000]\t Training Loss 0.8458\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [14750][55000]\t Training Loss 0.8464\t Accuracy 0.8440\n",
      "Epoch [18][20]\t Batch [14800][55000]\t Training Loss 0.8473\t Accuracy 0.8438\n",
      "Epoch [18][20]\t Batch [14850][55000]\t Training Loss 0.8479\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [14900][55000]\t Training Loss 0.8478\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [14950][55000]\t Training Loss 0.8479\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [15000][55000]\t Training Loss 0.8479\t Accuracy 0.8433\n",
      "Epoch [18][20]\t Batch [15050][55000]\t Training Loss 0.8475\t Accuracy 0.8437\n",
      "Epoch [18][20]\t Batch [15100][55000]\t Training Loss 0.8473\t Accuracy 0.8439\n",
      "Epoch [18][20]\t Batch [15150][55000]\t Training Loss 0.8477\t Accuracy 0.8438\n",
      "Epoch [18][20]\t Batch [15200][55000]\t Training Loss 0.8479\t Accuracy 0.8438\n",
      "Epoch [18][20]\t Batch [15250][55000]\t Training Loss 0.8478\t Accuracy 0.8439\n",
      "Epoch [18][20]\t Batch [15300][55000]\t Training Loss 0.8478\t Accuracy 0.8439\n",
      "Epoch [18][20]\t Batch [15350][55000]\t Training Loss 0.8475\t Accuracy 0.8441\n",
      "Epoch [18][20]\t Batch [15400][55000]\t Training Loss 0.8479\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [15450][55000]\t Training Loss 0.8480\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [15500][55000]\t Training Loss 0.8477\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [15550][55000]\t Training Loss 0.8476\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [15600][55000]\t Training Loss 0.8475\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [15650][55000]\t Training Loss 0.8474\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [15700][55000]\t Training Loss 0.8473\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [15750][55000]\t Training Loss 0.8481\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [15800][55000]\t Training Loss 0.8486\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [15850][55000]\t Training Loss 0.8489\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [15900][55000]\t Training Loss 0.8494\t Accuracy 0.8442\n",
      "Epoch [18][20]\t Batch [15950][55000]\t Training Loss 0.8495\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [16000][55000]\t Training Loss 0.8495\t Accuracy 0.8440\n",
      "Epoch [18][20]\t Batch [16050][55000]\t Training Loss 0.8505\t Accuracy 0.8436\n",
      "Epoch [18][20]\t Batch [16100][55000]\t Training Loss 0.8505\t Accuracy 0.8436\n",
      "Epoch [18][20]\t Batch [16150][55000]\t Training Loss 0.8504\t Accuracy 0.8437\n",
      "Epoch [18][20]\t Batch [16200][55000]\t Training Loss 0.8505\t Accuracy 0.8435\n",
      "Epoch [18][20]\t Batch [16250][55000]\t Training Loss 0.8503\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [16300][55000]\t Training Loss 0.8500\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [16350][55000]\t Training Loss 0.8497\t Accuracy 0.8437\n",
      "Epoch [18][20]\t Batch [16400][55000]\t Training Loss 0.8498\t Accuracy 0.8437\n",
      "Epoch [18][20]\t Batch [16450][55000]\t Training Loss 0.8495\t Accuracy 0.8440\n",
      "Epoch [18][20]\t Batch [16500][55000]\t Training Loss 0.8493\t Accuracy 0.8441\n",
      "Epoch [18][20]\t Batch [16550][55000]\t Training Loss 0.8489\t Accuracy 0.8442\n",
      "Epoch [18][20]\t Batch [16600][55000]\t Training Loss 0.8490\t Accuracy 0.8442\n",
      "Epoch [18][20]\t Batch [16650][55000]\t Training Loss 0.8490\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [16700][55000]\t Training Loss 0.8491\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [16750][55000]\t Training Loss 0.8489\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [16800][55000]\t Training Loss 0.8495\t Accuracy 0.8442\n",
      "Epoch [18][20]\t Batch [16850][55000]\t Training Loss 0.8501\t Accuracy 0.8440\n",
      "Epoch [18][20]\t Batch [16900][55000]\t Training Loss 0.8503\t Accuracy 0.8440\n",
      "Epoch [18][20]\t Batch [16950][55000]\t Training Loss 0.8503\t Accuracy 0.8439\n",
      "Epoch [18][20]\t Batch [17000][55000]\t Training Loss 0.8511\t Accuracy 0.8435\n",
      "Epoch [18][20]\t Batch [17050][55000]\t Training Loss 0.8510\t Accuracy 0.8435\n",
      "Epoch [18][20]\t Batch [17100][55000]\t Training Loss 0.8514\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [17150][55000]\t Training Loss 0.8512\t Accuracy 0.8435\n",
      "Epoch [18][20]\t Batch [17200][55000]\t Training Loss 0.8513\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [17250][55000]\t Training Loss 0.8518\t Accuracy 0.8433\n",
      "Epoch [18][20]\t Batch [17300][55000]\t Training Loss 0.8516\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [17350][55000]\t Training Loss 0.8510\t Accuracy 0.8436\n",
      "Epoch [18][20]\t Batch [17400][55000]\t Training Loss 0.8512\t Accuracy 0.8436\n",
      "Epoch [18][20]\t Batch [17450][55000]\t Training Loss 0.8513\t Accuracy 0.8436\n",
      "Epoch [18][20]\t Batch [17500][55000]\t Training Loss 0.8513\t Accuracy 0.8437\n",
      "Epoch [18][20]\t Batch [17550][55000]\t Training Loss 0.8520\t Accuracy 0.8435\n",
      "Epoch [18][20]\t Batch [17600][55000]\t Training Loss 0.8526\t Accuracy 0.8432\n",
      "Epoch [18][20]\t Batch [17650][55000]\t Training Loss 0.8528\t Accuracy 0.8435\n",
      "Epoch [18][20]\t Batch [17700][55000]\t Training Loss 0.8536\t Accuracy 0.8432\n",
      "Epoch [18][20]\t Batch [17750][55000]\t Training Loss 0.8539\t Accuracy 0.8433\n",
      "Epoch [18][20]\t Batch [17800][55000]\t Training Loss 0.8543\t Accuracy 0.8430\n",
      "Epoch [18][20]\t Batch [17850][55000]\t Training Loss 0.8546\t Accuracy 0.8428\n",
      "Epoch [18][20]\t Batch [17900][55000]\t Training Loss 0.8549\t Accuracy 0.8427\n",
      "Epoch [18][20]\t Batch [17950][55000]\t Training Loss 0.8547\t Accuracy 0.8425\n",
      "Epoch [18][20]\t Batch [18000][55000]\t Training Loss 0.8544\t Accuracy 0.8427\n",
      "Epoch [18][20]\t Batch [18050][55000]\t Training Loss 0.8546\t Accuracy 0.8427\n",
      "Epoch [18][20]\t Batch [18100][55000]\t Training Loss 0.8545\t Accuracy 0.8428\n",
      "Epoch [18][20]\t Batch [18150][55000]\t Training Loss 0.8541\t Accuracy 0.8429\n",
      "Epoch [18][20]\t Batch [18200][55000]\t Training Loss 0.8536\t Accuracy 0.8431\n",
      "Epoch [18][20]\t Batch [18250][55000]\t Training Loss 0.8536\t Accuracy 0.8432\n",
      "Epoch [18][20]\t Batch [18300][55000]\t Training Loss 0.8531\t Accuracy 0.8433\n",
      "Epoch [18][20]\t Batch [18350][55000]\t Training Loss 0.8530\t Accuracy 0.8436\n",
      "Epoch [18][20]\t Batch [18400][55000]\t Training Loss 0.8531\t Accuracy 0.8435\n",
      "Epoch [18][20]\t Batch [18450][55000]\t Training Loss 0.8535\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [18500][55000]\t Training Loss 0.8535\t Accuracy 0.8434\n",
      "Epoch [18][20]\t Batch [18550][55000]\t Training Loss 0.8533\t Accuracy 0.8436\n",
      "Epoch [18][20]\t Batch [18600][55000]\t Training Loss 0.8533\t Accuracy 0.8439\n",
      "Epoch [18][20]\t Batch [18650][55000]\t Training Loss 0.8531\t Accuracy 0.8440\n",
      "Epoch [18][20]\t Batch [18700][55000]\t Training Loss 0.8533\t Accuracy 0.8439\n",
      "Epoch [18][20]\t Batch [18750][55000]\t Training Loss 0.8536\t Accuracy 0.8439\n",
      "Epoch [18][20]\t Batch [18800][55000]\t Training Loss 0.8532\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [18850][55000]\t Training Loss 0.8535\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [18900][55000]\t Training Loss 0.8529\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [18950][55000]\t Training Loss 0.8527\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [19000][55000]\t Training Loss 0.8526\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [19050][55000]\t Training Loss 0.8530\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [19100][55000]\t Training Loss 0.8533\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [19150][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [19200][55000]\t Training Loss 0.8535\t Accuracy 0.8442\n",
      "Epoch [18][20]\t Batch [19250][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [19300][55000]\t Training Loss 0.8532\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [19350][55000]\t Training Loss 0.8532\t Accuracy 0.8442\n",
      "Epoch [18][20]\t Batch [19400][55000]\t Training Loss 0.8530\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [19450][55000]\t Training Loss 0.8528\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [19500][55000]\t Training Loss 0.8524\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [19550][55000]\t Training Loss 0.8525\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [19600][55000]\t Training Loss 0.8524\t Accuracy 0.8441\n",
      "Epoch [18][20]\t Batch [19650][55000]\t Training Loss 0.8520\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [19700][55000]\t Training Loss 0.8515\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [19750][55000]\t Training Loss 0.8509\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [19800][55000]\t Training Loss 0.8503\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [19850][55000]\t Training Loss 0.8504\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [19900][55000]\t Training Loss 0.8501\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [19950][55000]\t Training Loss 0.8503\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [20000][55000]\t Training Loss 0.8502\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [20050][55000]\t Training Loss 0.8507\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [20100][55000]\t Training Loss 0.8509\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [20150][55000]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [20200][55000]\t Training Loss 0.8510\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [20250][55000]\t Training Loss 0.8511\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [20300][55000]\t Training Loss 0.8512\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [20350][55000]\t Training Loss 0.8512\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [20400][55000]\t Training Loss 0.8509\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [20450][55000]\t Training Loss 0.8505\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [20500][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [20550][55000]\t Training Loss 0.8502\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [20600][55000]\t Training Loss 0.8502\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [20650][55000]\t Training Loss 0.8499\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [20700][55000]\t Training Loss 0.8496\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [20750][55000]\t Training Loss 0.8496\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [20800][55000]\t Training Loss 0.8497\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [20850][55000]\t Training Loss 0.8496\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [20900][55000]\t Training Loss 0.8499\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [20950][55000]\t Training Loss 0.8504\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [21000][55000]\t Training Loss 0.8506\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [21050][55000]\t Training Loss 0.8509\t Accuracy 0.8443\n",
      "Epoch [18][20]\t Batch [21100][55000]\t Training Loss 0.8506\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [21150][55000]\t Training Loss 0.8506\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [21200][55000]\t Training Loss 0.8503\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [21250][55000]\t Training Loss 0.8501\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [21300][55000]\t Training Loss 0.8497\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [21350][55000]\t Training Loss 0.8498\t Accuracy 0.8454\n",
      "Epoch [18][20]\t Batch [21400][55000]\t Training Loss 0.8498\t Accuracy 0.8454\n",
      "Epoch [18][20]\t Batch [21450][55000]\t Training Loss 0.8499\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [21500][55000]\t Training Loss 0.8495\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [21550][55000]\t Training Loss 0.8493\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [21600][55000]\t Training Loss 0.8495\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [21650][55000]\t Training Loss 0.8495\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [21700][55000]\t Training Loss 0.8494\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [21750][55000]\t Training Loss 0.8494\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [21800][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [21850][55000]\t Training Loss 0.8485\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [21900][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [21950][55000]\t Training Loss 0.8476\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [22000][55000]\t Training Loss 0.8472\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [22050][55000]\t Training Loss 0.8468\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [22100][55000]\t Training Loss 0.8469\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [22150][55000]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [22200][55000]\t Training Loss 0.8476\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [22250][55000]\t Training Loss 0.8476\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [22300][55000]\t Training Loss 0.8478\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [22350][55000]\t Training Loss 0.8476\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [22400][55000]\t Training Loss 0.8475\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [22450][55000]\t Training Loss 0.8475\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [22500][55000]\t Training Loss 0.8479\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [22550][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [22600][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [22650][55000]\t Training Loss 0.8491\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [22700][55000]\t Training Loss 0.8490\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [22750][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [22800][55000]\t Training Loss 0.8488\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [22850][55000]\t Training Loss 0.8488\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [22900][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [22950][55000]\t Training Loss 0.8484\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [23000][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [23050][55000]\t Training Loss 0.8479\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [23100][55000]\t Training Loss 0.8481\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [23150][55000]\t Training Loss 0.8480\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [23200][55000]\t Training Loss 0.8480\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [23250][55000]\t Training Loss 0.8478\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [23300][55000]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [23350][55000]\t Training Loss 0.8473\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [23400][55000]\t Training Loss 0.8470\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [23450][55000]\t Training Loss 0.8469\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [23500][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [23550][55000]\t Training Loss 0.8466\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [23600][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [23650][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [23700][55000]\t Training Loss 0.8470\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [23750][55000]\t Training Loss 0.8476\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [23800][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [23850][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [23900][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [23950][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [24000][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [24050][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [24100][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [24150][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [24200][55000]\t Training Loss 0.8468\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [24250][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [24300][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [24350][55000]\t Training Loss 0.8471\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [24400][55000]\t Training Loss 0.8471\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [24450][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [24500][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [24550][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [24600][55000]\t Training Loss 0.8473\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [24650][55000]\t Training Loss 0.8473\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [24700][55000]\t Training Loss 0.8473\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [24750][55000]\t Training Loss 0.8476\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [24800][55000]\t Training Loss 0.8479\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [24850][55000]\t Training Loss 0.8478\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [24900][55000]\t Training Loss 0.8479\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [24950][55000]\t Training Loss 0.8482\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [25000][55000]\t Training Loss 0.8484\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [25050][55000]\t Training Loss 0.8481\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [25100][55000]\t Training Loss 0.8481\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [25150][55000]\t Training Loss 0.8479\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [25200][55000]\t Training Loss 0.8477\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [25250][55000]\t Training Loss 0.8477\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [25300][55000]\t Training Loss 0.8476\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [25350][55000]\t Training Loss 0.8477\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [25400][55000]\t Training Loss 0.8473\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [25450][55000]\t Training Loss 0.8468\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [25500][55000]\t Training Loss 0.8466\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [25550][55000]\t Training Loss 0.8463\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [25600][55000]\t Training Loss 0.8462\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [25650][55000]\t Training Loss 0.8461\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [25700][55000]\t Training Loss 0.8459\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [25750][55000]\t Training Loss 0.8456\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [25800][55000]\t Training Loss 0.8456\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [25850][55000]\t Training Loss 0.8457\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [25900][55000]\t Training Loss 0.8458\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [25950][55000]\t Training Loss 0.8458\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [26000][55000]\t Training Loss 0.8457\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [26050][55000]\t Training Loss 0.8455\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [26100][55000]\t Training Loss 0.8453\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [26150][55000]\t Training Loss 0.8451\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [26200][55000]\t Training Loss 0.8449\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [26250][55000]\t Training Loss 0.8448\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [26300][55000]\t Training Loss 0.8451\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [26350][55000]\t Training Loss 0.8449\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [26400][55000]\t Training Loss 0.8453\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [26450][55000]\t Training Loss 0.8455\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [26500][55000]\t Training Loss 0.8457\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [26550][55000]\t Training Loss 0.8459\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [26600][55000]\t Training Loss 0.8461\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [26650][55000]\t Training Loss 0.8465\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [26700][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [26750][55000]\t Training Loss 0.8465\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [26800][55000]\t Training Loss 0.8464\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [26850][55000]\t Training Loss 0.8463\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [26900][55000]\t Training Loss 0.8466\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [26950][55000]\t Training Loss 0.8465\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [27000][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [27050][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [27100][55000]\t Training Loss 0.8460\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [27150][55000]\t Training Loss 0.8460\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [27200][55000]\t Training Loss 0.8463\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [27250][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [27300][55000]\t Training Loss 0.8462\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [27350][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [27400][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [27450][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [27500][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [27550][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [27600][55000]\t Training Loss 0.8459\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [27650][55000]\t Training Loss 0.8460\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [27700][55000]\t Training Loss 0.8460\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [27750][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [27800][55000]\t Training Loss 0.8462\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [27850][55000]\t Training Loss 0.8462\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [27900][55000]\t Training Loss 0.8461\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [27950][55000]\t Training Loss 0.8459\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [28000][55000]\t Training Loss 0.8456\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [28050][55000]\t Training Loss 0.8453\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [28100][55000]\t Training Loss 0.8450\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [28150][55000]\t Training Loss 0.8448\t Accuracy 0.8480\n",
      "Epoch [18][20]\t Batch [28200][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [28250][55000]\t Training Loss 0.8445\t Accuracy 0.8480\n",
      "Epoch [18][20]\t Batch [28300][55000]\t Training Loss 0.8444\t Accuracy 0.8480\n",
      "Epoch [18][20]\t Batch [28350][55000]\t Training Loss 0.8442\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [28400][55000]\t Training Loss 0.8446\t Accuracy 0.8480\n",
      "Epoch [18][20]\t Batch [28450][55000]\t Training Loss 0.8443\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [28500][55000]\t Training Loss 0.8442\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [28550][55000]\t Training Loss 0.8441\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [28600][55000]\t Training Loss 0.8439\t Accuracy 0.8484\n",
      "Epoch [18][20]\t Batch [28650][55000]\t Training Loss 0.8442\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [28700][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [28750][55000]\t Training Loss 0.8446\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [28800][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [28850][55000]\t Training Loss 0.8444\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [28900][55000]\t Training Loss 0.8443\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [28950][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [29000][55000]\t Training Loss 0.8442\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [29050][55000]\t Training Loss 0.8442\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [29100][55000]\t Training Loss 0.8444\t Accuracy 0.8481\n",
      "Epoch [18][20]\t Batch [29150][55000]\t Training Loss 0.8448\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [29200][55000]\t Training Loss 0.8451\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [29250][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [29300][55000]\t Training Loss 0.8451\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [29350][55000]\t Training Loss 0.8453\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [29400][55000]\t Training Loss 0.8452\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [29450][55000]\t Training Loss 0.8448\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [29500][55000]\t Training Loss 0.8446\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [29550][55000]\t Training Loss 0.8445\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [29600][55000]\t Training Loss 0.8445\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [29650][55000]\t Training Loss 0.8444\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [29700][55000]\t Training Loss 0.8445\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [29750][55000]\t Training Loss 0.8449\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [29800][55000]\t Training Loss 0.8451\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [29850][55000]\t Training Loss 0.8454\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [29900][55000]\t Training Loss 0.8457\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [29950][55000]\t Training Loss 0.8460\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [30000][55000]\t Training Loss 0.8464\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [30050][55000]\t Training Loss 0.8465\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [30100][55000]\t Training Loss 0.8468\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [30150][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [30200][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [30250][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [30300][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [30350][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [30400][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [30450][55000]\t Training Loss 0.8473\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [30500][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [30550][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [30600][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [30650][55000]\t Training Loss 0.8483\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [30700][55000]\t Training Loss 0.8486\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [30750][55000]\t Training Loss 0.8488\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [30800][55000]\t Training Loss 0.8490\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [30850][55000]\t Training Loss 0.8489\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [30900][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [30950][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [31000][55000]\t Training Loss 0.8491\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [31050][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [31100][55000]\t Training Loss 0.8492\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [31150][55000]\t Training Loss 0.8493\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [31200][55000]\t Training Loss 0.8493\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [31250][55000]\t Training Loss 0.8493\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [31300][55000]\t Training Loss 0.8496\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [31350][55000]\t Training Loss 0.8502\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [31400][55000]\t Training Loss 0.8504\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [31450][55000]\t Training Loss 0.8507\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [31500][55000]\t Training Loss 0.8507\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [31550][55000]\t Training Loss 0.8507\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [31600][55000]\t Training Loss 0.8509\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [31650][55000]\t Training Loss 0.8511\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [31700][55000]\t Training Loss 0.8513\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [31750][55000]\t Training Loss 0.8517\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [31800][55000]\t Training Loss 0.8519\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [31850][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [31900][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [31950][55000]\t Training Loss 0.8517\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [32000][55000]\t Training Loss 0.8518\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [32050][55000]\t Training Loss 0.8517\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [32100][55000]\t Training Loss 0.8516\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [32150][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [32200][55000]\t Training Loss 0.8521\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [32250][55000]\t Training Loss 0.8524\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [32300][55000]\t Training Loss 0.8527\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [32350][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [32400][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [32450][55000]\t Training Loss 0.8533\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [32500][55000]\t Training Loss 0.8535\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [32550][55000]\t Training Loss 0.8538\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [32600][55000]\t Training Loss 0.8538\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [32650][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [32700][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [32750][55000]\t Training Loss 0.8537\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [32800][55000]\t Training Loss 0.8538\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [32850][55000]\t Training Loss 0.8538\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [32900][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [32950][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [33000][55000]\t Training Loss 0.8534\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [33050][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [33100][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [33150][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [33200][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [33250][55000]\t Training Loss 0.8537\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [33300][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [33350][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [33400][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [33450][55000]\t Training Loss 0.8539\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [33500][55000]\t Training Loss 0.8538\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [33550][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [33600][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [33650][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [33700][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [33750][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [33800][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [33850][55000]\t Training Loss 0.8533\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [33900][55000]\t Training Loss 0.8529\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [33950][55000]\t Training Loss 0.8526\t Accuracy 0.8466\n",
      "Epoch [18][20]\t Batch [34000][55000]\t Training Loss 0.8525\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [34050][55000]\t Training Loss 0.8527\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [34100][55000]\t Training Loss 0.8528\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [34150][55000]\t Training Loss 0.8529\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [34200][55000]\t Training Loss 0.8526\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [34250][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [34300][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [34350][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [34400][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [34450][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [34500][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [34550][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [34600][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [34650][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [34700][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [34750][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [34800][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [34850][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [34900][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [34950][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [35000][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [35050][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [35100][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [35150][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [35200][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [35250][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [35300][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [35350][55000]\t Training Loss 0.8530\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [35400][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [35450][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [35500][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [35550][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [35600][55000]\t Training Loss 0.8526\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [35650][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [35700][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [35750][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [35800][55000]\t Training Loss 0.8528\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [35850][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [35900][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [35950][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [36000][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [36050][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [36100][55000]\t Training Loss 0.8531\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [36150][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [36200][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [36250][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [36300][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [36350][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [36400][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [36450][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [36500][55000]\t Training Loss 0.8525\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [36550][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [36600][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [36650][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [36700][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [36750][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [36800][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [36850][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [36900][55000]\t Training Loss 0.8514\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [36950][55000]\t Training Loss 0.8513\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [37000][55000]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [37050][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [37100][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [37150][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [37200][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [37250][55000]\t Training Loss 0.8509\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [37300][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [18][20]\t Batch [37350][55000]\t Training Loss 0.8511\t Accuracy 0.8478\n",
      "Epoch [18][20]\t Batch [37400][55000]\t Training Loss 0.8513\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [37450][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [37500][55000]\t Training Loss 0.8517\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [37550][55000]\t Training Loss 0.8518\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [37600][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [37650][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [37700][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [37750][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [37800][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [37850][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [37900][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [37950][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [38000][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [38050][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [38100][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [38150][55000]\t Training Loss 0.8521\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [38200][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [38250][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [38300][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [38350][55000]\t Training Loss 0.8523\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [38400][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [38450][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [38500][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [38550][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [38600][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [38650][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [38700][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [38750][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [38800][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [38850][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [38900][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [38950][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39000][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39050][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39100][55000]\t Training Loss 0.8515\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [39150][55000]\t Training Loss 0.8514\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [39200][55000]\t Training Loss 0.8512\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [39250][55000]\t Training Loss 0.8510\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [39300][55000]\t Training Loss 0.8510\t Accuracy 0.8477\n",
      "Epoch [18][20]\t Batch [39350][55000]\t Training Loss 0.8513\t Accuracy 0.8475\n",
      "Epoch [18][20]\t Batch [39400][55000]\t Training Loss 0.8513\t Accuracy 0.8476\n",
      "Epoch [18][20]\t Batch [39450][55000]\t Training Loss 0.8516\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [39500][55000]\t Training Loss 0.8516\t Accuracy 0.8474\n",
      "Epoch [18][20]\t Batch [39550][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39600][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39650][55000]\t Training Loss 0.8516\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39700][55000]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39750][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39800][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39850][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [39900][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [39950][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40000][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40050][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40100][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40150][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40200][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40250][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40300][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40350][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40400][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40450][55000]\t Training Loss 0.8520\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40500][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40550][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40600][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40650][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40700][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40750][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [40800][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40850][55000]\t Training Loss 0.8520\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40900][55000]\t Training Loss 0.8518\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [40950][55000]\t Training Loss 0.8517\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [41000][55000]\t Training Loss 0.8516\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [41050][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [41100][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [41150][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [41200][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [41250][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [41300][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [41350][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [41400][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [41450][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [41500][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [41550][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [41600][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [41650][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [41700][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [18][20]\t Batch [41750][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [41800][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [18][20]\t Batch [41850][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [41900][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [41950][55000]\t Training Loss 0.8533\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [42000][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [42050][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [18][20]\t Batch [42100][55000]\t Training Loss 0.8530\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [42150][55000]\t Training Loss 0.8532\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [42200][55000]\t Training Loss 0.8532\t Accuracy 0.8468\n",
      "Epoch [18][20]\t Batch [42250][55000]\t Training Loss 0.8533\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [42300][55000]\t Training Loss 0.8533\t Accuracy 0.8467\n",
      "Epoch [18][20]\t Batch [42350][55000]\t Training Loss 0.8535\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [42400][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [42450][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [42500][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [42550][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [42600][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [42650][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [42700][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [42750][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [42800][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [42850][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [42900][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [42950][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [43000][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [43050][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [43100][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [43150][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [43200][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [43250][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [43300][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [43350][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [43400][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [43450][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [43500][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [43550][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [43600][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [43650][55000]\t Training Loss 0.8532\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [43700][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [43750][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [43800][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [43850][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [43900][55000]\t Training Loss 0.8533\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [43950][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [44000][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [44050][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [44100][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [44150][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [44200][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [44250][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [44300][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [44350][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [44400][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [44450][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [44500][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [44550][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [44600][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [44650][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [44700][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [44750][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [44800][55000]\t Training Loss 0.8536\t Accuracy 0.8465\n",
      "Epoch [18][20]\t Batch [44850][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [18][20]\t Batch [44900][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [44950][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45000][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45050][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [45100][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45150][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [45200][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [45250][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45300][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45350][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [45400][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [45450][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [18][20]\t Batch [45500][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [45550][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45600][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45650][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [45700][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [45750][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45800][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45850][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [45900][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [45950][55000]\t Training Loss 0.8544\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [46000][55000]\t Training Loss 0.8545\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [46050][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [46100][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [46150][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46200][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46250][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46300][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [46350][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [46400][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [46450][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [46500][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [46550][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46600][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46650][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46700][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46750][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46800][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [46850][55000]\t Training Loss 0.8546\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46900][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [46950][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47000][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47050][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47100][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47150][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47200][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47250][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47300][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47350][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47400][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47450][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47500][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47550][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47600][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47650][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47700][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47750][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47800][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [47850][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47900][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [47950][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [48000][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [48050][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [48100][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [48150][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [48200][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [48250][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [48300][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [48350][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [48400][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [48450][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [48500][55000]\t Training Loss 0.8532\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [48550][55000]\t Training Loss 0.8531\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [48600][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [48650][55000]\t Training Loss 0.8530\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [48700][55000]\t Training Loss 0.8529\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [48750][55000]\t Training Loss 0.8527\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [48800][55000]\t Training Loss 0.8527\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [48850][55000]\t Training Loss 0.8525\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [48900][55000]\t Training Loss 0.8525\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [48950][55000]\t Training Loss 0.8527\t Accuracy 0.8461\n",
      "Epoch [18][20]\t Batch [49000][55000]\t Training Loss 0.8528\t Accuracy 0.8460\n",
      "Epoch [18][20]\t Batch [49050][55000]\t Training Loss 0.8531\t Accuracy 0.8459\n",
      "Epoch [18][20]\t Batch [49100][55000]\t Training Loss 0.8533\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [49150][55000]\t Training Loss 0.8533\t Accuracy 0.8458\n",
      "Epoch [18][20]\t Batch [49200][55000]\t Training Loss 0.8533\t Accuracy 0.8457\n",
      "Epoch [18][20]\t Batch [49250][55000]\t Training Loss 0.8535\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [49300][55000]\t Training Loss 0.8534\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [49350][55000]\t Training Loss 0.8532\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [49400][55000]\t Training Loss 0.8531\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [49450][55000]\t Training Loss 0.8531\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [49500][55000]\t Training Loss 0.8533\t Accuracy 0.8455\n",
      "Epoch [18][20]\t Batch [49550][55000]\t Training Loss 0.8536\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [49600][55000]\t Training Loss 0.8539\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [49650][55000]\t Training Loss 0.8540\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [49700][55000]\t Training Loss 0.8542\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [49750][55000]\t Training Loss 0.8542\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [49800][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [49850][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [49900][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [49950][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [50000][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [50050][55000]\t Training Loss 0.8546\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [50100][55000]\t Training Loss 0.8546\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [50150][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [50200][55000]\t Training Loss 0.8545\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [50250][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [50300][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [50350][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [50400][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [50450][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [50500][55000]\t Training Loss 0.8552\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [50550][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [50600][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [50650][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [50700][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [50750][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [50800][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [50850][55000]\t Training Loss 0.8554\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [50900][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [50950][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [51000][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [51050][55000]\t Training Loss 0.8549\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [51100][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [51150][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [51200][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [51250][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [51300][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [51350][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [51400][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [51450][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [51500][55000]\t Training Loss 0.8546\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [51550][55000]\t Training Loss 0.8546\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [51600][55000]\t Training Loss 0.8544\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [51650][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [51700][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [51750][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [51800][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [51850][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [51900][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [51950][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [52000][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [52050][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [52100][55000]\t Training Loss 0.8545\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [52150][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [52200][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [52250][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [52300][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [52350][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [52400][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [52450][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [52500][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [52550][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [52600][55000]\t Training Loss 0.8543\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [52650][55000]\t Training Loss 0.8542\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [52700][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [52750][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [52800][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [52850][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [52900][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [18][20]\t Batch [52950][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [53000][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53050][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [53100][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53150][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [53200][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53250][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53300][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53350][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53400][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [53450][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [53500][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53550][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53600][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [53650][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [53700][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [53750][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53800][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [53850][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53900][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [53950][55000]\t Training Loss 0.8548\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54000][55000]\t Training Loss 0.8549\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54050][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54100][55000]\t Training Loss 0.8552\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [54150][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54200][55000]\t Training Loss 0.8552\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [54250][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54300][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54350][55000]\t Training Loss 0.8549\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54400][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [54450][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [54500][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [54550][55000]\t Training Loss 0.8546\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [54600][55000]\t Training Loss 0.8547\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54650][55000]\t Training Loss 0.8545\t Accuracy 0.8447\n",
      "Epoch [18][20]\t Batch [54700][55000]\t Training Loss 0.8544\t Accuracy 0.8448\n",
      "Epoch [18][20]\t Batch [54750][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [54800][55000]\t Training Loss 0.8542\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [54850][55000]\t Training Loss 0.8541\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [54900][55000]\t Training Loss 0.8542\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [54950][55000]\t Training Loss 0.8546\t Accuracy 0.8448\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8547\t Average training accuracy 0.8447\n",
      "Epoch [18]\t Average validation loss 0.7920\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [19][20]\t Batch [0][55000]\t Training Loss 1.4692\t Accuracy 0.0000\n",
      "Epoch [19][20]\t Batch [50][55000]\t Training Loss 0.9379\t Accuracy 0.7255\n",
      "Epoch [19][20]\t Batch [100][55000]\t Training Loss 0.8486\t Accuracy 0.8020\n",
      "Epoch [19][20]\t Batch [150][55000]\t Training Loss 0.8500\t Accuracy 0.8146\n",
      "Epoch [19][20]\t Batch [200][55000]\t Training Loss 0.8609\t Accuracy 0.8159\n",
      "Epoch [19][20]\t Batch [250][55000]\t Training Loss 0.8431\t Accuracy 0.8247\n",
      "Epoch [19][20]\t Batch [300][55000]\t Training Loss 0.8447\t Accuracy 0.8206\n",
      "Epoch [19][20]\t Batch [350][55000]\t Training Loss 0.8249\t Accuracy 0.8234\n",
      "Epoch [19][20]\t Batch [400][55000]\t Training Loss 0.8110\t Accuracy 0.8279\n",
      "Epoch [19][20]\t Batch [450][55000]\t Training Loss 0.8169\t Accuracy 0.8293\n",
      "Epoch [19][20]\t Batch [500][55000]\t Training Loss 0.8232\t Accuracy 0.8303\n",
      "Epoch [19][20]\t Batch [550][55000]\t Training Loss 0.8405\t Accuracy 0.8276\n",
      "Epoch [19][20]\t Batch [600][55000]\t Training Loss 0.8450\t Accuracy 0.8303\n",
      "Epoch [19][20]\t Batch [650][55000]\t Training Loss 0.8648\t Accuracy 0.8203\n",
      "Epoch [19][20]\t Batch [700][55000]\t Training Loss 0.8625\t Accuracy 0.8260\n",
      "Epoch [19][20]\t Batch [750][55000]\t Training Loss 0.8587\t Accuracy 0.8269\n",
      "Epoch [19][20]\t Batch [800][55000]\t Training Loss 0.8527\t Accuracy 0.8290\n",
      "Epoch [19][20]\t Batch [850][55000]\t Training Loss 0.8513\t Accuracy 0.8308\n",
      "Epoch [19][20]\t Batch [900][55000]\t Training Loss 0.8577\t Accuracy 0.8302\n",
      "Epoch [19][20]\t Batch [950][55000]\t Training Loss 0.8645\t Accuracy 0.8254\n",
      "Epoch [19][20]\t Batch [1000][55000]\t Training Loss 0.8639\t Accuracy 0.8272\n",
      "Epoch [19][20]\t Batch [1050][55000]\t Training Loss 0.8716\t Accuracy 0.8259\n",
      "Epoch [19][20]\t Batch [1100][55000]\t Training Loss 0.8801\t Accuracy 0.8256\n",
      "Epoch [19][20]\t Batch [1150][55000]\t Training Loss 0.8893\t Accuracy 0.8228\n",
      "Epoch [19][20]\t Batch [1200][55000]\t Training Loss 0.8835\t Accuracy 0.8260\n",
      "Epoch [19][20]\t Batch [1250][55000]\t Training Loss 0.8849\t Accuracy 0.8257\n",
      "Epoch [19][20]\t Batch [1300][55000]\t Training Loss 0.8851\t Accuracy 0.8255\n",
      "Epoch [19][20]\t Batch [1350][55000]\t Training Loss 0.8819\t Accuracy 0.8268\n",
      "Epoch [19][20]\t Batch [1400][55000]\t Training Loss 0.8825\t Accuracy 0.8258\n",
      "Epoch [19][20]\t Batch [1450][55000]\t Training Loss 0.8834\t Accuracy 0.8249\n",
      "Epoch [19][20]\t Batch [1500][55000]\t Training Loss 0.8814\t Accuracy 0.8274\n",
      "Epoch [19][20]\t Batch [1550][55000]\t Training Loss 0.8809\t Accuracy 0.8279\n",
      "Epoch [19][20]\t Batch [1600][55000]\t Training Loss 0.8835\t Accuracy 0.8264\n",
      "Epoch [19][20]\t Batch [1650][55000]\t Training Loss 0.8804\t Accuracy 0.8280\n",
      "Epoch [19][20]\t Batch [1700][55000]\t Training Loss 0.8787\t Accuracy 0.8295\n",
      "Epoch [19][20]\t Batch [1750][55000]\t Training Loss 0.8731\t Accuracy 0.8310\n",
      "Epoch [19][20]\t Batch [1800][55000]\t Training Loss 0.8709\t Accuracy 0.8329\n",
      "Epoch [19][20]\t Batch [1850][55000]\t Training Loss 0.8692\t Accuracy 0.8336\n",
      "Epoch [19][20]\t Batch [1900][55000]\t Training Loss 0.8670\t Accuracy 0.8338\n",
      "Epoch [19][20]\t Batch [1950][55000]\t Training Loss 0.8656\t Accuracy 0.8350\n",
      "Epoch [19][20]\t Batch [2000][55000]\t Training Loss 0.8631\t Accuracy 0.8361\n",
      "Epoch [19][20]\t Batch [2050][55000]\t Training Loss 0.8622\t Accuracy 0.8367\n",
      "Epoch [19][20]\t Batch [2100][55000]\t Training Loss 0.8593\t Accuracy 0.8372\n",
      "Epoch [19][20]\t Batch [2150][55000]\t Training Loss 0.8560\t Accuracy 0.8391\n",
      "Epoch [19][20]\t Batch [2200][55000]\t Training Loss 0.8512\t Accuracy 0.8410\n",
      "Epoch [19][20]\t Batch [2250][55000]\t Training Loss 0.8505\t Accuracy 0.8414\n",
      "Epoch [19][20]\t Batch [2300][55000]\t Training Loss 0.8475\t Accuracy 0.8409\n",
      "Epoch [19][20]\t Batch [2350][55000]\t Training Loss 0.8460\t Accuracy 0.8413\n",
      "Epoch [19][20]\t Batch [2400][55000]\t Training Loss 0.8470\t Accuracy 0.8397\n",
      "Epoch [19][20]\t Batch [2450][55000]\t Training Loss 0.8499\t Accuracy 0.8384\n",
      "Epoch [19][20]\t Batch [2500][55000]\t Training Loss 0.8477\t Accuracy 0.8405\n",
      "Epoch [19][20]\t Batch [2550][55000]\t Training Loss 0.8464\t Accuracy 0.8405\n",
      "Epoch [19][20]\t Batch [2600][55000]\t Training Loss 0.8458\t Accuracy 0.8393\n",
      "Epoch [19][20]\t Batch [2650][55000]\t Training Loss 0.8446\t Accuracy 0.8397\n",
      "Epoch [19][20]\t Batch [2700][55000]\t Training Loss 0.8444\t Accuracy 0.8401\n",
      "Epoch [19][20]\t Batch [2750][55000]\t Training Loss 0.8443\t Accuracy 0.8401\n",
      "Epoch [19][20]\t Batch [2800][55000]\t Training Loss 0.8446\t Accuracy 0.8383\n",
      "Epoch [19][20]\t Batch [2850][55000]\t Training Loss 0.8435\t Accuracy 0.8383\n",
      "Epoch [19][20]\t Batch [2900][55000]\t Training Loss 0.8402\t Accuracy 0.8394\n",
      "Epoch [19][20]\t Batch [2950][55000]\t Training Loss 0.8403\t Accuracy 0.8390\n",
      "Epoch [19][20]\t Batch [3000][55000]\t Training Loss 0.8401\t Accuracy 0.8397\n",
      "Epoch [19][20]\t Batch [3050][55000]\t Training Loss 0.8412\t Accuracy 0.8394\n",
      "Epoch [19][20]\t Batch [3100][55000]\t Training Loss 0.8435\t Accuracy 0.8391\n",
      "Epoch [19][20]\t Batch [3150][55000]\t Training Loss 0.8434\t Accuracy 0.8394\n",
      "Epoch [19][20]\t Batch [3200][55000]\t Training Loss 0.8432\t Accuracy 0.8407\n",
      "Epoch [19][20]\t Batch [3250][55000]\t Training Loss 0.8426\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [3300][55000]\t Training Loss 0.8439\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [3350][55000]\t Training Loss 0.8421\t Accuracy 0.8415\n",
      "Epoch [19][20]\t Batch [3400][55000]\t Training Loss 0.8444\t Accuracy 0.8403\n",
      "Epoch [19][20]\t Batch [3450][55000]\t Training Loss 0.8441\t Accuracy 0.8412\n",
      "Epoch [19][20]\t Batch [3500][55000]\t Training Loss 0.8441\t Accuracy 0.8409\n",
      "Epoch [19][20]\t Batch [3550][55000]\t Training Loss 0.8457\t Accuracy 0.8403\n",
      "Epoch [19][20]\t Batch [3600][55000]\t Training Loss 0.8455\t Accuracy 0.8398\n",
      "Epoch [19][20]\t Batch [3650][55000]\t Training Loss 0.8441\t Accuracy 0.8406\n",
      "Epoch [19][20]\t Batch [3700][55000]\t Training Loss 0.8449\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [3750][55000]\t Training Loss 0.8448\t Accuracy 0.8403\n",
      "Epoch [19][20]\t Batch [3800][55000]\t Training Loss 0.8451\t Accuracy 0.8403\n",
      "Epoch [19][20]\t Batch [3850][55000]\t Training Loss 0.8447\t Accuracy 0.8406\n",
      "Epoch [19][20]\t Batch [3900][55000]\t Training Loss 0.8436\t Accuracy 0.8418\n",
      "Epoch [19][20]\t Batch [3950][55000]\t Training Loss 0.8425\t Accuracy 0.8431\n",
      "Epoch [19][20]\t Batch [4000][55000]\t Training Loss 0.8421\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [4050][55000]\t Training Loss 0.8406\t Accuracy 0.8445\n",
      "Epoch [19][20]\t Batch [4100][55000]\t Training Loss 0.8410\t Accuracy 0.8442\n",
      "Epoch [19][20]\t Batch [4150][55000]\t Training Loss 0.8415\t Accuracy 0.8439\n",
      "Epoch [19][20]\t Batch [4200][55000]\t Training Loss 0.8420\t Accuracy 0.8429\n",
      "Epoch [19][20]\t Batch [4250][55000]\t Training Loss 0.8406\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [4300][55000]\t Training Loss 0.8406\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [4350][55000]\t Training Loss 0.8411\t Accuracy 0.8444\n",
      "Epoch [19][20]\t Batch [4400][55000]\t Training Loss 0.8410\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [4450][55000]\t Training Loss 0.8412\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [4500][55000]\t Training Loss 0.8414\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [4550][55000]\t Training Loss 0.8398\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [4600][55000]\t Training Loss 0.8380\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [4650][55000]\t Training Loss 0.8379\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [4700][55000]\t Training Loss 0.8375\t Accuracy 0.8445\n",
      "Epoch [19][20]\t Batch [4750][55000]\t Training Loss 0.8363\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [4800][55000]\t Training Loss 0.8370\t Accuracy 0.8454\n",
      "Epoch [19][20]\t Batch [4850][55000]\t Training Loss 0.8382\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [4900][55000]\t Training Loss 0.8368\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [4950][55000]\t Training Loss 0.8370\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [5000][55000]\t Training Loss 0.8372\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [5050][55000]\t Training Loss 0.8367\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [5100][55000]\t Training Loss 0.8367\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [5150][55000]\t Training Loss 0.8370\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [5200][55000]\t Training Loss 0.8385\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [5250][55000]\t Training Loss 0.8376\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [5300][55000]\t Training Loss 0.8375\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [5350][55000]\t Training Loss 0.8381\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [5400][55000]\t Training Loss 0.8374\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [5450][55000]\t Training Loss 0.8366\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [5500][55000]\t Training Loss 0.8349\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [5550][55000]\t Training Loss 0.8348\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [5600][55000]\t Training Loss 0.8339\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [5650][55000]\t Training Loss 0.8345\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [5700][55000]\t Training Loss 0.8342\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [5750][55000]\t Training Loss 0.8346\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [5800][55000]\t Training Loss 0.8350\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [5850][55000]\t Training Loss 0.8351\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [5900][55000]\t Training Loss 0.8357\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [5950][55000]\t Training Loss 0.8355\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [6000][55000]\t Training Loss 0.8345\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [6050][55000]\t Training Loss 0.8331\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [6100][55000]\t Training Loss 0.8317\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [6150][55000]\t Training Loss 0.8296\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [6200][55000]\t Training Loss 0.8297\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [6250][55000]\t Training Loss 0.8285\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [6300][55000]\t Training Loss 0.8289\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [6350][55000]\t Training Loss 0.8285\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [6400][55000]\t Training Loss 0.8279\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [6450][55000]\t Training Loss 0.8274\t Accuracy 0.8486\n",
      "Epoch [19][20]\t Batch [6500][55000]\t Training Loss 0.8282\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [6550][55000]\t Training Loss 0.8272\t Accuracy 0.8484\n",
      "Epoch [19][20]\t Batch [6600][55000]\t Training Loss 0.8259\t Accuracy 0.8490\n",
      "Epoch [19][20]\t Batch [6650][55000]\t Training Loss 0.8247\t Accuracy 0.8490\n",
      "Epoch [19][20]\t Batch [6700][55000]\t Training Loss 0.8248\t Accuracy 0.8490\n",
      "Epoch [19][20]\t Batch [6750][55000]\t Training Loss 0.8251\t Accuracy 0.8498\n",
      "Epoch [19][20]\t Batch [6800][55000]\t Training Loss 0.8255\t Accuracy 0.8497\n",
      "Epoch [19][20]\t Batch [6850][55000]\t Training Loss 0.8276\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [6900][55000]\t Training Loss 0.8276\t Accuracy 0.8493\n",
      "Epoch [19][20]\t Batch [6950][55000]\t Training Loss 0.8283\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [7000][55000]\t Training Loss 0.8281\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [7050][55000]\t Training Loss 0.8287\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [7100][55000]\t Training Loss 0.8289\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [7150][55000]\t Training Loss 0.8292\t Accuracy 0.8481\n",
      "Epoch [19][20]\t Batch [7200][55000]\t Training Loss 0.8299\t Accuracy 0.8481\n",
      "Epoch [19][20]\t Batch [7250][55000]\t Training Loss 0.8323\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [7300][55000]\t Training Loss 0.8341\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [7350][55000]\t Training Loss 0.8356\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [7400][55000]\t Training Loss 0.8366\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [7450][55000]\t Training Loss 0.8366\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [7500][55000]\t Training Loss 0.8366\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [7550][55000]\t Training Loss 0.8371\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [7600][55000]\t Training Loss 0.8366\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [7650][55000]\t Training Loss 0.8371\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [7700][55000]\t Training Loss 0.8376\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [7750][55000]\t Training Loss 0.8377\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [7800][55000]\t Training Loss 0.8385\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [7850][55000]\t Training Loss 0.8386\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [7900][55000]\t Training Loss 0.8388\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [7950][55000]\t Training Loss 0.8389\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [8000][55000]\t Training Loss 0.8389\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [8050][55000]\t Training Loss 0.8394\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [8100][55000]\t Training Loss 0.8380\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [8150][55000]\t Training Loss 0.8388\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [8200][55000]\t Training Loss 0.8388\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [8250][55000]\t Training Loss 0.8400\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [8300][55000]\t Training Loss 0.8403\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [8350][55000]\t Training Loss 0.8410\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [8400][55000]\t Training Loss 0.8403\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [8450][55000]\t Training Loss 0.8418\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [8500][55000]\t Training Loss 0.8410\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [8550][55000]\t Training Loss 0.8401\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [8600][55000]\t Training Loss 0.8393\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [8650][55000]\t Training Loss 0.8393\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [8700][55000]\t Training Loss 0.8399\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [8750][55000]\t Training Loss 0.8415\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [8800][55000]\t Training Loss 0.8423\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [8850][55000]\t Training Loss 0.8423\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [8900][55000]\t Training Loss 0.8441\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [8950][55000]\t Training Loss 0.8436\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [9000][55000]\t Training Loss 0.8429\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [9050][55000]\t Training Loss 0.8421\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [9100][55000]\t Training Loss 0.8418\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [9150][55000]\t Training Loss 0.8423\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [9200][55000]\t Training Loss 0.8419\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [9250][55000]\t Training Loss 0.8422\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [9300][55000]\t Training Loss 0.8425\t Accuracy 0.8454\n",
      "Epoch [19][20]\t Batch [9350][55000]\t Training Loss 0.8426\t Accuracy 0.8454\n",
      "Epoch [19][20]\t Batch [9400][55000]\t Training Loss 0.8429\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [9450][55000]\t Training Loss 0.8431\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [9500][55000]\t Training Loss 0.8422\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [9550][55000]\t Training Loss 0.8420\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [9600][55000]\t Training Loss 0.8425\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [9650][55000]\t Training Loss 0.8423\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [9700][55000]\t Training Loss 0.8417\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [9750][55000]\t Training Loss 0.8409\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [9800][55000]\t Training Loss 0.8415\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [9850][55000]\t Training Loss 0.8411\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [9900][55000]\t Training Loss 0.8406\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [9950][55000]\t Training Loss 0.8399\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [10000][55000]\t Training Loss 0.8397\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [10050][55000]\t Training Loss 0.8399\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [10100][55000]\t Training Loss 0.8397\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [10150][55000]\t Training Loss 0.8395\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [10200][55000]\t Training Loss 0.8397\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [10250][55000]\t Training Loss 0.8398\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [10300][55000]\t Training Loss 0.8397\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [10350][55000]\t Training Loss 0.8387\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [10400][55000]\t Training Loss 0.8379\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [10450][55000]\t Training Loss 0.8377\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [10500][55000]\t Training Loss 0.8368\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [10550][55000]\t Training Loss 0.8363\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [10600][55000]\t Training Loss 0.8359\t Accuracy 0.8481\n",
      "Epoch [19][20]\t Batch [10650][55000]\t Training Loss 0.8356\t Accuracy 0.8485\n",
      "Epoch [19][20]\t Batch [10700][55000]\t Training Loss 0.8354\t Accuracy 0.8488\n",
      "Epoch [19][20]\t Batch [10750][55000]\t Training Loss 0.8359\t Accuracy 0.8485\n",
      "Epoch [19][20]\t Batch [10800][55000]\t Training Loss 0.8364\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [10850][55000]\t Training Loss 0.8357\t Accuracy 0.8485\n",
      "Epoch [19][20]\t Batch [10900][55000]\t Training Loss 0.8352\t Accuracy 0.8487\n",
      "Epoch [19][20]\t Batch [10950][55000]\t Training Loss 0.8349\t Accuracy 0.8485\n",
      "Epoch [19][20]\t Batch [11000][55000]\t Training Loss 0.8348\t Accuracy 0.8487\n",
      "Epoch [19][20]\t Batch [11050][55000]\t Training Loss 0.8340\t Accuracy 0.8491\n",
      "Epoch [19][20]\t Batch [11100][55000]\t Training Loss 0.8334\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [11150][55000]\t Training Loss 0.8334\t Accuracy 0.8494\n",
      "Epoch [19][20]\t Batch [11200][55000]\t Training Loss 0.8330\t Accuracy 0.8495\n",
      "Epoch [19][20]\t Batch [11250][55000]\t Training Loss 0.8334\t Accuracy 0.8493\n",
      "Epoch [19][20]\t Batch [11300][55000]\t Training Loss 0.8330\t Accuracy 0.8494\n",
      "Epoch [19][20]\t Batch [11350][55000]\t Training Loss 0.8324\t Accuracy 0.8494\n",
      "Epoch [19][20]\t Batch [11400][55000]\t Training Loss 0.8326\t Accuracy 0.8494\n",
      "Epoch [19][20]\t Batch [11450][55000]\t Training Loss 0.8323\t Accuracy 0.8494\n",
      "Epoch [19][20]\t Batch [11500][55000]\t Training Loss 0.8320\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [11550][55000]\t Training Loss 0.8321\t Accuracy 0.8491\n",
      "Epoch [19][20]\t Batch [11600][55000]\t Training Loss 0.8333\t Accuracy 0.8485\n",
      "Epoch [19][20]\t Batch [11650][55000]\t Training Loss 0.8338\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [11700][55000]\t Training Loss 0.8338\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [11750][55000]\t Training Loss 0.8347\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [11800][55000]\t Training Loss 0.8349\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [11850][55000]\t Training Loss 0.8350\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [11900][55000]\t Training Loss 0.8353\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [11950][55000]\t Training Loss 0.8354\t Accuracy 0.8481\n",
      "Epoch [19][20]\t Batch [12000][55000]\t Training Loss 0.8354\t Accuracy 0.8484\n",
      "Epoch [19][20]\t Batch [12050][55000]\t Training Loss 0.8352\t Accuracy 0.8487\n",
      "Epoch [19][20]\t Batch [12100][55000]\t Training Loss 0.8351\t Accuracy 0.8486\n",
      "Epoch [19][20]\t Batch [12150][55000]\t Training Loss 0.8345\t Accuracy 0.8491\n",
      "Epoch [19][20]\t Batch [12200][55000]\t Training Loss 0.8348\t Accuracy 0.8490\n",
      "Epoch [19][20]\t Batch [12250][55000]\t Training Loss 0.8347\t Accuracy 0.8491\n",
      "Epoch [19][20]\t Batch [12300][55000]\t Training Loss 0.8347\t Accuracy 0.8490\n",
      "Epoch [19][20]\t Batch [12350][55000]\t Training Loss 0.8349\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [12400][55000]\t Training Loss 0.8352\t Accuracy 0.8492\n",
      "Epoch [19][20]\t Batch [12450][55000]\t Training Loss 0.8354\t Accuracy 0.8488\n",
      "Epoch [19][20]\t Batch [12500][55000]\t Training Loss 0.8356\t Accuracy 0.8485\n",
      "Epoch [19][20]\t Batch [12550][55000]\t Training Loss 0.8356\t Accuracy 0.8486\n",
      "Epoch [19][20]\t Batch [12600][55000]\t Training Loss 0.8363\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [12650][55000]\t Training Loss 0.8369\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [12700][55000]\t Training Loss 0.8375\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [12750][55000]\t Training Loss 0.8371\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [12800][55000]\t Training Loss 0.8377\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [12850][55000]\t Training Loss 0.8379\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [12900][55000]\t Training Loss 0.8379\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [12950][55000]\t Training Loss 0.8384\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [13000][55000]\t Training Loss 0.8383\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [13050][55000]\t Training Loss 0.8390\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [13100][55000]\t Training Loss 0.8396\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [13150][55000]\t Training Loss 0.8400\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [13200][55000]\t Training Loss 0.8402\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [13250][55000]\t Training Loss 0.8397\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [13300][55000]\t Training Loss 0.8396\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [13350][55000]\t Training Loss 0.8400\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [13400][55000]\t Training Loss 0.8402\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [13450][55000]\t Training Loss 0.8397\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [13500][55000]\t Training Loss 0.8394\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [13550][55000]\t Training Loss 0.8390\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [13600][55000]\t Training Loss 0.8381\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [13650][55000]\t Training Loss 0.8380\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [13700][55000]\t Training Loss 0.8388\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [13750][55000]\t Training Loss 0.8394\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [13800][55000]\t Training Loss 0.8395\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [13850][55000]\t Training Loss 0.8395\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [13900][55000]\t Training Loss 0.8397\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [13950][55000]\t Training Loss 0.8401\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [14000][55000]\t Training Loss 0.8408\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [14050][55000]\t Training Loss 0.8409\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [14100][55000]\t Training Loss 0.8410\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [14150][55000]\t Training Loss 0.8413\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [14200][55000]\t Training Loss 0.8412\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [14250][55000]\t Training Loss 0.8414\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [14300][55000]\t Training Loss 0.8418\t Accuracy 0.8454\n",
      "Epoch [19][20]\t Batch [14350][55000]\t Training Loss 0.8422\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [14400][55000]\t Training Loss 0.8430\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [14450][55000]\t Training Loss 0.8432\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [14500][55000]\t Training Loss 0.8433\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [14550][55000]\t Training Loss 0.8440\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [14600][55000]\t Training Loss 0.8440\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [14650][55000]\t Training Loss 0.8450\t Accuracy 0.8445\n",
      "Epoch [19][20]\t Batch [14700][55000]\t Training Loss 0.8458\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [14750][55000]\t Training Loss 0.8464\t Accuracy 0.8440\n",
      "Epoch [19][20]\t Batch [14800][55000]\t Training Loss 0.8473\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [14850][55000]\t Training Loss 0.8479\t Accuracy 0.8434\n",
      "Epoch [19][20]\t Batch [14900][55000]\t Training Loss 0.8478\t Accuracy 0.8434\n",
      "Epoch [19][20]\t Batch [14950][55000]\t Training Loss 0.8479\t Accuracy 0.8434\n",
      "Epoch [19][20]\t Batch [15000][55000]\t Training Loss 0.8479\t Accuracy 0.8433\n",
      "Epoch [19][20]\t Batch [15050][55000]\t Training Loss 0.8475\t Accuracy 0.8437\n",
      "Epoch [19][20]\t Batch [15100][55000]\t Training Loss 0.8473\t Accuracy 0.8439\n",
      "Epoch [19][20]\t Batch [15150][55000]\t Training Loss 0.8476\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [15200][55000]\t Training Loss 0.8479\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [15250][55000]\t Training Loss 0.8478\t Accuracy 0.8439\n",
      "Epoch [19][20]\t Batch [15300][55000]\t Training Loss 0.8478\t Accuracy 0.8439\n",
      "Epoch [19][20]\t Batch [15350][55000]\t Training Loss 0.8475\t Accuracy 0.8441\n",
      "Epoch [19][20]\t Batch [15400][55000]\t Training Loss 0.8478\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [15450][55000]\t Training Loss 0.8479\t Accuracy 0.8444\n",
      "Epoch [19][20]\t Batch [15500][55000]\t Training Loss 0.8477\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [15550][55000]\t Training Loss 0.8476\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [15600][55000]\t Training Loss 0.8474\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [15650][55000]\t Training Loss 0.8474\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [15700][55000]\t Training Loss 0.8472\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [15750][55000]\t Training Loss 0.8481\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [15800][55000]\t Training Loss 0.8485\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [15850][55000]\t Training Loss 0.8489\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [15900][55000]\t Training Loss 0.8494\t Accuracy 0.8442\n",
      "Epoch [19][20]\t Batch [15950][55000]\t Training Loss 0.8495\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [16000][55000]\t Training Loss 0.8495\t Accuracy 0.8440\n",
      "Epoch [19][20]\t Batch [16050][55000]\t Training Loss 0.8505\t Accuracy 0.8436\n",
      "Epoch [19][20]\t Batch [16100][55000]\t Training Loss 0.8504\t Accuracy 0.8436\n",
      "Epoch [19][20]\t Batch [16150][55000]\t Training Loss 0.8504\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [16200][55000]\t Training Loss 0.8505\t Accuracy 0.8435\n",
      "Epoch [19][20]\t Batch [16250][55000]\t Training Loss 0.8503\t Accuracy 0.8435\n",
      "Epoch [19][20]\t Batch [16300][55000]\t Training Loss 0.8500\t Accuracy 0.8435\n",
      "Epoch [19][20]\t Batch [16350][55000]\t Training Loss 0.8497\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [16400][55000]\t Training Loss 0.8497\t Accuracy 0.8438\n",
      "Epoch [19][20]\t Batch [16450][55000]\t Training Loss 0.8494\t Accuracy 0.8440\n",
      "Epoch [19][20]\t Batch [16500][55000]\t Training Loss 0.8493\t Accuracy 0.8442\n",
      "Epoch [19][20]\t Batch [16550][55000]\t Training Loss 0.8488\t Accuracy 0.8442\n",
      "Epoch [19][20]\t Batch [16600][55000]\t Training Loss 0.8489\t Accuracy 0.8442\n",
      "Epoch [19][20]\t Batch [16650][55000]\t Training Loss 0.8489\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [16700][55000]\t Training Loss 0.8490\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [16750][55000]\t Training Loss 0.8489\t Accuracy 0.8445\n",
      "Epoch [19][20]\t Batch [16800][55000]\t Training Loss 0.8495\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [16850][55000]\t Training Loss 0.8501\t Accuracy 0.8440\n",
      "Epoch [19][20]\t Batch [16900][55000]\t Training Loss 0.8503\t Accuracy 0.8441\n",
      "Epoch [19][20]\t Batch [16950][55000]\t Training Loss 0.8503\t Accuracy 0.8440\n",
      "Epoch [19][20]\t Batch [17000][55000]\t Training Loss 0.8510\t Accuracy 0.8436\n",
      "Epoch [19][20]\t Batch [17050][55000]\t Training Loss 0.8509\t Accuracy 0.8436\n",
      "Epoch [19][20]\t Batch [17100][55000]\t Training Loss 0.8514\t Accuracy 0.8435\n",
      "Epoch [19][20]\t Batch [17150][55000]\t Training Loss 0.8512\t Accuracy 0.8436\n",
      "Epoch [19][20]\t Batch [17200][55000]\t Training Loss 0.8512\t Accuracy 0.8435\n",
      "Epoch [19][20]\t Batch [17250][55000]\t Training Loss 0.8517\t Accuracy 0.8434\n",
      "Epoch [19][20]\t Batch [17300][55000]\t Training Loss 0.8516\t Accuracy 0.8434\n",
      "Epoch [19][20]\t Batch [17350][55000]\t Training Loss 0.8510\t Accuracy 0.8437\n",
      "Epoch [19][20]\t Batch [17400][55000]\t Training Loss 0.8512\t Accuracy 0.8437\n",
      "Epoch [19][20]\t Batch [17450][55000]\t Training Loss 0.8513\t Accuracy 0.8437\n",
      "Epoch [19][20]\t Batch [17500][55000]\t Training Loss 0.8513\t Accuracy 0.8437\n",
      "Epoch [19][20]\t Batch [17550][55000]\t Training Loss 0.8520\t Accuracy 0.8435\n",
      "Epoch [19][20]\t Batch [17600][55000]\t Training Loss 0.8526\t Accuracy 0.8433\n",
      "Epoch [19][20]\t Batch [17650][55000]\t Training Loss 0.8528\t Accuracy 0.8435\n",
      "Epoch [19][20]\t Batch [17700][55000]\t Training Loss 0.8535\t Accuracy 0.8433\n",
      "Epoch [19][20]\t Batch [17750][55000]\t Training Loss 0.8539\t Accuracy 0.8433\n",
      "Epoch [19][20]\t Batch [17800][55000]\t Training Loss 0.8543\t Accuracy 0.8431\n",
      "Epoch [19][20]\t Batch [17850][55000]\t Training Loss 0.8545\t Accuracy 0.8429\n",
      "Epoch [19][20]\t Batch [17900][55000]\t Training Loss 0.8549\t Accuracy 0.8427\n",
      "Epoch [19][20]\t Batch [17950][55000]\t Training Loss 0.8546\t Accuracy 0.8426\n",
      "Epoch [19][20]\t Batch [18000][55000]\t Training Loss 0.8543\t Accuracy 0.8427\n",
      "Epoch [19][20]\t Batch [18050][55000]\t Training Loss 0.8546\t Accuracy 0.8427\n",
      "Epoch [19][20]\t Batch [18100][55000]\t Training Loss 0.8544\t Accuracy 0.8429\n",
      "Epoch [19][20]\t Batch [18150][55000]\t Training Loss 0.8540\t Accuracy 0.8429\n",
      "Epoch [19][20]\t Batch [18200][55000]\t Training Loss 0.8536\t Accuracy 0.8432\n",
      "Epoch [19][20]\t Batch [18250][55000]\t Training Loss 0.8536\t Accuracy 0.8432\n",
      "Epoch [19][20]\t Batch [18300][55000]\t Training Loss 0.8531\t Accuracy 0.8433\n",
      "Epoch [19][20]\t Batch [18350][55000]\t Training Loss 0.8530\t Accuracy 0.8436\n",
      "Epoch [19][20]\t Batch [18400][55000]\t Training Loss 0.8530\t Accuracy 0.8436\n",
      "Epoch [19][20]\t Batch [18450][55000]\t Training Loss 0.8534\t Accuracy 0.8434\n",
      "Epoch [19][20]\t Batch [18500][55000]\t Training Loss 0.8535\t Accuracy 0.8434\n",
      "Epoch [19][20]\t Batch [18550][55000]\t Training Loss 0.8532\t Accuracy 0.8437\n",
      "Epoch [19][20]\t Batch [18600][55000]\t Training Loss 0.8532\t Accuracy 0.8439\n",
      "Epoch [19][20]\t Batch [18650][55000]\t Training Loss 0.8531\t Accuracy 0.8441\n",
      "Epoch [19][20]\t Batch [18700][55000]\t Training Loss 0.8533\t Accuracy 0.8439\n",
      "Epoch [19][20]\t Batch [18750][55000]\t Training Loss 0.8535\t Accuracy 0.8440\n",
      "Epoch [19][20]\t Batch [18800][55000]\t Training Loss 0.8531\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [18850][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [18900][55000]\t Training Loss 0.8529\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [18950][55000]\t Training Loss 0.8527\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [19000][55000]\t Training Loss 0.8526\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [19050][55000]\t Training Loss 0.8530\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [19100][55000]\t Training Loss 0.8533\t Accuracy 0.8445\n",
      "Epoch [19][20]\t Batch [19150][55000]\t Training Loss 0.8534\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [19200][55000]\t Training Loss 0.8535\t Accuracy 0.8442\n",
      "Epoch [19][20]\t Batch [19250][55000]\t Training Loss 0.8533\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [19300][55000]\t Training Loss 0.8532\t Accuracy 0.8444\n",
      "Epoch [19][20]\t Batch [19350][55000]\t Training Loss 0.8532\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [19400][55000]\t Training Loss 0.8530\t Accuracy 0.8444\n",
      "Epoch [19][20]\t Batch [19450][55000]\t Training Loss 0.8527\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [19500][55000]\t Training Loss 0.8524\t Accuracy 0.8445\n",
      "Epoch [19][20]\t Batch [19550][55000]\t Training Loss 0.8524\t Accuracy 0.8444\n",
      "Epoch [19][20]\t Batch [19600][55000]\t Training Loss 0.8524\t Accuracy 0.8441\n",
      "Epoch [19][20]\t Batch [19650][55000]\t Training Loss 0.8520\t Accuracy 0.8444\n",
      "Epoch [19][20]\t Batch [19700][55000]\t Training Loss 0.8514\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [19750][55000]\t Training Loss 0.8509\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [19800][55000]\t Training Loss 0.8503\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [19850][55000]\t Training Loss 0.8503\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [19900][55000]\t Training Loss 0.8501\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [19950][55000]\t Training Loss 0.8503\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [20000][55000]\t Training Loss 0.8502\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [20050][55000]\t Training Loss 0.8507\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [20100][55000]\t Training Loss 0.8508\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [20150][55000]\t Training Loss 0.8506\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [20200][55000]\t Training Loss 0.8510\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [20250][55000]\t Training Loss 0.8511\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [20300][55000]\t Training Loss 0.8512\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [20350][55000]\t Training Loss 0.8512\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [20400][55000]\t Training Loss 0.8509\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [20450][55000]\t Training Loss 0.8505\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [20500][55000]\t Training Loss 0.8502\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [20550][55000]\t Training Loss 0.8502\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [20600][55000]\t Training Loss 0.8502\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [20650][55000]\t Training Loss 0.8498\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [20700][55000]\t Training Loss 0.8496\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [20750][55000]\t Training Loss 0.8496\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [20800][55000]\t Training Loss 0.8496\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [20850][55000]\t Training Loss 0.8496\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [20900][55000]\t Training Loss 0.8499\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [20950][55000]\t Training Loss 0.8504\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [21000][55000]\t Training Loss 0.8506\t Accuracy 0.8444\n",
      "Epoch [19][20]\t Batch [21050][55000]\t Training Loss 0.8508\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [21100][55000]\t Training Loss 0.8506\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [21150][55000]\t Training Loss 0.8506\t Accuracy 0.8446\n",
      "Epoch [19][20]\t Batch [21200][55000]\t Training Loss 0.8503\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [21250][55000]\t Training Loss 0.8501\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [21300][55000]\t Training Loss 0.8497\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [21350][55000]\t Training Loss 0.8498\t Accuracy 0.8454\n",
      "Epoch [19][20]\t Batch [21400][55000]\t Training Loss 0.8498\t Accuracy 0.8454\n",
      "Epoch [19][20]\t Batch [21450][55000]\t Training Loss 0.8499\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [21500][55000]\t Training Loss 0.8495\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [21550][55000]\t Training Loss 0.8493\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [21600][55000]\t Training Loss 0.8494\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [21650][55000]\t Training Loss 0.8494\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [21700][55000]\t Training Loss 0.8494\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [21750][55000]\t Training Loss 0.8494\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [21800][55000]\t Training Loss 0.8488\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [21850][55000]\t Training Loss 0.8484\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [21900][55000]\t Training Loss 0.8480\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [21950][55000]\t Training Loss 0.8475\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [22000][55000]\t Training Loss 0.8471\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [22050][55000]\t Training Loss 0.8467\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [22100][55000]\t Training Loss 0.8469\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [22150][55000]\t Training Loss 0.8473\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [22200][55000]\t Training Loss 0.8476\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [22250][55000]\t Training Loss 0.8476\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [22300][55000]\t Training Loss 0.8478\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [22350][55000]\t Training Loss 0.8475\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [22400][55000]\t Training Loss 0.8475\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [22450][55000]\t Training Loss 0.8474\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [22500][55000]\t Training Loss 0.8479\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [22550][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [22600][55000]\t Training Loss 0.8489\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [22650][55000]\t Training Loss 0.8491\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [22700][55000]\t Training Loss 0.8490\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [22750][55000]\t Training Loss 0.8488\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [22800][55000]\t Training Loss 0.8487\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [22850][55000]\t Training Loss 0.8488\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [22900][55000]\t Training Loss 0.8485\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [22950][55000]\t Training Loss 0.8483\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [23000][55000]\t Training Loss 0.8481\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [23050][55000]\t Training Loss 0.8479\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [23100][55000]\t Training Loss 0.8480\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [23150][55000]\t Training Loss 0.8479\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [23200][55000]\t Training Loss 0.8480\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [23250][55000]\t Training Loss 0.8477\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [23300][55000]\t Training Loss 0.8474\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [23350][55000]\t Training Loss 0.8472\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [23400][55000]\t Training Loss 0.8469\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [23450][55000]\t Training Loss 0.8469\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [23500][55000]\t Training Loss 0.8467\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [23550][55000]\t Training Loss 0.8466\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [23600][55000]\t Training Loss 0.8466\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [23650][55000]\t Training Loss 0.8468\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [23700][55000]\t Training Loss 0.8469\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [23750][55000]\t Training Loss 0.8475\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [23800][55000]\t Training Loss 0.8472\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [23850][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [23900][55000]\t Training Loss 0.8473\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [23950][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [24000][55000]\t Training Loss 0.8473\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [24050][55000]\t Training Loss 0.8472\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [24100][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [24150][55000]\t Training Loss 0.8470\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [24200][55000]\t Training Loss 0.8468\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [24250][55000]\t Training Loss 0.8469\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [24300][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [24350][55000]\t Training Loss 0.8471\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [24400][55000]\t Training Loss 0.8471\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [24450][55000]\t Training Loss 0.8471\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [24500][55000]\t Training Loss 0.8471\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [24550][55000]\t Training Loss 0.8472\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [24600][55000]\t Training Loss 0.8472\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [24650][55000]\t Training Loss 0.8473\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [24700][55000]\t Training Loss 0.8473\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [24750][55000]\t Training Loss 0.8475\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [24800][55000]\t Training Loss 0.8479\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [24850][55000]\t Training Loss 0.8477\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [24900][55000]\t Training Loss 0.8478\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [24950][55000]\t Training Loss 0.8481\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [25000][55000]\t Training Loss 0.8484\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [25050][55000]\t Training Loss 0.8481\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [25100][55000]\t Training Loss 0.8480\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [25150][55000]\t Training Loss 0.8478\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [25200][55000]\t Training Loss 0.8477\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [25250][55000]\t Training Loss 0.8476\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [25300][55000]\t Training Loss 0.8476\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [25350][55000]\t Training Loss 0.8477\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [25400][55000]\t Training Loss 0.8473\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [25450][55000]\t Training Loss 0.8468\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [25500][55000]\t Training Loss 0.8466\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [25550][55000]\t Training Loss 0.8462\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [25600][55000]\t Training Loss 0.8462\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [25650][55000]\t Training Loss 0.8460\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [25700][55000]\t Training Loss 0.8459\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [25750][55000]\t Training Loss 0.8456\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [25800][55000]\t Training Loss 0.8456\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [25850][55000]\t Training Loss 0.8457\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [25900][55000]\t Training Loss 0.8457\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [25950][55000]\t Training Loss 0.8457\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [26000][55000]\t Training Loss 0.8457\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [26050][55000]\t Training Loss 0.8455\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [26100][55000]\t Training Loss 0.8452\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [26150][55000]\t Training Loss 0.8450\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [26200][55000]\t Training Loss 0.8449\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [26250][55000]\t Training Loss 0.8448\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [26300][55000]\t Training Loss 0.8450\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [26350][55000]\t Training Loss 0.8449\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [26400][55000]\t Training Loss 0.8453\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [26450][55000]\t Training Loss 0.8455\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [26500][55000]\t Training Loss 0.8456\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [26550][55000]\t Training Loss 0.8458\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [26600][55000]\t Training Loss 0.8460\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [26650][55000]\t Training Loss 0.8464\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [26700][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [26750][55000]\t Training Loss 0.8465\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [26800][55000]\t Training Loss 0.8464\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [26850][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [26900][55000]\t Training Loss 0.8466\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [26950][55000]\t Training Loss 0.8465\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [27000][55000]\t Training Loss 0.8463\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [27050][55000]\t Training Loss 0.8460\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [27100][55000]\t Training Loss 0.8459\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [27150][55000]\t Training Loss 0.8459\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [27200][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [27250][55000]\t Training Loss 0.8462\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [27300][55000]\t Training Loss 0.8462\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [27350][55000]\t Training Loss 0.8461\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [27400][55000]\t Training Loss 0.8461\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [27450][55000]\t Training Loss 0.8460\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [27500][55000]\t Training Loss 0.8460\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [27550][55000]\t Training Loss 0.8461\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [27600][55000]\t Training Loss 0.8459\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [27650][55000]\t Training Loss 0.8460\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [27700][55000]\t Training Loss 0.8460\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [27750][55000]\t Training Loss 0.8461\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [27800][55000]\t Training Loss 0.8462\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [27850][55000]\t Training Loss 0.8462\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [27900][55000]\t Training Loss 0.8461\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [27950][55000]\t Training Loss 0.8458\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [28000][55000]\t Training Loss 0.8456\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [28050][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [28100][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [28150][55000]\t Training Loss 0.8448\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [28200][55000]\t Training Loss 0.8449\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [28250][55000]\t Training Loss 0.8444\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [28300][55000]\t Training Loss 0.8444\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [28350][55000]\t Training Loss 0.8441\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [28400][55000]\t Training Loss 0.8446\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [28450][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [28500][55000]\t Training Loss 0.8441\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [28550][55000]\t Training Loss 0.8440\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [28600][55000]\t Training Loss 0.8439\t Accuracy 0.8484\n",
      "Epoch [19][20]\t Batch [28650][55000]\t Training Loss 0.8442\t Accuracy 0.8484\n",
      "Epoch [19][20]\t Batch [28700][55000]\t Training Loss 0.8444\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [28750][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [28800][55000]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [28850][55000]\t Training Loss 0.8444\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [28900][55000]\t Training Loss 0.8443\t Accuracy 0.8483\n",
      "Epoch [19][20]\t Batch [28950][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [29000][55000]\t Training Loss 0.8442\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [29050][55000]\t Training Loss 0.8442\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [29100][55000]\t Training Loss 0.8443\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [29150][55000]\t Training Loss 0.8447\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [29200][55000]\t Training Loss 0.8451\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [29250][55000]\t Training Loss 0.8453\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [29300][55000]\t Training Loss 0.8451\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [29350][55000]\t Training Loss 0.8453\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [29400][55000]\t Training Loss 0.8452\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [29450][55000]\t Training Loss 0.8448\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [29500][55000]\t Training Loss 0.8445\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [29550][55000]\t Training Loss 0.8445\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [29600][55000]\t Training Loss 0.8445\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [29650][55000]\t Training Loss 0.8444\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [29700][55000]\t Training Loss 0.8445\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [29750][55000]\t Training Loss 0.8449\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [29800][55000]\t Training Loss 0.8450\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [29850][55000]\t Training Loss 0.8453\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [29900][55000]\t Training Loss 0.8457\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [29950][55000]\t Training Loss 0.8459\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [30000][55000]\t Training Loss 0.8463\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [30050][55000]\t Training Loss 0.8464\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [30100][55000]\t Training Loss 0.8468\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [30150][55000]\t Training Loss 0.8473\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [30200][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [30250][55000]\t Training Loss 0.8476\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [30300][55000]\t Training Loss 0.8474\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [30350][55000]\t Training Loss 0.8474\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [30400][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [30450][55000]\t Training Loss 0.8472\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [30500][55000]\t Training Loss 0.8475\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [30550][55000]\t Training Loss 0.8479\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [30600][55000]\t Training Loss 0.8479\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [30650][55000]\t Training Loss 0.8483\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [30700][55000]\t Training Loss 0.8486\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [30750][55000]\t Training Loss 0.8487\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [30800][55000]\t Training Loss 0.8489\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [30850][55000]\t Training Loss 0.8488\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [30900][55000]\t Training Loss 0.8492\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [30950][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [31000][55000]\t Training Loss 0.8491\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [31050][55000]\t Training Loss 0.8492\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [31100][55000]\t Training Loss 0.8492\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [31150][55000]\t Training Loss 0.8492\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [31200][55000]\t Training Loss 0.8493\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [31250][55000]\t Training Loss 0.8492\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [31300][55000]\t Training Loss 0.8495\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [31350][55000]\t Training Loss 0.8502\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [31400][55000]\t Training Loss 0.8503\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [31450][55000]\t Training Loss 0.8506\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [31500][55000]\t Training Loss 0.8507\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [31550][55000]\t Training Loss 0.8507\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [31600][55000]\t Training Loss 0.8508\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [31650][55000]\t Training Loss 0.8510\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [31700][55000]\t Training Loss 0.8513\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [31750][55000]\t Training Loss 0.8517\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [31800][55000]\t Training Loss 0.8518\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [31850][55000]\t Training Loss 0.8518\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [31900][55000]\t Training Loss 0.8517\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [31950][55000]\t Training Loss 0.8517\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [32000][55000]\t Training Loss 0.8517\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [32050][55000]\t Training Loss 0.8517\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [32100][55000]\t Training Loss 0.8516\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [32150][55000]\t Training Loss 0.8519\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [32200][55000]\t Training Loss 0.8520\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [32250][55000]\t Training Loss 0.8523\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [32300][55000]\t Training Loss 0.8527\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [32350][55000]\t Training Loss 0.8530\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [32400][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [32450][55000]\t Training Loss 0.8533\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [32500][55000]\t Training Loss 0.8535\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [32550][55000]\t Training Loss 0.8538\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [32600][55000]\t Training Loss 0.8537\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [32650][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [32700][55000]\t Training Loss 0.8536\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [32750][55000]\t Training Loss 0.8536\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [32800][55000]\t Training Loss 0.8537\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [32850][55000]\t Training Loss 0.8537\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [32900][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [32950][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [33000][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [33050][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [33100][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [33150][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [33200][55000]\t Training Loss 0.8534\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [33250][55000]\t Training Loss 0.8536\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [33300][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [33350][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [33400][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [33450][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [33500][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [33550][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [33600][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [33650][55000]\t Training Loss 0.8538\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [33700][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [33750][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [33800][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [33850][55000]\t Training Loss 0.8532\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [33900][55000]\t Training Loss 0.8529\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [33950][55000]\t Training Loss 0.8525\t Accuracy 0.8466\n",
      "Epoch [19][20]\t Batch [34000][55000]\t Training Loss 0.8525\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [34050][55000]\t Training Loss 0.8527\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [34100][55000]\t Training Loss 0.8528\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [34150][55000]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [34200][55000]\t Training Loss 0.8526\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [34250][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [34300][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [34350][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [34400][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [34450][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [34500][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [34550][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [34600][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [34650][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [34700][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [34750][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [34800][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [34850][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [34900][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [34950][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [35000][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [35050][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [35100][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [35150][55000]\t Training Loss 0.8529\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [35200][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [35250][55000]\t Training Loss 0.8531\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [35300][55000]\t Training Loss 0.8531\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [35350][55000]\t Training Loss 0.8529\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [35400][55000]\t Training Loss 0.8529\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [35450][55000]\t Training Loss 0.8528\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [35500][55000]\t Training Loss 0.8528\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [35550][55000]\t Training Loss 0.8527\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [35600][55000]\t Training Loss 0.8526\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [35650][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [35700][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [35750][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [35800][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [35850][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [35900][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [35950][55000]\t Training Loss 0.8526\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [36000][55000]\t Training Loss 0.8527\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [36050][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [36100][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [36150][55000]\t Training Loss 0.8530\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [36200][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [36250][55000]\t Training Loss 0.8527\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [36300][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [36350][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [36400][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [36450][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [36500][55000]\t Training Loss 0.8524\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [36550][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [36600][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [36650][55000]\t Training Loss 0.8519\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [36700][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [36750][55000]\t Training Loss 0.8515\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [36800][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [36850][55000]\t Training Loss 0.8514\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [36900][55000]\t Training Loss 0.8514\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [36950][55000]\t Training Loss 0.8513\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [37000][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [37050][55000]\t Training Loss 0.8510\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [37100][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [37150][55000]\t Training Loss 0.8511\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [37200][55000]\t Training Loss 0.8510\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [37250][55000]\t Training Loss 0.8509\t Accuracy 0.8480\n",
      "Epoch [19][20]\t Batch [37300][55000]\t Training Loss 0.8509\t Accuracy 0.8479\n",
      "Epoch [19][20]\t Batch [37350][55000]\t Training Loss 0.8510\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [37400][55000]\t Training Loss 0.8513\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [37450][55000]\t Training Loss 0.8516\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [37500][55000]\t Training Loss 0.8517\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [37550][55000]\t Training Loss 0.8518\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [37600][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [37650][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [37700][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [37750][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [37800][55000]\t Training Loss 0.8519\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [37850][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [37900][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [37950][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [38000][55000]\t Training Loss 0.8520\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [38050][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [38100][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [38150][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [38200][55000]\t Training Loss 0.8520\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [38250][55000]\t Training Loss 0.8519\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [38300][55000]\t Training Loss 0.8521\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [38350][55000]\t Training Loss 0.8522\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [38400][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [38450][55000]\t Training Loss 0.8523\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [38500][55000]\t Training Loss 0.8521\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [38550][55000]\t Training Loss 0.8522\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [38600][55000]\t Training Loss 0.8523\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [38650][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [38700][55000]\t Training Loss 0.8525\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [38750][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [38800][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [38850][55000]\t Training Loss 0.8523\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [38900][55000]\t Training Loss 0.8521\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [38950][55000]\t Training Loss 0.8519\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [39000][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [39050][55000]\t Training Loss 0.8518\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [39100][55000]\t Training Loss 0.8515\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [39150][55000]\t Training Loss 0.8513\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [39200][55000]\t Training Loss 0.8511\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [39250][55000]\t Training Loss 0.8510\t Accuracy 0.8477\n",
      "Epoch [19][20]\t Batch [39300][55000]\t Training Loss 0.8510\t Accuracy 0.8478\n",
      "Epoch [19][20]\t Batch [39350][55000]\t Training Loss 0.8513\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [39400][55000]\t Training Loss 0.8513\t Accuracy 0.8476\n",
      "Epoch [19][20]\t Batch [39450][55000]\t Training Loss 0.8516\t Accuracy 0.8475\n",
      "Epoch [19][20]\t Batch [39500][55000]\t Training Loss 0.8516\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [39550][55000]\t Training Loss 0.8517\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [39600][55000]\t Training Loss 0.8516\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [39650][55000]\t Training Loss 0.8516\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [39700][55000]\t Training Loss 0.8516\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [39750][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [39800][55000]\t Training Loss 0.8520\t Accuracy 0.8474\n",
      "Epoch [19][20]\t Batch [39850][55000]\t Training Loss 0.8521\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [39900][55000]\t Training Loss 0.8522\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [39950][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40000][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40050][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [40100][55000]\t Training Loss 0.8524\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [40150][55000]\t Training Loss 0.8523\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [40200][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40250][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [40300][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40350][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40400][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40450][55000]\t Training Loss 0.8520\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40500][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [40550][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40600][55000]\t Training Loss 0.8522\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [40650][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [40700][55000]\t Training Loss 0.8521\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [40750][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40800][55000]\t Training Loss 0.8521\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40850][55000]\t Training Loss 0.8519\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [40900][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [40950][55000]\t Training Loss 0.8516\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [41000][55000]\t Training Loss 0.8516\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [41050][55000]\t Training Loss 0.8517\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [41100][55000]\t Training Loss 0.8518\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [41150][55000]\t Training Loss 0.8519\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [41200][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [41250][55000]\t Training Loss 0.8518\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [41300][55000]\t Training Loss 0.8520\t Accuracy 0.8472\n",
      "Epoch [19][20]\t Batch [41350][55000]\t Training Loss 0.8522\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [41400][55000]\t Training Loss 0.8524\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [41450][55000]\t Training Loss 0.8525\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [41500][55000]\t Training Loss 0.8527\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [41550][55000]\t Training Loss 0.8529\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [41600][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [41650][55000]\t Training Loss 0.8529\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [41700][55000]\t Training Loss 0.8528\t Accuracy 0.8471\n",
      "Epoch [19][20]\t Batch [41750][55000]\t Training Loss 0.8530\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [41800][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [41850][55000]\t Training Loss 0.8533\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [41900][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [41950][55000]\t Training Loss 0.8532\t Accuracy 0.8470\n",
      "Epoch [19][20]\t Batch [42000][55000]\t Training Loss 0.8532\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [42050][55000]\t Training Loss 0.8531\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [42100][55000]\t Training Loss 0.8530\t Accuracy 0.8469\n",
      "Epoch [19][20]\t Batch [42150][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [42200][55000]\t Training Loss 0.8531\t Accuracy 0.8468\n",
      "Epoch [19][20]\t Batch [42250][55000]\t Training Loss 0.8532\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [42300][55000]\t Training Loss 0.8532\t Accuracy 0.8467\n",
      "Epoch [19][20]\t Batch [42350][55000]\t Training Loss 0.8535\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [42400][55000]\t Training Loss 0.8536\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [42450][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [42500][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [42550][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [42600][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [42650][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [42700][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [42750][55000]\t Training Loss 0.8537\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [42800][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [42850][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [42900][55000]\t Training Loss 0.8536\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [42950][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [43000][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [43050][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [43100][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [43150][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [43200][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [43250][55000]\t Training Loss 0.8542\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [43300][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [43350][55000]\t Training Loss 0.8540\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [43400][55000]\t Training Loss 0.8538\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [43450][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [43500][55000]\t Training Loss 0.8534\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [43550][55000]\t Training Loss 0.8535\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [43600][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [43650][55000]\t Training Loss 0.8532\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [43700][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [43750][55000]\t Training Loss 0.8531\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [43800][55000]\t Training Loss 0.8531\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [43850][55000]\t Training Loss 0.8532\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [43900][55000]\t Training Loss 0.8533\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [43950][55000]\t Training Loss 0.8534\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [44000][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [44050][55000]\t Training Loss 0.8534\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [44100][55000]\t Training Loss 0.8535\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [44150][55000]\t Training Loss 0.8537\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [44200][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [44250][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [44300][55000]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [44350][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [44400][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [44450][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [44500][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [44550][55000]\t Training Loss 0.8539\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [44600][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [44650][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [44700][55000]\t Training Loss 0.8536\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [44750][55000]\t Training Loss 0.8536\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [44800][55000]\t Training Loss 0.8536\t Accuracy 0.8465\n",
      "Epoch [19][20]\t Batch [44850][55000]\t Training Loss 0.8537\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [44900][55000]\t Training Loss 0.8538\t Accuracy 0.8464\n",
      "Epoch [19][20]\t Batch [44950][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [45000][55000]\t Training Loss 0.8538\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45050][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45100][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45150][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [45200][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [45250][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45300][55000]\t Training Loss 0.8540\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45350][55000]\t Training Loss 0.8539\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [45400][55000]\t Training Loss 0.8538\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [45450][55000]\t Training Loss 0.8540\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [45500][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45550][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45600][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45650][55000]\t Training Loss 0.8542\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [45700][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45750][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45800][55000]\t Training Loss 0.8541\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45850][55000]\t Training Loss 0.8542\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [45900][55000]\t Training Loss 0.8543\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [45950][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [46000][55000]\t Training Loss 0.8544\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [46050][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [46100][55000]\t Training Loss 0.8547\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [46150][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46200][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46250][55000]\t Training Loss 0.8547\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46300][55000]\t Training Loss 0.8548\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [46350][55000]\t Training Loss 0.8549\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [46400][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [46450][55000]\t Training Loss 0.8552\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [46500][55000]\t Training Loss 0.8551\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [46550][55000]\t Training Loss 0.8550\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46600][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46650][55000]\t Training Loss 0.8549\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46700][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46750][55000]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46800][55000]\t Training Loss 0.8546\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [46850][55000]\t Training Loss 0.8545\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [46900][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [46950][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47000][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47050][55000]\t Training Loss 0.8542\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47100][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [47150][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [47200][55000]\t Training Loss 0.8540\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [47250][55000]\t Training Loss 0.8541\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [47300][55000]\t Training Loss 0.8543\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47350][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47400][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47450][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47500][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [47550][55000]\t Training Loss 0.8545\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47600][55000]\t Training Loss 0.8544\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47650][55000]\t Training Loss 0.8546\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [47700][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [47750][55000]\t Training Loss 0.8545\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [47800][55000]\t Training Loss 0.8544\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [47850][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47900][55000]\t Training Loss 0.8541\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [47950][55000]\t Training Loss 0.8543\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [48000][55000]\t Training Loss 0.8543\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [48050][55000]\t Training Loss 0.8542\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [48100][55000]\t Training Loss 0.8541\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [48150][55000]\t Training Loss 0.8539\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [48200][55000]\t Training Loss 0.8537\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48250][55000]\t Training Loss 0.8536\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48300][55000]\t Training Loss 0.8535\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48350][55000]\t Training Loss 0.8533\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48400][55000]\t Training Loss 0.8535\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [48450][55000]\t Training Loss 0.8532\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48500][55000]\t Training Loss 0.8531\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48550][55000]\t Training Loss 0.8531\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48600][55000]\t Training Loss 0.8531\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [48650][55000]\t Training Loss 0.8530\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48700][55000]\t Training Loss 0.8529\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [48750][55000]\t Training Loss 0.8527\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [48800][55000]\t Training Loss 0.8526\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [48850][55000]\t Training Loss 0.8525\t Accuracy 0.8462\n",
      "Epoch [19][20]\t Batch [48900][55000]\t Training Loss 0.8524\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [48950][55000]\t Training Loss 0.8526\t Accuracy 0.8461\n",
      "Epoch [19][20]\t Batch [49000][55000]\t Training Loss 0.8528\t Accuracy 0.8460\n",
      "Epoch [19][20]\t Batch [49050][55000]\t Training Loss 0.8531\t Accuracy 0.8459\n",
      "Epoch [19][20]\t Batch [49100][55000]\t Training Loss 0.8532\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [49150][55000]\t Training Loss 0.8533\t Accuracy 0.8458\n",
      "Epoch [19][20]\t Batch [49200][55000]\t Training Loss 0.8533\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [49250][55000]\t Training Loss 0.8535\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [49300][55000]\t Training Loss 0.8534\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [49350][55000]\t Training Loss 0.8532\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [49400][55000]\t Training Loss 0.8531\t Accuracy 0.8457\n",
      "Epoch [19][20]\t Batch [49450][55000]\t Training Loss 0.8530\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [49500][55000]\t Training Loss 0.8533\t Accuracy 0.8455\n",
      "Epoch [19][20]\t Batch [49550][55000]\t Training Loss 0.8536\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [49600][55000]\t Training Loss 0.8539\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [49650][55000]\t Training Loss 0.8539\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [49700][55000]\t Training Loss 0.8542\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [49750][55000]\t Training Loss 0.8542\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [49800][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [49850][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [49900][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [49950][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [50000][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [50050][55000]\t Training Loss 0.8545\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [50100][55000]\t Training Loss 0.8546\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [50150][55000]\t Training Loss 0.8545\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [50200][55000]\t Training Loss 0.8544\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [50250][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [50300][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [50350][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [50400][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [50450][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [50500][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [50550][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [50600][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [50650][55000]\t Training Loss 0.8553\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [50700][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [50750][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [50800][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [50850][55000]\t Training Loss 0.8553\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [50900][55000]\t Training Loss 0.8552\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [50950][55000]\t Training Loss 0.8551\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [51000][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [51050][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [51100][55000]\t Training Loss 0.8547\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [51150][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [51200][55000]\t Training Loss 0.8548\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [51250][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [51300][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [51350][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [51400][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [51450][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [51500][55000]\t Training Loss 0.8546\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [51550][55000]\t Training Loss 0.8545\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [51600][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [51650][55000]\t Training Loss 0.8543\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [51700][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [51750][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [51800][55000]\t Training Loss 0.8544\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [51850][55000]\t Training Loss 0.8543\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [51900][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [51950][55000]\t Training Loss 0.8541\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [52000][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [52050][55000]\t Training Loss 0.8542\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [52100][55000]\t Training Loss 0.8545\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [52150][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [52200][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [52250][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [52300][55000]\t Training Loss 0.8551\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [52350][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [52400][55000]\t Training Loss 0.8550\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [52450][55000]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [52500][55000]\t Training Loss 0.8547\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [52550][55000]\t Training Loss 0.8546\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [52600][55000]\t Training Loss 0.8543\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [52650][55000]\t Training Loss 0.8542\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [52700][55000]\t Training Loss 0.8543\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [52750][55000]\t Training Loss 0.8544\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [52800][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [52850][55000]\t Training Loss 0.8546\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [52900][55000]\t Training Loss 0.8547\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [52950][55000]\t Training Loss 0.8549\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [53000][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53050][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53100][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53150][55000]\t Training Loss 0.8549\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53200][55000]\t Training Loss 0.8550\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53250][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53300][55000]\t Training Loss 0.8551\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53350][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53400][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53450][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53500][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53550][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53600][55000]\t Training Loss 0.8549\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [53650][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53700][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53750][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53800][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53850][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [53900][55000]\t Training Loss 0.8547\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [53950][55000]\t Training Loss 0.8548\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [54000][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [54050][55000]\t Training Loss 0.8550\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [54100][55000]\t Training Loss 0.8552\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [54150][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [54200][55000]\t Training Loss 0.8551\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [54250][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [54300][55000]\t Training Loss 0.8550\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [54350][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [54400][55000]\t Training Loss 0.8549\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [54450][55000]\t Training Loss 0.8548\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [54500][55000]\t Training Loss 0.8547\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [54550][55000]\t Training Loss 0.8546\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [54600][55000]\t Training Loss 0.8546\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [54650][55000]\t Training Loss 0.8545\t Accuracy 0.8448\n",
      "Epoch [19][20]\t Batch [54700][55000]\t Training Loss 0.8543\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [54750][55000]\t Training Loss 0.8542\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [54800][55000]\t Training Loss 0.8542\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [54850][55000]\t Training Loss 0.8540\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [54900][55000]\t Training Loss 0.8542\t Accuracy 0.8450\n",
      "Epoch [19][20]\t Batch [54950][55000]\t Training Loss 0.8545\t Accuracy 0.8448\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8547\t Average training accuracy 0.8448\n",
      "Epoch [19]\t Average validation loss 0.7920\t Average validation accuracy 0.8702\n",
      "\n",
      "Epoch [0][20]\t Batch [0][5500]\t Training Loss 0.7526\t Accuracy 0.9000\n",
      "Epoch [0][20]\t Batch [50][5500]\t Training Loss 0.8346\t Accuracy 0.8275\n",
      "Epoch [0][20]\t Batch [100][5500]\t Training Loss 0.8701\t Accuracy 0.8238\n",
      "Epoch [0][20]\t Batch [150][5500]\t Training Loss 0.8845\t Accuracy 0.8252\n",
      "Epoch [0][20]\t Batch [200][5500]\t Training Loss 0.8640\t Accuracy 0.8323\n",
      "Epoch [0][20]\t Batch [250][5500]\t Training Loss 0.8498\t Accuracy 0.8390\n",
      "Epoch [0][20]\t Batch [300][5500]\t Training Loss 0.8416\t Accuracy 0.8415\n",
      "Epoch [0][20]\t Batch [350][5500]\t Training Loss 0.8469\t Accuracy 0.8425\n",
      "Epoch [0][20]\t Batch [400][5500]\t Training Loss 0.8447\t Accuracy 0.8454\n",
      "Epoch [0][20]\t Batch [450][5500]\t Training Loss 0.8443\t Accuracy 0.8466\n",
      "Epoch [0][20]\t Batch [500][5500]\t Training Loss 0.8393\t Accuracy 0.8465\n",
      "Epoch [0][20]\t Batch [550][5500]\t Training Loss 0.8373\t Accuracy 0.8465\n",
      "Epoch [0][20]\t Batch [600][5500]\t Training Loss 0.8373\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [650][5500]\t Training Loss 0.8311\t Accuracy 0.8499\n",
      "Epoch [0][20]\t Batch [700][5500]\t Training Loss 0.8307\t Accuracy 0.8506\n",
      "Epoch [0][20]\t Batch [750][5500]\t Training Loss 0.8397\t Accuracy 0.8491\n",
      "Epoch [0][20]\t Batch [800][5500]\t Training Loss 0.8420\t Accuracy 0.8484\n",
      "Epoch [0][20]\t Batch [850][5500]\t Training Loss 0.8445\t Accuracy 0.8482\n",
      "Epoch [0][20]\t Batch [900][5500]\t Training Loss 0.8466\t Accuracy 0.8463\n",
      "Epoch [0][20]\t Batch [950][5500]\t Training Loss 0.8452\t Accuracy 0.8475\n",
      "Epoch [0][20]\t Batch [1000][5500]\t Training Loss 0.8425\t Accuracy 0.8486\n",
      "Epoch [0][20]\t Batch [1050][5500]\t Training Loss 0.8392\t Accuracy 0.8499\n",
      "Epoch [0][20]\t Batch [1100][5500]\t Training Loss 0.8372\t Accuracy 0.8505\n",
      "Epoch [0][20]\t Batch [1150][5500]\t Training Loss 0.8347\t Accuracy 0.8506\n",
      "Epoch [0][20]\t Batch [1200][5500]\t Training Loss 0.8388\t Accuracy 0.8484\n",
      "Epoch [0][20]\t Batch [1250][5500]\t Training Loss 0.8395\t Accuracy 0.8482\n",
      "Epoch [0][20]\t Batch [1300][5500]\t Training Loss 0.8417\t Accuracy 0.8472\n",
      "Epoch [0][20]\t Batch [1350][5500]\t Training Loss 0.8428\t Accuracy 0.8471\n",
      "Epoch [0][20]\t Batch [1400][5500]\t Training Loss 0.8443\t Accuracy 0.8463\n",
      "Epoch [0][20]\t Batch [1450][5500]\t Training Loss 0.8462\t Accuracy 0.8454\n",
      "Epoch [0][20]\t Batch [1500][5500]\t Training Loss 0.8499\t Accuracy 0.8436\n",
      "Epoch [0][20]\t Batch [1550][5500]\t Training Loss 0.8496\t Accuracy 0.8448\n",
      "Epoch [0][20]\t Batch [1600][5500]\t Training Loss 0.8516\t Accuracy 0.8438\n",
      "Epoch [0][20]\t Batch [1650][5500]\t Training Loss 0.8509\t Accuracy 0.8441\n",
      "Epoch [0][20]\t Batch [1700][5500]\t Training Loss 0.8526\t Accuracy 0.8437\n",
      "Epoch [0][20]\t Batch [1750][5500]\t Training Loss 0.8531\t Accuracy 0.8436\n",
      "Epoch [0][20]\t Batch [1800][5500]\t Training Loss 0.8563\t Accuracy 0.8425\n",
      "Epoch [0][20]\t Batch [1850][5500]\t Training Loss 0.8552\t Accuracy 0.8436\n",
      "Epoch [0][20]\t Batch [1900][5500]\t Training Loss 0.8544\t Accuracy 0.8453\n",
      "Epoch [0][20]\t Batch [1950][5500]\t Training Loss 0.8545\t Accuracy 0.8453\n",
      "Epoch [0][20]\t Batch [2000][5500]\t Training Loss 0.8524\t Accuracy 0.8457\n",
      "Epoch [0][20]\t Batch [2050][5500]\t Training Loss 0.8525\t Accuracy 0.8458\n",
      "Epoch [0][20]\t Batch [2100][5500]\t Training Loss 0.8529\t Accuracy 0.8452\n",
      "Epoch [0][20]\t Batch [2150][5500]\t Training Loss 0.8516\t Accuracy 0.8461\n",
      "Epoch [0][20]\t Batch [2200][5500]\t Training Loss 0.8492\t Accuracy 0.8472\n",
      "Epoch [0][20]\t Batch [2250][5500]\t Training Loss 0.8505\t Accuracy 0.8469\n",
      "Epoch [0][20]\t Batch [2300][5500]\t Training Loss 0.8505\t Accuracy 0.8469\n",
      "Epoch [0][20]\t Batch [2350][5500]\t Training Loss 0.8494\t Accuracy 0.8473\n",
      "Epoch [0][20]\t Batch [2400][5500]\t Training Loss 0.8499\t Accuracy 0.8472\n",
      "Epoch [0][20]\t Batch [2450][5500]\t Training Loss 0.8497\t Accuracy 0.8472\n",
      "Epoch [0][20]\t Batch [2500][5500]\t Training Loss 0.8510\t Accuracy 0.8464\n",
      "Epoch [0][20]\t Batch [2550][5500]\t Training Loss 0.8493\t Accuracy 0.8469\n",
      "Epoch [0][20]\t Batch [2600][5500]\t Training Loss 0.8485\t Accuracy 0.8473\n",
      "Epoch [0][20]\t Batch [2650][5500]\t Training Loss 0.8486\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [2700][5500]\t Training Loss 0.8493\t Accuracy 0.8475\n",
      "Epoch [0][20]\t Batch [2750][5500]\t Training Loss 0.8495\t Accuracy 0.8474\n",
      "Epoch [0][20]\t Batch [2800][5500]\t Training Loss 0.8490\t Accuracy 0.8477\n",
      "Epoch [0][20]\t Batch [2850][5500]\t Training Loss 0.8477\t Accuracy 0.8485\n",
      "Epoch [0][20]\t Batch [2900][5500]\t Training Loss 0.8476\t Accuracy 0.8484\n",
      "Epoch [0][20]\t Batch [2950][5500]\t Training Loss 0.8477\t Accuracy 0.8482\n",
      "Epoch [0][20]\t Batch [3000][5500]\t Training Loss 0.8493\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [3050][5500]\t Training Loss 0.8504\t Accuracy 0.8477\n",
      "Epoch [0][20]\t Batch [3100][5500]\t Training Loss 0.8519\t Accuracy 0.8476\n",
      "Epoch [0][20]\t Batch [3150][5500]\t Training Loss 0.8533\t Accuracy 0.8475\n",
      "Epoch [0][20]\t Batch [3200][5500]\t Training Loss 0.8544\t Accuracy 0.8473\n",
      "Epoch [0][20]\t Batch [3250][5500]\t Training Loss 0.8563\t Accuracy 0.8463\n",
      "Epoch [0][20]\t Batch [3300][5500]\t Training Loss 0.8559\t Accuracy 0.8466\n",
      "Epoch [0][20]\t Batch [3350][5500]\t Training Loss 0.8565\t Accuracy 0.8464\n",
      "Epoch [0][20]\t Batch [3400][5500]\t Training Loss 0.8551\t Accuracy 0.8474\n",
      "Epoch [0][20]\t Batch [3450][5500]\t Training Loss 0.8548\t Accuracy 0.8480\n",
      "Epoch [0][20]\t Batch [3500][5500]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "Epoch [0][20]\t Batch [3550][5500]\t Training Loss 0.8554\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [3600][5500]\t Training Loss 0.8555\t Accuracy 0.8479\n",
      "Epoch [0][20]\t Batch [3650][5500]\t Training Loss 0.8554\t Accuracy 0.8480\n",
      "Epoch [0][20]\t Batch [3700][5500]\t Training Loss 0.8541\t Accuracy 0.8485\n",
      "Epoch [0][20]\t Batch [3750][5500]\t Training Loss 0.8548\t Accuracy 0.8482\n",
      "Epoch [0][20]\t Batch [3800][5500]\t Training Loss 0.8550\t Accuracy 0.8481\n",
      "Epoch [0][20]\t Batch [3850][5500]\t Training Loss 0.8551\t Accuracy 0.8481\n",
      "Epoch [0][20]\t Batch [3900][5500]\t Training Loss 0.8547\t Accuracy 0.8482\n",
      "Epoch [0][20]\t Batch [3950][5500]\t Training Loss 0.8544\t Accuracy 0.8484\n",
      "Epoch [0][20]\t Batch [4000][5500]\t Training Loss 0.8552\t Accuracy 0.8482\n",
      "Epoch [0][20]\t Batch [4050][5500]\t Training Loss 0.8549\t Accuracy 0.8483\n",
      "Epoch [0][20]\t Batch [4100][5500]\t Training Loss 0.8543\t Accuracy 0.8485\n",
      "Epoch [0][20]\t Batch [4150][5500]\t Training Loss 0.8556\t Accuracy 0.8484\n",
      "Epoch [0][20]\t Batch [4200][5500]\t Training Loss 0.8561\t Accuracy 0.8485\n",
      "Epoch [0][20]\t Batch [4250][5500]\t Training Loss 0.8569\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [4300][5500]\t Training Loss 0.8567\t Accuracy 0.8476\n",
      "Epoch [0][20]\t Batch [4350][5500]\t Training Loss 0.8562\t Accuracy 0.8479\n",
      "Epoch [0][20]\t Batch [4400][5500]\t Training Loss 0.8562\t Accuracy 0.8479\n",
      "Epoch [0][20]\t Batch [4450][5500]\t Training Loss 0.8568\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [4500][5500]\t Training Loss 0.8564\t Accuracy 0.8481\n",
      "Epoch [0][20]\t Batch [4550][5500]\t Training Loss 0.8568\t Accuracy 0.8479\n",
      "Epoch [0][20]\t Batch [4600][5500]\t Training Loss 0.8570\t Accuracy 0.8479\n",
      "Epoch [0][20]\t Batch [4650][5500]\t Training Loss 0.8576\t Accuracy 0.8479\n",
      "Epoch [0][20]\t Batch [4700][5500]\t Training Loss 0.8567\t Accuracy 0.8481\n",
      "Epoch [0][20]\t Batch [4750][5500]\t Training Loss 0.8570\t Accuracy 0.8480\n",
      "Epoch [0][20]\t Batch [4800][5500]\t Training Loss 0.8568\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [4850][5500]\t Training Loss 0.8555\t Accuracy 0.8483\n",
      "Epoch [0][20]\t Batch [4900][5500]\t Training Loss 0.8552\t Accuracy 0.8483\n",
      "Epoch [0][20]\t Batch [4950][5500]\t Training Loss 0.8555\t Accuracy 0.8480\n",
      "Epoch [0][20]\t Batch [5000][5500]\t Training Loss 0.8568\t Accuracy 0.8477\n",
      "Epoch [0][20]\t Batch [5050][5500]\t Training Loss 0.8573\t Accuracy 0.8476\n",
      "Epoch [0][20]\t Batch [5100][5500]\t Training Loss 0.8570\t Accuracy 0.8476\n",
      "Epoch [0][20]\t Batch [5150][5500]\t Training Loss 0.8566\t Accuracy 0.8475\n",
      "Epoch [0][20]\t Batch [5200][5500]\t Training Loss 0.8561\t Accuracy 0.8478\n",
      "Epoch [0][20]\t Batch [5250][5500]\t Training Loss 0.8565\t Accuracy 0.8476\n",
      "Epoch [0][20]\t Batch [5300][5500]\t Training Loss 0.8568\t Accuracy 0.8473\n",
      "Epoch [0][20]\t Batch [5350][5500]\t Training Loss 0.8565\t Accuracy 0.8474\n",
      "Epoch [0][20]\t Batch [5400][5500]\t Training Loss 0.8566\t Accuracy 0.8475\n",
      "Epoch [0][20]\t Batch [5450][5500]\t Training Loss 0.8562\t Accuracy 0.8476\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8562\t Average training accuracy 0.8476\n",
      "Epoch [0]\t Average validation loss 0.7875\t Average validation accuracy 0.8858\n",
      "\n",
      "Epoch [1][20]\t Batch [0][5500]\t Training Loss 0.7438\t Accuracy 0.9000\n",
      "Epoch [1][20]\t Batch [50][5500]\t Training Loss 0.8272\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [100][5500]\t Training Loss 0.8644\t Accuracy 0.8396\n",
      "Epoch [1][20]\t Batch [150][5500]\t Training Loss 0.8787\t Accuracy 0.8364\n",
      "Epoch [1][20]\t Batch [200][5500]\t Training Loss 0.8584\t Accuracy 0.8443\n",
      "Epoch [1][20]\t Batch [250][5500]\t Training Loss 0.8451\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [300][5500]\t Training Loss 0.8373\t Accuracy 0.8512\n",
      "Epoch [1][20]\t Batch [350][5500]\t Training Loss 0.8427\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [400][5500]\t Training Loss 0.8404\t Accuracy 0.8541\n",
      "Epoch [1][20]\t Batch [450][5500]\t Training Loss 0.8403\t Accuracy 0.8548\n",
      "Epoch [1][20]\t Batch [500][5500]\t Training Loss 0.8355\t Accuracy 0.8549\n",
      "Epoch [1][20]\t Batch [550][5500]\t Training Loss 0.8336\t Accuracy 0.8543\n",
      "Epoch [1][20]\t Batch [600][5500]\t Training Loss 0.8336\t Accuracy 0.8552\n",
      "Epoch [1][20]\t Batch [650][5500]\t Training Loss 0.8275\t Accuracy 0.8571\n",
      "Epoch [1][20]\t Batch [700][5500]\t Training Loss 0.8274\t Accuracy 0.8575\n",
      "Epoch [1][20]\t Batch [750][5500]\t Training Loss 0.8365\t Accuracy 0.8561\n",
      "Epoch [1][20]\t Batch [800][5500]\t Training Loss 0.8387\t Accuracy 0.8549\n",
      "Epoch [1][20]\t Batch [850][5500]\t Training Loss 0.8413\t Accuracy 0.8543\n",
      "Epoch [1][20]\t Batch [900][5500]\t Training Loss 0.8434\t Accuracy 0.8524\n",
      "Epoch [1][20]\t Batch [950][5500]\t Training Loss 0.8422\t Accuracy 0.8533\n",
      "Epoch [1][20]\t Batch [1000][5500]\t Training Loss 0.8395\t Accuracy 0.8540\n",
      "Epoch [1][20]\t Batch [1050][5500]\t Training Loss 0.8363\t Accuracy 0.8552\n",
      "Epoch [1][20]\t Batch [1100][5500]\t Training Loss 0.8344\t Accuracy 0.8558\n",
      "Epoch [1][20]\t Batch [1150][5500]\t Training Loss 0.8320\t Accuracy 0.8556\n",
      "Epoch [1][20]\t Batch [1200][5500]\t Training Loss 0.8360\t Accuracy 0.8533\n",
      "Epoch [1][20]\t Batch [1250][5500]\t Training Loss 0.8366\t Accuracy 0.8531\n",
      "Epoch [1][20]\t Batch [1300][5500]\t Training Loss 0.8389\t Accuracy 0.8522\n",
      "Epoch [1][20]\t Batch [1350][5500]\t Training Loss 0.8399\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [1400][5500]\t Training Loss 0.8415\t Accuracy 0.8511\n",
      "Epoch [1][20]\t Batch [1450][5500]\t Training Loss 0.8434\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [1500][5500]\t Training Loss 0.8472\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [1550][5500]\t Training Loss 0.8470\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [1600][5500]\t Training Loss 0.8490\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [1650][5500]\t Training Loss 0.8484\t Accuracy 0.8480\n",
      "Epoch [1][20]\t Batch [1700][5500]\t Training Loss 0.8501\t Accuracy 0.8475\n",
      "Epoch [1][20]\t Batch [1750][5500]\t Training Loss 0.8506\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [1800][5500]\t Training Loss 0.8539\t Accuracy 0.8461\n",
      "Epoch [1][20]\t Batch [1850][5500]\t Training Loss 0.8528\t Accuracy 0.8469\n",
      "Epoch [1][20]\t Batch [1900][5500]\t Training Loss 0.8521\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [1950][5500]\t Training Loss 0.8522\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [2000][5500]\t Training Loss 0.8502\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [2050][5500]\t Training Loss 0.8502\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [2100][5500]\t Training Loss 0.8507\t Accuracy 0.8483\n",
      "Epoch [1][20]\t Batch [2150][5500]\t Training Loss 0.8495\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [2200][5500]\t Training Loss 0.8472\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [2250][5500]\t Training Loss 0.8484\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [2300][5500]\t Training Loss 0.8484\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [2350][5500]\t Training Loss 0.8473\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [2400][5500]\t Training Loss 0.8478\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [2450][5500]\t Training Loss 0.8476\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [2500][5500]\t Training Loss 0.8489\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [2550][5500]\t Training Loss 0.8472\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [2600][5500]\t Training Loss 0.8465\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [2650][5500]\t Training Loss 0.8466\t Accuracy 0.8502\n",
      "Epoch [1][20]\t Batch [2700][5500]\t Training Loss 0.8474\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [2750][5500]\t Training Loss 0.8477\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [2800][5500]\t Training Loss 0.8472\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [2850][5500]\t Training Loss 0.8459\t Accuracy 0.8507\n",
      "Epoch [1][20]\t Batch [2900][5500]\t Training Loss 0.8458\t Accuracy 0.8506\n",
      "Epoch [1][20]\t Batch [2950][5500]\t Training Loss 0.8459\t Accuracy 0.8504\n",
      "Epoch [1][20]\t Batch [3000][5500]\t Training Loss 0.8476\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [3050][5500]\t Training Loss 0.8486\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [3100][5500]\t Training Loss 0.8502\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [3150][5500]\t Training Loss 0.8516\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [3200][5500]\t Training Loss 0.8527\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [3250][5500]\t Training Loss 0.8545\t Accuracy 0.8482\n",
      "Epoch [1][20]\t Batch [3300][5500]\t Training Loss 0.8542\t Accuracy 0.8486\n",
      "Epoch [1][20]\t Batch [3350][5500]\t Training Loss 0.8548\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [3400][5500]\t Training Loss 0.8534\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [3450][5500]\t Training Loss 0.8531\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [3500][5500]\t Training Loss 0.8537\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [3550][5500]\t Training Loss 0.8537\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [3600][5500]\t Training Loss 0.8537\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [3650][5500]\t Training Loss 0.8537\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [3700][5500]\t Training Loss 0.8524\t Accuracy 0.8504\n",
      "Epoch [1][20]\t Batch [3750][5500]\t Training Loss 0.8531\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [3800][5500]\t Training Loss 0.8533\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [3850][5500]\t Training Loss 0.8535\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [3900][5500]\t Training Loss 0.8531\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [3950][5500]\t Training Loss 0.8528\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [4000][5500]\t Training Loss 0.8536\t Accuracy 0.8499\n",
      "Epoch [1][20]\t Batch [4050][5500]\t Training Loss 0.8533\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [4100][5500]\t Training Loss 0.8527\t Accuracy 0.8502\n",
      "Epoch [1][20]\t Batch [4150][5500]\t Training Loss 0.8540\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [4200][5500]\t Training Loss 0.8545\t Accuracy 0.8501\n",
      "Epoch [1][20]\t Batch [4250][5500]\t Training Loss 0.8553\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [4300][5500]\t Training Loss 0.8551\t Accuracy 0.8492\n",
      "Epoch [1][20]\t Batch [4350][5500]\t Training Loss 0.8545\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [4400][5500]\t Training Loss 0.8546\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [4450][5500]\t Training Loss 0.8552\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [4500][5500]\t Training Loss 0.8548\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [4550][5500]\t Training Loss 0.8552\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [4600][5500]\t Training Loss 0.8555\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [4650][5500]\t Training Loss 0.8561\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [4700][5500]\t Training Loss 0.8552\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [4750][5500]\t Training Loss 0.8555\t Accuracy 0.8495\n",
      "Epoch [1][20]\t Batch [4800][5500]\t Training Loss 0.8553\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [4850][5500]\t Training Loss 0.8541\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [4900][5500]\t Training Loss 0.8537\t Accuracy 0.8498\n",
      "Epoch [1][20]\t Batch [4950][5500]\t Training Loss 0.8541\t Accuracy 0.8494\n",
      "Epoch [1][20]\t Batch [5000][5500]\t Training Loss 0.8553\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [5050][5500]\t Training Loss 0.8559\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [5100][5500]\t Training Loss 0.8556\t Accuracy 0.8490\n",
      "Epoch [1][20]\t Batch [5150][5500]\t Training Loss 0.8552\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [5200][5500]\t Training Loss 0.8548\t Accuracy 0.8491\n",
      "Epoch [1][20]\t Batch [5250][5500]\t Training Loss 0.8552\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [5300][5500]\t Training Loss 0.8555\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [5350][5500]\t Training Loss 0.8552\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [5400][5500]\t Training Loss 0.8553\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [5450][5500]\t Training Loss 0.8550\t Accuracy 0.8489\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8549\t Average training accuracy 0.8489\n",
      "Epoch [1]\t Average validation loss 0.7869\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [2][20]\t Batch [0][5500]\t Training Loss 0.7434\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [50][5500]\t Training Loss 0.8261\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [100][5500]\t Training Loss 0.8630\t Accuracy 0.8386\n",
      "Epoch [2][20]\t Batch [150][5500]\t Training Loss 0.8772\t Accuracy 0.8358\n",
      "Epoch [2][20]\t Batch [200][5500]\t Training Loss 0.8570\t Accuracy 0.8443\n",
      "Epoch [2][20]\t Batch [250][5500]\t Training Loss 0.8438\t Accuracy 0.8482\n",
      "Epoch [2][20]\t Batch [300][5500]\t Training Loss 0.8360\t Accuracy 0.8508\n",
      "Epoch [2][20]\t Batch [350][5500]\t Training Loss 0.8415\t Accuracy 0.8519\n",
      "Epoch [2][20]\t Batch [400][5500]\t Training Loss 0.8392\t Accuracy 0.8546\n",
      "Epoch [2][20]\t Batch [450][5500]\t Training Loss 0.8390\t Accuracy 0.8550\n",
      "Epoch [2][20]\t Batch [500][5500]\t Training Loss 0.8343\t Accuracy 0.8555\n",
      "Epoch [2][20]\t Batch [550][5500]\t Training Loss 0.8324\t Accuracy 0.8550\n",
      "Epoch [2][20]\t Batch [600][5500]\t Training Loss 0.8324\t Accuracy 0.8562\n",
      "Epoch [2][20]\t Batch [650][5500]\t Training Loss 0.8263\t Accuracy 0.8581\n",
      "Epoch [2][20]\t Batch [700][5500]\t Training Loss 0.8262\t Accuracy 0.8581\n",
      "Epoch [2][20]\t Batch [750][5500]\t Training Loss 0.8353\t Accuracy 0.8566\n",
      "Epoch [2][20]\t Batch [800][5500]\t Training Loss 0.8375\t Accuracy 0.8557\n",
      "Epoch [2][20]\t Batch [850][5500]\t Training Loss 0.8402\t Accuracy 0.8550\n",
      "Epoch [2][20]\t Batch [900][5500]\t Training Loss 0.8422\t Accuracy 0.8529\n",
      "Epoch [2][20]\t Batch [950][5500]\t Training Loss 0.8410\t Accuracy 0.8539\n",
      "Epoch [2][20]\t Batch [1000][5500]\t Training Loss 0.8383\t Accuracy 0.8547\n",
      "Epoch [2][20]\t Batch [1050][5500]\t Training Loss 0.8352\t Accuracy 0.8558\n",
      "Epoch [2][20]\t Batch [1100][5500]\t Training Loss 0.8332\t Accuracy 0.8564\n",
      "Epoch [2][20]\t Batch [1150][5500]\t Training Loss 0.8308\t Accuracy 0.8562\n",
      "Epoch [2][20]\t Batch [1200][5500]\t Training Loss 0.8348\t Accuracy 0.8540\n",
      "Epoch [2][20]\t Batch [1250][5500]\t Training Loss 0.8353\t Accuracy 0.8536\n",
      "Epoch [2][20]\t Batch [1300][5500]\t Training Loss 0.8377\t Accuracy 0.8527\n",
      "Epoch [2][20]\t Batch [1350][5500]\t Training Loss 0.8386\t Accuracy 0.8525\n",
      "Epoch [2][20]\t Batch [1400][5500]\t Training Loss 0.8402\t Accuracy 0.8515\n",
      "Epoch [2][20]\t Batch [1450][5500]\t Training Loss 0.8422\t Accuracy 0.8505\n",
      "Epoch [2][20]\t Batch [1500][5500]\t Training Loss 0.8460\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [1550][5500]\t Training Loss 0.8458\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [1600][5500]\t Training Loss 0.8478\t Accuracy 0.8483\n",
      "Epoch [2][20]\t Batch [1650][5500]\t Training Loss 0.8472\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [1700][5500]\t Training Loss 0.8490\t Accuracy 0.8479\n",
      "Epoch [2][20]\t Batch [1750][5500]\t Training Loss 0.8495\t Accuracy 0.8477\n",
      "Epoch [2][20]\t Batch [1800][5500]\t Training Loss 0.8528\t Accuracy 0.8464\n",
      "Epoch [2][20]\t Batch [1850][5500]\t Training Loss 0.8517\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [1900][5500]\t Training Loss 0.8510\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [1950][5500]\t Training Loss 0.8511\t Accuracy 0.8490\n",
      "Epoch [2][20]\t Batch [2000][5500]\t Training Loss 0.8491\t Accuracy 0.8493\n",
      "Epoch [2][20]\t Batch [2050][5500]\t Training Loss 0.8491\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [2100][5500]\t Training Loss 0.8496\t Accuracy 0.8487\n",
      "Epoch [2][20]\t Batch [2150][5500]\t Training Loss 0.8484\t Accuracy 0.8496\n",
      "Epoch [2][20]\t Batch [2200][5500]\t Training Loss 0.8461\t Accuracy 0.8505\n",
      "Epoch [2][20]\t Batch [2250][5500]\t Training Loss 0.8474\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [2300][5500]\t Training Loss 0.8474\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [2350][5500]\t Training Loss 0.8462\t Accuracy 0.8504\n",
      "Epoch [2][20]\t Batch [2400][5500]\t Training Loss 0.8468\t Accuracy 0.8505\n",
      "Epoch [2][20]\t Batch [2450][5500]\t Training Loss 0.8466\t Accuracy 0.8504\n",
      "Epoch [2][20]\t Batch [2500][5500]\t Training Loss 0.8479\t Accuracy 0.8496\n",
      "Epoch [2][20]\t Batch [2550][5500]\t Training Loss 0.8462\t Accuracy 0.8501\n",
      "Epoch [2][20]\t Batch [2600][5500]\t Training Loss 0.8455\t Accuracy 0.8503\n",
      "Epoch [2][20]\t Batch [2650][5500]\t Training Loss 0.8456\t Accuracy 0.8507\n",
      "Epoch [2][20]\t Batch [2700][5500]\t Training Loss 0.8464\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [2750][5500]\t Training Loss 0.8467\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [2800][5500]\t Training Loss 0.8462\t Accuracy 0.8504\n",
      "Epoch [2][20]\t Batch [2850][5500]\t Training Loss 0.8449\t Accuracy 0.8511\n",
      "Epoch [2][20]\t Batch [2900][5500]\t Training Loss 0.8448\t Accuracy 0.8511\n",
      "Epoch [2][20]\t Batch [2950][5500]\t Training Loss 0.8450\t Accuracy 0.8509\n",
      "Epoch [2][20]\t Batch [3000][5500]\t Training Loss 0.8466\t Accuracy 0.8503\n",
      "Epoch [2][20]\t Batch [3050][5500]\t Training Loss 0.8477\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [3100][5500]\t Training Loss 0.8493\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [3150][5500]\t Training Loss 0.8507\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [3200][5500]\t Training Loss 0.8518\t Accuracy 0.8498\n",
      "Epoch [2][20]\t Batch [3250][5500]\t Training Loss 0.8536\t Accuracy 0.8488\n",
      "Epoch [2][20]\t Batch [3300][5500]\t Training Loss 0.8533\t Accuracy 0.8491\n",
      "Epoch [2][20]\t Batch [3350][5500]\t Training Loss 0.8538\t Accuracy 0.8489\n",
      "Epoch [2][20]\t Batch [3400][5500]\t Training Loss 0.8525\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [3450][5500]\t Training Loss 0.8522\t Accuracy 0.8504\n",
      "Epoch [2][20]\t Batch [3500][5500]\t Training Loss 0.8528\t Accuracy 0.8501\n",
      "Epoch [2][20]\t Batch [3550][5500]\t Training Loss 0.8527\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [3600][5500]\t Training Loss 0.8528\t Accuracy 0.8503\n",
      "Epoch [2][20]\t Batch [3650][5500]\t Training Loss 0.8527\t Accuracy 0.8503\n",
      "Epoch [2][20]\t Batch [3700][5500]\t Training Loss 0.8515\t Accuracy 0.8509\n",
      "Epoch [2][20]\t Batch [3750][5500]\t Training Loss 0.8522\t Accuracy 0.8506\n",
      "Epoch [2][20]\t Batch [3800][5500]\t Training Loss 0.8524\t Accuracy 0.8505\n",
      "Epoch [2][20]\t Batch [3850][5500]\t Training Loss 0.8526\t Accuracy 0.8504\n",
      "Epoch [2][20]\t Batch [3900][5500]\t Training Loss 0.8522\t Accuracy 0.8506\n",
      "Epoch [2][20]\t Batch [3950][5500]\t Training Loss 0.8519\t Accuracy 0.8506\n",
      "Epoch [2][20]\t Batch [4000][5500]\t Training Loss 0.8527\t Accuracy 0.8504\n",
      "Epoch [2][20]\t Batch [4050][5500]\t Training Loss 0.8524\t Accuracy 0.8505\n",
      "Epoch [2][20]\t Batch [4100][5500]\t Training Loss 0.8519\t Accuracy 0.8508\n",
      "Epoch [2][20]\t Batch [4150][5500]\t Training Loss 0.8531\t Accuracy 0.8506\n",
      "Epoch [2][20]\t Batch [4200][5500]\t Training Loss 0.8536\t Accuracy 0.8507\n",
      "Epoch [2][20]\t Batch [4250][5500]\t Training Loss 0.8544\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [4300][5500]\t Training Loss 0.8542\t Accuracy 0.8498\n",
      "Epoch [2][20]\t Batch [4350][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [4400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [4450][5500]\t Training Loss 0.8543\t Accuracy 0.8498\n",
      "Epoch [2][20]\t Batch [4500][5500]\t Training Loss 0.8540\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [4550][5500]\t Training Loss 0.8544\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [4600][5500]\t Training Loss 0.8546\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [4650][5500]\t Training Loss 0.8553\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [4700][5500]\t Training Loss 0.8544\t Accuracy 0.8501\n",
      "Epoch [2][20]\t Batch [4750][5500]\t Training Loss 0.8547\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [4800][5500]\t Training Loss 0.8545\t Accuracy 0.8498\n",
      "Epoch [2][20]\t Batch [4850][5500]\t Training Loss 0.8533\t Accuracy 0.8502\n",
      "Epoch [2][20]\t Batch [4900][5500]\t Training Loss 0.8529\t Accuracy 0.8503\n",
      "Epoch [2][20]\t Batch [4950][5500]\t Training Loss 0.8533\t Accuracy 0.8499\n",
      "Epoch [2][20]\t Batch [5000][5500]\t Training Loss 0.8546\t Accuracy 0.8496\n",
      "Epoch [2][20]\t Batch [5050][5500]\t Training Loss 0.8551\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [5100][5500]\t Training Loss 0.8549\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [5150][5500]\t Training Loss 0.8545\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [5200][5500]\t Training Loss 0.8541\t Accuracy 0.8496\n",
      "Epoch [2][20]\t Batch [5250][5500]\t Training Loss 0.8544\t Accuracy 0.8494\n",
      "Epoch [2][20]\t Batch [5300][5500]\t Training Loss 0.8547\t Accuracy 0.8492\n",
      "Epoch [2][20]\t Batch [5350][5500]\t Training Loss 0.8545\t Accuracy 0.8493\n",
      "Epoch [2][20]\t Batch [5400][5500]\t Training Loss 0.8546\t Accuracy 0.8493\n",
      "Epoch [2][20]\t Batch [5450][5500]\t Training Loss 0.8543\t Accuracy 0.8494\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8542\t Average training accuracy 0.8494\n",
      "Epoch [2]\t Average validation loss 0.7866\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [3][20]\t Batch [0][5500]\t Training Loss 0.7434\t Accuracy 0.9000\n",
      "Epoch [3][20]\t Batch [50][5500]\t Training Loss 0.8255\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [100][5500]\t Training Loss 0.8624\t Accuracy 0.8386\n",
      "Epoch [3][20]\t Batch [150][5500]\t Training Loss 0.8765\t Accuracy 0.8358\n",
      "Epoch [3][20]\t Batch [200][5500]\t Training Loss 0.8563\t Accuracy 0.8443\n",
      "Epoch [3][20]\t Batch [250][5500]\t Training Loss 0.8431\t Accuracy 0.8478\n",
      "Epoch [3][20]\t Batch [300][5500]\t Training Loss 0.8354\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [350][5500]\t Training Loss 0.8409\t Accuracy 0.8513\n",
      "Epoch [3][20]\t Batch [400][5500]\t Training Loss 0.8386\t Accuracy 0.8541\n",
      "Epoch [3][20]\t Batch [450][5500]\t Training Loss 0.8384\t Accuracy 0.8545\n",
      "Epoch [3][20]\t Batch [500][5500]\t Training Loss 0.8337\t Accuracy 0.8551\n",
      "Epoch [3][20]\t Batch [550][5500]\t Training Loss 0.8318\t Accuracy 0.8546\n",
      "Epoch [3][20]\t Batch [600][5500]\t Training Loss 0.8318\t Accuracy 0.8561\n",
      "Epoch [3][20]\t Batch [650][5500]\t Training Loss 0.8257\t Accuracy 0.8579\n",
      "Epoch [3][20]\t Batch [700][5500]\t Training Loss 0.8257\t Accuracy 0.8579\n",
      "Epoch [3][20]\t Batch [750][5500]\t Training Loss 0.8347\t Accuracy 0.8565\n",
      "Epoch [3][20]\t Batch [800][5500]\t Training Loss 0.8370\t Accuracy 0.8556\n",
      "Epoch [3][20]\t Batch [850][5500]\t Training Loss 0.8396\t Accuracy 0.8549\n",
      "Epoch [3][20]\t Batch [900][5500]\t Training Loss 0.8416\t Accuracy 0.8528\n",
      "Epoch [3][20]\t Batch [950][5500]\t Training Loss 0.8404\t Accuracy 0.8539\n",
      "Epoch [3][20]\t Batch [1000][5500]\t Training Loss 0.8377\t Accuracy 0.8548\n",
      "Epoch [3][20]\t Batch [1050][5500]\t Training Loss 0.8346\t Accuracy 0.8559\n",
      "Epoch [3][20]\t Batch [1100][5500]\t Training Loss 0.8326\t Accuracy 0.8566\n",
      "Epoch [3][20]\t Batch [1150][5500]\t Training Loss 0.8302\t Accuracy 0.8566\n",
      "Epoch [3][20]\t Batch [1200][5500]\t Training Loss 0.8342\t Accuracy 0.8543\n",
      "Epoch [3][20]\t Batch [1250][5500]\t Training Loss 0.8347\t Accuracy 0.8540\n",
      "Epoch [3][20]\t Batch [1300][5500]\t Training Loss 0.8370\t Accuracy 0.8530\n",
      "Epoch [3][20]\t Batch [1350][5500]\t Training Loss 0.8380\t Accuracy 0.8527\n",
      "Epoch [3][20]\t Batch [1400][5500]\t Training Loss 0.8396\t Accuracy 0.8517\n",
      "Epoch [3][20]\t Batch [1450][5500]\t Training Loss 0.8416\t Accuracy 0.8507\n",
      "Epoch [3][20]\t Batch [1500][5500]\t Training Loss 0.8454\t Accuracy 0.8486\n",
      "Epoch [3][20]\t Batch [1550][5500]\t Training Loss 0.8452\t Accuracy 0.8496\n",
      "Epoch [3][20]\t Batch [1600][5500]\t Training Loss 0.8472\t Accuracy 0.8487\n",
      "Epoch [3][20]\t Batch [1650][5500]\t Training Loss 0.8466\t Accuracy 0.8487\n",
      "Epoch [3][20]\t Batch [1700][5500]\t Training Loss 0.8484\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [1750][5500]\t Training Loss 0.8489\t Accuracy 0.8481\n",
      "Epoch [3][20]\t Batch [1800][5500]\t Training Loss 0.8522\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [1850][5500]\t Training Loss 0.8512\t Accuracy 0.8478\n",
      "Epoch [3][20]\t Batch [1900][5500]\t Training Loss 0.8505\t Accuracy 0.8493\n",
      "Epoch [3][20]\t Batch [1950][5500]\t Training Loss 0.8506\t Accuracy 0.8494\n",
      "Epoch [3][20]\t Batch [2000][5500]\t Training Loss 0.8485\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [2050][5500]\t Training Loss 0.8486\t Accuracy 0.8498\n",
      "Epoch [3][20]\t Batch [2100][5500]\t Training Loss 0.8491\t Accuracy 0.8491\n",
      "Epoch [3][20]\t Batch [2150][5500]\t Training Loss 0.8478\t Accuracy 0.8499\n",
      "Epoch [3][20]\t Batch [2200][5500]\t Training Loss 0.8456\t Accuracy 0.8508\n",
      "Epoch [3][20]\t Batch [2250][5500]\t Training Loss 0.8468\t Accuracy 0.8506\n",
      "Epoch [3][20]\t Batch [2300][5500]\t Training Loss 0.8468\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [2350][5500]\t Training Loss 0.8457\t Accuracy 0.8507\n",
      "Epoch [3][20]\t Batch [2400][5500]\t Training Loss 0.8462\t Accuracy 0.8508\n",
      "Epoch [3][20]\t Batch [2450][5500]\t Training Loss 0.8460\t Accuracy 0.8507\n",
      "Epoch [3][20]\t Batch [2500][5500]\t Training Loss 0.8473\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [2550][5500]\t Training Loss 0.8457\t Accuracy 0.8504\n",
      "Epoch [3][20]\t Batch [2600][5500]\t Training Loss 0.8450\t Accuracy 0.8506\n",
      "Epoch [3][20]\t Batch [2650][5500]\t Training Loss 0.8451\t Accuracy 0.8510\n",
      "Epoch [3][20]\t Batch [2700][5500]\t Training Loss 0.8459\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [2750][5500]\t Training Loss 0.8462\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [2800][5500]\t Training Loss 0.8457\t Accuracy 0.8508\n",
      "Epoch [3][20]\t Batch [2850][5500]\t Training Loss 0.8444\t Accuracy 0.8515\n",
      "Epoch [3][20]\t Batch [2900][5500]\t Training Loss 0.8443\t Accuracy 0.8514\n",
      "Epoch [3][20]\t Batch [2950][5500]\t Training Loss 0.8445\t Accuracy 0.8512\n",
      "Epoch [3][20]\t Batch [3000][5500]\t Training Loss 0.8461\t Accuracy 0.8506\n",
      "Epoch [3][20]\t Batch [3050][5500]\t Training Loss 0.8472\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [3100][5500]\t Training Loss 0.8488\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [3150][5500]\t Training Loss 0.8502\t Accuracy 0.8504\n",
      "Epoch [3][20]\t Batch [3200][5500]\t Training Loss 0.8513\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [3250][5500]\t Training Loss 0.8531\t Accuracy 0.8491\n",
      "Epoch [3][20]\t Batch [3300][5500]\t Training Loss 0.8528\t Accuracy 0.8494\n",
      "Epoch [3][20]\t Batch [3350][5500]\t Training Loss 0.8534\t Accuracy 0.8493\n",
      "Epoch [3][20]\t Batch [3400][5500]\t Training Loss 0.8520\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [3450][5500]\t Training Loss 0.8518\t Accuracy 0.8507\n",
      "Epoch [3][20]\t Batch [3500][5500]\t Training Loss 0.8523\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [3550][5500]\t Training Loss 0.8523\t Accuracy 0.8506\n",
      "Epoch [3][20]\t Batch [3600][5500]\t Training Loss 0.8524\t Accuracy 0.8506\n",
      "Epoch [3][20]\t Batch [3650][5500]\t Training Loss 0.8523\t Accuracy 0.8507\n",
      "Epoch [3][20]\t Batch [3700][5500]\t Training Loss 0.8510\t Accuracy 0.8513\n",
      "Epoch [3][20]\t Batch [3750][5500]\t Training Loss 0.8518\t Accuracy 0.8509\n",
      "Epoch [3][20]\t Batch [3800][5500]\t Training Loss 0.8520\t Accuracy 0.8508\n",
      "Epoch [3][20]\t Batch [3850][5500]\t Training Loss 0.8521\t Accuracy 0.8508\n",
      "Epoch [3][20]\t Batch [3900][5500]\t Training Loss 0.8517\t Accuracy 0.8509\n",
      "Epoch [3][20]\t Batch [3950][5500]\t Training Loss 0.8515\t Accuracy 0.8510\n",
      "Epoch [3][20]\t Batch [4000][5500]\t Training Loss 0.8523\t Accuracy 0.8508\n",
      "Epoch [3][20]\t Batch [4050][5500]\t Training Loss 0.8519\t Accuracy 0.8509\n",
      "Epoch [3][20]\t Batch [4100][5500]\t Training Loss 0.8514\t Accuracy 0.8511\n",
      "Epoch [3][20]\t Batch [4150][5500]\t Training Loss 0.8527\t Accuracy 0.8510\n",
      "Epoch [3][20]\t Batch [4200][5500]\t Training Loss 0.8532\t Accuracy 0.8510\n",
      "Epoch [3][20]\t Batch [4250][5500]\t Training Loss 0.8540\t Accuracy 0.8503\n",
      "Epoch [3][20]\t Batch [4300][5500]\t Training Loss 0.8538\t Accuracy 0.8501\n",
      "Epoch [3][20]\t Batch [4350][5500]\t Training Loss 0.8532\t Accuracy 0.8504\n",
      "Epoch [3][20]\t Batch [4400][5500]\t Training Loss 0.8533\t Accuracy 0.8503\n",
      "Epoch [3][20]\t Batch [4450][5500]\t Training Loss 0.8539\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [4500][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [3][20]\t Batch [4550][5500]\t Training Loss 0.8540\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [4600][5500]\t Training Loss 0.8542\t Accuracy 0.8503\n",
      "Epoch [3][20]\t Batch [4650][5500]\t Training Loss 0.8548\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [4700][5500]\t Training Loss 0.8540\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [4750][5500]\t Training Loss 0.8543\t Accuracy 0.8503\n",
      "Epoch [3][20]\t Batch [4800][5500]\t Training Loss 0.8541\t Accuracy 0.8501\n",
      "Epoch [3][20]\t Batch [4850][5500]\t Training Loss 0.8529\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [4900][5500]\t Training Loss 0.8525\t Accuracy 0.8506\n",
      "Epoch [3][20]\t Batch [4950][5500]\t Training Loss 0.8529\t Accuracy 0.8502\n",
      "Epoch [3][20]\t Batch [5000][5500]\t Training Loss 0.8542\t Accuracy 0.8499\n",
      "Epoch [3][20]\t Batch [5050][5500]\t Training Loss 0.8547\t Accuracy 0.8498\n",
      "Epoch [3][20]\t Batch [5100][5500]\t Training Loss 0.8545\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [5150][5500]\t Training Loss 0.8541\t Accuracy 0.8496\n",
      "Epoch [3][20]\t Batch [5200][5500]\t Training Loss 0.8537\t Accuracy 0.8499\n",
      "Epoch [3][20]\t Batch [5250][5500]\t Training Loss 0.8540\t Accuracy 0.8497\n",
      "Epoch [3][20]\t Batch [5300][5500]\t Training Loss 0.8544\t Accuracy 0.8494\n",
      "Epoch [3][20]\t Batch [5350][5500]\t Training Loss 0.8541\t Accuracy 0.8495\n",
      "Epoch [3][20]\t Batch [5400][5500]\t Training Loss 0.8542\t Accuracy 0.8495\n",
      "Epoch [3][20]\t Batch [5450][5500]\t Training Loss 0.8539\t Accuracy 0.8496\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8539\t Average training accuracy 0.8496\n",
      "Epoch [3]\t Average validation loss 0.7864\t Average validation accuracy 0.8866\n",
      "\n",
      "Epoch [4][20]\t Batch [0][5500]\t Training Loss 0.7434\t Accuracy 0.9000\n",
      "Epoch [4][20]\t Batch [50][5500]\t Training Loss 0.8252\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [100][5500]\t Training Loss 0.8620\t Accuracy 0.8386\n",
      "Epoch [4][20]\t Batch [150][5500]\t Training Loss 0.8762\t Accuracy 0.8358\n",
      "Epoch [4][20]\t Batch [200][5500]\t Training Loss 0.8560\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [250][5500]\t Training Loss 0.8428\t Accuracy 0.8478\n",
      "Epoch [4][20]\t Batch [300][5500]\t Training Loss 0.8351\t Accuracy 0.8502\n",
      "Epoch [4][20]\t Batch [350][5500]\t Training Loss 0.8406\t Accuracy 0.8513\n",
      "Epoch [4][20]\t Batch [400][5500]\t Training Loss 0.8383\t Accuracy 0.8541\n",
      "Epoch [4][20]\t Batch [450][5500]\t Training Loss 0.8381\t Accuracy 0.8545\n",
      "Epoch [4][20]\t Batch [500][5500]\t Training Loss 0.8334\t Accuracy 0.8551\n",
      "Epoch [4][20]\t Batch [550][5500]\t Training Loss 0.8315\t Accuracy 0.8548\n",
      "Epoch [4][20]\t Batch [600][5500]\t Training Loss 0.8315\t Accuracy 0.8562\n",
      "Epoch [4][20]\t Batch [650][5500]\t Training Loss 0.8254\t Accuracy 0.8581\n",
      "Epoch [4][20]\t Batch [700][5500]\t Training Loss 0.8254\t Accuracy 0.8581\n",
      "Epoch [4][20]\t Batch [750][5500]\t Training Loss 0.8344\t Accuracy 0.8567\n",
      "Epoch [4][20]\t Batch [800][5500]\t Training Loss 0.8367\t Accuracy 0.8558\n",
      "Epoch [4][20]\t Batch [850][5500]\t Training Loss 0.8393\t Accuracy 0.8551\n",
      "Epoch [4][20]\t Batch [900][5500]\t Training Loss 0.8413\t Accuracy 0.8531\n",
      "Epoch [4][20]\t Batch [950][5500]\t Training Loss 0.8401\t Accuracy 0.8542\n",
      "Epoch [4][20]\t Batch [1000][5500]\t Training Loss 0.8374\t Accuracy 0.8550\n",
      "Epoch [4][20]\t Batch [1050][5500]\t Training Loss 0.8343\t Accuracy 0.8560\n",
      "Epoch [4][20]\t Batch [1100][5500]\t Training Loss 0.8323\t Accuracy 0.8568\n",
      "Epoch [4][20]\t Batch [1150][5500]\t Training Loss 0.8298\t Accuracy 0.8567\n",
      "Epoch [4][20]\t Batch [1200][5500]\t Training Loss 0.8339\t Accuracy 0.8545\n",
      "Epoch [4][20]\t Batch [1250][5500]\t Training Loss 0.8344\t Accuracy 0.8541\n",
      "Epoch [4][20]\t Batch [1300][5500]\t Training Loss 0.8367\t Accuracy 0.8532\n",
      "Epoch [4][20]\t Batch [1350][5500]\t Training Loss 0.8377\t Accuracy 0.8528\n",
      "Epoch [4][20]\t Batch [1400][5500]\t Training Loss 0.8393\t Accuracy 0.8519\n",
      "Epoch [4][20]\t Batch [1450][5500]\t Training Loss 0.8413\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [1500][5500]\t Training Loss 0.8451\t Accuracy 0.8487\n",
      "Epoch [4][20]\t Batch [1550][5500]\t Training Loss 0.8449\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [1600][5500]\t Training Loss 0.8469\t Accuracy 0.8488\n",
      "Epoch [4][20]\t Batch [1650][5500]\t Training Loss 0.8463\t Accuracy 0.8488\n",
      "Epoch [4][20]\t Batch [1700][5500]\t Training Loss 0.8481\t Accuracy 0.8484\n",
      "Epoch [4][20]\t Batch [1750][5500]\t Training Loss 0.8486\t Accuracy 0.8483\n",
      "Epoch [4][20]\t Batch [1800][5500]\t Training Loss 0.8519\t Accuracy 0.8470\n",
      "Epoch [4][20]\t Batch [1850][5500]\t Training Loss 0.8509\t Accuracy 0.8479\n",
      "Epoch [4][20]\t Batch [1900][5500]\t Training Loss 0.8502\t Accuracy 0.8495\n",
      "Epoch [4][20]\t Batch [1950][5500]\t Training Loss 0.8503\t Accuracy 0.8496\n",
      "Epoch [4][20]\t Batch [2000][5500]\t Training Loss 0.8483\t Accuracy 0.8498\n",
      "Epoch [4][20]\t Batch [2050][5500]\t Training Loss 0.8483\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [2100][5500]\t Training Loss 0.8488\t Accuracy 0.8493\n",
      "Epoch [4][20]\t Batch [2150][5500]\t Training Loss 0.8476\t Accuracy 0.8501\n",
      "Epoch [4][20]\t Batch [2200][5500]\t Training Loss 0.8453\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [2250][5500]\t Training Loss 0.8466\t Accuracy 0.8508\n",
      "Epoch [4][20]\t Batch [2300][5500]\t Training Loss 0.8465\t Accuracy 0.8507\n",
      "Epoch [4][20]\t Batch [2350][5500]\t Training Loss 0.8454\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [2400][5500]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [2450][5500]\t Training Loss 0.8458\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [2500][5500]\t Training Loss 0.8471\t Accuracy 0.8502\n",
      "Epoch [4][20]\t Batch [2550][5500]\t Training Loss 0.8454\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [2600][5500]\t Training Loss 0.8447\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [2650][5500]\t Training Loss 0.8449\t Accuracy 0.8513\n",
      "Epoch [4][20]\t Batch [2700][5500]\t Training Loss 0.8456\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [2750][5500]\t Training Loss 0.8459\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [2800][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [4][20]\t Batch [2850][5500]\t Training Loss 0.8442\t Accuracy 0.8518\n",
      "Epoch [4][20]\t Batch [2900][5500]\t Training Loss 0.8441\t Accuracy 0.8517\n",
      "Epoch [4][20]\t Batch [2950][5500]\t Training Loss 0.8442\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [3000][5500]\t Training Loss 0.8459\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [3050][5500]\t Training Loss 0.8470\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [3100][5500]\t Training Loss 0.8486\t Accuracy 0.8508\n",
      "Epoch [4][20]\t Batch [3150][5500]\t Training Loss 0.8500\t Accuracy 0.8507\n",
      "Epoch [4][20]\t Batch [3200][5500]\t Training Loss 0.8510\t Accuracy 0.8504\n",
      "Epoch [4][20]\t Batch [3250][5500]\t Training Loss 0.8529\t Accuracy 0.8495\n",
      "Epoch [4][20]\t Batch [3300][5500]\t Training Loss 0.8526\t Accuracy 0.8498\n",
      "Epoch [4][20]\t Batch [3350][5500]\t Training Loss 0.8531\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [3400][5500]\t Training Loss 0.8518\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [3450][5500]\t Training Loss 0.8515\t Accuracy 0.8511\n",
      "Epoch [4][20]\t Batch [3500][5500]\t Training Loss 0.8521\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [3550][5500]\t Training Loss 0.8520\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [3600][5500]\t Training Loss 0.8521\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [3650][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [4][20]\t Batch [3700][5500]\t Training Loss 0.8508\t Accuracy 0.8516\n",
      "Epoch [4][20]\t Batch [3750][5500]\t Training Loss 0.8515\t Accuracy 0.8513\n",
      "Epoch [4][20]\t Batch [3800][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [4][20]\t Batch [3850][5500]\t Training Loss 0.8519\t Accuracy 0.8511\n",
      "Epoch [4][20]\t Batch [3900][5500]\t Training Loss 0.8515\t Accuracy 0.8513\n",
      "Epoch [4][20]\t Batch [3950][5500]\t Training Loss 0.8512\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [4000][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [4][20]\t Batch [4050][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [4][20]\t Batch [4100][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [4][20]\t Batch [4150][5500]\t Training Loss 0.8524\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [4200][5500]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [4250][5500]\t Training Loss 0.8537\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [4300][5500]\t Training Loss 0.8535\t Accuracy 0.8505\n",
      "Epoch [4][20]\t Batch [4350][5500]\t Training Loss 0.8530\t Accuracy 0.8507\n",
      "Epoch [4][20]\t Batch [4400][5500]\t Training Loss 0.8530\t Accuracy 0.8507\n",
      "Epoch [4][20]\t Batch [4450][5500]\t Training Loss 0.8537\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [4500][5500]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "Epoch [4][20]\t Batch [4550][5500]\t Training Loss 0.8537\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [4600][5500]\t Training Loss 0.8540\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [4650][5500]\t Training Loss 0.8546\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [4700][5500]\t Training Loss 0.8538\t Accuracy 0.8508\n",
      "Epoch [4][20]\t Batch [4750][5500]\t Training Loss 0.8541\t Accuracy 0.8506\n",
      "Epoch [4][20]\t Batch [4800][5500]\t Training Loss 0.8539\t Accuracy 0.8504\n",
      "Epoch [4][20]\t Batch [4850][5500]\t Training Loss 0.8527\t Accuracy 0.8508\n",
      "Epoch [4][20]\t Batch [4900][5500]\t Training Loss 0.8523\t Accuracy 0.8509\n",
      "Epoch [4][20]\t Batch [4950][5500]\t Training Loss 0.8527\t Accuracy 0.8505\n",
      "Epoch [4][20]\t Batch [5000][5500]\t Training Loss 0.8540\t Accuracy 0.8502\n",
      "Epoch [4][20]\t Batch [5050][5500]\t Training Loss 0.8545\t Accuracy 0.8501\n",
      "Epoch [4][20]\t Batch [5100][5500]\t Training Loss 0.8543\t Accuracy 0.8501\n",
      "Epoch [4][20]\t Batch [5150][5500]\t Training Loss 0.8539\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [5200][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [4][20]\t Batch [5250][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [5300][5500]\t Training Loss 0.8542\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [5350][5500]\t Training Loss 0.8539\t Accuracy 0.8498\n",
      "Epoch [4][20]\t Batch [5400][5500]\t Training Loss 0.8540\t Accuracy 0.8498\n",
      "Epoch [4][20]\t Batch [5450][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8537\t Average training accuracy 0.8499\n",
      "Epoch [4]\t Average validation loss 0.7863\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [5][20]\t Batch [0][5500]\t Training Loss 0.7433\t Accuracy 0.9000\n",
      "Epoch [5][20]\t Batch [50][5500]\t Training Loss 0.8250\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [100][5500]\t Training Loss 0.8618\t Accuracy 0.8386\n",
      "Epoch [5][20]\t Batch [150][5500]\t Training Loss 0.8760\t Accuracy 0.8358\n",
      "Epoch [5][20]\t Batch [200][5500]\t Training Loss 0.8558\t Accuracy 0.8443\n",
      "Epoch [5][20]\t Batch [250][5500]\t Training Loss 0.8426\t Accuracy 0.8478\n",
      "Epoch [5][20]\t Batch [300][5500]\t Training Loss 0.8349\t Accuracy 0.8505\n",
      "Epoch [5][20]\t Batch [350][5500]\t Training Loss 0.8404\t Accuracy 0.8516\n",
      "Epoch [5][20]\t Batch [400][5500]\t Training Loss 0.8382\t Accuracy 0.8544\n",
      "Epoch [5][20]\t Batch [450][5500]\t Training Loss 0.8380\t Accuracy 0.8548\n",
      "Epoch [5][20]\t Batch [500][5500]\t Training Loss 0.8332\t Accuracy 0.8553\n",
      "Epoch [5][20]\t Batch [550][5500]\t Training Loss 0.8314\t Accuracy 0.8550\n",
      "Epoch [5][20]\t Batch [600][5500]\t Training Loss 0.8313\t Accuracy 0.8564\n",
      "Epoch [5][20]\t Batch [650][5500]\t Training Loss 0.8253\t Accuracy 0.8582\n",
      "Epoch [5][20]\t Batch [700][5500]\t Training Loss 0.8252\t Accuracy 0.8582\n",
      "Epoch [5][20]\t Batch [750][5500]\t Training Loss 0.8343\t Accuracy 0.8567\n",
      "Epoch [5][20]\t Batch [800][5500]\t Training Loss 0.8365\t Accuracy 0.8558\n",
      "Epoch [5][20]\t Batch [850][5500]\t Training Loss 0.8391\t Accuracy 0.8551\n",
      "Epoch [5][20]\t Batch [900][5500]\t Training Loss 0.8411\t Accuracy 0.8531\n",
      "Epoch [5][20]\t Batch [950][5500]\t Training Loss 0.8400\t Accuracy 0.8542\n",
      "Epoch [5][20]\t Batch [1000][5500]\t Training Loss 0.8372\t Accuracy 0.8550\n",
      "Epoch [5][20]\t Batch [1050][5500]\t Training Loss 0.8341\t Accuracy 0.8560\n",
      "Epoch [5][20]\t Batch [1100][5500]\t Training Loss 0.8321\t Accuracy 0.8568\n",
      "Epoch [5][20]\t Batch [1150][5500]\t Training Loss 0.8297\t Accuracy 0.8567\n",
      "Epoch [5][20]\t Batch [1200][5500]\t Training Loss 0.8337\t Accuracy 0.8545\n",
      "Epoch [5][20]\t Batch [1250][5500]\t Training Loss 0.8343\t Accuracy 0.8541\n",
      "Epoch [5][20]\t Batch [1300][5500]\t Training Loss 0.8366\t Accuracy 0.8532\n",
      "Epoch [5][20]\t Batch [1350][5500]\t Training Loss 0.8376\t Accuracy 0.8528\n",
      "Epoch [5][20]\t Batch [1400][5500]\t Training Loss 0.8391\t Accuracy 0.8519\n",
      "Epoch [5][20]\t Batch [1450][5500]\t Training Loss 0.8411\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [1500][5500]\t Training Loss 0.8450\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [1550][5500]\t Training Loss 0.8447\t Accuracy 0.8496\n",
      "Epoch [5][20]\t Batch [1600][5500]\t Training Loss 0.8467\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [1650][5500]\t Training Loss 0.8462\t Accuracy 0.8488\n",
      "Epoch [5][20]\t Batch [1700][5500]\t Training Loss 0.8479\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [1750][5500]\t Training Loss 0.8484\t Accuracy 0.8482\n",
      "Epoch [5][20]\t Batch [1800][5500]\t Training Loss 0.8518\t Accuracy 0.8470\n",
      "Epoch [5][20]\t Batch [1850][5500]\t Training Loss 0.8507\t Accuracy 0.8479\n",
      "Epoch [5][20]\t Batch [1900][5500]\t Training Loss 0.8500\t Accuracy 0.8494\n",
      "Epoch [5][20]\t Batch [1950][5500]\t Training Loss 0.8501\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [2000][5500]\t Training Loss 0.8481\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [2050][5500]\t Training Loss 0.8482\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [2100][5500]\t Training Loss 0.8486\t Accuracy 0.8493\n",
      "Epoch [5][20]\t Batch [2150][5500]\t Training Loss 0.8474\t Accuracy 0.8501\n",
      "Epoch [5][20]\t Batch [2200][5500]\t Training Loss 0.8452\t Accuracy 0.8510\n",
      "Epoch [5][20]\t Batch [2250][5500]\t Training Loss 0.8464\t Accuracy 0.8508\n",
      "Epoch [5][20]\t Batch [2300][5500]\t Training Loss 0.8464\t Accuracy 0.8507\n",
      "Epoch [5][20]\t Batch [2350][5500]\t Training Loss 0.8452\t Accuracy 0.8510\n",
      "Epoch [5][20]\t Batch [2400][5500]\t Training Loss 0.8458\t Accuracy 0.8510\n",
      "Epoch [5][20]\t Batch [2450][5500]\t Training Loss 0.8456\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [2500][5500]\t Training Loss 0.8469\t Accuracy 0.8502\n",
      "Epoch [5][20]\t Batch [2550][5500]\t Training Loss 0.8453\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [2600][5500]\t Training Loss 0.8446\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [2650][5500]\t Training Loss 0.8447\t Accuracy 0.8513\n",
      "Epoch [5][20]\t Batch [2700][5500]\t Training Loss 0.8455\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [2750][5500]\t Training Loss 0.8458\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [2800][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [5][20]\t Batch [2850][5500]\t Training Loss 0.8440\t Accuracy 0.8518\n",
      "Epoch [5][20]\t Batch [2900][5500]\t Training Loss 0.8439\t Accuracy 0.8517\n",
      "Epoch [5][20]\t Batch [2950][5500]\t Training Loss 0.8441\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [3000][5500]\t Training Loss 0.8458\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [3050][5500]\t Training Loss 0.8468\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [3100][5500]\t Training Loss 0.8484\t Accuracy 0.8508\n",
      "Epoch [5][20]\t Batch [3150][5500]\t Training Loss 0.8498\t Accuracy 0.8507\n",
      "Epoch [5][20]\t Batch [3200][5500]\t Training Loss 0.8509\t Accuracy 0.8504\n",
      "Epoch [5][20]\t Batch [3250][5500]\t Training Loss 0.8528\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [3300][5500]\t Training Loss 0.8525\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [3350][5500]\t Training Loss 0.8530\t Accuracy 0.8497\n",
      "Epoch [5][20]\t Batch [3400][5500]\t Training Loss 0.8516\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [3450][5500]\t Training Loss 0.8514\t Accuracy 0.8512\n",
      "Epoch [5][20]\t Batch [3500][5500]\t Training Loss 0.8519\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [3550][5500]\t Training Loss 0.8519\t Accuracy 0.8510\n",
      "Epoch [5][20]\t Batch [3600][5500]\t Training Loss 0.8520\t Accuracy 0.8510\n",
      "Epoch [5][20]\t Batch [3650][5500]\t Training Loss 0.8519\t Accuracy 0.8511\n",
      "Epoch [5][20]\t Batch [3700][5500]\t Training Loss 0.8507\t Accuracy 0.8517\n",
      "Epoch [5][20]\t Batch [3750][5500]\t Training Loss 0.8514\t Accuracy 0.8513\n",
      "Epoch [5][20]\t Batch [3800][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [5][20]\t Batch [3850][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [5][20]\t Batch [3900][5500]\t Training Loss 0.8514\t Accuracy 0.8513\n",
      "Epoch [5][20]\t Batch [3950][5500]\t Training Loss 0.8511\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [4000][5500]\t Training Loss 0.8519\t Accuracy 0.8511\n",
      "Epoch [5][20]\t Batch [4050][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [5][20]\t Batch [4100][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [5][20]\t Batch [4150][5500]\t Training Loss 0.8523\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [4200][5500]\t Training Loss 0.8528\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [4250][5500]\t Training Loss 0.8536\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [4300][5500]\t Training Loss 0.8534\t Accuracy 0.8505\n",
      "Epoch [5][20]\t Batch [4350][5500]\t Training Loss 0.8529\t Accuracy 0.8507\n",
      "Epoch [5][20]\t Batch [4400][5500]\t Training Loss 0.8529\t Accuracy 0.8507\n",
      "Epoch [5][20]\t Batch [4450][5500]\t Training Loss 0.8536\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [4500][5500]\t Training Loss 0.8532\t Accuracy 0.8508\n",
      "Epoch [5][20]\t Batch [4550][5500]\t Training Loss 0.8536\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [4600][5500]\t Training Loss 0.8539\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [4650][5500]\t Training Loss 0.8545\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [4700][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [5][20]\t Batch [4750][5500]\t Training Loss 0.8539\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [4800][5500]\t Training Loss 0.8538\t Accuracy 0.8505\n",
      "Epoch [5][20]\t Batch [4850][5500]\t Training Loss 0.8525\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [4900][5500]\t Training Loss 0.8522\t Accuracy 0.8509\n",
      "Epoch [5][20]\t Batch [4950][5500]\t Training Loss 0.8526\t Accuracy 0.8506\n",
      "Epoch [5][20]\t Batch [5000][5500]\t Training Loss 0.8538\t Accuracy 0.8503\n",
      "Epoch [5][20]\t Batch [5050][5500]\t Training Loss 0.8544\t Accuracy 0.8502\n",
      "Epoch [5][20]\t Batch [5100][5500]\t Training Loss 0.8542\t Accuracy 0.8501\n",
      "Epoch [5][20]\t Batch [5150][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [5200][5500]\t Training Loss 0.8534\t Accuracy 0.8503\n",
      "Epoch [5][20]\t Batch [5250][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [5300][5500]\t Training Loss 0.8541\t Accuracy 0.8498\n",
      "Epoch [5][20]\t Batch [5350][5500]\t Training Loss 0.8538\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [5400][5500]\t Training Loss 0.8539\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [5450][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8536\t Average training accuracy 0.8500\n",
      "Epoch [5]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [6][20]\t Batch [0][5500]\t Training Loss 0.7433\t Accuracy 0.9000\n",
      "Epoch [6][20]\t Batch [50][5500]\t Training Loss 0.8249\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [100][5500]\t Training Loss 0.8617\t Accuracy 0.8386\n",
      "Epoch [6][20]\t Batch [150][5500]\t Training Loss 0.8759\t Accuracy 0.8358\n",
      "Epoch [6][20]\t Batch [200][5500]\t Training Loss 0.8557\t Accuracy 0.8443\n",
      "Epoch [6][20]\t Batch [250][5500]\t Training Loss 0.8425\t Accuracy 0.8478\n",
      "Epoch [6][20]\t Batch [300][5500]\t Training Loss 0.8348\t Accuracy 0.8505\n",
      "Epoch [6][20]\t Batch [350][5500]\t Training Loss 0.8403\t Accuracy 0.8519\n",
      "Epoch [6][20]\t Batch [400][5500]\t Training Loss 0.8381\t Accuracy 0.8549\n",
      "Epoch [6][20]\t Batch [450][5500]\t Training Loss 0.8379\t Accuracy 0.8552\n",
      "Epoch [6][20]\t Batch [500][5500]\t Training Loss 0.8331\t Accuracy 0.8557\n",
      "Epoch [6][20]\t Batch [550][5500]\t Training Loss 0.8313\t Accuracy 0.8555\n",
      "Epoch [6][20]\t Batch [600][5500]\t Training Loss 0.8312\t Accuracy 0.8569\n",
      "Epoch [6][20]\t Batch [650][5500]\t Training Loss 0.8252\t Accuracy 0.8587\n",
      "Epoch [6][20]\t Batch [700][5500]\t Training Loss 0.8251\t Accuracy 0.8586\n",
      "Epoch [6][20]\t Batch [750][5500]\t Training Loss 0.8342\t Accuracy 0.8571\n",
      "Epoch [6][20]\t Batch [800][5500]\t Training Loss 0.8364\t Accuracy 0.8562\n",
      "Epoch [6][20]\t Batch [850][5500]\t Training Loss 0.8390\t Accuracy 0.8555\n",
      "Epoch [6][20]\t Batch [900][5500]\t Training Loss 0.8410\t Accuracy 0.8534\n",
      "Epoch [6][20]\t Batch [950][5500]\t Training Loss 0.8399\t Accuracy 0.8545\n",
      "Epoch [6][20]\t Batch [1000][5500]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [6][20]\t Batch [1050][5500]\t Training Loss 0.8340\t Accuracy 0.8563\n",
      "Epoch [6][20]\t Batch [1100][5500]\t Training Loss 0.8320\t Accuracy 0.8570\n",
      "Epoch [6][20]\t Batch [1150][5500]\t Training Loss 0.8296\t Accuracy 0.8570\n",
      "Epoch [6][20]\t Batch [1200][5500]\t Training Loss 0.8336\t Accuracy 0.8547\n",
      "Epoch [6][20]\t Batch [1250][5500]\t Training Loss 0.8342\t Accuracy 0.8544\n",
      "Epoch [6][20]\t Batch [1300][5500]\t Training Loss 0.8365\t Accuracy 0.8534\n",
      "Epoch [6][20]\t Batch [1350][5500]\t Training Loss 0.8375\t Accuracy 0.8531\n",
      "Epoch [6][20]\t Batch [1400][5500]\t Training Loss 0.8390\t Accuracy 0.8521\n",
      "Epoch [6][20]\t Batch [1450][5500]\t Training Loss 0.8411\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [1500][5500]\t Training Loss 0.8449\t Accuracy 0.8488\n",
      "Epoch [6][20]\t Batch [1550][5500]\t Training Loss 0.8446\t Accuracy 0.8498\n",
      "Epoch [6][20]\t Batch [1600][5500]\t Training Loss 0.8467\t Accuracy 0.8489\n",
      "Epoch [6][20]\t Batch [1650][5500]\t Training Loss 0.8461\t Accuracy 0.8489\n",
      "Epoch [6][20]\t Batch [1700][5500]\t Training Loss 0.8479\t Accuracy 0.8486\n",
      "Epoch [6][20]\t Batch [1750][5500]\t Training Loss 0.8484\t Accuracy 0.8484\n",
      "Epoch [6][20]\t Batch [1800][5500]\t Training Loss 0.8517\t Accuracy 0.8471\n",
      "Epoch [6][20]\t Batch [1850][5500]\t Training Loss 0.8506\t Accuracy 0.8481\n",
      "Epoch [6][20]\t Batch [1900][5500]\t Training Loss 0.8500\t Accuracy 0.8497\n",
      "Epoch [6][20]\t Batch [1950][5500]\t Training Loss 0.8500\t Accuracy 0.8497\n",
      "Epoch [6][20]\t Batch [2000][5500]\t Training Loss 0.8480\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [2050][5500]\t Training Loss 0.8481\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [2100][5500]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [6][20]\t Batch [2150][5500]\t Training Loss 0.8473\t Accuracy 0.8503\n",
      "Epoch [6][20]\t Batch [2200][5500]\t Training Loss 0.8451\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [2250][5500]\t Training Loss 0.8463\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [2300][5500]\t Training Loss 0.8463\t Accuracy 0.8508\n",
      "Epoch [6][20]\t Batch [2350][5500]\t Training Loss 0.8451\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [2400][5500]\t Training Loss 0.8457\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [2450][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [2500][5500]\t Training Loss 0.8468\t Accuracy 0.8503\n",
      "Epoch [6][20]\t Batch [2550][5500]\t Training Loss 0.8452\t Accuracy 0.8507\n",
      "Epoch [6][20]\t Batch [2600][5500]\t Training Loss 0.8445\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [2650][5500]\t Training Loss 0.8446\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [2700][5500]\t Training Loss 0.8454\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [2750][5500]\t Training Loss 0.8457\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [2800][5500]\t Training Loss 0.8453\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [2850][5500]\t Training Loss 0.8440\t Accuracy 0.8520\n",
      "Epoch [6][20]\t Batch [2900][5500]\t Training Loss 0.8439\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [2950][5500]\t Training Loss 0.8440\t Accuracy 0.8516\n",
      "Epoch [6][20]\t Batch [3000][5500]\t Training Loss 0.8457\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [3050][5500]\t Training Loss 0.8467\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [3100][5500]\t Training Loss 0.8483\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [3150][5500]\t Training Loss 0.8497\t Accuracy 0.8509\n",
      "Epoch [6][20]\t Batch [3200][5500]\t Training Loss 0.8508\t Accuracy 0.8506\n",
      "Epoch [6][20]\t Batch [3250][5500]\t Training Loss 0.8527\t Accuracy 0.8496\n",
      "Epoch [6][20]\t Batch [3300][5500]\t Training Loss 0.8524\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [3350][5500]\t Training Loss 0.8529\t Accuracy 0.8499\n",
      "Epoch [6][20]\t Batch [3400][5500]\t Training Loss 0.8516\t Accuracy 0.8508\n",
      "Epoch [6][20]\t Batch [3450][5500]\t Training Loss 0.8513\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [3500][5500]\t Training Loss 0.8519\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [3550][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [3600][5500]\t Training Loss 0.8519\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [3650][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [3700][5500]\t Training Loss 0.8506\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [3750][5500]\t Training Loss 0.8513\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [3800][5500]\t Training Loss 0.8515\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [3850][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [3900][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [3950][5500]\t Training Loss 0.8510\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [4000][5500]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [4050][5500]\t Training Loss 0.8515\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [4100][5500]\t Training Loss 0.8510\t Accuracy 0.8516\n",
      "Epoch [6][20]\t Batch [4150][5500]\t Training Loss 0.8522\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [4200][5500]\t Training Loss 0.8527\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [4250][5500]\t Training Loss 0.8535\t Accuracy 0.8508\n",
      "Epoch [6][20]\t Batch [4300][5500]\t Training Loss 0.8533\t Accuracy 0.8506\n",
      "Epoch [6][20]\t Batch [4350][5500]\t Training Loss 0.8528\t Accuracy 0.8509\n",
      "Epoch [6][20]\t Batch [4400][5500]\t Training Loss 0.8528\t Accuracy 0.8508\n",
      "Epoch [6][20]\t Batch [4450][5500]\t Training Loss 0.8535\t Accuracy 0.8507\n",
      "Epoch [6][20]\t Batch [4500][5500]\t Training Loss 0.8531\t Accuracy 0.8509\n",
      "Epoch [6][20]\t Batch [4550][5500]\t Training Loss 0.8536\t Accuracy 0.8507\n",
      "Epoch [6][20]\t Batch [4600][5500]\t Training Loss 0.8538\t Accuracy 0.8507\n",
      "Epoch [6][20]\t Batch [4650][5500]\t Training Loss 0.8544\t Accuracy 0.8507\n",
      "Epoch [6][20]\t Batch [4700][5500]\t Training Loss 0.8536\t Accuracy 0.8509\n",
      "Epoch [6][20]\t Batch [4750][5500]\t Training Loss 0.8539\t Accuracy 0.8507\n",
      "Epoch [6][20]\t Batch [4800][5500]\t Training Loss 0.8537\t Accuracy 0.8506\n",
      "Epoch [6][20]\t Batch [4850][5500]\t Training Loss 0.8525\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [4900][5500]\t Training Loss 0.8521\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [4950][5500]\t Training Loss 0.8525\t Accuracy 0.8507\n",
      "Epoch [6][20]\t Batch [5000][5500]\t Training Loss 0.8538\t Accuracy 0.8504\n",
      "Epoch [6][20]\t Batch [5050][5500]\t Training Loss 0.8543\t Accuracy 0.8503\n",
      "Epoch [6][20]\t Batch [5100][5500]\t Training Loss 0.8541\t Accuracy 0.8502\n",
      "Epoch [6][20]\t Batch [5150][5500]\t Training Loss 0.8537\t Accuracy 0.8501\n",
      "Epoch [6][20]\t Batch [5200][5500]\t Training Loss 0.8533\t Accuracy 0.8504\n",
      "Epoch [6][20]\t Batch [5250][5500]\t Training Loss 0.8537\t Accuracy 0.8501\n",
      "Epoch [6][20]\t Batch [5300][5500]\t Training Loss 0.8540\t Accuracy 0.8499\n",
      "Epoch [6][20]\t Batch [5350][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [5400][5500]\t Training Loss 0.8539\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [5450][5500]\t Training Loss 0.8536\t Accuracy 0.8501\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8535\t Average training accuracy 0.8501\n",
      "Epoch [6]\t Average validation loss 0.7862\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [7][20]\t Batch [0][5500]\t Training Loss 0.7433\t Accuracy 0.9000\n",
      "Epoch [7][20]\t Batch [50][5500]\t Training Loss 0.8248\t Accuracy 0.8451\n",
      "Epoch [7][20]\t Batch [100][5500]\t Training Loss 0.8617\t Accuracy 0.8386\n",
      "Epoch [7][20]\t Batch [150][5500]\t Training Loss 0.8758\t Accuracy 0.8358\n",
      "Epoch [7][20]\t Batch [200][5500]\t Training Loss 0.8557\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [250][5500]\t Training Loss 0.8425\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [300][5500]\t Training Loss 0.8347\t Accuracy 0.8508\n",
      "Epoch [7][20]\t Batch [350][5500]\t Training Loss 0.8402\t Accuracy 0.8521\n",
      "Epoch [7][20]\t Batch [400][5500]\t Training Loss 0.8380\t Accuracy 0.8551\n",
      "Epoch [7][20]\t Batch [450][5500]\t Training Loss 0.8378\t Accuracy 0.8554\n",
      "Epoch [7][20]\t Batch [500][5500]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [7][20]\t Batch [550][5500]\t Training Loss 0.8312\t Accuracy 0.8557\n",
      "Epoch [7][20]\t Batch [600][5500]\t Training Loss 0.8312\t Accuracy 0.8571\n",
      "Epoch [7][20]\t Batch [650][5500]\t Training Loss 0.8251\t Accuracy 0.8588\n",
      "Epoch [7][20]\t Batch [700][5500]\t Training Loss 0.8251\t Accuracy 0.8588\n",
      "Epoch [7][20]\t Batch [750][5500]\t Training Loss 0.8341\t Accuracy 0.8573\n",
      "Epoch [7][20]\t Batch [800][5500]\t Training Loss 0.8363\t Accuracy 0.8563\n",
      "Epoch [7][20]\t Batch [850][5500]\t Training Loss 0.8389\t Accuracy 0.8556\n",
      "Epoch [7][20]\t Batch [900][5500]\t Training Loss 0.8410\t Accuracy 0.8535\n",
      "Epoch [7][20]\t Batch [950][5500]\t Training Loss 0.8398\t Accuracy 0.8546\n",
      "Epoch [7][20]\t Batch [1000][5500]\t Training Loss 0.8371\t Accuracy 0.8554\n",
      "Epoch [7][20]\t Batch [1050][5500]\t Training Loss 0.8340\t Accuracy 0.8564\n",
      "Epoch [7][20]\t Batch [1100][5500]\t Training Loss 0.8320\t Accuracy 0.8571\n",
      "Epoch [7][20]\t Batch [1150][5500]\t Training Loss 0.8295\t Accuracy 0.8572\n",
      "Epoch [7][20]\t Batch [1200][5500]\t Training Loss 0.8335\t Accuracy 0.8549\n",
      "Epoch [7][20]\t Batch [1250][5500]\t Training Loss 0.8341\t Accuracy 0.8545\n",
      "Epoch [7][20]\t Batch [1300][5500]\t Training Loss 0.8364\t Accuracy 0.8536\n",
      "Epoch [7][20]\t Batch [1350][5500]\t Training Loss 0.8374\t Accuracy 0.8532\n",
      "Epoch [7][20]\t Batch [1400][5500]\t Training Loss 0.8390\t Accuracy 0.8522\n",
      "Epoch [7][20]\t Batch [1450][5500]\t Training Loss 0.8410\t Accuracy 0.8512\n",
      "Epoch [7][20]\t Batch [1500][5500]\t Training Loss 0.8448\t Accuracy 0.8490\n",
      "Epoch [7][20]\t Batch [1550][5500]\t Training Loss 0.8446\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [1600][5500]\t Training Loss 0.8466\t Accuracy 0.8490\n",
      "Epoch [7][20]\t Batch [1650][5500]\t Training Loss 0.8460\t Accuracy 0.8491\n",
      "Epoch [7][20]\t Batch [1700][5500]\t Training Loss 0.8478\t Accuracy 0.8487\n",
      "Epoch [7][20]\t Batch [1750][5500]\t Training Loss 0.8483\t Accuracy 0.8485\n",
      "Epoch [7][20]\t Batch [1800][5500]\t Training Loss 0.8516\t Accuracy 0.8473\n",
      "Epoch [7][20]\t Batch [1850][5500]\t Training Loss 0.8506\t Accuracy 0.8482\n",
      "Epoch [7][20]\t Batch [1900][5500]\t Training Loss 0.8499\t Accuracy 0.8498\n",
      "Epoch [7][20]\t Batch [1950][5500]\t Training Loss 0.8500\t Accuracy 0.8498\n",
      "Epoch [7][20]\t Batch [2000][5500]\t Training Loss 0.8480\t Accuracy 0.8501\n",
      "Epoch [7][20]\t Batch [2050][5500]\t Training Loss 0.8480\t Accuracy 0.8503\n",
      "Epoch [7][20]\t Batch [2100][5500]\t Training Loss 0.8485\t Accuracy 0.8496\n",
      "Epoch [7][20]\t Batch [2150][5500]\t Training Loss 0.8473\t Accuracy 0.8504\n",
      "Epoch [7][20]\t Batch [2200][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [7][20]\t Batch [2250][5500]\t Training Loss 0.8463\t Accuracy 0.8511\n",
      "Epoch [7][20]\t Batch [2300][5500]\t Training Loss 0.8463\t Accuracy 0.8509\n",
      "Epoch [7][20]\t Batch [2350][5500]\t Training Loss 0.8451\t Accuracy 0.8512\n",
      "Epoch [7][20]\t Batch [2400][5500]\t Training Loss 0.8457\t Accuracy 0.8512\n",
      "Epoch [7][20]\t Batch [2450][5500]\t Training Loss 0.8455\t Accuracy 0.8512\n",
      "Epoch [7][20]\t Batch [2500][5500]\t Training Loss 0.8468\t Accuracy 0.8504\n",
      "Epoch [7][20]\t Batch [2550][5500]\t Training Loss 0.8451\t Accuracy 0.8508\n",
      "Epoch [7][20]\t Batch [2600][5500]\t Training Loss 0.8444\t Accuracy 0.8511\n",
      "Epoch [7][20]\t Batch [2650][5500]\t Training Loss 0.8446\t Accuracy 0.8515\n",
      "Epoch [7][20]\t Batch [2700][5500]\t Training Loss 0.8454\t Accuracy 0.8511\n",
      "Epoch [7][20]\t Batch [2750][5500]\t Training Loss 0.8457\t Accuracy 0.8511\n",
      "Epoch [7][20]\t Batch [2800][5500]\t Training Loss 0.8452\t Accuracy 0.8513\n",
      "Epoch [7][20]\t Batch [2850][5500]\t Training Loss 0.8439\t Accuracy 0.8521\n",
      "Epoch [7][20]\t Batch [2900][5500]\t Training Loss 0.8438\t Accuracy 0.8519\n",
      "Epoch [7][20]\t Batch [2950][5500]\t Training Loss 0.8440\t Accuracy 0.8517\n",
      "Epoch [7][20]\t Batch [3000][5500]\t Training Loss 0.8456\t Accuracy 0.8511\n",
      "Epoch [7][20]\t Batch [3050][5500]\t Training Loss 0.8467\t Accuracy 0.8511\n",
      "Epoch [7][20]\t Batch [3100][5500]\t Training Loss 0.8483\t Accuracy 0.8510\n",
      "Epoch [7][20]\t Batch [3150][5500]\t Training Loss 0.8497\t Accuracy 0.8510\n",
      "Epoch [7][20]\t Batch [3200][5500]\t Training Loss 0.8508\t Accuracy 0.8506\n",
      "Epoch [7][20]\t Batch [3250][5500]\t Training Loss 0.8526\t Accuracy 0.8497\n",
      "Epoch [7][20]\t Batch [3300][5500]\t Training Loss 0.8523\t Accuracy 0.8501\n",
      "Epoch [7][20]\t Batch [3350][5500]\t Training Loss 0.8529\t Accuracy 0.8499\n",
      "Epoch [7][20]\t Batch [3400][5500]\t Training Loss 0.8515\t Accuracy 0.8509\n",
      "Epoch [7][20]\t Batch [3450][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [7][20]\t Batch [3500][5500]\t Training Loss 0.8518\t Accuracy 0.8511\n",
      "Epoch [7][20]\t Batch [3550][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [7][20]\t Batch [3600][5500]\t Training Loss 0.8519\t Accuracy 0.8512\n",
      "Epoch [7][20]\t Batch [3650][5500]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [7][20]\t Batch [3700][5500]\t Training Loss 0.8505\t Accuracy 0.8519\n",
      "Epoch [7][20]\t Batch [3750][5500]\t Training Loss 0.8513\t Accuracy 0.8515\n",
      "Epoch [7][20]\t Batch [3800][5500]\t Training Loss 0.8515\t Accuracy 0.8514\n",
      "Epoch [7][20]\t Batch [3850][5500]\t Training Loss 0.8516\t Accuracy 0.8514\n",
      "Epoch [7][20]\t Batch [3900][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [7][20]\t Batch [3950][5500]\t Training Loss 0.8510\t Accuracy 0.8516\n",
      "Epoch [7][20]\t Batch [4000][5500]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [7][20]\t Batch [4050][5500]\t Training Loss 0.8515\t Accuracy 0.8514\n",
      "Epoch [7][20]\t Batch [4100][5500]\t Training Loss 0.8509\t Accuracy 0.8517\n",
      "Epoch [7][20]\t Batch [4150][5500]\t Training Loss 0.8522\t Accuracy 0.8516\n",
      "Epoch [7][20]\t Batch [4200][5500]\t Training Loss 0.8527\t Accuracy 0.8516\n",
      "Epoch [7][20]\t Batch [4250][5500]\t Training Loss 0.8535\t Accuracy 0.8508\n",
      "Epoch [7][20]\t Batch [4300][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [7][20]\t Batch [4350][5500]\t Training Loss 0.8528\t Accuracy 0.8509\n",
      "Epoch [7][20]\t Batch [4400][5500]\t Training Loss 0.8528\t Accuracy 0.8509\n",
      "Epoch [7][20]\t Batch [4450][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [7][20]\t Batch [4500][5500]\t Training Loss 0.8531\t Accuracy 0.8509\n",
      "Epoch [7][20]\t Batch [4550][5500]\t Training Loss 0.8535\t Accuracy 0.8508\n",
      "Epoch [7][20]\t Batch [4600][5500]\t Training Loss 0.8538\t Accuracy 0.8508\n",
      "Epoch [7][20]\t Batch [4650][5500]\t Training Loss 0.8544\t Accuracy 0.8508\n",
      "Epoch [7][20]\t Batch [4700][5500]\t Training Loss 0.8536\t Accuracy 0.8510\n",
      "Epoch [7][20]\t Batch [4750][5500]\t Training Loss 0.8538\t Accuracy 0.8508\n",
      "Epoch [7][20]\t Batch [4800][5500]\t Training Loss 0.8537\t Accuracy 0.8506\n",
      "Epoch [7][20]\t Batch [4850][5500]\t Training Loss 0.8524\t Accuracy 0.8510\n",
      "Epoch [7][20]\t Batch [4900][5500]\t Training Loss 0.8521\t Accuracy 0.8511\n",
      "Epoch [7][20]\t Batch [4950][5500]\t Training Loss 0.8525\t Accuracy 0.8507\n",
      "Epoch [7][20]\t Batch [5000][5500]\t Training Loss 0.8537\t Accuracy 0.8504\n",
      "Epoch [7][20]\t Batch [5050][5500]\t Training Loss 0.8543\t Accuracy 0.8503\n",
      "Epoch [7][20]\t Batch [5100][5500]\t Training Loss 0.8541\t Accuracy 0.8503\n",
      "Epoch [7][20]\t Batch [5150][5500]\t Training Loss 0.8537\t Accuracy 0.8502\n",
      "Epoch [7][20]\t Batch [5200][5500]\t Training Loss 0.8533\t Accuracy 0.8504\n",
      "Epoch [7][20]\t Batch [5250][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [7][20]\t Batch [5300][5500]\t Training Loss 0.8540\t Accuracy 0.8499\n",
      "Epoch [7][20]\t Batch [5350][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [5400][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [5450][5500]\t Training Loss 0.8535\t Accuracy 0.8501\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8535\t Average training accuracy 0.8501\n",
      "Epoch [7]\t Average validation loss 0.7862\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [8][20]\t Batch [0][5500]\t Training Loss 0.7432\t Accuracy 0.9000\n",
      "Epoch [8][20]\t Batch [50][5500]\t Training Loss 0.8248\t Accuracy 0.8451\n",
      "Epoch [8][20]\t Batch [100][5500]\t Training Loss 0.8616\t Accuracy 0.8386\n",
      "Epoch [8][20]\t Batch [150][5500]\t Training Loss 0.8758\t Accuracy 0.8358\n",
      "Epoch [8][20]\t Batch [200][5500]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [250][5500]\t Training Loss 0.8424\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [300][5500]\t Training Loss 0.8347\t Accuracy 0.8508\n",
      "Epoch [8][20]\t Batch [350][5500]\t Training Loss 0.8402\t Accuracy 0.8521\n",
      "Epoch [8][20]\t Batch [400][5500]\t Training Loss 0.8380\t Accuracy 0.8551\n",
      "Epoch [8][20]\t Batch [450][5500]\t Training Loss 0.8378\t Accuracy 0.8554\n",
      "Epoch [8][20]\t Batch [500][5500]\t Training Loss 0.8330\t Accuracy 0.8559\n",
      "Epoch [8][20]\t Batch [550][5500]\t Training Loss 0.8312\t Accuracy 0.8557\n",
      "Epoch [8][20]\t Batch [600][5500]\t Training Loss 0.8311\t Accuracy 0.8571\n",
      "Epoch [8][20]\t Batch [650][5500]\t Training Loss 0.8251\t Accuracy 0.8588\n",
      "Epoch [8][20]\t Batch [700][5500]\t Training Loss 0.8250\t Accuracy 0.8588\n",
      "Epoch [8][20]\t Batch [750][5500]\t Training Loss 0.8341\t Accuracy 0.8573\n",
      "Epoch [8][20]\t Batch [800][5500]\t Training Loss 0.8363\t Accuracy 0.8563\n",
      "Epoch [8][20]\t Batch [850][5500]\t Training Loss 0.8389\t Accuracy 0.8556\n",
      "Epoch [8][20]\t Batch [900][5500]\t Training Loss 0.8409\t Accuracy 0.8535\n",
      "Epoch [8][20]\t Batch [950][5500]\t Training Loss 0.8398\t Accuracy 0.8546\n",
      "Epoch [8][20]\t Batch [1000][5500]\t Training Loss 0.8370\t Accuracy 0.8554\n",
      "Epoch [8][20]\t Batch [1050][5500]\t Training Loss 0.8339\t Accuracy 0.8564\n",
      "Epoch [8][20]\t Batch [1100][5500]\t Training Loss 0.8319\t Accuracy 0.8571\n",
      "Epoch [8][20]\t Batch [1150][5500]\t Training Loss 0.8295\t Accuracy 0.8572\n",
      "Epoch [8][20]\t Batch [1200][5500]\t Training Loss 0.8335\t Accuracy 0.8549\n",
      "Epoch [8][20]\t Batch [1250][5500]\t Training Loss 0.8341\t Accuracy 0.8545\n",
      "Epoch [8][20]\t Batch [1300][5500]\t Training Loss 0.8364\t Accuracy 0.8536\n",
      "Epoch [8][20]\t Batch [1350][5500]\t Training Loss 0.8374\t Accuracy 0.8532\n",
      "Epoch [8][20]\t Batch [1400][5500]\t Training Loss 0.8389\t Accuracy 0.8522\n",
      "Epoch [8][20]\t Batch [1450][5500]\t Training Loss 0.8410\t Accuracy 0.8512\n",
      "Epoch [8][20]\t Batch [1500][5500]\t Training Loss 0.8448\t Accuracy 0.8490\n",
      "Epoch [8][20]\t Batch [1550][5500]\t Training Loss 0.8446\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [1600][5500]\t Training Loss 0.8466\t Accuracy 0.8490\n",
      "Epoch [8][20]\t Batch [1650][5500]\t Training Loss 0.8460\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [1700][5500]\t Training Loss 0.8478\t Accuracy 0.8487\n",
      "Epoch [8][20]\t Batch [1750][5500]\t Training Loss 0.8483\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [1800][5500]\t Training Loss 0.8516\t Accuracy 0.8473\n",
      "Epoch [8][20]\t Batch [1850][5500]\t Training Loss 0.8506\t Accuracy 0.8482\n",
      "Epoch [8][20]\t Batch [1900][5500]\t Training Loss 0.8499\t Accuracy 0.8498\n",
      "Epoch [8][20]\t Batch [1950][5500]\t Training Loss 0.8499\t Accuracy 0.8498\n",
      "Epoch [8][20]\t Batch [2000][5500]\t Training Loss 0.8479\t Accuracy 0.8501\n",
      "Epoch [8][20]\t Batch [2050][5500]\t Training Loss 0.8480\t Accuracy 0.8503\n",
      "Epoch [8][20]\t Batch [2100][5500]\t Training Loss 0.8485\t Accuracy 0.8496\n",
      "Epoch [8][20]\t Batch [2150][5500]\t Training Loss 0.8472\t Accuracy 0.8504\n",
      "Epoch [8][20]\t Batch [2200][5500]\t Training Loss 0.8450\t Accuracy 0.8513\n",
      "Epoch [8][20]\t Batch [2250][5500]\t Training Loss 0.8462\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [2300][5500]\t Training Loss 0.8462\t Accuracy 0.8509\n",
      "Epoch [8][20]\t Batch [2350][5500]\t Training Loss 0.8451\t Accuracy 0.8512\n",
      "Epoch [8][20]\t Batch [2400][5500]\t Training Loss 0.8456\t Accuracy 0.8512\n",
      "Epoch [8][20]\t Batch [2450][5500]\t Training Loss 0.8454\t Accuracy 0.8512\n",
      "Epoch [8][20]\t Batch [2500][5500]\t Training Loss 0.8467\t Accuracy 0.8504\n",
      "Epoch [8][20]\t Batch [2550][5500]\t Training Loss 0.8451\t Accuracy 0.8508\n",
      "Epoch [8][20]\t Batch [2600][5500]\t Training Loss 0.8444\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [2650][5500]\t Training Loss 0.8446\t Accuracy 0.8515\n",
      "Epoch [8][20]\t Batch [2700][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [2750][5500]\t Training Loss 0.8456\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [2800][5500]\t Training Loss 0.8452\t Accuracy 0.8513\n",
      "Epoch [8][20]\t Batch [2850][5500]\t Training Loss 0.8439\t Accuracy 0.8521\n",
      "Epoch [8][20]\t Batch [2900][5500]\t Training Loss 0.8438\t Accuracy 0.8519\n",
      "Epoch [8][20]\t Batch [2950][5500]\t Training Loss 0.8439\t Accuracy 0.8517\n",
      "Epoch [8][20]\t Batch [3000][5500]\t Training Loss 0.8456\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [3050][5500]\t Training Loss 0.8467\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [3100][5500]\t Training Loss 0.8483\t Accuracy 0.8510\n",
      "Epoch [8][20]\t Batch [3150][5500]\t Training Loss 0.8497\t Accuracy 0.8510\n",
      "Epoch [8][20]\t Batch [3200][5500]\t Training Loss 0.8507\t Accuracy 0.8506\n",
      "Epoch [8][20]\t Batch [3250][5500]\t Training Loss 0.8526\t Accuracy 0.8497\n",
      "Epoch [8][20]\t Batch [3300][5500]\t Training Loss 0.8523\t Accuracy 0.8501\n",
      "Epoch [8][20]\t Batch [3350][5500]\t Training Loss 0.8528\t Accuracy 0.8499\n",
      "Epoch [8][20]\t Batch [3400][5500]\t Training Loss 0.8515\t Accuracy 0.8509\n",
      "Epoch [8][20]\t Batch [3450][5500]\t Training Loss 0.8512\t Accuracy 0.8514\n",
      "Epoch [8][20]\t Batch [3500][5500]\t Training Loss 0.8518\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [3550][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [8][20]\t Batch [3600][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [8][20]\t Batch [3650][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [8][20]\t Batch [3700][5500]\t Training Loss 0.8505\t Accuracy 0.8519\n",
      "Epoch [8][20]\t Batch [3750][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [8][20]\t Batch [3800][5500]\t Training Loss 0.8515\t Accuracy 0.8514\n",
      "Epoch [8][20]\t Batch [3850][5500]\t Training Loss 0.8516\t Accuracy 0.8514\n",
      "Epoch [8][20]\t Batch [3900][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [8][20]\t Batch [3950][5500]\t Training Loss 0.8510\t Accuracy 0.8516\n",
      "Epoch [8][20]\t Batch [4000][5500]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [8][20]\t Batch [4050][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [8][20]\t Batch [4100][5500]\t Training Loss 0.8509\t Accuracy 0.8517\n",
      "Epoch [8][20]\t Batch [4150][5500]\t Training Loss 0.8521\t Accuracy 0.8516\n",
      "Epoch [8][20]\t Batch [4200][5500]\t Training Loss 0.8526\t Accuracy 0.8516\n",
      "Epoch [8][20]\t Batch [4250][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [8][20]\t Batch [4300][5500]\t Training Loss 0.8532\t Accuracy 0.8507\n",
      "Epoch [8][20]\t Batch [4350][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [8][20]\t Batch [4400][5500]\t Training Loss 0.8528\t Accuracy 0.8509\n",
      "Epoch [8][20]\t Batch [4450][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [8][20]\t Batch [4500][5500]\t Training Loss 0.8531\t Accuracy 0.8509\n",
      "Epoch [8][20]\t Batch [4550][5500]\t Training Loss 0.8535\t Accuracy 0.8508\n",
      "Epoch [8][20]\t Batch [4600][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [8][20]\t Batch [4650][5500]\t Training Loss 0.8544\t Accuracy 0.8508\n",
      "Epoch [8][20]\t Batch [4700][5500]\t Training Loss 0.8535\t Accuracy 0.8510\n",
      "Epoch [8][20]\t Batch [4750][5500]\t Training Loss 0.8538\t Accuracy 0.8508\n",
      "Epoch [8][20]\t Batch [4800][5500]\t Training Loss 0.8536\t Accuracy 0.8506\n",
      "Epoch [8][20]\t Batch [4850][5500]\t Training Loss 0.8524\t Accuracy 0.8510\n",
      "Epoch [8][20]\t Batch [4900][5500]\t Training Loss 0.8521\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [4950][5500]\t Training Loss 0.8524\t Accuracy 0.8507\n",
      "Epoch [8][20]\t Batch [5000][5500]\t Training Loss 0.8537\t Accuracy 0.8504\n",
      "Epoch [8][20]\t Batch [5050][5500]\t Training Loss 0.8542\t Accuracy 0.8503\n",
      "Epoch [8][20]\t Batch [5100][5500]\t Training Loss 0.8540\t Accuracy 0.8503\n",
      "Epoch [8][20]\t Batch [5150][5500]\t Training Loss 0.8537\t Accuracy 0.8502\n",
      "Epoch [8][20]\t Batch [5200][5500]\t Training Loss 0.8532\t Accuracy 0.8504\n",
      "Epoch [8][20]\t Batch [5250][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [8][20]\t Batch [5300][5500]\t Training Loss 0.8539\t Accuracy 0.8499\n",
      "Epoch [8][20]\t Batch [5350][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [5400][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [5450][5500]\t Training Loss 0.8535\t Accuracy 0.8501\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8535\t Average training accuracy 0.8501\n",
      "Epoch [8]\t Average validation loss 0.7862\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [9][20]\t Batch [0][5500]\t Training Loss 0.7432\t Accuracy 0.9000\n",
      "Epoch [9][20]\t Batch [50][5500]\t Training Loss 0.8248\t Accuracy 0.8451\n",
      "Epoch [9][20]\t Batch [100][5500]\t Training Loss 0.8616\t Accuracy 0.8386\n",
      "Epoch [9][20]\t Batch [150][5500]\t Training Loss 0.8757\t Accuracy 0.8358\n",
      "Epoch [9][20]\t Batch [200][5500]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [9][20]\t Batch [250][5500]\t Training Loss 0.8424\t Accuracy 0.8482\n",
      "Epoch [9][20]\t Batch [300][5500]\t Training Loss 0.8347\t Accuracy 0.8508\n",
      "Epoch [9][20]\t Batch [350][5500]\t Training Loss 0.8402\t Accuracy 0.8521\n",
      "Epoch [9][20]\t Batch [400][5500]\t Training Loss 0.8379\t Accuracy 0.8551\n",
      "Epoch [9][20]\t Batch [450][5500]\t Training Loss 0.8377\t Accuracy 0.8554\n",
      "Epoch [9][20]\t Batch [500][5500]\t Training Loss 0.8330\t Accuracy 0.8559\n",
      "Epoch [9][20]\t Batch [550][5500]\t Training Loss 0.8311\t Accuracy 0.8557\n",
      "Epoch [9][20]\t Batch [600][5500]\t Training Loss 0.8311\t Accuracy 0.8571\n",
      "Epoch [9][20]\t Batch [650][5500]\t Training Loss 0.8250\t Accuracy 0.8588\n",
      "Epoch [9][20]\t Batch [700][5500]\t Training Loss 0.8250\t Accuracy 0.8586\n",
      "Epoch [9][20]\t Batch [750][5500]\t Training Loss 0.8341\t Accuracy 0.8571\n",
      "Epoch [9][20]\t Batch [800][5500]\t Training Loss 0.8363\t Accuracy 0.8562\n",
      "Epoch [9][20]\t Batch [850][5500]\t Training Loss 0.8389\t Accuracy 0.8555\n",
      "Epoch [9][20]\t Batch [900][5500]\t Training Loss 0.8409\t Accuracy 0.8534\n",
      "Epoch [9][20]\t Batch [950][5500]\t Training Loss 0.8398\t Accuracy 0.8545\n",
      "Epoch [9][20]\t Batch [1000][5500]\t Training Loss 0.8370\t Accuracy 0.8553\n",
      "Epoch [9][20]\t Batch [1050][5500]\t Training Loss 0.8339\t Accuracy 0.8563\n",
      "Epoch [9][20]\t Batch [1100][5500]\t Training Loss 0.8319\t Accuracy 0.8570\n",
      "Epoch [9][20]\t Batch [1150][5500]\t Training Loss 0.8295\t Accuracy 0.8571\n",
      "Epoch [9][20]\t Batch [1200][5500]\t Training Loss 0.8335\t Accuracy 0.8548\n",
      "Epoch [9][20]\t Batch [1250][5500]\t Training Loss 0.8340\t Accuracy 0.8544\n",
      "Epoch [9][20]\t Batch [1300][5500]\t Training Loss 0.8364\t Accuracy 0.8535\n",
      "Epoch [9][20]\t Batch [1350][5500]\t Training Loss 0.8373\t Accuracy 0.8531\n",
      "Epoch [9][20]\t Batch [1400][5500]\t Training Loss 0.8389\t Accuracy 0.8522\n",
      "Epoch [9][20]\t Batch [1450][5500]\t Training Loss 0.8409\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [1500][5500]\t Training Loss 0.8447\t Accuracy 0.8489\n",
      "Epoch [9][20]\t Batch [1550][5500]\t Training Loss 0.8445\t Accuracy 0.8499\n",
      "Epoch [9][20]\t Batch [1600][5500]\t Training Loss 0.8465\t Accuracy 0.8490\n",
      "Epoch [9][20]\t Batch [1650][5500]\t Training Loss 0.8459\t Accuracy 0.8490\n",
      "Epoch [9][20]\t Batch [1700][5500]\t Training Loss 0.8477\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [1750][5500]\t Training Loss 0.8482\t Accuracy 0.8484\n",
      "Epoch [9][20]\t Batch [1800][5500]\t Training Loss 0.8515\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [1850][5500]\t Training Loss 0.8505\t Accuracy 0.8481\n",
      "Epoch [9][20]\t Batch [1900][5500]\t Training Loss 0.8498\t Accuracy 0.8497\n",
      "Epoch [9][20]\t Batch [1950][5500]\t Training Loss 0.8499\t Accuracy 0.8498\n",
      "Epoch [9][20]\t Batch [2000][5500]\t Training Loss 0.8479\t Accuracy 0.8501\n",
      "Epoch [9][20]\t Batch [2050][5500]\t Training Loss 0.8480\t Accuracy 0.8503\n",
      "Epoch [9][20]\t Batch [2100][5500]\t Training Loss 0.8484\t Accuracy 0.8495\n",
      "Epoch [9][20]\t Batch [2150][5500]\t Training Loss 0.8472\t Accuracy 0.8503\n",
      "Epoch [9][20]\t Batch [2200][5500]\t Training Loss 0.8450\t Accuracy 0.8512\n",
      "Epoch [9][20]\t Batch [2250][5500]\t Training Loss 0.8462\t Accuracy 0.8510\n",
      "Epoch [9][20]\t Batch [2300][5500]\t Training Loss 0.8462\t Accuracy 0.8509\n",
      "Epoch [9][20]\t Batch [2350][5500]\t Training Loss 0.8450\t Accuracy 0.8512\n",
      "Epoch [9][20]\t Batch [2400][5500]\t Training Loss 0.8456\t Accuracy 0.8512\n",
      "Epoch [9][20]\t Batch [2450][5500]\t Training Loss 0.8454\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [2500][5500]\t Training Loss 0.8467\t Accuracy 0.8504\n",
      "Epoch [9][20]\t Batch [2550][5500]\t Training Loss 0.8451\t Accuracy 0.8508\n",
      "Epoch [9][20]\t Batch [2600][5500]\t Training Loss 0.8444\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [2650][5500]\t Training Loss 0.8445\t Accuracy 0.8515\n",
      "Epoch [9][20]\t Batch [2700][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [2750][5500]\t Training Loss 0.8456\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [2800][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [9][20]\t Batch [2850][5500]\t Training Loss 0.8438\t Accuracy 0.8520\n",
      "Epoch [9][20]\t Batch [2900][5500]\t Training Loss 0.8437\t Accuracy 0.8519\n",
      "Epoch [9][20]\t Batch [2950][5500]\t Training Loss 0.8439\t Accuracy 0.8516\n",
      "Epoch [9][20]\t Batch [3000][5500]\t Training Loss 0.8456\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [3050][5500]\t Training Loss 0.8466\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [3100][5500]\t Training Loss 0.8482\t Accuracy 0.8510\n",
      "Epoch [9][20]\t Batch [3150][5500]\t Training Loss 0.8496\t Accuracy 0.8509\n",
      "Epoch [9][20]\t Batch [3200][5500]\t Training Loss 0.8507\t Accuracy 0.8506\n",
      "Epoch [9][20]\t Batch [3250][5500]\t Training Loss 0.8526\t Accuracy 0.8496\n",
      "Epoch [9][20]\t Batch [3300][5500]\t Training Loss 0.8523\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [3350][5500]\t Training Loss 0.8528\t Accuracy 0.8499\n",
      "Epoch [9][20]\t Batch [3400][5500]\t Training Loss 0.8515\t Accuracy 0.8508\n",
      "Epoch [9][20]\t Batch [3450][5500]\t Training Loss 0.8512\t Accuracy 0.8514\n",
      "Epoch [9][20]\t Batch [3500][5500]\t Training Loss 0.8517\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [3550][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [9][20]\t Batch [3600][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [9][20]\t Batch [3650][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [9][20]\t Batch [3700][5500]\t Training Loss 0.8505\t Accuracy 0.8519\n",
      "Epoch [9][20]\t Batch [3750][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [9][20]\t Batch [3800][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [9][20]\t Batch [3850][5500]\t Training Loss 0.8516\t Accuracy 0.8513\n",
      "Epoch [9][20]\t Batch [3900][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [9][20]\t Batch [3950][5500]\t Training Loss 0.8509\t Accuracy 0.8515\n",
      "Epoch [9][20]\t Batch [4000][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [9][20]\t Batch [4050][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [9][20]\t Batch [4100][5500]\t Training Loss 0.8509\t Accuracy 0.8517\n",
      "Epoch [9][20]\t Batch [4150][5500]\t Training Loss 0.8521\t Accuracy 0.8515\n",
      "Epoch [9][20]\t Batch [4200][5500]\t Training Loss 0.8526\t Accuracy 0.8516\n",
      "Epoch [9][20]\t Batch [4250][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [9][20]\t Batch [4300][5500]\t Training Loss 0.8532\t Accuracy 0.8507\n",
      "Epoch [9][20]\t Batch [4350][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [9][20]\t Batch [4400][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [9][20]\t Batch [4450][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [9][20]\t Batch [4500][5500]\t Training Loss 0.8530\t Accuracy 0.8509\n",
      "Epoch [9][20]\t Batch [4550][5500]\t Training Loss 0.8535\t Accuracy 0.8507\n",
      "Epoch [9][20]\t Batch [4600][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [9][20]\t Batch [4650][5500]\t Training Loss 0.8543\t Accuracy 0.8507\n",
      "Epoch [9][20]\t Batch [4700][5500]\t Training Loss 0.8535\t Accuracy 0.8510\n",
      "Epoch [9][20]\t Batch [4750][5500]\t Training Loss 0.8538\t Accuracy 0.8508\n",
      "Epoch [9][20]\t Batch [4800][5500]\t Training Loss 0.8536\t Accuracy 0.8506\n",
      "Epoch [9][20]\t Batch [4850][5500]\t Training Loss 0.8524\t Accuracy 0.8510\n",
      "Epoch [9][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [4950][5500]\t Training Loss 0.8524\t Accuracy 0.8507\n",
      "Epoch [9][20]\t Batch [5000][5500]\t Training Loss 0.8537\t Accuracy 0.8504\n",
      "Epoch [9][20]\t Batch [5050][5500]\t Training Loss 0.8542\t Accuracy 0.8503\n",
      "Epoch [9][20]\t Batch [5100][5500]\t Training Loss 0.8540\t Accuracy 0.8502\n",
      "Epoch [9][20]\t Batch [5150][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [9][20]\t Batch [5200][5500]\t Training Loss 0.8532\t Accuracy 0.8504\n",
      "Epoch [9][20]\t Batch [5250][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [9][20]\t Batch [5300][5500]\t Training Loss 0.8539\t Accuracy 0.8499\n",
      "Epoch [9][20]\t Batch [5350][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [5400][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [5450][5500]\t Training Loss 0.8535\t Accuracy 0.8501\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8535\t Average training accuracy 0.8501\n",
      "Epoch [9]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [10][20]\t Batch [0][5500]\t Training Loss 0.7432\t Accuracy 0.9000\n",
      "Epoch [10][20]\t Batch [50][5500]\t Training Loss 0.8247\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [100][5500]\t Training Loss 0.8616\t Accuracy 0.8386\n",
      "Epoch [10][20]\t Batch [150][5500]\t Training Loss 0.8757\t Accuracy 0.8358\n",
      "Epoch [10][20]\t Batch [200][5500]\t Training Loss 0.8556\t Accuracy 0.8448\n",
      "Epoch [10][20]\t Batch [250][5500]\t Training Loss 0.8424\t Accuracy 0.8482\n",
      "Epoch [10][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8508\n",
      "Epoch [10][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8521\n",
      "Epoch [10][20]\t Batch [400][5500]\t Training Loss 0.8379\t Accuracy 0.8551\n",
      "Epoch [10][20]\t Batch [450][5500]\t Training Loss 0.8377\t Accuracy 0.8554\n",
      "Epoch [10][20]\t Batch [500][5500]\t Training Loss 0.8330\t Accuracy 0.8559\n",
      "Epoch [10][20]\t Batch [550][5500]\t Training Loss 0.8311\t Accuracy 0.8557\n",
      "Epoch [10][20]\t Batch [600][5500]\t Training Loss 0.8311\t Accuracy 0.8571\n",
      "Epoch [10][20]\t Batch [650][5500]\t Training Loss 0.8250\t Accuracy 0.8588\n",
      "Epoch [10][20]\t Batch [700][5500]\t Training Loss 0.8250\t Accuracy 0.8586\n",
      "Epoch [10][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8571\n",
      "Epoch [10][20]\t Batch [800][5500]\t Training Loss 0.8363\t Accuracy 0.8562\n",
      "Epoch [10][20]\t Batch [850][5500]\t Training Loss 0.8389\t Accuracy 0.8555\n",
      "Epoch [10][20]\t Batch [900][5500]\t Training Loss 0.8409\t Accuracy 0.8534\n",
      "Epoch [10][20]\t Batch [950][5500]\t Training Loss 0.8398\t Accuracy 0.8545\n",
      "Epoch [10][20]\t Batch [1000][5500]\t Training Loss 0.8370\t Accuracy 0.8553\n",
      "Epoch [10][20]\t Batch [1050][5500]\t Training Loss 0.8339\t Accuracy 0.8563\n",
      "Epoch [10][20]\t Batch [1100][5500]\t Training Loss 0.8319\t Accuracy 0.8570\n",
      "Epoch [10][20]\t Batch [1150][5500]\t Training Loss 0.8295\t Accuracy 0.8571\n",
      "Epoch [10][20]\t Batch [1200][5500]\t Training Loss 0.8335\t Accuracy 0.8548\n",
      "Epoch [10][20]\t Batch [1250][5500]\t Training Loss 0.8340\t Accuracy 0.8544\n",
      "Epoch [10][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8535\n",
      "Epoch [10][20]\t Batch [1350][5500]\t Training Loss 0.8373\t Accuracy 0.8531\n",
      "Epoch [10][20]\t Batch [1400][5500]\t Training Loss 0.8389\t Accuracy 0.8522\n",
      "Epoch [10][20]\t Batch [1450][5500]\t Training Loss 0.8409\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [1500][5500]\t Training Loss 0.8447\t Accuracy 0.8489\n",
      "Epoch [10][20]\t Batch [1550][5500]\t Training Loss 0.8445\t Accuracy 0.8499\n",
      "Epoch [10][20]\t Batch [1600][5500]\t Training Loss 0.8465\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [1650][5500]\t Training Loss 0.8459\t Accuracy 0.8490\n",
      "Epoch [10][20]\t Batch [1700][5500]\t Training Loss 0.8477\t Accuracy 0.8486\n",
      "Epoch [10][20]\t Batch [1750][5500]\t Training Loss 0.8482\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [1800][5500]\t Training Loss 0.8515\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [1850][5500]\t Training Loss 0.8505\t Accuracy 0.8481\n",
      "Epoch [10][20]\t Batch [1900][5500]\t Training Loss 0.8498\t Accuracy 0.8497\n",
      "Epoch [10][20]\t Batch [1950][5500]\t Training Loss 0.8499\t Accuracy 0.8498\n",
      "Epoch [10][20]\t Batch [2000][5500]\t Training Loss 0.8479\t Accuracy 0.8501\n",
      "Epoch [10][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [10][20]\t Batch [2100][5500]\t Training Loss 0.8484\t Accuracy 0.8495\n",
      "Epoch [10][20]\t Batch [2150][5500]\t Training Loss 0.8472\t Accuracy 0.8503\n",
      "Epoch [10][20]\t Batch [2200][5500]\t Training Loss 0.8450\t Accuracy 0.8512\n",
      "Epoch [10][20]\t Batch [2250][5500]\t Training Loss 0.8462\t Accuracy 0.8510\n",
      "Epoch [10][20]\t Batch [2300][5500]\t Training Loss 0.8462\t Accuracy 0.8509\n",
      "Epoch [10][20]\t Batch [2350][5500]\t Training Loss 0.8450\t Accuracy 0.8512\n",
      "Epoch [10][20]\t Batch [2400][5500]\t Training Loss 0.8456\t Accuracy 0.8512\n",
      "Epoch [10][20]\t Batch [2450][5500]\t Training Loss 0.8454\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [2500][5500]\t Training Loss 0.8467\t Accuracy 0.8504\n",
      "Epoch [10][20]\t Batch [2550][5500]\t Training Loss 0.8451\t Accuracy 0.8508\n",
      "Epoch [10][20]\t Batch [2600][5500]\t Training Loss 0.8443\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [2650][5500]\t Training Loss 0.8445\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [2700][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [2750][5500]\t Training Loss 0.8456\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [2800][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [10][20]\t Batch [2850][5500]\t Training Loss 0.8438\t Accuracy 0.8520\n",
      "Epoch [10][20]\t Batch [2900][5500]\t Training Loss 0.8437\t Accuracy 0.8519\n",
      "Epoch [10][20]\t Batch [2950][5500]\t Training Loss 0.8439\t Accuracy 0.8516\n",
      "Epoch [10][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [3050][5500]\t Training Loss 0.8466\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [3100][5500]\t Training Loss 0.8482\t Accuracy 0.8510\n",
      "Epoch [10][20]\t Batch [3150][5500]\t Training Loss 0.8496\t Accuracy 0.8509\n",
      "Epoch [10][20]\t Batch [3200][5500]\t Training Loss 0.8507\t Accuracy 0.8506\n",
      "Epoch [10][20]\t Batch [3250][5500]\t Training Loss 0.8526\t Accuracy 0.8496\n",
      "Epoch [10][20]\t Batch [3300][5500]\t Training Loss 0.8523\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [3350][5500]\t Training Loss 0.8528\t Accuracy 0.8499\n",
      "Epoch [10][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [10][20]\t Batch [3450][5500]\t Training Loss 0.8512\t Accuracy 0.8513\n",
      "Epoch [10][20]\t Batch [3500][5500]\t Training Loss 0.8517\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [3550][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [10][20]\t Batch [3600][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [10][20]\t Batch [3650][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [10][20]\t Batch [3700][5500]\t Training Loss 0.8505\t Accuracy 0.8518\n",
      "Epoch [10][20]\t Batch [3750][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [3800][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [10][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8513\n",
      "Epoch [10][20]\t Batch [3900][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [3950][5500]\t Training Loss 0.8509\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [4000][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [10][20]\t Batch [4050][5500]\t Training Loss 0.8514\t Accuracy 0.8513\n",
      "Epoch [10][20]\t Batch [4100][5500]\t Training Loss 0.8509\t Accuracy 0.8516\n",
      "Epoch [10][20]\t Batch [4150][5500]\t Training Loss 0.8521\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [4200][5500]\t Training Loss 0.8526\t Accuracy 0.8516\n",
      "Epoch [10][20]\t Batch [4250][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [10][20]\t Batch [4300][5500]\t Training Loss 0.8532\t Accuracy 0.8506\n",
      "Epoch [10][20]\t Batch [4350][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [10][20]\t Batch [4400][5500]\t Training Loss 0.8527\t Accuracy 0.8508\n",
      "Epoch [10][20]\t Batch [4450][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [10][20]\t Batch [4500][5500]\t Training Loss 0.8530\t Accuracy 0.8509\n",
      "Epoch [10][20]\t Batch [4550][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [10][20]\t Batch [4600][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [10][20]\t Batch [4650][5500]\t Training Loss 0.8543\t Accuracy 0.8507\n",
      "Epoch [10][20]\t Batch [4700][5500]\t Training Loss 0.8535\t Accuracy 0.8510\n",
      "Epoch [10][20]\t Batch [4750][5500]\t Training Loss 0.8538\t Accuracy 0.8508\n",
      "Epoch [10][20]\t Batch [4800][5500]\t Training Loss 0.8536\t Accuracy 0.8506\n",
      "Epoch [10][20]\t Batch [4850][5500]\t Training Loss 0.8524\t Accuracy 0.8510\n",
      "Epoch [10][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [4950][5500]\t Training Loss 0.8524\t Accuracy 0.8507\n",
      "Epoch [10][20]\t Batch [5000][5500]\t Training Loss 0.8537\t Accuracy 0.8504\n",
      "Epoch [10][20]\t Batch [5050][5500]\t Training Loss 0.8542\t Accuracy 0.8503\n",
      "Epoch [10][20]\t Batch [5100][5500]\t Training Loss 0.8540\t Accuracy 0.8502\n",
      "Epoch [10][20]\t Batch [5150][5500]\t Training Loss 0.8536\t Accuracy 0.8501\n",
      "Epoch [10][20]\t Batch [5200][5500]\t Training Loss 0.8532\t Accuracy 0.8504\n",
      "Epoch [10][20]\t Batch [5250][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [10][20]\t Batch [5300][5500]\t Training Loss 0.8539\t Accuracy 0.8499\n",
      "Epoch [10][20]\t Batch [5350][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [5400][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [5450][5500]\t Training Loss 0.8535\t Accuracy 0.8501\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [10]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [11][20]\t Batch [0][5500]\t Training Loss 0.7432\t Accuracy 0.9000\n",
      "Epoch [11][20]\t Batch [50][5500]\t Training Loss 0.8247\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [100][5500]\t Training Loss 0.8616\t Accuracy 0.8386\n",
      "Epoch [11][20]\t Batch [150][5500]\t Training Loss 0.8757\t Accuracy 0.8358\n",
      "Epoch [11][20]\t Batch [200][5500]\t Training Loss 0.8556\t Accuracy 0.8453\n",
      "Epoch [11][20]\t Batch [250][5500]\t Training Loss 0.8424\t Accuracy 0.8486\n",
      "Epoch [11][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [11][20]\t Batch [400][5500]\t Training Loss 0.8379\t Accuracy 0.8554\n",
      "Epoch [11][20]\t Batch [450][5500]\t Training Loss 0.8377\t Accuracy 0.8557\n",
      "Epoch [11][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [11][20]\t Batch [550][5500]\t Training Loss 0.8311\t Accuracy 0.8559\n",
      "Epoch [11][20]\t Batch [600][5500]\t Training Loss 0.8311\t Accuracy 0.8572\n",
      "Epoch [11][20]\t Batch [650][5500]\t Training Loss 0.8250\t Accuracy 0.8590\n",
      "Epoch [11][20]\t Batch [700][5500]\t Training Loss 0.8250\t Accuracy 0.8588\n",
      "Epoch [11][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8573\n",
      "Epoch [11][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8563\n",
      "Epoch [11][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8556\n",
      "Epoch [11][20]\t Batch [900][5500]\t Training Loss 0.8409\t Accuracy 0.8535\n",
      "Epoch [11][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8546\n",
      "Epoch [11][20]\t Batch [1000][5500]\t Training Loss 0.8370\t Accuracy 0.8554\n",
      "Epoch [11][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8564\n",
      "Epoch [11][20]\t Batch [1100][5500]\t Training Loss 0.8319\t Accuracy 0.8571\n",
      "Epoch [11][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8572\n",
      "Epoch [11][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8549\n",
      "Epoch [11][20]\t Batch [1250][5500]\t Training Loss 0.8340\t Accuracy 0.8545\n",
      "Epoch [11][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8536\n",
      "Epoch [11][20]\t Batch [1350][5500]\t Training Loss 0.8373\t Accuracy 0.8532\n",
      "Epoch [11][20]\t Batch [1400][5500]\t Training Loss 0.8389\t Accuracy 0.8522\n",
      "Epoch [11][20]\t Batch [1450][5500]\t Training Loss 0.8409\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [1500][5500]\t Training Loss 0.8447\t Accuracy 0.8490\n",
      "Epoch [11][20]\t Batch [1550][5500]\t Training Loss 0.8445\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [1600][5500]\t Training Loss 0.8465\t Accuracy 0.8490\n",
      "Epoch [11][20]\t Batch [1650][5500]\t Training Loss 0.8459\t Accuracy 0.8491\n",
      "Epoch [11][20]\t Batch [1700][5500]\t Training Loss 0.8477\t Accuracy 0.8487\n",
      "Epoch [11][20]\t Batch [1750][5500]\t Training Loss 0.8482\t Accuracy 0.8485\n",
      "Epoch [11][20]\t Batch [1800][5500]\t Training Loss 0.8515\t Accuracy 0.8473\n",
      "Epoch [11][20]\t Batch [1850][5500]\t Training Loss 0.8505\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [1900][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [11][20]\t Batch [1950][5500]\t Training Loss 0.8499\t Accuracy 0.8498\n",
      "Epoch [11][20]\t Batch [2000][5500]\t Training Loss 0.8479\t Accuracy 0.8501\n",
      "Epoch [11][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [11][20]\t Batch [2100][5500]\t Training Loss 0.8484\t Accuracy 0.8496\n",
      "Epoch [11][20]\t Batch [2150][5500]\t Training Loss 0.8472\t Accuracy 0.8504\n",
      "Epoch [11][20]\t Batch [2200][5500]\t Training Loss 0.8450\t Accuracy 0.8513\n",
      "Epoch [11][20]\t Batch [2250][5500]\t Training Loss 0.8462\t Accuracy 0.8510\n",
      "Epoch [11][20]\t Batch [2300][5500]\t Training Loss 0.8462\t Accuracy 0.8509\n",
      "Epoch [11][20]\t Batch [2350][5500]\t Training Loss 0.8450\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [2400][5500]\t Training Loss 0.8456\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [2450][5500]\t Training Loss 0.8454\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [2500][5500]\t Training Loss 0.8467\t Accuracy 0.8504\n",
      "Epoch [11][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [11][20]\t Batch [2600][5500]\t Training Loss 0.8443\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [2650][5500]\t Training Loss 0.8445\t Accuracy 0.8515\n",
      "Epoch [11][20]\t Batch [2700][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [2750][5500]\t Training Loss 0.8456\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [2800][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [11][20]\t Batch [2850][5500]\t Training Loss 0.8438\t Accuracy 0.8520\n",
      "Epoch [11][20]\t Batch [2900][5500]\t Training Loss 0.8437\t Accuracy 0.8519\n",
      "Epoch [11][20]\t Batch [2950][5500]\t Training Loss 0.8439\t Accuracy 0.8516\n",
      "Epoch [11][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [3050][5500]\t Training Loss 0.8466\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [3100][5500]\t Training Loss 0.8482\t Accuracy 0.8510\n",
      "Epoch [11][20]\t Batch [3150][5500]\t Training Loss 0.8496\t Accuracy 0.8509\n",
      "Epoch [11][20]\t Batch [3200][5500]\t Training Loss 0.8507\t Accuracy 0.8506\n",
      "Epoch [11][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [11][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [3350][5500]\t Training Loss 0.8528\t Accuracy 0.8499\n",
      "Epoch [11][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [11][20]\t Batch [3450][5500]\t Training Loss 0.8512\t Accuracy 0.8513\n",
      "Epoch [11][20]\t Batch [3500][5500]\t Training Loss 0.8517\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [3550][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [3600][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [3650][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [3700][5500]\t Training Loss 0.8505\t Accuracy 0.8518\n",
      "Epoch [11][20]\t Batch [3750][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [11][20]\t Batch [3800][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [11][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8513\n",
      "Epoch [11][20]\t Batch [3900][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [11][20]\t Batch [3950][5500]\t Training Loss 0.8509\t Accuracy 0.8515\n",
      "Epoch [11][20]\t Batch [4000][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [11][20]\t Batch [4050][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [11][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [11][20]\t Batch [4150][5500]\t Training Loss 0.8521\t Accuracy 0.8515\n",
      "Epoch [11][20]\t Batch [4200][5500]\t Training Loss 0.8526\t Accuracy 0.8516\n",
      "Epoch [11][20]\t Batch [4250][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [11][20]\t Batch [4300][5500]\t Training Loss 0.8532\t Accuracy 0.8507\n",
      "Epoch [11][20]\t Batch [4350][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [11][20]\t Batch [4400][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [11][20]\t Batch [4450][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [11][20]\t Batch [4500][5500]\t Training Loss 0.8530\t Accuracy 0.8509\n",
      "Epoch [11][20]\t Batch [4550][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [11][20]\t Batch [4600][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [11][20]\t Batch [4650][5500]\t Training Loss 0.8543\t Accuracy 0.8507\n",
      "Epoch [11][20]\t Batch [4700][5500]\t Training Loss 0.8535\t Accuracy 0.8510\n",
      "Epoch [11][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [11][20]\t Batch [4800][5500]\t Training Loss 0.8536\t Accuracy 0.8506\n",
      "Epoch [11][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [11][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [4950][5500]\t Training Loss 0.8524\t Accuracy 0.8507\n",
      "Epoch [11][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [11][20]\t Batch [5050][5500]\t Training Loss 0.8542\t Accuracy 0.8503\n",
      "Epoch [11][20]\t Batch [5100][5500]\t Training Loss 0.8540\t Accuracy 0.8502\n",
      "Epoch [11][20]\t Batch [5150][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [11][20]\t Batch [5200][5500]\t Training Loss 0.8532\t Accuracy 0.8504\n",
      "Epoch [11][20]\t Batch [5250][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [11][20]\t Batch [5300][5500]\t Training Loss 0.8539\t Accuracy 0.8499\n",
      "Epoch [11][20]\t Batch [5350][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [5450][5500]\t Training Loss 0.8535\t Accuracy 0.8501\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [11]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [12][20]\t Batch [0][5500]\t Training Loss 0.7432\t Accuracy 0.9000\n",
      "Epoch [12][20]\t Batch [50][5500]\t Training Loss 0.8247\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [100][5500]\t Training Loss 0.8616\t Accuracy 0.8386\n",
      "Epoch [12][20]\t Batch [150][5500]\t Training Loss 0.8757\t Accuracy 0.8358\n",
      "Epoch [12][20]\t Batch [200][5500]\t Training Loss 0.8556\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [250][5500]\t Training Loss 0.8424\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [12][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [12][20]\t Batch [400][5500]\t Training Loss 0.8379\t Accuracy 0.8554\n",
      "Epoch [12][20]\t Batch [450][5500]\t Training Loss 0.8377\t Accuracy 0.8557\n",
      "Epoch [12][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [12][20]\t Batch [550][5500]\t Training Loss 0.8311\t Accuracy 0.8559\n",
      "Epoch [12][20]\t Batch [600][5500]\t Training Loss 0.8311\t Accuracy 0.8572\n",
      "Epoch [12][20]\t Batch [650][5500]\t Training Loss 0.8250\t Accuracy 0.8590\n",
      "Epoch [12][20]\t Batch [700][5500]\t Training Loss 0.8249\t Accuracy 0.8588\n",
      "Epoch [12][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8573\n",
      "Epoch [12][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8563\n",
      "Epoch [12][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8556\n",
      "Epoch [12][20]\t Batch [900][5500]\t Training Loss 0.8409\t Accuracy 0.8535\n",
      "Epoch [12][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8546\n",
      "Epoch [12][20]\t Batch [1000][5500]\t Training Loss 0.8370\t Accuracy 0.8554\n",
      "Epoch [12][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8564\n",
      "Epoch [12][20]\t Batch [1100][5500]\t Training Loss 0.8318\t Accuracy 0.8571\n",
      "Epoch [12][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8572\n",
      "Epoch [12][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8549\n",
      "Epoch [12][20]\t Batch [1250][5500]\t Training Loss 0.8340\t Accuracy 0.8545\n",
      "Epoch [12][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8536\n",
      "Epoch [12][20]\t Batch [1350][5500]\t Training Loss 0.8373\t Accuracy 0.8532\n",
      "Epoch [12][20]\t Batch [1400][5500]\t Training Loss 0.8389\t Accuracy 0.8522\n",
      "Epoch [12][20]\t Batch [1450][5500]\t Training Loss 0.8409\t Accuracy 0.8512\n",
      "Epoch [12][20]\t Batch [1500][5500]\t Training Loss 0.8447\t Accuracy 0.8490\n",
      "Epoch [12][20]\t Batch [1550][5500]\t Training Loss 0.8445\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [1600][5500]\t Training Loss 0.8465\t Accuracy 0.8490\n",
      "Epoch [12][20]\t Batch [1650][5500]\t Training Loss 0.8459\t Accuracy 0.8491\n",
      "Epoch [12][20]\t Batch [1700][5500]\t Training Loss 0.8477\t Accuracy 0.8487\n",
      "Epoch [12][20]\t Batch [1750][5500]\t Training Loss 0.8482\t Accuracy 0.8485\n",
      "Epoch [12][20]\t Batch [1800][5500]\t Training Loss 0.8515\t Accuracy 0.8473\n",
      "Epoch [12][20]\t Batch [1850][5500]\t Training Loss 0.8505\t Accuracy 0.8482\n",
      "Epoch [12][20]\t Batch [1900][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [12][20]\t Batch [1950][5500]\t Training Loss 0.8499\t Accuracy 0.8498\n",
      "Epoch [12][20]\t Batch [2000][5500]\t Training Loss 0.8479\t Accuracy 0.8501\n",
      "Epoch [12][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [12][20]\t Batch [2100][5500]\t Training Loss 0.8484\t Accuracy 0.8495\n",
      "Epoch [12][20]\t Batch [2150][5500]\t Training Loss 0.8472\t Accuracy 0.8503\n",
      "Epoch [12][20]\t Batch [2200][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [12][20]\t Batch [2250][5500]\t Training Loss 0.8462\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [2300][5500]\t Training Loss 0.8462\t Accuracy 0.8508\n",
      "Epoch [12][20]\t Batch [2350][5500]\t Training Loss 0.8450\t Accuracy 0.8511\n",
      "Epoch [12][20]\t Batch [2400][5500]\t Training Loss 0.8456\t Accuracy 0.8511\n",
      "Epoch [12][20]\t Batch [2450][5500]\t Training Loss 0.8454\t Accuracy 0.8511\n",
      "Epoch [12][20]\t Batch [2500][5500]\t Training Loss 0.8467\t Accuracy 0.8503\n",
      "Epoch [12][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8507\n",
      "Epoch [12][20]\t Batch [2600][5500]\t Training Loss 0.8443\t Accuracy 0.8511\n",
      "Epoch [12][20]\t Batch [2650][5500]\t Training Loss 0.8445\t Accuracy 0.8515\n",
      "Epoch [12][20]\t Batch [2700][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [12][20]\t Batch [2750][5500]\t Training Loss 0.8456\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [2800][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [12][20]\t Batch [2850][5500]\t Training Loss 0.8438\t Accuracy 0.8520\n",
      "Epoch [12][20]\t Batch [2900][5500]\t Training Loss 0.8437\t Accuracy 0.8518\n",
      "Epoch [12][20]\t Batch [2950][5500]\t Training Loss 0.8438\t Accuracy 0.8516\n",
      "Epoch [12][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [3050][5500]\t Training Loss 0.8466\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [3100][5500]\t Training Loss 0.8482\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [3150][5500]\t Training Loss 0.8496\t Accuracy 0.8509\n",
      "Epoch [12][20]\t Batch [3200][5500]\t Training Loss 0.8507\t Accuracy 0.8506\n",
      "Epoch [12][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [12][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [3350][5500]\t Training Loss 0.8528\t Accuracy 0.8499\n",
      "Epoch [12][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [12][20]\t Batch [3450][5500]\t Training Loss 0.8511\t Accuracy 0.8513\n",
      "Epoch [12][20]\t Batch [3500][5500]\t Training Loss 0.8517\t Accuracy 0.8511\n",
      "Epoch [12][20]\t Batch [3550][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [12][20]\t Batch [3600][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [12][20]\t Batch [3650][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [12][20]\t Batch [3700][5500]\t Training Loss 0.8504\t Accuracy 0.8518\n",
      "Epoch [12][20]\t Batch [3750][5500]\t Training Loss 0.8512\t Accuracy 0.8515\n",
      "Epoch [12][20]\t Batch [3800][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [12][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8513\n",
      "Epoch [12][20]\t Batch [3900][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [12][20]\t Batch [3950][5500]\t Training Loss 0.8509\t Accuracy 0.8515\n",
      "Epoch [12][20]\t Batch [4000][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [12][20]\t Batch [4050][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [12][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [12][20]\t Batch [4150][5500]\t Training Loss 0.8521\t Accuracy 0.8515\n",
      "Epoch [12][20]\t Batch [4200][5500]\t Training Loss 0.8526\t Accuracy 0.8516\n",
      "Epoch [12][20]\t Batch [4250][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [12][20]\t Batch [4300][5500]\t Training Loss 0.8532\t Accuracy 0.8507\n",
      "Epoch [12][20]\t Batch [4350][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [12][20]\t Batch [4400][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [12][20]\t Batch [4450][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [12][20]\t Batch [4500][5500]\t Training Loss 0.8530\t Accuracy 0.8509\n",
      "Epoch [12][20]\t Batch [4550][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [12][20]\t Batch [4600][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [12][20]\t Batch [4650][5500]\t Training Loss 0.8543\t Accuracy 0.8507\n",
      "Epoch [12][20]\t Batch [4700][5500]\t Training Loss 0.8535\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [12][20]\t Batch [4800][5500]\t Training Loss 0.8536\t Accuracy 0.8506\n",
      "Epoch [12][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [12][20]\t Batch [4950][5500]\t Training Loss 0.8524\t Accuracy 0.8507\n",
      "Epoch [12][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [12][20]\t Batch [5050][5500]\t Training Loss 0.8542\t Accuracy 0.8503\n",
      "Epoch [12][20]\t Batch [5100][5500]\t Training Loss 0.8540\t Accuracy 0.8502\n",
      "Epoch [12][20]\t Batch [5150][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [12][20]\t Batch [5200][5500]\t Training Loss 0.8532\t Accuracy 0.8504\n",
      "Epoch [12][20]\t Batch [5250][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [12][20]\t Batch [5300][5500]\t Training Loss 0.8539\t Accuracy 0.8499\n",
      "Epoch [12][20]\t Batch [5350][5500]\t Training Loss 0.8536\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [5450][5500]\t Training Loss 0.8535\t Accuracy 0.8501\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [12]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [13][20]\t Batch [0][5500]\t Training Loss 0.7431\t Accuracy 0.9000\n",
      "Epoch [13][20]\t Batch [50][5500]\t Training Loss 0.8247\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [100][5500]\t Training Loss 0.8615\t Accuracy 0.8386\n",
      "Epoch [13][20]\t Batch [150][5500]\t Training Loss 0.8757\t Accuracy 0.8358\n",
      "Epoch [13][20]\t Batch [200][5500]\t Training Loss 0.8556\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [250][5500]\t Training Loss 0.8424\t Accuracy 0.8486\n",
      "Epoch [13][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [13][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [13][20]\t Batch [400][5500]\t Training Loss 0.8379\t Accuracy 0.8554\n",
      "Epoch [13][20]\t Batch [450][5500]\t Training Loss 0.8377\t Accuracy 0.8557\n",
      "Epoch [13][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [13][20]\t Batch [550][5500]\t Training Loss 0.8311\t Accuracy 0.8559\n",
      "Epoch [13][20]\t Batch [600][5500]\t Training Loss 0.8310\t Accuracy 0.8572\n",
      "Epoch [13][20]\t Batch [650][5500]\t Training Loss 0.8250\t Accuracy 0.8590\n",
      "Epoch [13][20]\t Batch [700][5500]\t Training Loss 0.8249\t Accuracy 0.8588\n",
      "Epoch [13][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8573\n",
      "Epoch [13][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8563\n",
      "Epoch [13][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8556\n",
      "Epoch [13][20]\t Batch [900][5500]\t Training Loss 0.8408\t Accuracy 0.8535\n",
      "Epoch [13][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8546\n",
      "Epoch [13][20]\t Batch [1000][5500]\t Training Loss 0.8370\t Accuracy 0.8554\n",
      "Epoch [13][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8564\n",
      "Epoch [13][20]\t Batch [1100][5500]\t Training Loss 0.8318\t Accuracy 0.8571\n",
      "Epoch [13][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8572\n",
      "Epoch [13][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8549\n",
      "Epoch [13][20]\t Batch [1250][5500]\t Training Loss 0.8340\t Accuracy 0.8545\n",
      "Epoch [13][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8536\n",
      "Epoch [13][20]\t Batch [1350][5500]\t Training Loss 0.8373\t Accuracy 0.8532\n",
      "Epoch [13][20]\t Batch [1400][5500]\t Training Loss 0.8389\t Accuracy 0.8522\n",
      "Epoch [13][20]\t Batch [1450][5500]\t Training Loss 0.8409\t Accuracy 0.8512\n",
      "Epoch [13][20]\t Batch [1500][5500]\t Training Loss 0.8447\t Accuracy 0.8490\n",
      "Epoch [13][20]\t Batch [1550][5500]\t Training Loss 0.8445\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [1600][5500]\t Training Loss 0.8465\t Accuracy 0.8490\n",
      "Epoch [13][20]\t Batch [1650][5500]\t Training Loss 0.8459\t Accuracy 0.8491\n",
      "Epoch [13][20]\t Batch [1700][5500]\t Training Loss 0.8477\t Accuracy 0.8487\n",
      "Epoch [13][20]\t Batch [1750][5500]\t Training Loss 0.8482\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [1800][5500]\t Training Loss 0.8515\t Accuracy 0.8473\n",
      "Epoch [13][20]\t Batch [1850][5500]\t Training Loss 0.8505\t Accuracy 0.8482\n",
      "Epoch [13][20]\t Batch [1900][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [13][20]\t Batch [1950][5500]\t Training Loss 0.8499\t Accuracy 0.8498\n",
      "Epoch [13][20]\t Batch [2000][5500]\t Training Loss 0.8479\t Accuracy 0.8501\n",
      "Epoch [13][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [13][20]\t Batch [2100][5500]\t Training Loss 0.8484\t Accuracy 0.8495\n",
      "Epoch [13][20]\t Batch [2150][5500]\t Training Loss 0.8472\t Accuracy 0.8503\n",
      "Epoch [13][20]\t Batch [2200][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [13][20]\t Batch [2250][5500]\t Training Loss 0.8462\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [2300][5500]\t Training Loss 0.8461\t Accuracy 0.8509\n",
      "Epoch [13][20]\t Batch [2350][5500]\t Training Loss 0.8450\t Accuracy 0.8512\n",
      "Epoch [13][20]\t Batch [2400][5500]\t Training Loss 0.8456\t Accuracy 0.8512\n",
      "Epoch [13][20]\t Batch [2450][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [2500][5500]\t Training Loss 0.8467\t Accuracy 0.8504\n",
      "Epoch [13][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [13][20]\t Batch [2600][5500]\t Training Loss 0.8443\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [2650][5500]\t Training Loss 0.8445\t Accuracy 0.8515\n",
      "Epoch [13][20]\t Batch [2700][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [2750][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [2800][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [13][20]\t Batch [2850][5500]\t Training Loss 0.8438\t Accuracy 0.8520\n",
      "Epoch [13][20]\t Batch [2900][5500]\t Training Loss 0.8437\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [2950][5500]\t Training Loss 0.8438\t Accuracy 0.8516\n",
      "Epoch [13][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [3050][5500]\t Training Loss 0.8466\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [3100][5500]\t Training Loss 0.8482\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [3150][5500]\t Training Loss 0.8496\t Accuracy 0.8509\n",
      "Epoch [13][20]\t Batch [3200][5500]\t Training Loss 0.8507\t Accuracy 0.8506\n",
      "Epoch [13][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [13][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [3350][5500]\t Training Loss 0.8528\t Accuracy 0.8499\n",
      "Epoch [13][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [13][20]\t Batch [3450][5500]\t Training Loss 0.8511\t Accuracy 0.8514\n",
      "Epoch [13][20]\t Batch [3500][5500]\t Training Loss 0.8517\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [3550][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [13][20]\t Batch [3600][5500]\t Training Loss 0.8518\t Accuracy 0.8512\n",
      "Epoch [13][20]\t Batch [3650][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [13][20]\t Batch [3700][5500]\t Training Loss 0.8504\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [3750][5500]\t Training Loss 0.8512\t Accuracy 0.8516\n",
      "Epoch [13][20]\t Batch [3800][5500]\t Training Loss 0.8514\t Accuracy 0.8515\n",
      "Epoch [13][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8514\n",
      "Epoch [13][20]\t Batch [3900][5500]\t Training Loss 0.8511\t Accuracy 0.8516\n",
      "Epoch [13][20]\t Batch [3950][5500]\t Training Loss 0.8509\t Accuracy 0.8516\n",
      "Epoch [13][20]\t Batch [4000][5500]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [13][20]\t Batch [4050][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [13][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [13][20]\t Batch [4150][5500]\t Training Loss 0.8521\t Accuracy 0.8516\n",
      "Epoch [13][20]\t Batch [4200][5500]\t Training Loss 0.8526\t Accuracy 0.8516\n",
      "Epoch [13][20]\t Batch [4250][5500]\t Training Loss 0.8534\t Accuracy 0.8509\n",
      "Epoch [13][20]\t Batch [4300][5500]\t Training Loss 0.8532\t Accuracy 0.8507\n",
      "Epoch [13][20]\t Batch [4350][5500]\t Training Loss 0.8527\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [4400][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [13][20]\t Batch [4450][5500]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "Epoch [13][20]\t Batch [4500][5500]\t Training Loss 0.8530\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [4550][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [13][20]\t Batch [4600][5500]\t Training Loss 0.8536\t Accuracy 0.8508\n",
      "Epoch [13][20]\t Batch [4650][5500]\t Training Loss 0.8543\t Accuracy 0.8508\n",
      "Epoch [13][20]\t Batch [4700][5500]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [13][20]\t Batch [4800][5500]\t Training Loss 0.8535\t Accuracy 0.8507\n",
      "Epoch [13][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [4950][5500]\t Training Loss 0.8524\t Accuracy 0.8507\n",
      "Epoch [13][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [13][20]\t Batch [5050][5500]\t Training Loss 0.8542\t Accuracy 0.8503\n",
      "Epoch [13][20]\t Batch [5100][5500]\t Training Loss 0.8540\t Accuracy 0.8503\n",
      "Epoch [13][20]\t Batch [5150][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [13][20]\t Batch [5200][5500]\t Training Loss 0.8532\t Accuracy 0.8505\n",
      "Epoch [13][20]\t Batch [5250][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [13][20]\t Batch [5300][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [5350][5500]\t Training Loss 0.8536\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [5450][5500]\t Training Loss 0.8534\t Accuracy 0.8502\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [13]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [14][20]\t Batch [0][5500]\t Training Loss 0.7431\t Accuracy 0.9000\n",
      "Epoch [14][20]\t Batch [50][5500]\t Training Loss 0.8247\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [100][5500]\t Training Loss 0.8615\t Accuracy 0.8386\n",
      "Epoch [14][20]\t Batch [150][5500]\t Training Loss 0.8757\t Accuracy 0.8358\n",
      "Epoch [14][20]\t Batch [200][5500]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [14][20]\t Batch [250][5500]\t Training Loss 0.8423\t Accuracy 0.8486\n",
      "Epoch [14][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [14][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [14][20]\t Batch [400][5500]\t Training Loss 0.8379\t Accuracy 0.8554\n",
      "Epoch [14][20]\t Batch [450][5500]\t Training Loss 0.8377\t Accuracy 0.8557\n",
      "Epoch [14][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [14][20]\t Batch [550][5500]\t Training Loss 0.8311\t Accuracy 0.8559\n",
      "Epoch [14][20]\t Batch [600][5500]\t Training Loss 0.8310\t Accuracy 0.8572\n",
      "Epoch [14][20]\t Batch [650][5500]\t Training Loss 0.8250\t Accuracy 0.8590\n",
      "Epoch [14][20]\t Batch [700][5500]\t Training Loss 0.8249\t Accuracy 0.8588\n",
      "Epoch [14][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8573\n",
      "Epoch [14][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8563\n",
      "Epoch [14][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8556\n",
      "Epoch [14][20]\t Batch [900][5500]\t Training Loss 0.8408\t Accuracy 0.8535\n",
      "Epoch [14][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8546\n",
      "Epoch [14][20]\t Batch [1000][5500]\t Training Loss 0.8369\t Accuracy 0.8554\n",
      "Epoch [14][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8564\n",
      "Epoch [14][20]\t Batch [1100][5500]\t Training Loss 0.8318\t Accuracy 0.8571\n",
      "Epoch [14][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8572\n",
      "Epoch [14][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8549\n",
      "Epoch [14][20]\t Batch [1250][5500]\t Training Loss 0.8340\t Accuracy 0.8545\n",
      "Epoch [14][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8536\n",
      "Epoch [14][20]\t Batch [1350][5500]\t Training Loss 0.8373\t Accuracy 0.8532\n",
      "Epoch [14][20]\t Batch [1400][5500]\t Training Loss 0.8388\t Accuracy 0.8522\n",
      "Epoch [14][20]\t Batch [1450][5500]\t Training Loss 0.8409\t Accuracy 0.8512\n",
      "Epoch [14][20]\t Batch [1500][5500]\t Training Loss 0.8447\t Accuracy 0.8490\n",
      "Epoch [14][20]\t Batch [1550][5500]\t Training Loss 0.8445\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [1600][5500]\t Training Loss 0.8465\t Accuracy 0.8490\n",
      "Epoch [14][20]\t Batch [1650][5500]\t Training Loss 0.8459\t Accuracy 0.8491\n",
      "Epoch [14][20]\t Batch [1700][5500]\t Training Loss 0.8477\t Accuracy 0.8487\n",
      "Epoch [14][20]\t Batch [1750][5500]\t Training Loss 0.8482\t Accuracy 0.8485\n",
      "Epoch [14][20]\t Batch [1800][5500]\t Training Loss 0.8515\t Accuracy 0.8473\n",
      "Epoch [14][20]\t Batch [1850][5500]\t Training Loss 0.8505\t Accuracy 0.8482\n",
      "Epoch [14][20]\t Batch [1900][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [14][20]\t Batch [1950][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [14][20]\t Batch [2000][5500]\t Training Loss 0.8478\t Accuracy 0.8501\n",
      "Epoch [14][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [14][20]\t Batch [2100][5500]\t Training Loss 0.8484\t Accuracy 0.8495\n",
      "Epoch [14][20]\t Batch [2150][5500]\t Training Loss 0.8472\t Accuracy 0.8503\n",
      "Epoch [14][20]\t Batch [2200][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [14][20]\t Batch [2250][5500]\t Training Loss 0.8461\t Accuracy 0.8510\n",
      "Epoch [14][20]\t Batch [2300][5500]\t Training Loss 0.8461\t Accuracy 0.8509\n",
      "Epoch [14][20]\t Batch [2350][5500]\t Training Loss 0.8450\t Accuracy 0.8512\n",
      "Epoch [14][20]\t Batch [2400][5500]\t Training Loss 0.8455\t Accuracy 0.8512\n",
      "Epoch [14][20]\t Batch [2450][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [14][20]\t Batch [2500][5500]\t Training Loss 0.8466\t Accuracy 0.8504\n",
      "Epoch [14][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [14][20]\t Batch [2600][5500]\t Training Loss 0.8443\t Accuracy 0.8511\n",
      "Epoch [14][20]\t Batch [2650][5500]\t Training Loss 0.8445\t Accuracy 0.8515\n",
      "Epoch [14][20]\t Batch [2700][5500]\t Training Loss 0.8452\t Accuracy 0.8511\n",
      "Epoch [14][20]\t Batch [2750][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [14][20]\t Batch [2800][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [2850][5500]\t Training Loss 0.8438\t Accuracy 0.8520\n",
      "Epoch [14][20]\t Batch [2900][5500]\t Training Loss 0.8437\t Accuracy 0.8518\n",
      "Epoch [14][20]\t Batch [2950][5500]\t Training Loss 0.8438\t Accuracy 0.8516\n",
      "Epoch [14][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8510\n",
      "Epoch [14][20]\t Batch [3050][5500]\t Training Loss 0.8466\t Accuracy 0.8510\n",
      "Epoch [14][20]\t Batch [3100][5500]\t Training Loss 0.8482\t Accuracy 0.8510\n",
      "Epoch [14][20]\t Batch [3150][5500]\t Training Loss 0.8496\t Accuracy 0.8509\n",
      "Epoch [14][20]\t Batch [3200][5500]\t Training Loss 0.8506\t Accuracy 0.8506\n",
      "Epoch [14][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [14][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [3350][5500]\t Training Loss 0.8527\t Accuracy 0.8499\n",
      "Epoch [14][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [14][20]\t Batch [3450][5500]\t Training Loss 0.8511\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [3500][5500]\t Training Loss 0.8517\t Accuracy 0.8511\n",
      "Epoch [14][20]\t Batch [3550][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [14][20]\t Batch [3600][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [14][20]\t Batch [3650][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [14][20]\t Batch [3700][5500]\t Training Loss 0.8504\t Accuracy 0.8518\n",
      "Epoch [14][20]\t Batch [3750][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [14][20]\t Batch [3800][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [14][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [3900][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [14][20]\t Batch [3950][5500]\t Training Loss 0.8509\t Accuracy 0.8515\n",
      "Epoch [14][20]\t Batch [4000][5500]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [4050][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [14][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [14][20]\t Batch [4150][5500]\t Training Loss 0.8521\t Accuracy 0.8515\n",
      "Epoch [14][20]\t Batch [4200][5500]\t Training Loss 0.8526\t Accuracy 0.8516\n",
      "Epoch [14][20]\t Batch [4250][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [14][20]\t Batch [4300][5500]\t Training Loss 0.8532\t Accuracy 0.8507\n",
      "Epoch [14][20]\t Batch [4350][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [14][20]\t Batch [4400][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [14][20]\t Batch [4450][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [14][20]\t Batch [4500][5500]\t Training Loss 0.8530\t Accuracy 0.8509\n",
      "Epoch [14][20]\t Batch [4550][5500]\t Training Loss 0.8534\t Accuracy 0.8507\n",
      "Epoch [14][20]\t Batch [4600][5500]\t Training Loss 0.8536\t Accuracy 0.8508\n",
      "Epoch [14][20]\t Batch [4650][5500]\t Training Loss 0.8543\t Accuracy 0.8507\n",
      "Epoch [14][20]\t Batch [4700][5500]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "Epoch [14][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [14][20]\t Batch [4800][5500]\t Training Loss 0.8535\t Accuracy 0.8506\n",
      "Epoch [14][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [14][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [14][20]\t Batch [4950][5500]\t Training Loss 0.8524\t Accuracy 0.8507\n",
      "Epoch [14][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [14][20]\t Batch [5050][5500]\t Training Loss 0.8542\t Accuracy 0.8503\n",
      "Epoch [14][20]\t Batch [5100][5500]\t Training Loss 0.8539\t Accuracy 0.8503\n",
      "Epoch [14][20]\t Batch [5150][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [14][20]\t Batch [5200][5500]\t Training Loss 0.8532\t Accuracy 0.8504\n",
      "Epoch [14][20]\t Batch [5250][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [14][20]\t Batch [5300][5500]\t Training Loss 0.8538\t Accuracy 0.8499\n",
      "Epoch [14][20]\t Batch [5350][5500]\t Training Loss 0.8536\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [5450][5500]\t Training Loss 0.8534\t Accuracy 0.8501\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [14]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [15][20]\t Batch [0][5500]\t Training Loss 0.7431\t Accuracy 0.9000\n",
      "Epoch [15][20]\t Batch [50][5500]\t Training Loss 0.8247\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [100][5500]\t Training Loss 0.8615\t Accuracy 0.8386\n",
      "Epoch [15][20]\t Batch [150][5500]\t Training Loss 0.8757\t Accuracy 0.8358\n",
      "Epoch [15][20]\t Batch [200][5500]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [15][20]\t Batch [250][5500]\t Training Loss 0.8423\t Accuracy 0.8486\n",
      "Epoch [15][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [15][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [15][20]\t Batch [400][5500]\t Training Loss 0.8379\t Accuracy 0.8554\n",
      "Epoch [15][20]\t Batch [450][5500]\t Training Loss 0.8377\t Accuracy 0.8557\n",
      "Epoch [15][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [15][20]\t Batch [550][5500]\t Training Loss 0.8311\t Accuracy 0.8559\n",
      "Epoch [15][20]\t Batch [600][5500]\t Training Loss 0.8310\t Accuracy 0.8572\n",
      "Epoch [15][20]\t Batch [650][5500]\t Training Loss 0.8250\t Accuracy 0.8590\n",
      "Epoch [15][20]\t Batch [700][5500]\t Training Loss 0.8249\t Accuracy 0.8588\n",
      "Epoch [15][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8573\n",
      "Epoch [15][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8563\n",
      "Epoch [15][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8556\n",
      "Epoch [15][20]\t Batch [900][5500]\t Training Loss 0.8408\t Accuracy 0.8535\n",
      "Epoch [15][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8546\n",
      "Epoch [15][20]\t Batch [1000][5500]\t Training Loss 0.8369\t Accuracy 0.8554\n",
      "Epoch [15][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8564\n",
      "Epoch [15][20]\t Batch [1100][5500]\t Training Loss 0.8318\t Accuracy 0.8571\n",
      "Epoch [15][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8572\n",
      "Epoch [15][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8549\n",
      "Epoch [15][20]\t Batch [1250][5500]\t Training Loss 0.8340\t Accuracy 0.8545\n",
      "Epoch [15][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8536\n",
      "Epoch [15][20]\t Batch [1350][5500]\t Training Loss 0.8373\t Accuracy 0.8532\n",
      "Epoch [15][20]\t Batch [1400][5500]\t Training Loss 0.8388\t Accuracy 0.8522\n",
      "Epoch [15][20]\t Batch [1450][5500]\t Training Loss 0.8408\t Accuracy 0.8512\n",
      "Epoch [15][20]\t Batch [1500][5500]\t Training Loss 0.8447\t Accuracy 0.8490\n",
      "Epoch [15][20]\t Batch [1550][5500]\t Training Loss 0.8444\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [1600][5500]\t Training Loss 0.8464\t Accuracy 0.8491\n",
      "Epoch [15][20]\t Batch [1650][5500]\t Training Loss 0.8459\t Accuracy 0.8491\n",
      "Epoch [15][20]\t Batch [1700][5500]\t Training Loss 0.8476\t Accuracy 0.8487\n",
      "Epoch [15][20]\t Batch [1750][5500]\t Training Loss 0.8481\t Accuracy 0.8485\n",
      "Epoch [15][20]\t Batch [1800][5500]\t Training Loss 0.8515\t Accuracy 0.8473\n",
      "Epoch [15][20]\t Batch [1850][5500]\t Training Loss 0.8505\t Accuracy 0.8482\n",
      "Epoch [15][20]\t Batch [1900][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [15][20]\t Batch [1950][5500]\t Training Loss 0.8498\t Accuracy 0.8499\n",
      "Epoch [15][20]\t Batch [2000][5500]\t Training Loss 0.8478\t Accuracy 0.8501\n",
      "Epoch [15][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [15][20]\t Batch [2100][5500]\t Training Loss 0.8484\t Accuracy 0.8496\n",
      "Epoch [15][20]\t Batch [2150][5500]\t Training Loss 0.8471\t Accuracy 0.8504\n",
      "Epoch [15][20]\t Batch [2200][5500]\t Training Loss 0.8449\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [2250][5500]\t Training Loss 0.8461\t Accuracy 0.8511\n",
      "Epoch [15][20]\t Batch [2300][5500]\t Training Loss 0.8461\t Accuracy 0.8509\n",
      "Epoch [15][20]\t Batch [2350][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [15][20]\t Batch [2400][5500]\t Training Loss 0.8455\t Accuracy 0.8512\n",
      "Epoch [15][20]\t Batch [2450][5500]\t Training Loss 0.8453\t Accuracy 0.8512\n",
      "Epoch [15][20]\t Batch [2500][5500]\t Training Loss 0.8466\t Accuracy 0.8504\n",
      "Epoch [15][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [15][20]\t Batch [2600][5500]\t Training Loss 0.8443\t Accuracy 0.8511\n",
      "Epoch [15][20]\t Batch [2650][5500]\t Training Loss 0.8445\t Accuracy 0.8515\n",
      "Epoch [15][20]\t Batch [2700][5500]\t Training Loss 0.8452\t Accuracy 0.8511\n",
      "Epoch [15][20]\t Batch [2750][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [15][20]\t Batch [2800][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [2850][5500]\t Training Loss 0.8438\t Accuracy 0.8520\n",
      "Epoch [15][20]\t Batch [2900][5500]\t Training Loss 0.8437\t Accuracy 0.8519\n",
      "Epoch [15][20]\t Batch [2950][5500]\t Training Loss 0.8438\t Accuracy 0.8516\n",
      "Epoch [15][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [15][20]\t Batch [3050][5500]\t Training Loss 0.8466\t Accuracy 0.8511\n",
      "Epoch [15][20]\t Batch [3100][5500]\t Training Loss 0.8481\t Accuracy 0.8510\n",
      "Epoch [15][20]\t Batch [3150][5500]\t Training Loss 0.8496\t Accuracy 0.8509\n",
      "Epoch [15][20]\t Batch [3200][5500]\t Training Loss 0.8506\t Accuracy 0.8506\n",
      "Epoch [15][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [15][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [3350][5500]\t Training Loss 0.8527\t Accuracy 0.8499\n",
      "Epoch [15][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [15][20]\t Batch [3450][5500]\t Training Loss 0.8511\t Accuracy 0.8514\n",
      "Epoch [15][20]\t Batch [3500][5500]\t Training Loss 0.8517\t Accuracy 0.8511\n",
      "Epoch [15][20]\t Batch [3550][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [15][20]\t Batch [3600][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [15][20]\t Batch [3650][5500]\t Training Loss 0.8516\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [3700][5500]\t Training Loss 0.8504\t Accuracy 0.8519\n",
      "Epoch [15][20]\t Batch [3750][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [15][20]\t Batch [3800][5500]\t Training Loss 0.8514\t Accuracy 0.8514\n",
      "Epoch [15][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8514\n",
      "Epoch [15][20]\t Batch [3900][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [15][20]\t Batch [3950][5500]\t Training Loss 0.8509\t Accuracy 0.8516\n",
      "Epoch [15][20]\t Batch [4000][5500]\t Training Loss 0.8516\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [4050][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [15][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [15][20]\t Batch [4150][5500]\t Training Loss 0.8520\t Accuracy 0.8516\n",
      "Epoch [15][20]\t Batch [4200][5500]\t Training Loss 0.8525\t Accuracy 0.8516\n",
      "Epoch [15][20]\t Batch [4250][5500]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "Epoch [15][20]\t Batch [4300][5500]\t Training Loss 0.8531\t Accuracy 0.8507\n",
      "Epoch [15][20]\t Batch [4350][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [15][20]\t Batch [4400][5500]\t Training Loss 0.8527\t Accuracy 0.8509\n",
      "Epoch [15][20]\t Batch [4450][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [15][20]\t Batch [4500][5500]\t Training Loss 0.8530\t Accuracy 0.8509\n",
      "Epoch [15][20]\t Batch [4550][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [15][20]\t Batch [4600][5500]\t Training Loss 0.8536\t Accuracy 0.8508\n",
      "Epoch [15][20]\t Batch [4650][5500]\t Training Loss 0.8543\t Accuracy 0.8508\n",
      "Epoch [15][20]\t Batch [4700][5500]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "Epoch [15][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [15][20]\t Batch [4800][5500]\t Training Loss 0.8535\t Accuracy 0.8506\n",
      "Epoch [15][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [15][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [15][20]\t Batch [4950][5500]\t Training Loss 0.8523\t Accuracy 0.8507\n",
      "Epoch [15][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [15][20]\t Batch [5050][5500]\t Training Loss 0.8541\t Accuracy 0.8503\n",
      "Epoch [15][20]\t Batch [5100][5500]\t Training Loss 0.8539\t Accuracy 0.8503\n",
      "Epoch [15][20]\t Batch [5150][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [15][20]\t Batch [5200][5500]\t Training Loss 0.8531\t Accuracy 0.8505\n",
      "Epoch [15][20]\t Batch [5250][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [15][20]\t Batch [5300][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [5350][5500]\t Training Loss 0.8536\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [5450][5500]\t Training Loss 0.8534\t Accuracy 0.8502\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [15]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [16][20]\t Batch [0][5500]\t Training Loss 0.7431\t Accuracy 0.9000\n",
      "Epoch [16][20]\t Batch [50][5500]\t Training Loss 0.8247\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [100][5500]\t Training Loss 0.8615\t Accuracy 0.8386\n",
      "Epoch [16][20]\t Batch [150][5500]\t Training Loss 0.8757\t Accuracy 0.8358\n",
      "Epoch [16][20]\t Batch [200][5500]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [250][5500]\t Training Loss 0.8423\t Accuracy 0.8486\n",
      "Epoch [16][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [16][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [16][20]\t Batch [400][5500]\t Training Loss 0.8379\t Accuracy 0.8554\n",
      "Epoch [16][20]\t Batch [450][5500]\t Training Loss 0.8376\t Accuracy 0.8557\n",
      "Epoch [16][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [16][20]\t Batch [550][5500]\t Training Loss 0.8311\t Accuracy 0.8559\n",
      "Epoch [16][20]\t Batch [600][5500]\t Training Loss 0.8310\t Accuracy 0.8572\n",
      "Epoch [16][20]\t Batch [650][5500]\t Training Loss 0.8249\t Accuracy 0.8590\n",
      "Epoch [16][20]\t Batch [700][5500]\t Training Loss 0.8249\t Accuracy 0.8588\n",
      "Epoch [16][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8573\n",
      "Epoch [16][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8563\n",
      "Epoch [16][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8556\n",
      "Epoch [16][20]\t Batch [900][5500]\t Training Loss 0.8408\t Accuracy 0.8535\n",
      "Epoch [16][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8546\n",
      "Epoch [16][20]\t Batch [1000][5500]\t Training Loss 0.8369\t Accuracy 0.8554\n",
      "Epoch [16][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8564\n",
      "Epoch [16][20]\t Batch [1100][5500]\t Training Loss 0.8318\t Accuracy 0.8571\n",
      "Epoch [16][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8572\n",
      "Epoch [16][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8549\n",
      "Epoch [16][20]\t Batch [1250][5500]\t Training Loss 0.8340\t Accuracy 0.8545\n",
      "Epoch [16][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8536\n",
      "Epoch [16][20]\t Batch [1350][5500]\t Training Loss 0.8372\t Accuracy 0.8532\n",
      "Epoch [16][20]\t Batch [1400][5500]\t Training Loss 0.8388\t Accuracy 0.8522\n",
      "Epoch [16][20]\t Batch [1450][5500]\t Training Loss 0.8408\t Accuracy 0.8512\n",
      "Epoch [16][20]\t Batch [1500][5500]\t Training Loss 0.8447\t Accuracy 0.8490\n",
      "Epoch [16][20]\t Batch [1550][5500]\t Training Loss 0.8444\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [1600][5500]\t Training Loss 0.8464\t Accuracy 0.8491\n",
      "Epoch [16][20]\t Batch [1650][5500]\t Training Loss 0.8458\t Accuracy 0.8491\n",
      "Epoch [16][20]\t Batch [1700][5500]\t Training Loss 0.8476\t Accuracy 0.8487\n",
      "Epoch [16][20]\t Batch [1750][5500]\t Training Loss 0.8481\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [1800][5500]\t Training Loss 0.8515\t Accuracy 0.8473\n",
      "Epoch [16][20]\t Batch [1850][5500]\t Training Loss 0.8504\t Accuracy 0.8482\n",
      "Epoch [16][20]\t Batch [1900][5500]\t Training Loss 0.8497\t Accuracy 0.8498\n",
      "Epoch [16][20]\t Batch [1950][5500]\t Training Loss 0.8498\t Accuracy 0.8499\n",
      "Epoch [16][20]\t Batch [2000][5500]\t Training Loss 0.8478\t Accuracy 0.8501\n",
      "Epoch [16][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [16][20]\t Batch [2100][5500]\t Training Loss 0.8483\t Accuracy 0.8496\n",
      "Epoch [16][20]\t Batch [2150][5500]\t Training Loss 0.8471\t Accuracy 0.8504\n",
      "Epoch [16][20]\t Batch [2200][5500]\t Training Loss 0.8449\t Accuracy 0.8513\n",
      "Epoch [16][20]\t Batch [2250][5500]\t Training Loss 0.8461\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [2300][5500]\t Training Loss 0.8461\t Accuracy 0.8509\n",
      "Epoch [16][20]\t Batch [2350][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [16][20]\t Batch [2400][5500]\t Training Loss 0.8455\t Accuracy 0.8512\n",
      "Epoch [16][20]\t Batch [2450][5500]\t Training Loss 0.8453\t Accuracy 0.8512\n",
      "Epoch [16][20]\t Batch [2500][5500]\t Training Loss 0.8466\t Accuracy 0.8504\n",
      "Epoch [16][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [16][20]\t Batch [2600][5500]\t Training Loss 0.8443\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [2650][5500]\t Training Loss 0.8444\t Accuracy 0.8515\n",
      "Epoch [16][20]\t Batch [2700][5500]\t Training Loss 0.8452\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [2750][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [2800][5500]\t Training Loss 0.8451\t Accuracy 0.8513\n",
      "Epoch [16][20]\t Batch [2850][5500]\t Training Loss 0.8438\t Accuracy 0.8520\n",
      "Epoch [16][20]\t Batch [2900][5500]\t Training Loss 0.8436\t Accuracy 0.8519\n",
      "Epoch [16][20]\t Batch [2950][5500]\t Training Loss 0.8438\t Accuracy 0.8516\n",
      "Epoch [16][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [3050][5500]\t Training Loss 0.8465\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [3100][5500]\t Training Loss 0.8481\t Accuracy 0.8510\n",
      "Epoch [16][20]\t Batch [3150][5500]\t Training Loss 0.8495\t Accuracy 0.8509\n",
      "Epoch [16][20]\t Batch [3200][5500]\t Training Loss 0.8506\t Accuracy 0.8506\n",
      "Epoch [16][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [16][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [3350][5500]\t Training Loss 0.8527\t Accuracy 0.8499\n",
      "Epoch [16][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [16][20]\t Batch [3450][5500]\t Training Loss 0.8511\t Accuracy 0.8514\n",
      "Epoch [16][20]\t Batch [3500][5500]\t Training Loss 0.8517\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [3550][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [16][20]\t Batch [3600][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [16][20]\t Batch [3650][5500]\t Training Loss 0.8516\t Accuracy 0.8513\n",
      "Epoch [16][20]\t Batch [3700][5500]\t Training Loss 0.8504\t Accuracy 0.8519\n",
      "Epoch [16][20]\t Batch [3750][5500]\t Training Loss 0.8511\t Accuracy 0.8516\n",
      "Epoch [16][20]\t Batch [3800][5500]\t Training Loss 0.8513\t Accuracy 0.8515\n",
      "Epoch [16][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8514\n",
      "Epoch [16][20]\t Batch [3900][5500]\t Training Loss 0.8511\t Accuracy 0.8516\n",
      "Epoch [16][20]\t Batch [3950][5500]\t Training Loss 0.8509\t Accuracy 0.8516\n",
      "Epoch [16][20]\t Batch [4000][5500]\t Training Loss 0.8516\t Accuracy 0.8514\n",
      "Epoch [16][20]\t Batch [4050][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [16][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [16][20]\t Batch [4150][5500]\t Training Loss 0.8520\t Accuracy 0.8516\n",
      "Epoch [16][20]\t Batch [4200][5500]\t Training Loss 0.8525\t Accuracy 0.8516\n",
      "Epoch [16][20]\t Batch [4250][5500]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "Epoch [16][20]\t Batch [4300][5500]\t Training Loss 0.8531\t Accuracy 0.8507\n",
      "Epoch [16][20]\t Batch [4350][5500]\t Training Loss 0.8526\t Accuracy 0.8510\n",
      "Epoch [16][20]\t Batch [4400][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [16][20]\t Batch [4450][5500]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "Epoch [16][20]\t Batch [4500][5500]\t Training Loss 0.8530\t Accuracy 0.8510\n",
      "Epoch [16][20]\t Batch [4550][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [16][20]\t Batch [4600][5500]\t Training Loss 0.8536\t Accuracy 0.8508\n",
      "Epoch [16][20]\t Batch [4650][5500]\t Training Loss 0.8542\t Accuracy 0.8508\n",
      "Epoch [16][20]\t Batch [4700][5500]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "Epoch [16][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [16][20]\t Batch [4800][5500]\t Training Loss 0.8535\t Accuracy 0.8507\n",
      "Epoch [16][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [16][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [16][20]\t Batch [4950][5500]\t Training Loss 0.8523\t Accuracy 0.8507\n",
      "Epoch [16][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [16][20]\t Batch [5050][5500]\t Training Loss 0.8541\t Accuracy 0.8503\n",
      "Epoch [16][20]\t Batch [5100][5500]\t Training Loss 0.8539\t Accuracy 0.8503\n",
      "Epoch [16][20]\t Batch [5150][5500]\t Training Loss 0.8536\t Accuracy 0.8502\n",
      "Epoch [16][20]\t Batch [5200][5500]\t Training Loss 0.8531\t Accuracy 0.8505\n",
      "Epoch [16][20]\t Batch [5250][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [16][20]\t Batch [5300][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [5350][5500]\t Training Loss 0.8536\t Accuracy 0.8501\n",
      "Epoch [16][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [5450][5500]\t Training Loss 0.8534\t Accuracy 0.8502\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [16]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [17][20]\t Batch [0][5500]\t Training Loss 0.7431\t Accuracy 0.9000\n",
      "Epoch [17][20]\t Batch [50][5500]\t Training Loss 0.8247\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [100][5500]\t Training Loss 0.8615\t Accuracy 0.8386\n",
      "Epoch [17][20]\t Batch [150][5500]\t Training Loss 0.8756\t Accuracy 0.8358\n",
      "Epoch [17][20]\t Batch [200][5500]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [250][5500]\t Training Loss 0.8423\t Accuracy 0.8486\n",
      "Epoch [17][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [17][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [17][20]\t Batch [400][5500]\t Training Loss 0.8378\t Accuracy 0.8554\n",
      "Epoch [17][20]\t Batch [450][5500]\t Training Loss 0.8376\t Accuracy 0.8557\n",
      "Epoch [17][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [17][20]\t Batch [550][5500]\t Training Loss 0.8310\t Accuracy 0.8559\n",
      "Epoch [17][20]\t Batch [600][5500]\t Training Loss 0.8310\t Accuracy 0.8572\n",
      "Epoch [17][20]\t Batch [650][5500]\t Training Loss 0.8249\t Accuracy 0.8590\n",
      "Epoch [17][20]\t Batch [700][5500]\t Training Loss 0.8249\t Accuracy 0.8588\n",
      "Epoch [17][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8571\n",
      "Epoch [17][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8562\n",
      "Epoch [17][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8555\n",
      "Epoch [17][20]\t Batch [900][5500]\t Training Loss 0.8408\t Accuracy 0.8534\n",
      "Epoch [17][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8545\n",
      "Epoch [17][20]\t Batch [1000][5500]\t Training Loss 0.8369\t Accuracy 0.8553\n",
      "Epoch [17][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8563\n",
      "Epoch [17][20]\t Batch [1100][5500]\t Training Loss 0.8318\t Accuracy 0.8570\n",
      "Epoch [17][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8571\n",
      "Epoch [17][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8548\n",
      "Epoch [17][20]\t Batch [1250][5500]\t Training Loss 0.8339\t Accuracy 0.8544\n",
      "Epoch [17][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8535\n",
      "Epoch [17][20]\t Batch [1350][5500]\t Training Loss 0.8372\t Accuracy 0.8531\n",
      "Epoch [17][20]\t Batch [1400][5500]\t Training Loss 0.8388\t Accuracy 0.8522\n",
      "Epoch [17][20]\t Batch [1450][5500]\t Training Loss 0.8408\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [1500][5500]\t Training Loss 0.8446\t Accuracy 0.8489\n",
      "Epoch [17][20]\t Batch [1550][5500]\t Training Loss 0.8444\t Accuracy 0.8499\n",
      "Epoch [17][20]\t Batch [1600][5500]\t Training Loss 0.8464\t Accuracy 0.8490\n",
      "Epoch [17][20]\t Batch [1650][5500]\t Training Loss 0.8458\t Accuracy 0.8491\n",
      "Epoch [17][20]\t Batch [1700][5500]\t Training Loss 0.8476\t Accuracy 0.8487\n",
      "Epoch [17][20]\t Batch [1750][5500]\t Training Loss 0.8481\t Accuracy 0.8485\n",
      "Epoch [17][20]\t Batch [1800][5500]\t Training Loss 0.8514\t Accuracy 0.8473\n",
      "Epoch [17][20]\t Batch [1850][5500]\t Training Loss 0.8504\t Accuracy 0.8482\n",
      "Epoch [17][20]\t Batch [1900][5500]\t Training Loss 0.8497\t Accuracy 0.8498\n",
      "Epoch [17][20]\t Batch [1950][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [17][20]\t Batch [2000][5500]\t Training Loss 0.8478\t Accuracy 0.8501\n",
      "Epoch [17][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [17][20]\t Batch [2100][5500]\t Training Loss 0.8483\t Accuracy 0.8495\n",
      "Epoch [17][20]\t Batch [2150][5500]\t Training Loss 0.8471\t Accuracy 0.8503\n",
      "Epoch [17][20]\t Batch [2200][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [17][20]\t Batch [2250][5500]\t Training Loss 0.8461\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [2300][5500]\t Training Loss 0.8461\t Accuracy 0.8509\n",
      "Epoch [17][20]\t Batch [2350][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [17][20]\t Batch [2400][5500]\t Training Loss 0.8455\t Accuracy 0.8512\n",
      "Epoch [17][20]\t Batch [2450][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [2500][5500]\t Training Loss 0.8466\t Accuracy 0.8504\n",
      "Epoch [17][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [17][20]\t Batch [2600][5500]\t Training Loss 0.8443\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [2650][5500]\t Training Loss 0.8444\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [2700][5500]\t Training Loss 0.8452\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [2750][5500]\t Training Loss 0.8455\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [2800][5500]\t Training Loss 0.8450\t Accuracy 0.8513\n",
      "Epoch [17][20]\t Batch [2850][5500]\t Training Loss 0.8437\t Accuracy 0.8520\n",
      "Epoch [17][20]\t Batch [2900][5500]\t Training Loss 0.8436\t Accuracy 0.8518\n",
      "Epoch [17][20]\t Batch [2950][5500]\t Training Loss 0.8438\t Accuracy 0.8516\n",
      "Epoch [17][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [3050][5500]\t Training Loss 0.8465\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [3100][5500]\t Training Loss 0.8481\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [3150][5500]\t Training Loss 0.8495\t Accuracy 0.8509\n",
      "Epoch [17][20]\t Batch [3200][5500]\t Training Loss 0.8506\t Accuracy 0.8506\n",
      "Epoch [17][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [17][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [3350][5500]\t Training Loss 0.8527\t Accuracy 0.8499\n",
      "Epoch [17][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [17][20]\t Batch [3450][5500]\t Training Loss 0.8511\t Accuracy 0.8513\n",
      "Epoch [17][20]\t Batch [3500][5500]\t Training Loss 0.8516\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [3550][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [17][20]\t Batch [3600][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [17][20]\t Batch [3650][5500]\t Training Loss 0.8516\t Accuracy 0.8513\n",
      "Epoch [17][20]\t Batch [3700][5500]\t Training Loss 0.8504\t Accuracy 0.8519\n",
      "Epoch [17][20]\t Batch [3750][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [3800][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [17][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8514\n",
      "Epoch [17][20]\t Batch [3900][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [3950][5500]\t Training Loss 0.8508\t Accuracy 0.8516\n",
      "Epoch [17][20]\t Batch [4000][5500]\t Training Loss 0.8516\t Accuracy 0.8513\n",
      "Epoch [17][20]\t Batch [4050][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [17][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [17][20]\t Batch [4150][5500]\t Training Loss 0.8520\t Accuracy 0.8516\n",
      "Epoch [17][20]\t Batch [4200][5500]\t Training Loss 0.8525\t Accuracy 0.8516\n",
      "Epoch [17][20]\t Batch [4250][5500]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "Epoch [17][20]\t Batch [4300][5500]\t Training Loss 0.8531\t Accuracy 0.8507\n",
      "Epoch [17][20]\t Batch [4350][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [17][20]\t Batch [4400][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [17][20]\t Batch [4450][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [17][20]\t Batch [4500][5500]\t Training Loss 0.8529\t Accuracy 0.8509\n",
      "Epoch [17][20]\t Batch [4550][5500]\t Training Loss 0.8534\t Accuracy 0.8508\n",
      "Epoch [17][20]\t Batch [4600][5500]\t Training Loss 0.8536\t Accuracy 0.8508\n",
      "Epoch [17][20]\t Batch [4650][5500]\t Training Loss 0.8542\t Accuracy 0.8508\n",
      "Epoch [17][20]\t Batch [4700][5500]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [17][20]\t Batch [4800][5500]\t Training Loss 0.8535\t Accuracy 0.8506\n",
      "Epoch [17][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [4900][5500]\t Training Loss 0.8520\t Accuracy 0.8511\n",
      "Epoch [17][20]\t Batch [4950][5500]\t Training Loss 0.8523\t Accuracy 0.8507\n",
      "Epoch [17][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [17][20]\t Batch [5050][5500]\t Training Loss 0.8541\t Accuracy 0.8503\n",
      "Epoch [17][20]\t Batch [5100][5500]\t Training Loss 0.8539\t Accuracy 0.8503\n",
      "Epoch [17][20]\t Batch [5150][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [17][20]\t Batch [5200][5500]\t Training Loss 0.8531\t Accuracy 0.8505\n",
      "Epoch [17][20]\t Batch [5250][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [17][20]\t Batch [5300][5500]\t Training Loss 0.8538\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [5350][5500]\t Training Loss 0.8536\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [5450][5500]\t Training Loss 0.8534\t Accuracy 0.8502\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [17]\t Average validation loss 0.7861\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [18][20]\t Batch [0][5500]\t Training Loss 0.7431\t Accuracy 0.9000\n",
      "Epoch [18][20]\t Batch [50][5500]\t Training Loss 0.8246\t Accuracy 0.8451\n",
      "Epoch [18][20]\t Batch [100][5500]\t Training Loss 0.8615\t Accuracy 0.8386\n",
      "Epoch [18][20]\t Batch [150][5500]\t Training Loss 0.8756\t Accuracy 0.8358\n",
      "Epoch [18][20]\t Batch [200][5500]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [250][5500]\t Training Loss 0.8423\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [18][20]\t Batch [400][5500]\t Training Loss 0.8378\t Accuracy 0.8554\n",
      "Epoch [18][20]\t Batch [450][5500]\t Training Loss 0.8376\t Accuracy 0.8557\n",
      "Epoch [18][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [18][20]\t Batch [550][5500]\t Training Loss 0.8310\t Accuracy 0.8559\n",
      "Epoch [18][20]\t Batch [600][5500]\t Training Loss 0.8310\t Accuracy 0.8572\n",
      "Epoch [18][20]\t Batch [650][5500]\t Training Loss 0.8249\t Accuracy 0.8590\n",
      "Epoch [18][20]\t Batch [700][5500]\t Training Loss 0.8249\t Accuracy 0.8588\n",
      "Epoch [18][20]\t Batch [750][5500]\t Training Loss 0.8340\t Accuracy 0.8571\n",
      "Epoch [18][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8562\n",
      "Epoch [18][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8555\n",
      "Epoch [18][20]\t Batch [900][5500]\t Training Loss 0.8408\t Accuracy 0.8534\n",
      "Epoch [18][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8545\n",
      "Epoch [18][20]\t Batch [1000][5500]\t Training Loss 0.8369\t Accuracy 0.8553\n",
      "Epoch [18][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8563\n",
      "Epoch [18][20]\t Batch [1100][5500]\t Training Loss 0.8318\t Accuracy 0.8570\n",
      "Epoch [18][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8571\n",
      "Epoch [18][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8548\n",
      "Epoch [18][20]\t Batch [1250][5500]\t Training Loss 0.8339\t Accuracy 0.8544\n",
      "Epoch [18][20]\t Batch [1300][5500]\t Training Loss 0.8363\t Accuracy 0.8535\n",
      "Epoch [18][20]\t Batch [1350][5500]\t Training Loss 0.8372\t Accuracy 0.8531\n",
      "Epoch [18][20]\t Batch [1400][5500]\t Training Loss 0.8388\t Accuracy 0.8522\n",
      "Epoch [18][20]\t Batch [1450][5500]\t Training Loss 0.8408\t Accuracy 0.8511\n",
      "Epoch [18][20]\t Batch [1500][5500]\t Training Loss 0.8446\t Accuracy 0.8489\n",
      "Epoch [18][20]\t Batch [1550][5500]\t Training Loss 0.8444\t Accuracy 0.8499\n",
      "Epoch [18][20]\t Batch [1600][5500]\t Training Loss 0.8464\t Accuracy 0.8490\n",
      "Epoch [18][20]\t Batch [1650][5500]\t Training Loss 0.8458\t Accuracy 0.8491\n",
      "Epoch [18][20]\t Batch [1700][5500]\t Training Loss 0.8476\t Accuracy 0.8487\n",
      "Epoch [18][20]\t Batch [1750][5500]\t Training Loss 0.8481\t Accuracy 0.8485\n",
      "Epoch [18][20]\t Batch [1800][5500]\t Training Loss 0.8514\t Accuracy 0.8473\n",
      "Epoch [18][20]\t Batch [1850][5500]\t Training Loss 0.8504\t Accuracy 0.8482\n",
      "Epoch [18][20]\t Batch [1900][5500]\t Training Loss 0.8497\t Accuracy 0.8498\n",
      "Epoch [18][20]\t Batch [1950][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [18][20]\t Batch [2000][5500]\t Training Loss 0.8478\t Accuracy 0.8501\n",
      "Epoch [18][20]\t Batch [2050][5500]\t Training Loss 0.8479\t Accuracy 0.8503\n",
      "Epoch [18][20]\t Batch [2100][5500]\t Training Loss 0.8483\t Accuracy 0.8495\n",
      "Epoch [18][20]\t Batch [2150][5500]\t Training Loss 0.8471\t Accuracy 0.8503\n",
      "Epoch [18][20]\t Batch [2200][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [2250][5500]\t Training Loss 0.8461\t Accuracy 0.8510\n",
      "Epoch [18][20]\t Batch [2300][5500]\t Training Loss 0.8461\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [2350][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [2400][5500]\t Training Loss 0.8455\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [2450][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [18][20]\t Batch [2500][5500]\t Training Loss 0.8466\t Accuracy 0.8504\n",
      "Epoch [18][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [18][20]\t Batch [2600][5500]\t Training Loss 0.8442\t Accuracy 0.8511\n",
      "Epoch [18][20]\t Batch [2650][5500]\t Training Loss 0.8444\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [2700][5500]\t Training Loss 0.8452\t Accuracy 0.8511\n",
      "Epoch [18][20]\t Batch [2750][5500]\t Training Loss 0.8455\t Accuracy 0.8510\n",
      "Epoch [18][20]\t Batch [2800][5500]\t Training Loss 0.8450\t Accuracy 0.8513\n",
      "Epoch [18][20]\t Batch [2850][5500]\t Training Loss 0.8437\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [2900][5500]\t Training Loss 0.8436\t Accuracy 0.8518\n",
      "Epoch [18][20]\t Batch [2950][5500]\t Training Loss 0.8438\t Accuracy 0.8516\n",
      "Epoch [18][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8510\n",
      "Epoch [18][20]\t Batch [3050][5500]\t Training Loss 0.8465\t Accuracy 0.8510\n",
      "Epoch [18][20]\t Batch [3100][5500]\t Training Loss 0.8481\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [3150][5500]\t Training Loss 0.8495\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [3200][5500]\t Training Loss 0.8506\t Accuracy 0.8505\n",
      "Epoch [18][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [18][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [3350][5500]\t Training Loss 0.8527\t Accuracy 0.8498\n",
      "Epoch [18][20]\t Batch [3400][5500]\t Training Loss 0.8514\t Accuracy 0.8508\n",
      "Epoch [18][20]\t Batch [3450][5500]\t Training Loss 0.8511\t Accuracy 0.8513\n",
      "Epoch [18][20]\t Batch [3500][5500]\t Training Loss 0.8516\t Accuracy 0.8510\n",
      "Epoch [18][20]\t Batch [3550][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [3600][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [3650][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [3700][5500]\t Training Loss 0.8504\t Accuracy 0.8518\n",
      "Epoch [18][20]\t Batch [3750][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [3800][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [18][20]\t Batch [3850][5500]\t Training Loss 0.8515\t Accuracy 0.8513\n",
      "Epoch [18][20]\t Batch [3900][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [3950][5500]\t Training Loss 0.8508\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [4000][5500]\t Training Loss 0.8516\t Accuracy 0.8513\n",
      "Epoch [18][20]\t Batch [4050][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [18][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [18][20]\t Batch [4150][5500]\t Training Loss 0.8520\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [4200][5500]\t Training Loss 0.8525\t Accuracy 0.8516\n",
      "Epoch [18][20]\t Batch [4250][5500]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "Epoch [18][20]\t Batch [4300][5500]\t Training Loss 0.8531\t Accuracy 0.8507\n",
      "Epoch [18][20]\t Batch [4350][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [4400][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [4450][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [18][20]\t Batch [4500][5500]\t Training Loss 0.8529\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [4550][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [18][20]\t Batch [4600][5500]\t Training Loss 0.8536\t Accuracy 0.8508\n",
      "Epoch [18][20]\t Batch [4650][5500]\t Training Loss 0.8542\t Accuracy 0.8507\n",
      "Epoch [18][20]\t Batch [4700][5500]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "Epoch [18][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [18][20]\t Batch [4800][5500]\t Training Loss 0.8535\t Accuracy 0.8506\n",
      "Epoch [18][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [18][20]\t Batch [4900][5500]\t Training Loss 0.8519\t Accuracy 0.8511\n",
      "Epoch [18][20]\t Batch [4950][5500]\t Training Loss 0.8523\t Accuracy 0.8507\n",
      "Epoch [18][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [18][20]\t Batch [5050][5500]\t Training Loss 0.8541\t Accuracy 0.8503\n",
      "Epoch [18][20]\t Batch [5100][5500]\t Training Loss 0.8539\t Accuracy 0.8503\n",
      "Epoch [18][20]\t Batch [5150][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [18][20]\t Batch [5200][5500]\t Training Loss 0.8531\t Accuracy 0.8504\n",
      "Epoch [18][20]\t Batch [5250][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [18][20]\t Batch [5300][5500]\t Training Loss 0.8538\t Accuracy 0.8499\n",
      "Epoch [18][20]\t Batch [5350][5500]\t Training Loss 0.8536\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [5450][5500]\t Training Loss 0.8534\t Accuracy 0.8501\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8534\t Average training accuracy 0.8501\n",
      "Epoch [18]\t Average validation loss 0.7860\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [19][20]\t Batch [0][5500]\t Training Loss 0.7431\t Accuracy 0.9000\n",
      "Epoch [19][20]\t Batch [50][5500]\t Training Loss 0.8246\t Accuracy 0.8451\n",
      "Epoch [19][20]\t Batch [100][5500]\t Training Loss 0.8615\t Accuracy 0.8386\n",
      "Epoch [19][20]\t Batch [150][5500]\t Training Loss 0.8756\t Accuracy 0.8358\n",
      "Epoch [19][20]\t Batch [200][5500]\t Training Loss 0.8555\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [250][5500]\t Training Loss 0.8423\t Accuracy 0.8486\n",
      "Epoch [19][20]\t Batch [300][5500]\t Training Loss 0.8346\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [350][5500]\t Training Loss 0.8401\t Accuracy 0.8524\n",
      "Epoch [19][20]\t Batch [400][5500]\t Training Loss 0.8378\t Accuracy 0.8554\n",
      "Epoch [19][20]\t Batch [450][5500]\t Training Loss 0.8376\t Accuracy 0.8557\n",
      "Epoch [19][20]\t Batch [500][5500]\t Training Loss 0.8329\t Accuracy 0.8561\n",
      "Epoch [19][20]\t Batch [550][5500]\t Training Loss 0.8310\t Accuracy 0.8559\n",
      "Epoch [19][20]\t Batch [600][5500]\t Training Loss 0.8310\t Accuracy 0.8572\n",
      "Epoch [19][20]\t Batch [650][5500]\t Training Loss 0.8249\t Accuracy 0.8590\n",
      "Epoch [19][20]\t Batch [700][5500]\t Training Loss 0.8249\t Accuracy 0.8588\n",
      "Epoch [19][20]\t Batch [750][5500]\t Training Loss 0.8339\t Accuracy 0.8571\n",
      "Epoch [19][20]\t Batch [800][5500]\t Training Loss 0.8362\t Accuracy 0.8562\n",
      "Epoch [19][20]\t Batch [850][5500]\t Training Loss 0.8388\t Accuracy 0.8555\n",
      "Epoch [19][20]\t Batch [900][5500]\t Training Loss 0.8408\t Accuracy 0.8534\n",
      "Epoch [19][20]\t Batch [950][5500]\t Training Loss 0.8397\t Accuracy 0.8545\n",
      "Epoch [19][20]\t Batch [1000][5500]\t Training Loss 0.8369\t Accuracy 0.8553\n",
      "Epoch [19][20]\t Batch [1050][5500]\t Training Loss 0.8338\t Accuracy 0.8563\n",
      "Epoch [19][20]\t Batch [1100][5500]\t Training Loss 0.8318\t Accuracy 0.8570\n",
      "Epoch [19][20]\t Batch [1150][5500]\t Training Loss 0.8294\t Accuracy 0.8571\n",
      "Epoch [19][20]\t Batch [1200][5500]\t Training Loss 0.8334\t Accuracy 0.8548\n",
      "Epoch [19][20]\t Batch [1250][5500]\t Training Loss 0.8339\t Accuracy 0.8544\n",
      "Epoch [19][20]\t Batch [1300][5500]\t Training Loss 0.8362\t Accuracy 0.8535\n",
      "Epoch [19][20]\t Batch [1350][5500]\t Training Loss 0.8372\t Accuracy 0.8531\n",
      "Epoch [19][20]\t Batch [1400][5500]\t Training Loss 0.8388\t Accuracy 0.8522\n",
      "Epoch [19][20]\t Batch [1450][5500]\t Training Loss 0.8408\t Accuracy 0.8511\n",
      "Epoch [19][20]\t Batch [1500][5500]\t Training Loss 0.8446\t Accuracy 0.8489\n",
      "Epoch [19][20]\t Batch [1550][5500]\t Training Loss 0.8444\t Accuracy 0.8499\n",
      "Epoch [19][20]\t Batch [1600][5500]\t Training Loss 0.8464\t Accuracy 0.8490\n",
      "Epoch [19][20]\t Batch [1650][5500]\t Training Loss 0.8458\t Accuracy 0.8491\n",
      "Epoch [19][20]\t Batch [1700][5500]\t Training Loss 0.8476\t Accuracy 0.8487\n",
      "Epoch [19][20]\t Batch [1750][5500]\t Training Loss 0.8481\t Accuracy 0.8485\n",
      "Epoch [19][20]\t Batch [1800][5500]\t Training Loss 0.8514\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [1850][5500]\t Training Loss 0.8504\t Accuracy 0.8482\n",
      "Epoch [19][20]\t Batch [1900][5500]\t Training Loss 0.8497\t Accuracy 0.8498\n",
      "Epoch [19][20]\t Batch [1950][5500]\t Training Loss 0.8498\t Accuracy 0.8498\n",
      "Epoch [19][20]\t Batch [2000][5500]\t Training Loss 0.8478\t Accuracy 0.8501\n",
      "Epoch [19][20]\t Batch [2050][5500]\t Training Loss 0.8478\t Accuracy 0.8503\n",
      "Epoch [19][20]\t Batch [2100][5500]\t Training Loss 0.8483\t Accuracy 0.8495\n",
      "Epoch [19][20]\t Batch [2150][5500]\t Training Loss 0.8471\t Accuracy 0.8503\n",
      "Epoch [19][20]\t Batch [2200][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [2250][5500]\t Training Loss 0.8461\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [2300][5500]\t Training Loss 0.8461\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [2350][5500]\t Training Loss 0.8449\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [2400][5500]\t Training Loss 0.8455\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [2450][5500]\t Training Loss 0.8453\t Accuracy 0.8511\n",
      "Epoch [19][20]\t Batch [2500][5500]\t Training Loss 0.8466\t Accuracy 0.8504\n",
      "Epoch [19][20]\t Batch [2550][5500]\t Training Loss 0.8450\t Accuracy 0.8508\n",
      "Epoch [19][20]\t Batch [2600][5500]\t Training Loss 0.8442\t Accuracy 0.8511\n",
      "Epoch [19][20]\t Batch [2650][5500]\t Training Loss 0.8444\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [2700][5500]\t Training Loss 0.8452\t Accuracy 0.8511\n",
      "Epoch [19][20]\t Batch [2750][5500]\t Training Loss 0.8455\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [2800][5500]\t Training Loss 0.8450\t Accuracy 0.8513\n",
      "Epoch [19][20]\t Batch [2850][5500]\t Training Loss 0.8437\t Accuracy 0.8519\n",
      "Epoch [19][20]\t Batch [2900][5500]\t Training Loss 0.8436\t Accuracy 0.8518\n",
      "Epoch [19][20]\t Batch [2950][5500]\t Training Loss 0.8438\t Accuracy 0.8516\n",
      "Epoch [19][20]\t Batch [3000][5500]\t Training Loss 0.8455\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [3050][5500]\t Training Loss 0.8465\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [3100][5500]\t Training Loss 0.8481\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [3150][5500]\t Training Loss 0.8495\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [3200][5500]\t Training Loss 0.8506\t Accuracy 0.8505\n",
      "Epoch [19][20]\t Batch [3250][5500]\t Training Loss 0.8525\t Accuracy 0.8496\n",
      "Epoch [19][20]\t Batch [3300][5500]\t Training Loss 0.8522\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [3350][5500]\t Training Loss 0.8527\t Accuracy 0.8498\n",
      "Epoch [19][20]\t Batch [3400][5500]\t Training Loss 0.8513\t Accuracy 0.8508\n",
      "Epoch [19][20]\t Batch [3450][5500]\t Training Loss 0.8511\t Accuracy 0.8513\n",
      "Epoch [19][20]\t Batch [3500][5500]\t Training Loss 0.8516\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [3550][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [3600][5500]\t Training Loss 0.8517\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [3650][5500]\t Training Loss 0.8516\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [3700][5500]\t Training Loss 0.8504\t Accuracy 0.8518\n",
      "Epoch [19][20]\t Batch [3750][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [3800][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [19][20]\t Batch [3850][5500]\t Training Loss 0.8514\t Accuracy 0.8513\n",
      "Epoch [19][20]\t Batch [3900][5500]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [3950][5500]\t Training Loss 0.8508\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [4000][5500]\t Training Loss 0.8516\t Accuracy 0.8513\n",
      "Epoch [19][20]\t Batch [4050][5500]\t Training Loss 0.8513\t Accuracy 0.8514\n",
      "Epoch [19][20]\t Batch [4100][5500]\t Training Loss 0.8508\t Accuracy 0.8517\n",
      "Epoch [19][20]\t Batch [4150][5500]\t Training Loss 0.8520\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [4200][5500]\t Training Loss 0.8525\t Accuracy 0.8516\n",
      "Epoch [19][20]\t Batch [4250][5500]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "Epoch [19][20]\t Batch [4300][5500]\t Training Loss 0.8531\t Accuracy 0.8507\n",
      "Epoch [19][20]\t Batch [4350][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [4400][5500]\t Training Loss 0.8526\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [4450][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [19][20]\t Batch [4500][5500]\t Training Loss 0.8529\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [4550][5500]\t Training Loss 0.8533\t Accuracy 0.8507\n",
      "Epoch [19][20]\t Batch [4600][5500]\t Training Loss 0.8536\t Accuracy 0.8508\n",
      "Epoch [19][20]\t Batch [4650][5500]\t Training Loss 0.8542\t Accuracy 0.8507\n",
      "Epoch [19][20]\t Batch [4700][5500]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [4750][5500]\t Training Loss 0.8537\t Accuracy 0.8508\n",
      "Epoch [19][20]\t Batch [4800][5500]\t Training Loss 0.8535\t Accuracy 0.8506\n",
      "Epoch [19][20]\t Batch [4850][5500]\t Training Loss 0.8523\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [4900][5500]\t Training Loss 0.8519\t Accuracy 0.8511\n",
      "Epoch [19][20]\t Batch [4950][5500]\t Training Loss 0.8523\t Accuracy 0.8507\n",
      "Epoch [19][20]\t Batch [5000][5500]\t Training Loss 0.8536\t Accuracy 0.8504\n",
      "Epoch [19][20]\t Batch [5050][5500]\t Training Loss 0.8541\t Accuracy 0.8503\n",
      "Epoch [19][20]\t Batch [5100][5500]\t Training Loss 0.8539\t Accuracy 0.8503\n",
      "Epoch [19][20]\t Batch [5150][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [19][20]\t Batch [5200][5500]\t Training Loss 0.8531\t Accuracy 0.8504\n",
      "Epoch [19][20]\t Batch [5250][5500]\t Training Loss 0.8535\t Accuracy 0.8502\n",
      "Epoch [19][20]\t Batch [5300][5500]\t Training Loss 0.8538\t Accuracy 0.8499\n",
      "Epoch [19][20]\t Batch [5350][5500]\t Training Loss 0.8536\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [5400][5500]\t Training Loss 0.8537\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [5450][5500]\t Training Loss 0.8534\t Accuracy 0.8502\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8533\t Average training accuracy 0.8501\n",
      "Epoch [19]\t Average validation loss 0.7860\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8277\t Accuracy 0.8600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8332\t Accuracy 0.8561\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8374\t Accuracy 0.8570\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8508\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8482\t Accuracy 0.8502\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8460\t Accuracy 0.8505\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8505\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8505\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8508\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8526\t Accuracy 0.8504\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8500\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8527\t Average training accuracy 0.8494\n",
      "Epoch [0]\t Average validation loss 0.7860\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8255\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8329\t Accuracy 0.8588\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8372\t Accuracy 0.8582\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8510\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8513\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8511\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8512\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8508\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8505\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8527\t Average training accuracy 0.8498\n",
      "Epoch [1]\t Average validation loss 0.7861\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8250\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8329\t Accuracy 0.8584\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8372\t Accuracy 0.8580\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8521\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8513\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8518\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8517\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8515\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8516\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8526\t Accuracy 0.8511\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8507\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8528\t Average training accuracy 0.8501\n",
      "Epoch [2]\t Average validation loss 0.7862\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8329\t Accuracy 0.8588\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8372\t Accuracy 0.8581\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8522\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8514\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8519\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8518\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8516\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8516\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8526\t Accuracy 0.8511\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8508\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8528\t Average training accuracy 0.8502\n",
      "Epoch [3]\t Average validation loss 0.7863\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8247\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8329\t Accuracy 0.8586\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8372\t Accuracy 0.8580\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8521\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8520\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8518\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8517\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8517\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8512\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8509\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8529\t Average training accuracy 0.8503\n",
      "Epoch [4]\t Average validation loss 0.7863\t Average validation accuracy 0.8880\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8246\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8329\t Accuracy 0.8588\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8372\t Accuracy 0.8578\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8520\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8520\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8518\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8516\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8517\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8512\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8509\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8529\t Average training accuracy 0.8503\n",
      "Epoch [5]\t Average validation loss 0.7863\t Average validation accuracy 0.8880\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8246\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8329\t Accuracy 0.8588\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8372\t Accuracy 0.8578\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8521\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8460\t Accuracy 0.8520\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8519\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8517\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8517\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8509\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8530\t Average training accuracy 0.8503\n",
      "Epoch [6]\t Average validation loss 0.7864\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8245\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8329\t Accuracy 0.8586\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8372\t Accuracy 0.8577\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8519\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8513\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8520\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8518\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8517\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8517\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8530\t Average training accuracy 0.8503\n",
      "Epoch [7]\t Average validation loss 0.7864\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8244\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8328\t Accuracy 0.8582\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8372\t Accuracy 0.8575\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8442\t Accuracy 0.8519\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8513\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8520\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8518\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8517\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8518\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8530\t Average training accuracy 0.8503\n",
      "Epoch [8]\t Average validation loss 0.7864\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8244\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8328\t Accuracy 0.8582\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8574\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8442\t Accuracy 0.8517\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8512\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8520\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8518\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8517\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8517\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8530\t Average training accuracy 0.8503\n",
      "Epoch [9]\t Average validation loss 0.7864\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8243\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8328\t Accuracy 0.8582\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8574\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8442\t Accuracy 0.8518\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8512\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8520\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8518\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8517\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8517\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8530\t Average training accuracy 0.8503\n",
      "Epoch [10]\t Average validation loss 0.7864\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8243\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8328\t Accuracy 0.8582\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8574\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8442\t Accuracy 0.8518\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8512\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8520\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8518\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8517\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8517\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8530\t Average training accuracy 0.8503\n",
      "Epoch [11]\t Average validation loss 0.7864\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8243\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8327\t Accuracy 0.8582\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8575\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8441\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8513\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8521\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8518\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8530\t Average training accuracy 0.8503\n",
      "Epoch [12]\t Average validation loss 0.7864\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8242\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8327\t Accuracy 0.8582\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8370\t Accuracy 0.8575\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8441\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8513\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8521\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8518\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8518\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8510\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8530\t Average training accuracy 0.8503\n",
      "Epoch [13]\t Average validation loss 0.7864\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8242\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8327\t Accuracy 0.8582\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8370\t Accuracy 0.8575\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8441\t Accuracy 0.8519\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8521\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8519\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8518\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8518\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8510\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8530\t Average training accuracy 0.8504\n",
      "Epoch [14]\t Average validation loss 0.7863\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8241\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8327\t Accuracy 0.8582\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8370\t Accuracy 0.8575\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8441\t Accuracy 0.8519\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8521\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8519\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8518\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8518\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8510\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8530\t Average training accuracy 0.8504\n",
      "Epoch [15]\t Average validation loss 0.7863\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8241\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8582\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8370\t Accuracy 0.8576\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8521\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8514\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8522\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8520\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8519\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8519\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8514\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8511\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8530\t Average training accuracy 0.8504\n",
      "Epoch [16]\t Average validation loss 0.7863\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8241\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8582\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8369\t Accuracy 0.8576\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8521\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8522\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8520\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8519\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8519\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8514\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8511\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8529\t Average training accuracy 0.8504\n",
      "Epoch [17]\t Average validation loss 0.7863\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8240\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8582\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8369\t Accuracy 0.8576\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8522\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8522\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8520\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8526\t Accuracy 0.8514\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8511\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8529\t Average training accuracy 0.8505\n",
      "Epoch [18]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8240\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8582\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8369\t Accuracy 0.8576\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8522\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8522\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8520\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8519\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8519\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8526\t Accuracy 0.8514\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8511\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8529\t Average training accuracy 0.8505\n",
      "Epoch [19]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [0][20]\t Batch [0][55]\t Training Loss 0.8588\t Accuracy 0.8400\n",
      "Epoch [0][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [0]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [1][20]\t Batch [0][55]\t Training Loss 0.8588\t Accuracy 0.8400\n",
      "Epoch [1][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [1]\t Average validation loss 0.7863\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [2][20]\t Batch [0][55]\t Training Loss 0.8588\t Accuracy 0.8400\n",
      "Epoch [2][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [2]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [3][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [3][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [3]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [4][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [4][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [4]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [5][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [5][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [5]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [6][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [6]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [7][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [7][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [7]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [8][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [8][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [8]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [9][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [9][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [9]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [10][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [10][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [10]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [11][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [11][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [11]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [12][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [12][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [12]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [13][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [13][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [13]\t Average validation loss 0.7863\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [14][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [14][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [14]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [15][20]\t Batch [0][55]\t Training Loss 0.8587\t Accuracy 0.8400\n",
      "Epoch [15][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [15]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [16][20]\t Batch [0][55]\t Training Loss 0.8586\t Accuracy 0.8400\n",
      "Epoch [16][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [16]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [17][20]\t Batch [0][55]\t Training Loss 0.8586\t Accuracy 0.8400\n",
      "Epoch [17][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [17]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [18][20]\t Batch [0][55]\t Training Loss 0.8586\t Accuracy 0.8400\n",
      "Epoch [18][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [18]\t Average validation loss 0.7863\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [19][20]\t Batch [0][55]\t Training Loss 0.8586\t Accuracy 0.8400\n",
      "Epoch [19][20]\t Batch [50][55]\t Training Loss 0.8533\t Accuracy 0.8508\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [19]\t Average validation loss 0.7863\t Average validation accuracy 0.8874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [1, 10, 100, 1000]\n",
    "losses, accs = [], []\n",
    "for batch_size in batch_sizes:\n",
    "    reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n",
    "    losses.append(relu_loss)\n",
    "    accs.append(relu_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'batch_size=1': [losses[0], accs[0]], \n",
    "                   'batch_size=10': [losses[1], accs[1]],\n",
    "                   'batch_size=100': [losses[2], accs[2]],\n",
    "                    'batch_size=1000':[losses[3], accs[3]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8586\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8582\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8526\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8520\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8525\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8523\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8521\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [0]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8586\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8526\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8525\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8523\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [1]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8586\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8526\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8525\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8520\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [2]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8586\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8525\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8524\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8519\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [3]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8586\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8580\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8525\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8518\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8524\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8519\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [4]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8586\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8580\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8525\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8518\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8519\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [5]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8586\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8525\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8519\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [6]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8525\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [7]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8526\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [8]\t Average validation loss 0.7863\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8526\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [9]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8526\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [10]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8526\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [11]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8526\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [12]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8526\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [13]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8526\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8518\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8510\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [14]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8236\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8581\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8526\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8519\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8518\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8510\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [15]\t Average validation loss 0.7863\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8235\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8581\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8526\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8518\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8513\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8510\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [16]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8235\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8581\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8526\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8511\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [17]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8235\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8581\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8526\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8521\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8511\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [18]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8235\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8586\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8581\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8526\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8520\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8525\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8522\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8511\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8528\t Average training accuracy 0.8504\n",
      "Epoch [19]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8235\t Accuracy 0.8500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8584\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8523\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8516\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8524\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8520\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8519\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8511\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8529\t Average training accuracy 0.8505\n",
      "Epoch [0]\t Average validation loss 0.7862\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8578\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8525\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8529\t Average training accuracy 0.8505\n",
      "Epoch [1]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8578\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8518\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8525\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8510\t Accuracy 0.8521\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [2]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8578\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8518\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8525\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [3]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8578\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8518\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8525\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [4]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8582\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8578\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8518\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8525\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [5]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8582\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8578\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8525\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8522\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [6]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8526\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [7]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8526\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8512\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [8]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8526\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8512\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [9]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8526\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8512\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8528\t Average training accuracy 0.8505\n",
      "Epoch [10]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8526\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [11]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8525\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8526\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [12]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8525\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8526\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [13]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8525\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8456\t Accuracy 0.8526\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [14]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8525\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8455\t Accuracy 0.8526\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [15]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8525\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8455\t Accuracy 0.8526\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [16]\t Average validation loss 0.7862\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8525\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8455\t Accuracy 0.8526\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8509\t Accuracy 0.8520\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [17]\t Average validation loss 0.7861\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8579\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8525\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8455\t Accuracy 0.8526\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8508\t Accuracy 0.8521\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8516\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [18]\t Average validation loss 0.7861\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8584\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8366\t Accuracy 0.8579\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8525\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8477\t Accuracy 0.8519\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8455\t Accuracy 0.8526\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8452\t Accuracy 0.8523\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8509\t Accuracy 0.8521\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8508\t Accuracy 0.8521\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8525\t Accuracy 0.8516\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8530\t Accuracy 0.8513\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8528\t Average training accuracy 0.8506\n",
      "Epoch [19]\t Average validation loss 0.7861\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8237\t Accuracy 0.8500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8582\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8366\t Accuracy 0.8575\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8516\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8512\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8521\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8522\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8533\t Accuracy 0.8515\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [0]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8246\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8323\t Accuracy 0.8582\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8366\t Accuracy 0.8573\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8511\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8521\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8522\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8522\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [1]\t Average validation loss 0.7860\t Average validation accuracy 0.8880\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8247\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8366\t Accuracy 0.8574\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8437\t Accuracy 0.8515\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8511\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8522\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8522\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [2]\t Average validation loss 0.7860\t Average validation accuracy 0.8880\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8247\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8511\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8522\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8522\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8526\t Accuracy 0.8518\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [3]\t Average validation loss 0.7860\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8247\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8511\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8521\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8522\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8522\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [4]\t Average validation loss 0.7860\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8247\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8511\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8523\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [5]\t Average validation loss 0.7860\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8523\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [6]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8512\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8520\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8523\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [7]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8511\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8510\t Accuracy 0.8523\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [8]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8511\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [9]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8511\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [10]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8511\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [11]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8511\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [12]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8511\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [13]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8511\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8519\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [14]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8512\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8520\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [15]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8512\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8520\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [16]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8512\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8520\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [17]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8574\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8512\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8520\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [18]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8324\t Accuracy 0.8582\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8575\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8438\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8479\t Accuracy 0.8512\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8457\t Accuracy 0.8520\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8454\t Accuracy 0.8522\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8511\t Accuracy 0.8523\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8527\t Accuracy 0.8518\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8532\t Accuracy 0.8515\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8530\t Average training accuracy 0.8508\n",
      "Epoch [19]\t Average validation loss 0.7860\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8248\t Accuracy 0.8500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8578\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8570\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8506\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8501\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8511\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8515\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8519\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8515\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8511\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8531\t Average training accuracy 0.8505\n",
      "Epoch [0]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8256\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8575\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8569\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8505\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8458\t Accuracy 0.8510\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8517\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8528\t Accuracy 0.8514\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [1]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8256\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8325\t Accuracy 0.8575\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8367\t Accuracy 0.8568\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8506\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8501\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8515\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8519\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8528\t Accuracy 0.8515\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [2]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8257\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8568\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8505\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8501\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8515\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8528\t Accuracy 0.8514\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [3]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8257\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8568\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8505\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8501\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8515\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8528\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [4]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8257\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8439\t Accuracy 0.8505\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8455\t Accuracy 0.8515\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [5]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8480\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [6]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [7]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [8]\t Average validation loss 0.7860\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [9]\t Average validation loss 0.7860\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [10]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [11]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [12]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8517\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [13]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8501\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [14]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8501\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [15]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8501\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [16]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8501\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [17]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8501\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [18]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8326\t Accuracy 0.8575\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8567\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8440\t Accuracy 0.8505\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8481\t Accuracy 0.8501\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8510\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8456\t Accuracy 0.8515\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8513\t Accuracy 0.8518\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8512\t Accuracy 0.8518\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8514\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8534\t Accuracy 0.8510\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8531\t Average training accuracy 0.8504\n",
      "Epoch [19]\t Average validation loss 0.7859\t Average validation accuracy 0.8868\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8258\t Accuracy 0.8500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8327\t Accuracy 0.8561\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8368\t Accuracy 0.8558\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8441\t Accuracy 0.8495\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8484\t Accuracy 0.8499\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8463\t Accuracy 0.8506\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8459\t Accuracy 0.8512\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8516\t Accuracy 0.8515\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8516\t Accuracy 0.8516\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8532\t Accuracy 0.8512\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8537\t Accuracy 0.8507\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8534\t Average training accuracy 0.8504\n",
      "Epoch [0]\t Average validation loss 0.7858\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8273\t Accuracy 0.8600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8328\t Accuracy 0.8559\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8369\t Accuracy 0.8555\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8442\t Accuracy 0.8493\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8484\t Accuracy 0.8497\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8463\t Accuracy 0.8505\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8459\t Accuracy 0.8511\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8516\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8516\t Accuracy 0.8515\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8532\t Accuracy 0.8511\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8537\t Accuracy 0.8507\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8534\t Average training accuracy 0.8503\n",
      "Epoch [1]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8275\t Accuracy 0.8600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8329\t Accuracy 0.8559\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8370\t Accuracy 0.8553\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8492\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8485\t Accuracy 0.8495\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8503\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8516\t Accuracy 0.8514\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8532\t Accuracy 0.8510\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8537\t Accuracy 0.8506\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8534\t Average training accuracy 0.8503\n",
      "Epoch [2]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8330\t Accuracy 0.8559\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8443\t Accuracy 0.8492\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8503\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8537\t Accuracy 0.8506\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8535\t Average training accuracy 0.8503\n",
      "Epoch [3]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8330\t Accuracy 0.8559\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8503\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8535\t Average training accuracy 0.8503\n",
      "Epoch [4]\t Average validation loss 0.7860\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8503\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8535\t Average training accuracy 0.8503\n",
      "Epoch [5]\t Average validation loss 0.7860\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8465\t Accuracy 0.8503\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [6]\t Average validation loss 0.7860\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8465\t Accuracy 0.8503\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [7]\t Average validation loss 0.7860\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8465\t Accuracy 0.8503\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [8]\t Average validation loss 0.7860\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8465\t Accuracy 0.8503\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [9]\t Average validation loss 0.7860\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8465\t Accuracy 0.8503\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [10]\t Average validation loss 0.7860\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8465\t Accuracy 0.8503\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8518\t Accuracy 0.8513\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [11]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8492\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8465\t Accuracy 0.8503\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8510\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8506\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [12]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8553\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8491\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8465\t Accuracy 0.8503\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8505\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [13]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8552\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8491\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8495\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8503\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8510\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8514\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8505\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [14]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8552\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8491\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8494\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8502\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8509\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8505\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [15]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8552\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8491\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8494\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8502\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8509\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8505\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [16]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8552\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8491\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8494\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8502\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8509\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8505\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [17]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8559\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8552\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8491\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8494\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8502\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8538\t Accuracy 0.8505\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [18]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8330\t Accuracy 0.8559\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8371\t Accuracy 0.8552\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8444\t Accuracy 0.8491\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8486\t Accuracy 0.8494\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8502\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8460\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8517\t Accuracy 0.8513\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8509\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8537\t Accuracy 0.8505\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8535\t Average training accuracy 0.8502\n",
      "Epoch [19]\t Average validation loss 0.7859\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8340\t Accuracy 0.8527\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8378\t Accuracy 0.8507\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8459\t Accuracy 0.8462\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8502\t Accuracy 0.8476\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8478\t Accuracy 0.8485\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8468\t Accuracy 0.8494\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8526\t Accuracy 0.8495\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8524\t Accuracy 0.8494\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8541\t Accuracy 0.8492\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8545\t Accuracy 0.8484\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8545\t Average training accuracy 0.8480\n",
      "Epoch [0]\t Average validation loss 0.7889\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8376\t Accuracy 0.8300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8368\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8401\t Accuracy 0.8488\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8479\t Accuracy 0.8448\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8464\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8493\t Accuracy 0.8473\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8481\t Accuracy 0.8484\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8534\t Accuracy 0.8487\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8485\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8478\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [1]\t Average validation loss 0.7890\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8377\t Accuracy 0.8300\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8488\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8448\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8463\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8473\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8484\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8487\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8487\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8485\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8555\t Accuracy 0.8477\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [2]\t Average validation loss 0.7890\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8377\t Accuracy 0.8300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8403\t Accuracy 0.8488\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8521\t Accuracy 0.8463\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8473\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8485\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8555\t Accuracy 0.8477\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [3]\t Average validation loss 0.7890\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8377\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8403\t Accuracy 0.8487\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8448\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8521\t Accuracy 0.8462\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8484\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8555\t Accuracy 0.8477\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8553\t Average training accuracy 0.8473\n",
      "Epoch [4]\t Average validation loss 0.7890\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8377\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8403\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8448\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8521\t Accuracy 0.8462\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8539\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8484\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8555\t Accuracy 0.8477\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [5]\t Average validation loss 0.7890\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8377\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8403\t Accuracy 0.8487\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8448\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8521\t Accuracy 0.8462\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8484\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8555\t Accuracy 0.8477\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [6]\t Average validation loss 0.7890\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8377\t Accuracy 0.8300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8403\t Accuracy 0.8487\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8448\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8521\t Accuracy 0.8462\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8485\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8555\t Accuracy 0.8477\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [7]\t Average validation loss 0.7889\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8376\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8448\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8521\t Accuracy 0.8462\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8485\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8555\t Accuracy 0.8477\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [8]\t Average validation loss 0.7889\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8376\t Accuracy 0.8300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8521\t Accuracy 0.8462\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8484\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8555\t Accuracy 0.8477\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [9]\t Average validation loss 0.7889\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8376\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [10]\t Average validation loss 0.7889\t Average validation accuracy 0.8786\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8376\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8485\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [11]\t Average validation loss 0.7889\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8376\t Accuracy 0.8300\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8484\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [12]\t Average validation loss 0.7889\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8376\t Accuracy 0.8300\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8500\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8485\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [13]\t Average validation loss 0.7889\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8376\t Accuracy 0.8300\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8368\t Accuracy 0.8500\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8483\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8485\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [14]\t Average validation loss 0.7889\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8375\t Accuracy 0.8300\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8368\t Accuracy 0.8500\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8472\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8481\t Accuracy 0.8483\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8485\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8553\t Average training accuracy 0.8474\n",
      "Epoch [15]\t Average validation loss 0.7889\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8375\t Accuracy 0.8300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8368\t Accuracy 0.8498\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8486\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8446\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8493\t Accuracy 0.8472\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8481\t Accuracy 0.8483\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8485\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8552\t Average training accuracy 0.8474\n",
      "Epoch [16]\t Average validation loss 0.7889\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8375\t Accuracy 0.8300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8368\t Accuracy 0.8500\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8493\t Accuracy 0.8472\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8481\t Accuracy 0.8483\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8486\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8484\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8552\t Average training accuracy 0.8474\n",
      "Epoch [17]\t Average validation loss 0.7888\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8375\t Accuracy 0.8300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8368\t Accuracy 0.8498\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8446\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8462\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8493\t Accuracy 0.8472\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8481\t Accuracy 0.8483\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8534\t Accuracy 0.8486\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8484\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8477\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8552\t Average training accuracy 0.8474\n",
      "Epoch [18]\t Average validation loss 0.7888\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8375\t Accuracy 0.8300\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8368\t Accuracy 0.8500\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8402\t Accuracy 0.8487\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8480\t Accuracy 0.8447\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8463\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8493\t Accuracy 0.8473\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8481\t Accuracy 0.8484\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8487\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8534\t Accuracy 0.8487\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8550\t Accuracy 0.8485\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8478\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8552\t Average training accuracy 0.8474\n",
      "Epoch [19]\t Average validation loss 0.7888\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.8375\t Accuracy 0.8300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.8379\t Accuracy 0.8469\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.8412\t Accuracy 0.8461\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.8493\t Accuracy 0.8428\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.8528\t Accuracy 0.8439\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.8501\t Accuracy 0.8455\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.8488\t Accuracy 0.8462\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.8548\t Accuracy 0.8460\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8544\t Accuracy 0.8457\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8562\t Accuracy 0.8454\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8567\t Accuracy 0.8445\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8568\t Average training accuracy 0.8439\n",
      "Epoch [0]\t Average validation loss 0.7930\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.8447\t Accuracy 0.8300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8451\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8443\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8418\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8431\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.8508\t Accuracy 0.8450\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.8494\t Accuracy 0.8456\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8455\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8442\n",
      "\n",
      "Epoch [1]\t Average training loss 0.8571\t Average training accuracy 0.8436\n",
      "Epoch [1]\t Average validation loss 0.7930\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.8447\t Accuracy 0.8300\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8446\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.8504\t Accuracy 0.8420\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8433\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.8508\t Accuracy 0.8451\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.8494\t Accuracy 0.8457\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.8554\t Accuracy 0.8456\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8442\n",
      "\n",
      "Epoch [2]\t Average training loss 0.8572\t Average training accuracy 0.8436\n",
      "Epoch [2]\t Average validation loss 0.7930\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.8447\t Accuracy 0.8300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8504\t Accuracy 0.8421\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8434\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8508\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8494\t Accuracy 0.8457\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8554\t Accuracy 0.8456\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8549\t Accuracy 0.8454\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8451\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8442\n",
      "\n",
      "Epoch [3]\t Average training loss 0.8572\t Average training accuracy 0.8436\n",
      "Epoch [3]\t Average validation loss 0.7930\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8447\t Accuracy 0.8300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8504\t Accuracy 0.8420\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8433\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8508\t Accuracy 0.8451\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8494\t Accuracy 0.8457\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8554\t Accuracy 0.8455\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8442\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8572\t Average training accuracy 0.8436\n",
      "Epoch [4]\t Average validation loss 0.7930\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.8447\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8446\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8421\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8433\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.8508\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.8494\t Accuracy 0.8457\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8455\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8443\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8571\t Average training accuracy 0.8437\n",
      "Epoch [5]\t Average validation loss 0.7930\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8447\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8446\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8421\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8433\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8508\t Accuracy 0.8451\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8494\t Accuracy 0.8457\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8455\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8549\t Accuracy 0.8453\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8442\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8571\t Average training accuracy 0.8436\n",
      "Epoch [6]\t Average validation loss 0.7930\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8444\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8432\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8508\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8456\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8455\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8549\t Accuracy 0.8452\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8450\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8442\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8571\t Average training accuracy 0.8436\n",
      "Epoch [7]\t Average validation loss 0.7930\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8444\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8432\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8456\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8449\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8442\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8571\t Average training accuracy 0.8436\n",
      "Epoch [8]\t Average validation loss 0.7929\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8444\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8432\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8449\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8442\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8571\t Average training accuracy 0.8436\n",
      "Epoch [9]\t Average validation loss 0.7929\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8444\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8432\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8449\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8441\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8571\t Average training accuracy 0.8435\n",
      "Epoch [10]\t Average validation loss 0.7929\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8444\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8432\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8455\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8449\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8441\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8571\t Average training accuracy 0.8435\n",
      "Epoch [11]\t Average validation loss 0.7929\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8444\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8432\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8455\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8448\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8571\t Accuracy 0.8441\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8571\t Average training accuracy 0.8435\n",
      "Epoch [12]\t Average validation loss 0.7929\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.8395\t Accuracy 0.8455\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8444\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8432\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8454\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8566\t Accuracy 0.8448\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8570\t Accuracy 0.8441\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8571\t Average training accuracy 0.8435\n",
      "Epoch [13]\t Average validation loss 0.7929\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8453\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8443\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8418\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8431\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8454\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8453\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8565\t Accuracy 0.8448\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8570\t Accuracy 0.8440\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8571\t Average training accuracy 0.8434\n",
      "Epoch [14]\t Average validation loss 0.7929\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8444\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8432\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8455\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8570\t Accuracy 0.8442\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8571\t Average training accuracy 0.8435\n",
      "Epoch [15]\t Average validation loss 0.7929\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8446\t Accuracy 0.8300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8453\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8444\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8432\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8448\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8455\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8553\t Accuracy 0.8454\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8570\t Accuracy 0.8442\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8571\t Average training accuracy 0.8435\n",
      "Epoch [16]\t Average validation loss 0.7929\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8445\t Accuracy 0.8300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8444\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8419\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8432\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8448\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8455\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8552\t Accuracy 0.8453\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8451\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8570\t Accuracy 0.8441\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8570\t Average training accuracy 0.8435\n",
      "Epoch [17]\t Average validation loss 0.7929\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8445\t Accuracy 0.8300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8453\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8444\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8502\t Accuracy 0.8420\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8433\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8456\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8552\t Accuracy 0.8454\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8570\t Accuracy 0.8441\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8570\t Average training accuracy 0.8435\n",
      "Epoch [18]\t Average validation loss 0.7929\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8445\t Accuracy 0.8300\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8453\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8443\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8502\t Accuracy 0.8419\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8432\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8456\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8552\t Accuracy 0.8454\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8548\t Accuracy 0.8452\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8565\t Accuracy 0.8449\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8570\t Accuracy 0.8441\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8570\t Average training accuracy 0.8435\n",
      "Epoch [19]\t Average validation loss 0.7929\t Average validation accuracy 0.8692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0005, 0.001, 0.003, 0.005, 0.01, 0.05, 0.1]\n",
    "losses, accs = [], []\n",
    "for learning_rate in learning_rates:\n",
    "    sgd = SGD(learning_rate, weight_decay)\n",
    "    reluMLP, relu_loss, relu_acc = train(model=reluMLP, criterion=criterion, optimizer=sgd, dataset=data_train, max_epoch=max_epoch, batch_size=100, disp_freq=disp_freq)\n",
    "    losses.append(relu_loss)\n",
    "    accs.append(relu_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'learning_rate=0.0005': [losses[0], accs[0]], \n",
    "                   'learning_rate=0.001': [losses[1], accs[1]],\n",
    "                   'learning_rate=0.003': [losses[2], accs[2]],\n",
    "                    'learning_rate=0.005': [losses[3], accs[3]],\n",
    "                    'learning_rate=0.01': [losses[4], accs[4]],\n",
    "                    'learning_rate=0.05': [losses[5], accs[5]],\n",
    "                    'learning_rate=0.1': [losses[6], accs[6]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][100]\t Batch [0][550]\t Training Loss 2.5152\t Accuracy 0.1100\n",
      "Epoch [0][100]\t Batch [50][550]\t Training Loss 2.5661\t Accuracy 0.1094\n",
      "Epoch [0][100]\t Batch [100][550]\t Training Loss 2.5010\t Accuracy 0.1196\n",
      "Epoch [0][100]\t Batch [150][550]\t Training Loss 2.4432\t Accuracy 0.1287\n",
      "Epoch [0][100]\t Batch [200][550]\t Training Loss 2.3931\t Accuracy 0.1436\n",
      "Epoch [0][100]\t Batch [250][550]\t Training Loss 2.3431\t Accuracy 0.1631\n",
      "Epoch [0][100]\t Batch [300][550]\t Training Loss 2.2997\t Accuracy 0.1834\n",
      "Epoch [0][100]\t Batch [350][550]\t Training Loss 2.2610\t Accuracy 0.2039\n",
      "Epoch [0][100]\t Batch [400][550]\t Training Loss 2.2238\t Accuracy 0.2236\n",
      "Epoch [0][100]\t Batch [450][550]\t Training Loss 2.1914\t Accuracy 0.2396\n",
      "Epoch [0][100]\t Batch [500][550]\t Training Loss 2.1588\t Accuracy 0.2569\n",
      "\n",
      "Epoch [0]\t Average training loss 2.1282\t Average training accuracy 0.2726\n",
      "Epoch [0]\t Average validation loss 1.7580\t Average validation accuracy 0.4650\n",
      "\n",
      "Epoch [1][100]\t Batch [0][550]\t Training Loss 1.6530\t Accuracy 0.5300\n",
      "Epoch [1][100]\t Batch [50][550]\t Training Loss 1.7539\t Accuracy 0.4688\n",
      "Epoch [1][100]\t Batch [100][550]\t Training Loss 1.7365\t Accuracy 0.4768\n",
      "Epoch [1][100]\t Batch [150][550]\t Training Loss 1.7187\t Accuracy 0.4836\n",
      "Epoch [1][100]\t Batch [200][550]\t Training Loss 1.7049\t Accuracy 0.4933\n",
      "Epoch [1][100]\t Batch [250][550]\t Training Loss 1.6844\t Accuracy 0.5053\n",
      "Epoch [1][100]\t Batch [300][550]\t Training Loss 1.6661\t Accuracy 0.5152\n",
      "Epoch [1][100]\t Batch [350][550]\t Training Loss 1.6532\t Accuracy 0.5237\n",
      "Epoch [1][100]\t Batch [400][550]\t Training Loss 1.6369\t Accuracy 0.5323\n",
      "Epoch [1][100]\t Batch [450][550]\t Training Loss 1.6241\t Accuracy 0.5387\n",
      "Epoch [1][100]\t Batch [500][550]\t Training Loss 1.6092\t Accuracy 0.5472\n",
      "\n",
      "Epoch [1]\t Average training loss 1.5943\t Average training accuracy 0.5558\n",
      "Epoch [1]\t Average validation loss 1.3778\t Average validation accuracy 0.6880\n",
      "\n",
      "Epoch [2][100]\t Batch [0][550]\t Training Loss 1.2982\t Accuracy 0.7400\n",
      "Epoch [2][100]\t Batch [50][550]\t Training Loss 1.3971\t Accuracy 0.6637\n",
      "Epoch [2][100]\t Batch [100][550]\t Training Loss 1.3906\t Accuracy 0.6622\n",
      "Epoch [2][100]\t Batch [150][550]\t Training Loss 1.3833\t Accuracy 0.6614\n",
      "Epoch [2][100]\t Batch [200][550]\t Training Loss 1.3790\t Accuracy 0.6635\n",
      "Epoch [2][100]\t Batch [250][550]\t Training Loss 1.3665\t Accuracy 0.6692\n",
      "Epoch [2][100]\t Batch [300][550]\t Training Loss 1.3556\t Accuracy 0.6720\n",
      "Epoch [2][100]\t Batch [350][550]\t Training Loss 1.3512\t Accuracy 0.6744\n",
      "Epoch [2][100]\t Batch [400][550]\t Training Loss 1.3419\t Accuracy 0.6780\n",
      "Epoch [2][100]\t Batch [450][550]\t Training Loss 1.3356\t Accuracy 0.6804\n",
      "Epoch [2][100]\t Batch [500][550]\t Training Loss 1.3272\t Accuracy 0.6842\n",
      "\n",
      "Epoch [2]\t Average training loss 1.3180\t Average training accuracy 0.6885\n",
      "Epoch [2]\t Average validation loss 1.1554\t Average validation accuracy 0.7780\n",
      "\n",
      "Epoch [3][100]\t Batch [0][550]\t Training Loss 1.1000\t Accuracy 0.8000\n",
      "Epoch [3][100]\t Batch [50][550]\t Training Loss 1.1893\t Accuracy 0.7429\n",
      "Epoch [3][100]\t Batch [100][550]\t Training Loss 1.1881\t Accuracy 0.7392\n",
      "Epoch [3][100]\t Batch [150][550]\t Training Loss 1.1862\t Accuracy 0.7358\n",
      "Epoch [3][100]\t Batch [200][550]\t Training Loss 1.1861\t Accuracy 0.7375\n",
      "Epoch [3][100]\t Batch [250][550]\t Training Loss 1.1777\t Accuracy 0.7395\n",
      "Epoch [3][100]\t Batch [300][550]\t Training Loss 1.1707\t Accuracy 0.7415\n",
      "Epoch [3][100]\t Batch [350][550]\t Training Loss 1.1705\t Accuracy 0.7426\n",
      "Epoch [3][100]\t Batch [400][550]\t Training Loss 1.1647\t Accuracy 0.7446\n",
      "Epoch [3][100]\t Batch [450][550]\t Training Loss 1.1618\t Accuracy 0.7456\n",
      "Epoch [3][100]\t Batch [500][550]\t Training Loss 1.1568\t Accuracy 0.7474\n",
      "\n",
      "Epoch [3]\t Average training loss 1.1506\t Average training accuracy 0.7500\n",
      "Epoch [3]\t Average validation loss 1.0164\t Average validation accuracy 0.8202\n",
      "\n",
      "Epoch [4][100]\t Batch [0][550]\t Training Loss 0.9826\t Accuracy 0.8300\n",
      "Epoch [4][100]\t Batch [50][550]\t Training Loss 1.0594\t Accuracy 0.7871\n",
      "Epoch [4][100]\t Batch [100][550]\t Training Loss 1.0611\t Accuracy 0.7817\n",
      "Epoch [4][100]\t Batch [150][550]\t Training Loss 1.0625\t Accuracy 0.7767\n",
      "Epoch [4][100]\t Batch [200][550]\t Training Loss 1.0643\t Accuracy 0.7779\n",
      "Epoch [4][100]\t Batch [250][550]\t Training Loss 1.0583\t Accuracy 0.7787\n",
      "Epoch [4][100]\t Batch [300][550]\t Training Loss 1.0533\t Accuracy 0.7801\n",
      "Epoch [4][100]\t Batch [350][550]\t Training Loss 1.0556\t Accuracy 0.7806\n",
      "Epoch [4][100]\t Batch [400][550]\t Training Loss 1.0519\t Accuracy 0.7819\n",
      "Epoch [4][100]\t Batch [450][550]\t Training Loss 1.0508\t Accuracy 0.7819\n",
      "Epoch [4][100]\t Batch [500][550]\t Training Loss 1.0478\t Accuracy 0.7828\n",
      "\n",
      "Epoch [4]\t Average training loss 1.0434\t Average training accuracy 0.7842\n",
      "Epoch [4]\t Average validation loss 0.9260\t Average validation accuracy 0.8478\n",
      "\n",
      "Epoch [5][100]\t Batch [0][550]\t Training Loss 0.9085\t Accuracy 0.8400\n",
      "Epoch [5][100]\t Batch [50][550]\t Training Loss 0.9745\t Accuracy 0.8069\n",
      "Epoch [5][100]\t Batch [100][550]\t Training Loss 0.9779\t Accuracy 0.8059\n",
      "Epoch [5][100]\t Batch [150][550]\t Training Loss 0.9812\t Accuracy 0.8007\n",
      "Epoch [5][100]\t Batch [200][550]\t Training Loss 0.9841\t Accuracy 0.8006\n",
      "Epoch [5][100]\t Batch [250][550]\t Training Loss 0.9795\t Accuracy 0.8006\n",
      "Epoch [5][100]\t Batch [300][550]\t Training Loss 0.9759\t Accuracy 0.8020\n",
      "Epoch [5][100]\t Batch [350][550]\t Training Loss 0.9796\t Accuracy 0.8025\n",
      "Epoch [5][100]\t Batch [400][550]\t Training Loss 0.9771\t Accuracy 0.8034\n",
      "Epoch [5][100]\t Batch [450][550]\t Training Loss 0.9771\t Accuracy 0.8032\n",
      "Epoch [5][100]\t Batch [500][550]\t Training Loss 0.9753\t Accuracy 0.8037\n",
      "\n",
      "Epoch [5]\t Average training loss 0.9720\t Average training accuracy 0.8047\n",
      "Epoch [5]\t Average validation loss 0.8655\t Average validation accuracy 0.8626\n",
      "\n",
      "Epoch [6][100]\t Batch [0][550]\t Training Loss 0.8596\t Accuracy 0.8500\n",
      "Epoch [6][100]\t Batch [50][550]\t Training Loss 0.9171\t Accuracy 0.8245\n",
      "Epoch [6][100]\t Batch [100][550]\t Training Loss 0.9215\t Accuracy 0.8232\n",
      "Epoch [6][100]\t Batch [150][550]\t Training Loss 0.9262\t Accuracy 0.8172\n",
      "Epoch [6][100]\t Batch [200][550]\t Training Loss 0.9297\t Accuracy 0.8164\n",
      "Epoch [6][100]\t Batch [250][550]\t Training Loss 0.9260\t Accuracy 0.8161\n",
      "Epoch [6][100]\t Batch [300][550]\t Training Loss 0.9232\t Accuracy 0.8175\n",
      "Epoch [6][100]\t Batch [350][550]\t Training Loss 0.9279\t Accuracy 0.8174\n",
      "Epoch [6][100]\t Batch [400][550]\t Training Loss 0.9262\t Accuracy 0.8179\n",
      "Epoch [6][100]\t Batch [450][550]\t Training Loss 0.9269\t Accuracy 0.8177\n",
      "Epoch [6][100]\t Batch [500][550]\t Training Loss 0.9259\t Accuracy 0.8178\n",
      "\n",
      "Epoch [6]\t Average training loss 0.9233\t Average training accuracy 0.8186\n",
      "Epoch [6]\t Average validation loss 0.8242\t Average validation accuracy 0.8686\n",
      "\n",
      "Epoch [7][100]\t Batch [0][550]\t Training Loss 0.8264\t Accuracy 0.8600\n",
      "Epoch [7][100]\t Batch [50][550]\t Training Loss 0.8776\t Accuracy 0.8355\n",
      "Epoch [7][100]\t Batch [100][550]\t Training Loss 0.8827\t Accuracy 0.8335\n",
      "Epoch [7][100]\t Batch [150][550]\t Training Loss 0.8883\t Accuracy 0.8281\n",
      "Epoch [7][100]\t Batch [200][550]\t Training Loss 0.8921\t Accuracy 0.8278\n",
      "Epoch [7][100]\t Batch [250][550]\t Training Loss 0.8890\t Accuracy 0.8275\n",
      "Epoch [7][100]\t Batch [300][550]\t Training Loss 0.8869\t Accuracy 0.8282\n",
      "Epoch [7][100]\t Batch [350][550]\t Training Loss 0.8921\t Accuracy 0.8281\n",
      "Epoch [7][100]\t Batch [400][550]\t Training Loss 0.8910\t Accuracy 0.8283\n",
      "Epoch [7][100]\t Batch [450][550]\t Training Loss 0.8920\t Accuracy 0.8280\n",
      "Epoch [7][100]\t Batch [500][550]\t Training Loss 0.8916\t Accuracy 0.8279\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8896\t Average training accuracy 0.8283\n",
      "Epoch [7]\t Average validation loss 0.7956\t Average validation accuracy 0.8746\n",
      "\n",
      "Epoch [8][100]\t Batch [0][550]\t Training Loss 0.8041\t Accuracy 0.8600\n",
      "Epoch [8][100]\t Batch [50][550]\t Training Loss 0.8501\t Accuracy 0.8425\n",
      "Epoch [8][100]\t Batch [100][550]\t Training Loss 0.8556\t Accuracy 0.8407\n",
      "Epoch [8][100]\t Batch [150][550]\t Training Loss 0.8618\t Accuracy 0.8350\n",
      "Epoch [8][100]\t Batch [200][550]\t Training Loss 0.8658\t Accuracy 0.8345\n",
      "Epoch [8][100]\t Batch [250][550]\t Training Loss 0.8631\t Accuracy 0.8343\n",
      "Epoch [8][100]\t Batch [300][550]\t Training Loss 0.8615\t Accuracy 0.8349\n",
      "Epoch [8][100]\t Batch [350][550]\t Training Loss 0.8670\t Accuracy 0.8348\n",
      "Epoch [8][100]\t Batch [400][550]\t Training Loss 0.8663\t Accuracy 0.8349\n",
      "Epoch [8][100]\t Batch [450][550]\t Training Loss 0.8676\t Accuracy 0.8346\n",
      "Epoch [8][100]\t Batch [500][550]\t Training Loss 0.8676\t Accuracy 0.8345\n",
      "\n",
      "Epoch [8]\t Average training loss 0.8659\t Average training accuracy 0.8348\n",
      "Epoch [8]\t Average validation loss 0.7757\t Average validation accuracy 0.8804\n",
      "\n",
      "Epoch [9][100]\t Batch [0][550]\t Training Loss 0.7888\t Accuracy 0.8600\n",
      "Epoch [9][100]\t Batch [50][550]\t Training Loss 0.8308\t Accuracy 0.8492\n",
      "Epoch [9][100]\t Batch [100][550]\t Training Loss 0.8365\t Accuracy 0.8469\n",
      "Epoch [9][100]\t Batch [150][550]\t Training Loss 0.8432\t Accuracy 0.8411\n",
      "Epoch [9][100]\t Batch [200][550]\t Training Loss 0.8473\t Accuracy 0.8406\n",
      "Epoch [9][100]\t Batch [250][550]\t Training Loss 0.8450\t Accuracy 0.8405\n",
      "Epoch [9][100]\t Batch [300][550]\t Training Loss 0.8436\t Accuracy 0.8412\n",
      "Epoch [9][100]\t Batch [350][550]\t Training Loss 0.8494\t Accuracy 0.8409\n",
      "Epoch [9][100]\t Batch [400][550]\t Training Loss 0.8489\t Accuracy 0.8408\n",
      "Epoch [9][100]\t Batch [450][550]\t Training Loss 0.8504\t Accuracy 0.8402\n",
      "Epoch [9][100]\t Batch [500][550]\t Training Loss 0.8507\t Accuracy 0.8401\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8492\t Average training accuracy 0.8402\n",
      "Epoch [9]\t Average validation loss 0.7620\t Average validation accuracy 0.8852\n",
      "\n",
      "Epoch [10][100]\t Batch [0][550]\t Training Loss 0.7787\t Accuracy 0.8600\n",
      "Epoch [10][100]\t Batch [50][550]\t Training Loss 0.8173\t Accuracy 0.8524\n",
      "Epoch [10][100]\t Batch [100][550]\t Training Loss 0.8231\t Accuracy 0.8497\n",
      "Epoch [10][100]\t Batch [150][550]\t Training Loss 0.8302\t Accuracy 0.8450\n",
      "Epoch [10][100]\t Batch [200][550]\t Training Loss 0.8344\t Accuracy 0.8446\n",
      "Epoch [10][100]\t Batch [250][550]\t Training Loss 0.8323\t Accuracy 0.8450\n",
      "Epoch [10][100]\t Batch [300][550]\t Training Loss 0.8311\t Accuracy 0.8456\n",
      "Epoch [10][100]\t Batch [350][550]\t Training Loss 0.8371\t Accuracy 0.8453\n",
      "Epoch [10][100]\t Batch [400][550]\t Training Loss 0.8368\t Accuracy 0.8451\n",
      "Epoch [10][100]\t Batch [450][550]\t Training Loss 0.8384\t Accuracy 0.8447\n",
      "Epoch [10][100]\t Batch [500][550]\t Training Loss 0.8389\t Accuracy 0.8445\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8376\t Average training accuracy 0.8444\n",
      "Epoch [10]\t Average validation loss 0.7527\t Average validation accuracy 0.8880\n",
      "\n",
      "Epoch [11][100]\t Batch [0][550]\t Training Loss 0.7720\t Accuracy 0.8700\n",
      "Epoch [11][100]\t Batch [50][550]\t Training Loss 0.8080\t Accuracy 0.8557\n",
      "Epoch [11][100]\t Batch [100][550]\t Training Loss 0.8138\t Accuracy 0.8537\n",
      "Epoch [11][100]\t Batch [150][550]\t Training Loss 0.8212\t Accuracy 0.8485\n",
      "Epoch [11][100]\t Batch [200][550]\t Training Loss 0.8255\t Accuracy 0.8481\n",
      "Epoch [11][100]\t Batch [250][550]\t Training Loss 0.8235\t Accuracy 0.8486\n",
      "Epoch [11][100]\t Batch [300][550]\t Training Loss 0.8225\t Accuracy 0.8492\n",
      "Epoch [11][100]\t Batch [350][550]\t Training Loss 0.8286\t Accuracy 0.8486\n",
      "Epoch [11][100]\t Batch [400][550]\t Training Loss 0.8285\t Accuracy 0.8483\n",
      "Epoch [11][100]\t Batch [450][550]\t Training Loss 0.8302\t Accuracy 0.8478\n",
      "Epoch [11][100]\t Batch [500][550]\t Training Loss 0.8307\t Accuracy 0.8475\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8297\t Average training accuracy 0.8473\n",
      "Epoch [11]\t Average validation loss 0.7465\t Average validation accuracy 0.8900\n",
      "\n",
      "Epoch [12][100]\t Batch [0][550]\t Training Loss 0.7683\t Accuracy 0.8800\n",
      "Epoch [12][100]\t Batch [50][550]\t Training Loss 0.8017\t Accuracy 0.8582\n",
      "Epoch [12][100]\t Batch [100][550]\t Training Loss 0.8076\t Accuracy 0.8566\n",
      "Epoch [12][100]\t Batch [150][550]\t Training Loss 0.8151\t Accuracy 0.8511\n",
      "Epoch [12][100]\t Batch [200][550]\t Training Loss 0.8195\t Accuracy 0.8509\n",
      "Epoch [12][100]\t Batch [250][550]\t Training Loss 0.8176\t Accuracy 0.8512\n",
      "Epoch [12][100]\t Batch [300][550]\t Training Loss 0.8168\t Accuracy 0.8519\n",
      "Epoch [12][100]\t Batch [350][550]\t Training Loss 0.8229\t Accuracy 0.8511\n",
      "Epoch [12][100]\t Batch [400][550]\t Training Loss 0.8229\t Accuracy 0.8508\n",
      "Epoch [12][100]\t Batch [450][550]\t Training Loss 0.8247\t Accuracy 0.8503\n",
      "Epoch [12][100]\t Batch [500][550]\t Training Loss 0.8253\t Accuracy 0.8500\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8244\t Average training accuracy 0.8500\n",
      "Epoch [12]\t Average validation loss 0.7428\t Average validation accuracy 0.8916\n",
      "\n",
      "Epoch [13][100]\t Batch [0][550]\t Training Loss 0.7663\t Accuracy 0.8800\n",
      "Epoch [13][100]\t Batch [50][550]\t Training Loss 0.7977\t Accuracy 0.8616\n",
      "Epoch [13][100]\t Batch [100][550]\t Training Loss 0.8036\t Accuracy 0.8589\n",
      "Epoch [13][100]\t Batch [150][550]\t Training Loss 0.8112\t Accuracy 0.8536\n",
      "Epoch [13][100]\t Batch [200][550]\t Training Loss 0.8156\t Accuracy 0.8533\n",
      "Epoch [13][100]\t Batch [250][550]\t Training Loss 0.8138\t Accuracy 0.8537\n",
      "Epoch [13][100]\t Batch [300][550]\t Training Loss 0.8131\t Accuracy 0.8539\n",
      "Epoch [13][100]\t Batch [350][550]\t Training Loss 0.8193\t Accuracy 0.8531\n",
      "Epoch [13][100]\t Batch [400][550]\t Training Loss 0.8193\t Accuracy 0.8528\n",
      "Epoch [13][100]\t Batch [450][550]\t Training Loss 0.8212\t Accuracy 0.8524\n",
      "Epoch [13][100]\t Batch [500][550]\t Training Loss 0.8219\t Accuracy 0.8520\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8211\t Average training accuracy 0.8521\n",
      "Epoch [13]\t Average validation loss 0.7408\t Average validation accuracy 0.8926\n",
      "\n",
      "Epoch [14][100]\t Batch [0][550]\t Training Loss 0.7658\t Accuracy 0.8900\n",
      "Epoch [14][100]\t Batch [50][550]\t Training Loss 0.7953\t Accuracy 0.8633\n",
      "Epoch [14][100]\t Batch [100][550]\t Training Loss 0.8012\t Accuracy 0.8601\n",
      "Epoch [14][100]\t Batch [150][550]\t Training Loss 0.8090\t Accuracy 0.8549\n",
      "Epoch [14][100]\t Batch [200][550]\t Training Loss 0.8133\t Accuracy 0.8546\n",
      "Epoch [14][100]\t Batch [250][550]\t Training Loss 0.8116\t Accuracy 0.8551\n",
      "Epoch [14][100]\t Batch [300][550]\t Training Loss 0.8110\t Accuracy 0.8554\n",
      "Epoch [14][100]\t Batch [350][550]\t Training Loss 0.8173\t Accuracy 0.8546\n",
      "Epoch [14][100]\t Batch [400][550]\t Training Loss 0.8173\t Accuracy 0.8542\n",
      "Epoch [14][100]\t Batch [450][550]\t Training Loss 0.8192\t Accuracy 0.8538\n",
      "Epoch [14][100]\t Batch [500][550]\t Training Loss 0.8200\t Accuracy 0.8534\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8192\t Average training accuracy 0.8535\n",
      "Epoch [14]\t Average validation loss 0.7401\t Average validation accuracy 0.8936\n",
      "\n",
      "Epoch [15][100]\t Batch [0][550]\t Training Loss 0.7662\t Accuracy 0.8900\n",
      "Epoch [15][100]\t Batch [50][550]\t Training Loss 0.7942\t Accuracy 0.8641\n",
      "Epoch [15][100]\t Batch [100][550]\t Training Loss 0.8001\t Accuracy 0.8606\n",
      "Epoch [15][100]\t Batch [150][550]\t Training Loss 0.8079\t Accuracy 0.8554\n",
      "Epoch [15][100]\t Batch [200][550]\t Training Loss 0.8123\t Accuracy 0.8554\n",
      "Epoch [15][100]\t Batch [250][550]\t Training Loss 0.8107\t Accuracy 0.8563\n",
      "Epoch [15][100]\t Batch [300][550]\t Training Loss 0.8102\t Accuracy 0.8566\n",
      "Epoch [15][100]\t Batch [350][550]\t Training Loss 0.8164\t Accuracy 0.8559\n",
      "Epoch [15][100]\t Batch [400][550]\t Training Loss 0.8165\t Accuracy 0.8555\n",
      "Epoch [15][100]\t Batch [450][550]\t Training Loss 0.8184\t Accuracy 0.8551\n",
      "Epoch [15][100]\t Batch [500][550]\t Training Loss 0.8192\t Accuracy 0.8546\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8185\t Average training accuracy 0.8545\n",
      "Epoch [15]\t Average validation loss 0.7404\t Average validation accuracy 0.8944\n",
      "\n",
      "Epoch [16][100]\t Batch [0][550]\t Training Loss 0.7675\t Accuracy 0.8900\n",
      "Epoch [16][100]\t Batch [50][550]\t Training Loss 0.7941\t Accuracy 0.8651\n",
      "Epoch [16][100]\t Batch [100][550]\t Training Loss 0.7999\t Accuracy 0.8615\n",
      "Epoch [16][100]\t Batch [150][550]\t Training Loss 0.8078\t Accuracy 0.8563\n",
      "Epoch [16][100]\t Batch [200][550]\t Training Loss 0.8122\t Accuracy 0.8565\n",
      "Epoch [16][100]\t Batch [250][550]\t Training Loss 0.8106\t Accuracy 0.8573\n",
      "Epoch [16][100]\t Batch [300][550]\t Training Loss 0.8102\t Accuracy 0.8576\n",
      "Epoch [16][100]\t Batch [350][550]\t Training Loss 0.8165\t Accuracy 0.8567\n",
      "Epoch [16][100]\t Batch [400][550]\t Training Loss 0.8166\t Accuracy 0.8565\n",
      "Epoch [16][100]\t Batch [450][550]\t Training Loss 0.8184\t Accuracy 0.8561\n",
      "Epoch [16][100]\t Batch [500][550]\t Training Loss 0.8193\t Accuracy 0.8555\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8187\t Average training accuracy 0.8554\n",
      "Epoch [16]\t Average validation loss 0.7414\t Average validation accuracy 0.8944\n",
      "\n",
      "Epoch [17][100]\t Batch [0][550]\t Training Loss 0.7694\t Accuracy 0.8900\n",
      "Epoch [17][100]\t Batch [50][550]\t Training Loss 0.7948\t Accuracy 0.8657\n",
      "Epoch [17][100]\t Batch [100][550]\t Training Loss 0.8005\t Accuracy 0.8621\n",
      "Epoch [17][100]\t Batch [150][550]\t Training Loss 0.8084\t Accuracy 0.8570\n",
      "Epoch [17][100]\t Batch [200][550]\t Training Loss 0.8128\t Accuracy 0.8571\n",
      "Epoch [17][100]\t Batch [250][550]\t Training Loss 0.8112\t Accuracy 0.8576\n",
      "Epoch [17][100]\t Batch [300][550]\t Training Loss 0.8109\t Accuracy 0.8581\n",
      "Epoch [17][100]\t Batch [350][550]\t Training Loss 0.8172\t Accuracy 0.8573\n",
      "Epoch [17][100]\t Batch [400][550]\t Training Loss 0.8173\t Accuracy 0.8572\n",
      "Epoch [17][100]\t Batch [450][550]\t Training Loss 0.8192\t Accuracy 0.8567\n",
      "Epoch [17][100]\t Batch [500][550]\t Training Loss 0.8201\t Accuracy 0.8562\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8195\t Average training accuracy 0.8561\n",
      "Epoch [17]\t Average validation loss 0.7430\t Average validation accuracy 0.8956\n",
      "\n",
      "Epoch [18][100]\t Batch [0][550]\t Training Loss 0.7718\t Accuracy 0.9000\n",
      "Epoch [18][100]\t Batch [50][550]\t Training Loss 0.7959\t Accuracy 0.8682\n",
      "Epoch [18][100]\t Batch [100][550]\t Training Loss 0.8016\t Accuracy 0.8642\n",
      "Epoch [18][100]\t Batch [150][550]\t Training Loss 0.8095\t Accuracy 0.8589\n",
      "Epoch [18][100]\t Batch [200][550]\t Training Loss 0.8139\t Accuracy 0.8588\n",
      "Epoch [18][100]\t Batch [250][550]\t Training Loss 0.8124\t Accuracy 0.8594\n",
      "Epoch [18][100]\t Batch [300][550]\t Training Loss 0.8121\t Accuracy 0.8597\n",
      "Epoch [18][100]\t Batch [350][550]\t Training Loss 0.8184\t Accuracy 0.8588\n",
      "Epoch [18][100]\t Batch [400][550]\t Training Loss 0.8185\t Accuracy 0.8586\n",
      "Epoch [18][100]\t Batch [450][550]\t Training Loss 0.8204\t Accuracy 0.8581\n",
      "Epoch [18][100]\t Batch [500][550]\t Training Loss 0.8213\t Accuracy 0.8576\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8207\t Average training accuracy 0.8575\n",
      "Epoch [18]\t Average validation loss 0.7450\t Average validation accuracy 0.8954\n",
      "\n",
      "Epoch [19][100]\t Batch [0][550]\t Training Loss 0.7745\t Accuracy 0.9000\n",
      "Epoch [19][100]\t Batch [50][550]\t Training Loss 0.7975\t Accuracy 0.8680\n",
      "Epoch [19][100]\t Batch [100][550]\t Training Loss 0.8031\t Accuracy 0.8641\n",
      "Epoch [19][100]\t Batch [150][550]\t Training Loss 0.8110\t Accuracy 0.8589\n",
      "Epoch [19][100]\t Batch [200][550]\t Training Loss 0.8154\t Accuracy 0.8588\n",
      "Epoch [19][100]\t Batch [250][550]\t Training Loss 0.8139\t Accuracy 0.8596\n",
      "Epoch [19][100]\t Batch [300][550]\t Training Loss 0.8136\t Accuracy 0.8600\n",
      "Epoch [19][100]\t Batch [350][550]\t Training Loss 0.8199\t Accuracy 0.8589\n",
      "Epoch [19][100]\t Batch [400][550]\t Training Loss 0.8200\t Accuracy 0.8587\n",
      "Epoch [19][100]\t Batch [450][550]\t Training Loss 0.8219\t Accuracy 0.8582\n",
      "Epoch [19][100]\t Batch [500][550]\t Training Loss 0.8228\t Accuracy 0.8577\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8223\t Average training accuracy 0.8575\n",
      "Epoch [19]\t Average validation loss 0.7472\t Average validation accuracy 0.8962\n",
      "\n",
      "Epoch [20][100]\t Batch [0][550]\t Training Loss 0.7774\t Accuracy 0.9000\n",
      "Epoch [20][100]\t Batch [50][550]\t Training Loss 0.7993\t Accuracy 0.8682\n",
      "Epoch [20][100]\t Batch [100][550]\t Training Loss 0.8048\t Accuracy 0.8644\n",
      "Epoch [20][100]\t Batch [150][550]\t Training Loss 0.8128\t Accuracy 0.8592\n",
      "Epoch [20][100]\t Batch [200][550]\t Training Loss 0.8172\t Accuracy 0.8592\n",
      "Epoch [20][100]\t Batch [250][550]\t Training Loss 0.8156\t Accuracy 0.8602\n",
      "Epoch [20][100]\t Batch [300][550]\t Training Loss 0.8154\t Accuracy 0.8605\n",
      "Epoch [20][100]\t Batch [350][550]\t Training Loss 0.8217\t Accuracy 0.8594\n",
      "Epoch [20][100]\t Batch [400][550]\t Training Loss 0.8218\t Accuracy 0.8593\n",
      "Epoch [20][100]\t Batch [450][550]\t Training Loss 0.8237\t Accuracy 0.8588\n",
      "Epoch [20][100]\t Batch [500][550]\t Training Loss 0.8246\t Accuracy 0.8582\n",
      "\n",
      "Epoch [20]\t Average training loss 0.8241\t Average training accuracy 0.8581\n",
      "Epoch [20]\t Average validation loss 0.7496\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [21][100]\t Batch [0][550]\t Training Loss 0.7803\t Accuracy 0.9000\n",
      "Epoch [21][100]\t Batch [50][550]\t Training Loss 0.8013\t Accuracy 0.8690\n",
      "Epoch [21][100]\t Batch [100][550]\t Training Loss 0.8067\t Accuracy 0.8649\n",
      "Epoch [21][100]\t Batch [150][550]\t Training Loss 0.8147\t Accuracy 0.8597\n",
      "Epoch [21][100]\t Batch [200][550]\t Training Loss 0.8191\t Accuracy 0.8597\n",
      "Epoch [21][100]\t Batch [250][550]\t Training Loss 0.8176\t Accuracy 0.8607\n",
      "Epoch [21][100]\t Batch [300][550]\t Training Loss 0.8173\t Accuracy 0.8611\n",
      "Epoch [21][100]\t Batch [350][550]\t Training Loss 0.8236\t Accuracy 0.8599\n",
      "Epoch [21][100]\t Batch [400][550]\t Training Loss 0.8237\t Accuracy 0.8598\n",
      "Epoch [21][100]\t Batch [450][550]\t Training Loss 0.8256\t Accuracy 0.8594\n",
      "Epoch [21][100]\t Batch [500][550]\t Training Loss 0.8265\t Accuracy 0.8588\n",
      "\n",
      "Epoch [21]\t Average training loss 0.8260\t Average training accuracy 0.8586\n",
      "Epoch [21]\t Average validation loss 0.7521\t Average validation accuracy 0.8952\n",
      "\n",
      "Epoch [22][100]\t Batch [0][550]\t Training Loss 0.7832\t Accuracy 0.9000\n",
      "Epoch [22][100]\t Batch [50][550]\t Training Loss 0.8033\t Accuracy 0.8682\n",
      "Epoch [22][100]\t Batch [100][550]\t Training Loss 0.8088\t Accuracy 0.8647\n",
      "Epoch [22][100]\t Batch [150][550]\t Training Loss 0.8167\t Accuracy 0.8598\n",
      "Epoch [22][100]\t Batch [200][550]\t Training Loss 0.8211\t Accuracy 0.8599\n",
      "Epoch [22][100]\t Batch [250][550]\t Training Loss 0.8195\t Accuracy 0.8610\n",
      "Epoch [22][100]\t Batch [300][550]\t Training Loss 0.8193\t Accuracy 0.8614\n",
      "Epoch [22][100]\t Batch [350][550]\t Training Loss 0.8256\t Accuracy 0.8603\n",
      "Epoch [22][100]\t Batch [400][550]\t Training Loss 0.8257\t Accuracy 0.8602\n",
      "Epoch [22][100]\t Batch [450][550]\t Training Loss 0.8276\t Accuracy 0.8599\n",
      "Epoch [22][100]\t Batch [500][550]\t Training Loss 0.8285\t Accuracy 0.8592\n",
      "\n",
      "Epoch [22]\t Average training loss 0.8280\t Average training accuracy 0.8590\n",
      "Epoch [22]\t Average validation loss 0.7545\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [23][100]\t Batch [0][550]\t Training Loss 0.7861\t Accuracy 0.9000\n",
      "Epoch [23][100]\t Batch [50][550]\t Training Loss 0.8055\t Accuracy 0.8680\n",
      "Epoch [23][100]\t Batch [100][550]\t Training Loss 0.8108\t Accuracy 0.8644\n",
      "Epoch [23][100]\t Batch [150][550]\t Training Loss 0.8187\t Accuracy 0.8597\n",
      "Epoch [23][100]\t Batch [200][550]\t Training Loss 0.8231\t Accuracy 0.8598\n",
      "Epoch [23][100]\t Batch [250][550]\t Training Loss 0.8216\t Accuracy 0.8611\n",
      "Epoch [23][100]\t Batch [300][550]\t Training Loss 0.8214\t Accuracy 0.8614\n",
      "Epoch [23][100]\t Batch [350][550]\t Training Loss 0.8276\t Accuracy 0.8604\n",
      "Epoch [23][100]\t Batch [400][550]\t Training Loss 0.8277\t Accuracy 0.8604\n",
      "Epoch [23][100]\t Batch [450][550]\t Training Loss 0.8296\t Accuracy 0.8600\n",
      "Epoch [23][100]\t Batch [500][550]\t Training Loss 0.8305\t Accuracy 0.8593\n",
      "\n",
      "Epoch [23]\t Average training loss 0.8300\t Average training accuracy 0.8591\n",
      "Epoch [23]\t Average validation loss 0.7570\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [24][100]\t Batch [0][550]\t Training Loss 0.7889\t Accuracy 0.9000\n",
      "Epoch [24][100]\t Batch [50][550]\t Training Loss 0.8075\t Accuracy 0.8684\n",
      "Epoch [24][100]\t Batch [100][550]\t Training Loss 0.8128\t Accuracy 0.8642\n",
      "Epoch [24][100]\t Batch [150][550]\t Training Loss 0.8208\t Accuracy 0.8595\n",
      "Epoch [24][100]\t Batch [200][550]\t Training Loss 0.8252\t Accuracy 0.8600\n",
      "Epoch [24][100]\t Batch [250][550]\t Training Loss 0.8236\t Accuracy 0.8609\n",
      "Epoch [24][100]\t Batch [300][550]\t Training Loss 0.8234\t Accuracy 0.8609\n",
      "Epoch [24][100]\t Batch [350][550]\t Training Loss 0.8296\t Accuracy 0.8599\n",
      "Epoch [24][100]\t Batch [400][550]\t Training Loss 0.8297\t Accuracy 0.8600\n",
      "Epoch [24][100]\t Batch [450][550]\t Training Loss 0.8316\t Accuracy 0.8596\n",
      "Epoch [24][100]\t Batch [500][550]\t Training Loss 0.8325\t Accuracy 0.8590\n",
      "\n",
      "Epoch [24]\t Average training loss 0.8320\t Average training accuracy 0.8588\n",
      "Epoch [24]\t Average validation loss 0.7594\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [25][100]\t Batch [0][550]\t Training Loss 0.7917\t Accuracy 0.9000\n",
      "Epoch [25][100]\t Batch [50][550]\t Training Loss 0.8096\t Accuracy 0.8686\n",
      "Epoch [25][100]\t Batch [100][550]\t Training Loss 0.8148\t Accuracy 0.8640\n",
      "Epoch [25][100]\t Batch [150][550]\t Training Loss 0.8227\t Accuracy 0.8593\n",
      "Epoch [25][100]\t Batch [200][550]\t Training Loss 0.8271\t Accuracy 0.8599\n",
      "Epoch [25][100]\t Batch [250][550]\t Training Loss 0.8255\t Accuracy 0.8608\n",
      "Epoch [25][100]\t Batch [300][550]\t Training Loss 0.8253\t Accuracy 0.8609\n",
      "Epoch [25][100]\t Batch [350][550]\t Training Loss 0.8316\t Accuracy 0.8600\n",
      "Epoch [25][100]\t Batch [400][550]\t Training Loss 0.8317\t Accuracy 0.8601\n",
      "Epoch [25][100]\t Batch [450][550]\t Training Loss 0.8335\t Accuracy 0.8598\n",
      "Epoch [25][100]\t Batch [500][550]\t Training Loss 0.8344\t Accuracy 0.8592\n",
      "\n",
      "Epoch [25]\t Average training loss 0.8339\t Average training accuracy 0.8590\n",
      "Epoch [25]\t Average validation loss 0.7618\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [26][100]\t Batch [0][550]\t Training Loss 0.7944\t Accuracy 0.8900\n",
      "Epoch [26][100]\t Batch [50][550]\t Training Loss 0.8116\t Accuracy 0.8684\n",
      "Epoch [26][100]\t Batch [100][550]\t Training Loss 0.8168\t Accuracy 0.8635\n",
      "Epoch [26][100]\t Batch [150][550]\t Training Loss 0.8247\t Accuracy 0.8589\n",
      "Epoch [26][100]\t Batch [200][550]\t Training Loss 0.8290\t Accuracy 0.8597\n",
      "Epoch [26][100]\t Batch [250][550]\t Training Loss 0.8274\t Accuracy 0.8606\n",
      "Epoch [26][100]\t Batch [300][550]\t Training Loss 0.8272\t Accuracy 0.8608\n",
      "Epoch [26][100]\t Batch [350][550]\t Training Loss 0.8335\t Accuracy 0.8598\n",
      "Epoch [26][100]\t Batch [400][550]\t Training Loss 0.8336\t Accuracy 0.8601\n",
      "Epoch [26][100]\t Batch [450][550]\t Training Loss 0.8354\t Accuracy 0.8598\n",
      "Epoch [26][100]\t Batch [500][550]\t Training Loss 0.8362\t Accuracy 0.8594\n",
      "\n",
      "Epoch [26]\t Average training loss 0.8358\t Average training accuracy 0.8591\n",
      "Epoch [26]\t Average validation loss 0.7640\t Average validation accuracy 0.8966\n",
      "\n",
      "Epoch [27][100]\t Batch [0][550]\t Training Loss 0.7969\t Accuracy 0.8900\n",
      "Epoch [27][100]\t Batch [50][550]\t Training Loss 0.8135\t Accuracy 0.8690\n",
      "Epoch [27][100]\t Batch [100][550]\t Training Loss 0.8186\t Accuracy 0.8642\n",
      "Epoch [27][100]\t Batch [150][550]\t Training Loss 0.8265\t Accuracy 0.8594\n",
      "Epoch [27][100]\t Batch [200][550]\t Training Loss 0.8309\t Accuracy 0.8600\n",
      "Epoch [27][100]\t Batch [250][550]\t Training Loss 0.8292\t Accuracy 0.8608\n",
      "Epoch [27][100]\t Batch [300][550]\t Training Loss 0.8291\t Accuracy 0.8610\n",
      "Epoch [27][100]\t Batch [350][550]\t Training Loss 0.8353\t Accuracy 0.8598\n",
      "Epoch [27][100]\t Batch [400][550]\t Training Loss 0.8353\t Accuracy 0.8601\n",
      "Epoch [27][100]\t Batch [450][550]\t Training Loss 0.8371\t Accuracy 0.8597\n",
      "Epoch [27][100]\t Batch [500][550]\t Training Loss 0.8380\t Accuracy 0.8593\n",
      "\n",
      "Epoch [27]\t Average training loss 0.8376\t Average training accuracy 0.8591\n",
      "Epoch [27]\t Average validation loss 0.7661\t Average validation accuracy 0.8968\n",
      "\n",
      "Epoch [28][100]\t Batch [0][550]\t Training Loss 0.7993\t Accuracy 0.8900\n",
      "Epoch [28][100]\t Batch [50][550]\t Training Loss 0.8153\t Accuracy 0.8686\n",
      "Epoch [28][100]\t Batch [100][550]\t Training Loss 0.8204\t Accuracy 0.8642\n",
      "Epoch [28][100]\t Batch [150][550]\t Training Loss 0.8282\t Accuracy 0.8594\n",
      "Epoch [28][100]\t Batch [200][550]\t Training Loss 0.8326\t Accuracy 0.8596\n",
      "Epoch [28][100]\t Batch [250][550]\t Training Loss 0.8309\t Accuracy 0.8605\n",
      "Epoch [28][100]\t Batch [300][550]\t Training Loss 0.8308\t Accuracy 0.8607\n",
      "Epoch [28][100]\t Batch [350][550]\t Training Loss 0.8370\t Accuracy 0.8596\n",
      "Epoch [28][100]\t Batch [400][550]\t Training Loss 0.8370\t Accuracy 0.8599\n",
      "Epoch [28][100]\t Batch [450][550]\t Training Loss 0.8388\t Accuracy 0.8595\n",
      "Epoch [28][100]\t Batch [500][550]\t Training Loss 0.8397\t Accuracy 0.8591\n",
      "\n",
      "Epoch [28]\t Average training loss 0.8393\t Average training accuracy 0.8589\n",
      "Epoch [28]\t Average validation loss 0.7681\t Average validation accuracy 0.8966\n",
      "\n",
      "Epoch [29][100]\t Batch [0][550]\t Training Loss 0.8015\t Accuracy 0.8900\n",
      "Epoch [29][100]\t Batch [50][550]\t Training Loss 0.8170\t Accuracy 0.8686\n",
      "Epoch [29][100]\t Batch [100][550]\t Training Loss 0.8220\t Accuracy 0.8645\n",
      "Epoch [29][100]\t Batch [150][550]\t Training Loss 0.8299\t Accuracy 0.8593\n",
      "Epoch [29][100]\t Batch [200][550]\t Training Loss 0.8342\t Accuracy 0.8595\n",
      "Epoch [29][100]\t Batch [250][550]\t Training Loss 0.8325\t Accuracy 0.8603\n",
      "Epoch [29][100]\t Batch [300][550]\t Training Loss 0.8324\t Accuracy 0.8604\n",
      "Epoch [29][100]\t Batch [350][550]\t Training Loss 0.8386\t Accuracy 0.8593\n",
      "Epoch [29][100]\t Batch [400][550]\t Training Loss 0.8386\t Accuracy 0.8595\n",
      "Epoch [29][100]\t Batch [450][550]\t Training Loss 0.8404\t Accuracy 0.8592\n",
      "Epoch [29][100]\t Batch [500][550]\t Training Loss 0.8412\t Accuracy 0.8588\n",
      "\n",
      "Epoch [29]\t Average training loss 0.8408\t Average training accuracy 0.8587\n",
      "Epoch [29]\t Average validation loss 0.7700\t Average validation accuracy 0.8964\n",
      "\n",
      "Epoch [30][100]\t Batch [0][550]\t Training Loss 0.8036\t Accuracy 0.8900\n",
      "Epoch [30][100]\t Batch [50][550]\t Training Loss 0.8187\t Accuracy 0.8680\n",
      "Epoch [30][100]\t Batch [100][550]\t Training Loss 0.8236\t Accuracy 0.8642\n",
      "Epoch [30][100]\t Batch [150][550]\t Training Loss 0.8314\t Accuracy 0.8591\n",
      "Epoch [30][100]\t Batch [200][550]\t Training Loss 0.8357\t Accuracy 0.8592\n",
      "Epoch [30][100]\t Batch [250][550]\t Training Loss 0.8340\t Accuracy 0.8600\n",
      "Epoch [30][100]\t Batch [300][550]\t Training Loss 0.8339\t Accuracy 0.8601\n",
      "Epoch [30][100]\t Batch [350][550]\t Training Loss 0.8401\t Accuracy 0.8590\n",
      "Epoch [30][100]\t Batch [400][550]\t Training Loss 0.8401\t Accuracy 0.8592\n",
      "Epoch [30][100]\t Batch [450][550]\t Training Loss 0.8419\t Accuracy 0.8590\n",
      "Epoch [30][100]\t Batch [500][550]\t Training Loss 0.8427\t Accuracy 0.8587\n",
      "\n",
      "Epoch [30]\t Average training loss 0.8423\t Average training accuracy 0.8586\n",
      "Epoch [30]\t Average validation loss 0.7718\t Average validation accuracy 0.8968\n",
      "\n",
      "Epoch [31][100]\t Batch [0][550]\t Training Loss 0.8057\t Accuracy 0.8900\n",
      "Epoch [31][100]\t Batch [50][550]\t Training Loss 0.8202\t Accuracy 0.8684\n",
      "Epoch [31][100]\t Batch [100][550]\t Training Loss 0.8251\t Accuracy 0.8643\n",
      "Epoch [31][100]\t Batch [150][550]\t Training Loss 0.8328\t Accuracy 0.8594\n",
      "Epoch [31][100]\t Batch [200][550]\t Training Loss 0.8371\t Accuracy 0.8594\n",
      "Epoch [31][100]\t Batch [250][550]\t Training Loss 0.8354\t Accuracy 0.8600\n",
      "Epoch [31][100]\t Batch [300][550]\t Training Loss 0.8353\t Accuracy 0.8601\n",
      "Epoch [31][100]\t Batch [350][550]\t Training Loss 0.8415\t Accuracy 0.8589\n",
      "Epoch [31][100]\t Batch [400][550]\t Training Loss 0.8415\t Accuracy 0.8592\n",
      "Epoch [31][100]\t Batch [450][550]\t Training Loss 0.8432\t Accuracy 0.8591\n",
      "Epoch [31][100]\t Batch [500][550]\t Training Loss 0.8441\t Accuracy 0.8588\n",
      "\n",
      "Epoch [31]\t Average training loss 0.8437\t Average training accuracy 0.8586\n",
      "Epoch [31]\t Average validation loss 0.7734\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [32][100]\t Batch [0][550]\t Training Loss 0.8076\t Accuracy 0.8900\n",
      "Epoch [32][100]\t Batch [50][550]\t Training Loss 0.8216\t Accuracy 0.8680\n",
      "Epoch [32][100]\t Batch [100][550]\t Training Loss 0.8264\t Accuracy 0.8640\n",
      "Epoch [32][100]\t Batch [150][550]\t Training Loss 0.8342\t Accuracy 0.8593\n",
      "Epoch [32][100]\t Batch [200][550]\t Training Loss 0.8385\t Accuracy 0.8591\n",
      "Epoch [32][100]\t Batch [250][550]\t Training Loss 0.8368\t Accuracy 0.8598\n",
      "Epoch [32][100]\t Batch [300][550]\t Training Loss 0.8366\t Accuracy 0.8598\n",
      "Epoch [32][100]\t Batch [350][550]\t Training Loss 0.8427\t Accuracy 0.8588\n",
      "Epoch [32][100]\t Batch [400][550]\t Training Loss 0.8428\t Accuracy 0.8591\n",
      "Epoch [32][100]\t Batch [450][550]\t Training Loss 0.8445\t Accuracy 0.8590\n",
      "Epoch [32][100]\t Batch [500][550]\t Training Loss 0.8453\t Accuracy 0.8587\n",
      "\n",
      "Epoch [32]\t Average training loss 0.8449\t Average training accuracy 0.8585\n",
      "Epoch [32]\t Average validation loss 0.7749\t Average validation accuracy 0.8962\n",
      "\n",
      "Epoch [33][100]\t Batch [0][550]\t Training Loss 0.8093\t Accuracy 0.8900\n",
      "Epoch [33][100]\t Batch [50][550]\t Training Loss 0.8229\t Accuracy 0.8675\n",
      "Epoch [33][100]\t Batch [100][550]\t Training Loss 0.8277\t Accuracy 0.8636\n",
      "Epoch [33][100]\t Batch [150][550]\t Training Loss 0.8354\t Accuracy 0.8590\n",
      "Epoch [33][100]\t Batch [200][550]\t Training Loss 0.8397\t Accuracy 0.8589\n",
      "Epoch [33][100]\t Batch [250][550]\t Training Loss 0.8380\t Accuracy 0.8596\n",
      "Epoch [33][100]\t Batch [300][550]\t Training Loss 0.8378\t Accuracy 0.8595\n",
      "Epoch [33][100]\t Batch [350][550]\t Training Loss 0.8439\t Accuracy 0.8585\n",
      "Epoch [33][100]\t Batch [400][550]\t Training Loss 0.8440\t Accuracy 0.8588\n",
      "Epoch [33][100]\t Batch [450][550]\t Training Loss 0.8457\t Accuracy 0.8586\n",
      "Epoch [33][100]\t Batch [500][550]\t Training Loss 0.8465\t Accuracy 0.8584\n",
      "\n",
      "Epoch [33]\t Average training loss 0.8461\t Average training accuracy 0.8583\n",
      "Epoch [33]\t Average validation loss 0.7763\t Average validation accuracy 0.8958\n",
      "\n",
      "Epoch [34][100]\t Batch [0][550]\t Training Loss 0.8110\t Accuracy 0.8900\n",
      "Epoch [34][100]\t Batch [50][550]\t Training Loss 0.8241\t Accuracy 0.8669\n",
      "Epoch [34][100]\t Batch [100][550]\t Training Loss 0.8288\t Accuracy 0.8635\n",
      "Epoch [34][100]\t Batch [150][550]\t Training Loss 0.8365\t Accuracy 0.8589\n",
      "Epoch [34][100]\t Batch [200][550]\t Training Loss 0.8408\t Accuracy 0.8589\n",
      "Epoch [34][100]\t Batch [250][550]\t Training Loss 0.8391\t Accuracy 0.8596\n",
      "Epoch [34][100]\t Batch [300][550]\t Training Loss 0.8390\t Accuracy 0.8595\n",
      "Epoch [34][100]\t Batch [350][550]\t Training Loss 0.8451\t Accuracy 0.8586\n",
      "Epoch [34][100]\t Batch [400][550]\t Training Loss 0.8451\t Accuracy 0.8589\n",
      "Epoch [34][100]\t Batch [450][550]\t Training Loss 0.8468\t Accuracy 0.8587\n",
      "Epoch [34][100]\t Batch [500][550]\t Training Loss 0.8476\t Accuracy 0.8584\n",
      "\n",
      "Epoch [34]\t Average training loss 0.8472\t Average training accuracy 0.8583\n",
      "Epoch [34]\t Average validation loss 0.7776\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [35][100]\t Batch [0][550]\t Training Loss 0.8125\t Accuracy 0.8900\n",
      "Epoch [35][100]\t Batch [50][550]\t Training Loss 0.8252\t Accuracy 0.8665\n",
      "Epoch [35][100]\t Batch [100][550]\t Training Loss 0.8299\t Accuracy 0.8634\n",
      "Epoch [35][100]\t Batch [150][550]\t Training Loss 0.8376\t Accuracy 0.8587\n",
      "Epoch [35][100]\t Batch [200][550]\t Training Loss 0.8419\t Accuracy 0.8586\n",
      "Epoch [35][100]\t Batch [250][550]\t Training Loss 0.8401\t Accuracy 0.8596\n",
      "Epoch [35][100]\t Batch [300][550]\t Training Loss 0.8400\t Accuracy 0.8594\n",
      "Epoch [35][100]\t Batch [350][550]\t Training Loss 0.8461\t Accuracy 0.8585\n",
      "Epoch [35][100]\t Batch [400][550]\t Training Loss 0.8461\t Accuracy 0.8589\n",
      "Epoch [35][100]\t Batch [450][550]\t Training Loss 0.8478\t Accuracy 0.8587\n",
      "Epoch [35][100]\t Batch [500][550]\t Training Loss 0.8486\t Accuracy 0.8585\n",
      "\n",
      "Epoch [35]\t Average training loss 0.8482\t Average training accuracy 0.8584\n",
      "Epoch [35]\t Average validation loss 0.7788\t Average validation accuracy 0.8956\n",
      "\n",
      "Epoch [36][100]\t Batch [0][550]\t Training Loss 0.8139\t Accuracy 0.8800\n",
      "Epoch [36][100]\t Batch [50][550]\t Training Loss 0.8263\t Accuracy 0.8661\n",
      "Epoch [36][100]\t Batch [100][550]\t Training Loss 0.8309\t Accuracy 0.8632\n",
      "Epoch [36][100]\t Batch [150][550]\t Training Loss 0.8386\t Accuracy 0.8585\n",
      "Epoch [36][100]\t Batch [200][550]\t Training Loss 0.8428\t Accuracy 0.8585\n",
      "Epoch [36][100]\t Batch [250][550]\t Training Loss 0.8411\t Accuracy 0.8595\n",
      "Epoch [36][100]\t Batch [300][550]\t Training Loss 0.8409\t Accuracy 0.8591\n",
      "Epoch [36][100]\t Batch [350][550]\t Training Loss 0.8470\t Accuracy 0.8582\n",
      "Epoch [36][100]\t Batch [400][550]\t Training Loss 0.8470\t Accuracy 0.8586\n",
      "Epoch [36][100]\t Batch [450][550]\t Training Loss 0.8487\t Accuracy 0.8584\n",
      "Epoch [36][100]\t Batch [500][550]\t Training Loss 0.8495\t Accuracy 0.8582\n",
      "\n",
      "Epoch [36]\t Average training loss 0.8491\t Average training accuracy 0.8581\n",
      "Epoch [36]\t Average validation loss 0.7800\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [37][100]\t Batch [0][550]\t Training Loss 0.8152\t Accuracy 0.8800\n",
      "Epoch [37][100]\t Batch [50][550]\t Training Loss 0.8272\t Accuracy 0.8663\n",
      "Epoch [37][100]\t Batch [100][550]\t Training Loss 0.8318\t Accuracy 0.8634\n",
      "Epoch [37][100]\t Batch [150][550]\t Training Loss 0.8395\t Accuracy 0.8587\n",
      "Epoch [37][100]\t Batch [200][550]\t Training Loss 0.8437\t Accuracy 0.8586\n",
      "Epoch [37][100]\t Batch [250][550]\t Training Loss 0.8419\t Accuracy 0.8596\n",
      "Epoch [37][100]\t Batch [300][550]\t Training Loss 0.8418\t Accuracy 0.8593\n",
      "Epoch [37][100]\t Batch [350][550]\t Training Loss 0.8479\t Accuracy 0.8582\n",
      "Epoch [37][100]\t Batch [400][550]\t Training Loss 0.8479\t Accuracy 0.8585\n",
      "Epoch [37][100]\t Batch [450][550]\t Training Loss 0.8496\t Accuracy 0.8583\n",
      "Epoch [37][100]\t Batch [500][550]\t Training Loss 0.8503\t Accuracy 0.8580\n",
      "\n",
      "Epoch [37]\t Average training loss 0.8500\t Average training accuracy 0.8579\n",
      "Epoch [37]\t Average validation loss 0.7810\t Average validation accuracy 0.8958\n",
      "\n",
      "Epoch [38][100]\t Batch [0][550]\t Training Loss 0.8164\t Accuracy 0.8800\n",
      "Epoch [38][100]\t Batch [50][550]\t Training Loss 0.8281\t Accuracy 0.8665\n",
      "Epoch [38][100]\t Batch [100][550]\t Training Loss 0.8327\t Accuracy 0.8637\n",
      "Epoch [38][100]\t Batch [150][550]\t Training Loss 0.8403\t Accuracy 0.8588\n",
      "Epoch [38][100]\t Batch [200][550]\t Training Loss 0.8445\t Accuracy 0.8586\n",
      "Epoch [38][100]\t Batch [250][550]\t Training Loss 0.8428\t Accuracy 0.8597\n",
      "Epoch [38][100]\t Batch [300][550]\t Training Loss 0.8426\t Accuracy 0.8594\n",
      "Epoch [38][100]\t Batch [350][550]\t Training Loss 0.8487\t Accuracy 0.8583\n",
      "Epoch [38][100]\t Batch [400][550]\t Training Loss 0.8487\t Accuracy 0.8586\n",
      "Epoch [38][100]\t Batch [450][550]\t Training Loss 0.8503\t Accuracy 0.8585\n",
      "Epoch [38][100]\t Batch [500][550]\t Training Loss 0.8511\t Accuracy 0.8582\n",
      "\n",
      "Epoch [38]\t Average training loss 0.8508\t Average training accuracy 0.8580\n",
      "Epoch [38]\t Average validation loss 0.7819\t Average validation accuracy 0.8956\n",
      "\n",
      "Epoch [39][100]\t Batch [0][550]\t Training Loss 0.8175\t Accuracy 0.8800\n",
      "Epoch [39][100]\t Batch [50][550]\t Training Loss 0.8289\t Accuracy 0.8663\n",
      "Epoch [39][100]\t Batch [100][550]\t Training Loss 0.8335\t Accuracy 0.8635\n",
      "Epoch [39][100]\t Batch [150][550]\t Training Loss 0.8411\t Accuracy 0.8587\n",
      "Epoch [39][100]\t Batch [200][550]\t Training Loss 0.8453\t Accuracy 0.8584\n",
      "Epoch [39][100]\t Batch [250][550]\t Training Loss 0.8435\t Accuracy 0.8595\n",
      "Epoch [39][100]\t Batch [300][550]\t Training Loss 0.8434\t Accuracy 0.8592\n",
      "Epoch [39][100]\t Batch [350][550]\t Training Loss 0.8494\t Accuracy 0.8582\n",
      "Epoch [39][100]\t Batch [400][550]\t Training Loss 0.8494\t Accuracy 0.8584\n",
      "Epoch [39][100]\t Batch [450][550]\t Training Loss 0.8511\t Accuracy 0.8583\n",
      "Epoch [39][100]\t Batch [500][550]\t Training Loss 0.8518\t Accuracy 0.8580\n",
      "\n",
      "Epoch [39]\t Average training loss 0.8515\t Average training accuracy 0.8579\n",
      "Epoch [39]\t Average validation loss 0.7828\t Average validation accuracy 0.8954\n",
      "\n",
      "Epoch [40][100]\t Batch [0][550]\t Training Loss 0.8186\t Accuracy 0.8800\n",
      "Epoch [40][100]\t Batch [50][550]\t Training Loss 0.8297\t Accuracy 0.8665\n",
      "Epoch [40][100]\t Batch [100][550]\t Training Loss 0.8342\t Accuracy 0.8636\n",
      "Epoch [40][100]\t Batch [150][550]\t Training Loss 0.8418\t Accuracy 0.8589\n",
      "Epoch [40][100]\t Batch [200][550]\t Training Loss 0.8460\t Accuracy 0.8585\n",
      "Epoch [40][100]\t Batch [250][550]\t Training Loss 0.8442\t Accuracy 0.8595\n",
      "Epoch [40][100]\t Batch [300][550]\t Training Loss 0.8440\t Accuracy 0.8592\n",
      "Epoch [40][100]\t Batch [350][550]\t Training Loss 0.8501\t Accuracy 0.8581\n",
      "Epoch [40][100]\t Batch [400][550]\t Training Loss 0.8501\t Accuracy 0.8584\n",
      "Epoch [40][100]\t Batch [450][550]\t Training Loss 0.8517\t Accuracy 0.8583\n",
      "Epoch [40][100]\t Batch [500][550]\t Training Loss 0.8525\t Accuracy 0.8579\n",
      "\n",
      "Epoch [40]\t Average training loss 0.8521\t Average training accuracy 0.8578\n",
      "Epoch [40]\t Average validation loss 0.7836\t Average validation accuracy 0.8956\n",
      "\n",
      "Epoch [41][100]\t Batch [0][550]\t Training Loss 0.8195\t Accuracy 0.8700\n",
      "Epoch [41][100]\t Batch [50][550]\t Training Loss 0.8304\t Accuracy 0.8661\n",
      "Epoch [41][100]\t Batch [100][550]\t Training Loss 0.8348\t Accuracy 0.8633\n",
      "Epoch [41][100]\t Batch [150][550]\t Training Loss 0.8424\t Accuracy 0.8587\n",
      "Epoch [41][100]\t Batch [200][550]\t Training Loss 0.8466\t Accuracy 0.8582\n",
      "Epoch [41][100]\t Batch [250][550]\t Training Loss 0.8448\t Accuracy 0.8594\n",
      "Epoch [41][100]\t Batch [300][550]\t Training Loss 0.8447\t Accuracy 0.8592\n",
      "Epoch [41][100]\t Batch [350][550]\t Training Loss 0.8507\t Accuracy 0.8582\n",
      "Epoch [41][100]\t Batch [400][550]\t Training Loss 0.8507\t Accuracy 0.8584\n",
      "Epoch [41][100]\t Batch [450][550]\t Training Loss 0.8523\t Accuracy 0.8582\n",
      "Epoch [41][100]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8579\n",
      "\n",
      "Epoch [41]\t Average training loss 0.8527\t Average training accuracy 0.8577\n",
      "Epoch [41]\t Average validation loss 0.7844\t Average validation accuracy 0.8956\n",
      "\n",
      "Epoch [42][100]\t Batch [0][550]\t Training Loss 0.8204\t Accuracy 0.8700\n",
      "Epoch [42][100]\t Batch [50][550]\t Training Loss 0.8310\t Accuracy 0.8659\n",
      "Epoch [42][100]\t Batch [100][550]\t Training Loss 0.8354\t Accuracy 0.8632\n",
      "Epoch [42][100]\t Batch [150][550]\t Training Loss 0.8430\t Accuracy 0.8584\n",
      "Epoch [42][100]\t Batch [200][550]\t Training Loss 0.8472\t Accuracy 0.8579\n",
      "Epoch [42][100]\t Batch [250][550]\t Training Loss 0.8454\t Accuracy 0.8591\n",
      "Epoch [42][100]\t Batch [300][550]\t Training Loss 0.8453\t Accuracy 0.8589\n",
      "Epoch [42][100]\t Batch [350][550]\t Training Loss 0.8512\t Accuracy 0.8578\n",
      "Epoch [42][100]\t Batch [400][550]\t Training Loss 0.8513\t Accuracy 0.8582\n",
      "Epoch [42][100]\t Batch [450][550]\t Training Loss 0.8529\t Accuracy 0.8580\n",
      "Epoch [42][100]\t Batch [500][550]\t Training Loss 0.8536\t Accuracy 0.8577\n",
      "\n",
      "Epoch [42]\t Average training loss 0.8533\t Average training accuracy 0.8575\n",
      "Epoch [42]\t Average validation loss 0.7850\t Average validation accuracy 0.8954\n",
      "\n",
      "Epoch [43][100]\t Batch [0][550]\t Training Loss 0.8212\t Accuracy 0.8700\n",
      "Epoch [43][100]\t Batch [50][550]\t Training Loss 0.8316\t Accuracy 0.8655\n",
      "Epoch [43][100]\t Batch [100][550]\t Training Loss 0.8360\t Accuracy 0.8633\n",
      "Epoch [43][100]\t Batch [150][550]\t Training Loss 0.8436\t Accuracy 0.8587\n",
      "Epoch [43][100]\t Batch [200][550]\t Training Loss 0.8478\t Accuracy 0.8581\n",
      "Epoch [43][100]\t Batch [250][550]\t Training Loss 0.8459\t Accuracy 0.8591\n",
      "Epoch [43][100]\t Batch [300][550]\t Training Loss 0.8458\t Accuracy 0.8590\n",
      "Epoch [43][100]\t Batch [350][550]\t Training Loss 0.8518\t Accuracy 0.8580\n",
      "Epoch [43][100]\t Batch [400][550]\t Training Loss 0.8518\t Accuracy 0.8583\n",
      "Epoch [43][100]\t Batch [450][550]\t Training Loss 0.8534\t Accuracy 0.8582\n",
      "Epoch [43][100]\t Batch [500][550]\t Training Loss 0.8541\t Accuracy 0.8578\n",
      "\n",
      "Epoch [43]\t Average training loss 0.8538\t Average training accuracy 0.8576\n",
      "Epoch [43]\t Average validation loss 0.7857\t Average validation accuracy 0.8948\n",
      "\n",
      "Epoch [44][100]\t Batch [0][550]\t Training Loss 0.8220\t Accuracy 0.8700\n",
      "Epoch [44][100]\t Batch [50][550]\t Training Loss 0.8321\t Accuracy 0.8651\n",
      "Epoch [44][100]\t Batch [100][550]\t Training Loss 0.8365\t Accuracy 0.8632\n",
      "Epoch [44][100]\t Batch [150][550]\t Training Loss 0.8441\t Accuracy 0.8585\n",
      "Epoch [44][100]\t Batch [200][550]\t Training Loss 0.8483\t Accuracy 0.8580\n",
      "Epoch [44][100]\t Batch [250][550]\t Training Loss 0.8464\t Accuracy 0.8588\n",
      "Epoch [44][100]\t Batch [300][550]\t Training Loss 0.8463\t Accuracy 0.8588\n",
      "Epoch [44][100]\t Batch [350][550]\t Training Loss 0.8522\t Accuracy 0.8577\n",
      "Epoch [44][100]\t Batch [400][550]\t Training Loss 0.8523\t Accuracy 0.8580\n",
      "Epoch [44][100]\t Batch [450][550]\t Training Loss 0.8539\t Accuracy 0.8579\n",
      "Epoch [44][100]\t Batch [500][550]\t Training Loss 0.8546\t Accuracy 0.8576\n",
      "\n",
      "Epoch [44]\t Average training loss 0.8543\t Average training accuracy 0.8575\n",
      "Epoch [44]\t Average validation loss 0.7863\t Average validation accuracy 0.8948\n",
      "\n",
      "Epoch [45][100]\t Batch [0][550]\t Training Loss 0.8227\t Accuracy 0.8700\n",
      "Epoch [45][100]\t Batch [50][550]\t Training Loss 0.8327\t Accuracy 0.8647\n",
      "Epoch [45][100]\t Batch [100][550]\t Training Loss 0.8370\t Accuracy 0.8628\n",
      "Epoch [45][100]\t Batch [150][550]\t Training Loss 0.8445\t Accuracy 0.8581\n",
      "Epoch [45][100]\t Batch [200][550]\t Training Loss 0.8487\t Accuracy 0.8577\n",
      "Epoch [45][100]\t Batch [250][550]\t Training Loss 0.8468\t Accuracy 0.8586\n",
      "Epoch [45][100]\t Batch [300][550]\t Training Loss 0.8467\t Accuracy 0.8586\n",
      "Epoch [45][100]\t Batch [350][550]\t Training Loss 0.8527\t Accuracy 0.8576\n",
      "Epoch [45][100]\t Batch [400][550]\t Training Loss 0.8527\t Accuracy 0.8578\n",
      "Epoch [45][100]\t Batch [450][550]\t Training Loss 0.8543\t Accuracy 0.8577\n",
      "Epoch [45][100]\t Batch [500][550]\t Training Loss 0.8550\t Accuracy 0.8574\n",
      "\n",
      "Epoch [45]\t Average training loss 0.8547\t Average training accuracy 0.8573\n",
      "Epoch [45]\t Average validation loss 0.7868\t Average validation accuracy 0.8948\n",
      "\n",
      "Epoch [46][100]\t Batch [0][550]\t Training Loss 0.8234\t Accuracy 0.8700\n",
      "Epoch [46][100]\t Batch [50][550]\t Training Loss 0.8331\t Accuracy 0.8645\n",
      "Epoch [46][100]\t Batch [100][550]\t Training Loss 0.8375\t Accuracy 0.8627\n",
      "Epoch [46][100]\t Batch [150][550]\t Training Loss 0.8450\t Accuracy 0.8581\n",
      "Epoch [46][100]\t Batch [200][550]\t Training Loss 0.8491\t Accuracy 0.8577\n",
      "Epoch [46][100]\t Batch [250][550]\t Training Loss 0.8473\t Accuracy 0.8584\n",
      "Epoch [46][100]\t Batch [300][550]\t Training Loss 0.8471\t Accuracy 0.8584\n",
      "Epoch [46][100]\t Batch [350][550]\t Training Loss 0.8531\t Accuracy 0.8574\n",
      "Epoch [46][100]\t Batch [400][550]\t Training Loss 0.8531\t Accuracy 0.8578\n",
      "Epoch [46][100]\t Batch [450][550]\t Training Loss 0.8547\t Accuracy 0.8576\n",
      "Epoch [46][100]\t Batch [500][550]\t Training Loss 0.8554\t Accuracy 0.8573\n",
      "\n",
      "Epoch [46]\t Average training loss 0.8551\t Average training accuracy 0.8572\n",
      "Epoch [46]\t Average validation loss 0.7873\t Average validation accuracy 0.8944\n",
      "\n",
      "Epoch [47][100]\t Batch [0][550]\t Training Loss 0.8240\t Accuracy 0.8700\n",
      "Epoch [47][100]\t Batch [50][550]\t Training Loss 0.8336\t Accuracy 0.8641\n",
      "Epoch [47][100]\t Batch [100][550]\t Training Loss 0.8379\t Accuracy 0.8625\n",
      "Epoch [47][100]\t Batch [150][550]\t Training Loss 0.8454\t Accuracy 0.8579\n",
      "Epoch [47][100]\t Batch [200][550]\t Training Loss 0.8495\t Accuracy 0.8576\n",
      "Epoch [47][100]\t Batch [250][550]\t Training Loss 0.8476\t Accuracy 0.8584\n",
      "Epoch [47][100]\t Batch [300][550]\t Training Loss 0.8475\t Accuracy 0.8583\n",
      "Epoch [47][100]\t Batch [350][550]\t Training Loss 0.8535\t Accuracy 0.8572\n",
      "Epoch [47][100]\t Batch [400][550]\t Training Loss 0.8535\t Accuracy 0.8575\n",
      "Epoch [47][100]\t Batch [450][550]\t Training Loss 0.8551\t Accuracy 0.8573\n",
      "Epoch [47][100]\t Batch [500][550]\t Training Loss 0.8558\t Accuracy 0.8571\n",
      "\n",
      "Epoch [47]\t Average training loss 0.8555\t Average training accuracy 0.8569\n",
      "Epoch [47]\t Average validation loss 0.7878\t Average validation accuracy 0.8946\n",
      "\n",
      "Epoch [48][100]\t Batch [0][550]\t Training Loss 0.8245\t Accuracy 0.8700\n",
      "Epoch [48][100]\t Batch [50][550]\t Training Loss 0.8340\t Accuracy 0.8641\n",
      "Epoch [48][100]\t Batch [100][550]\t Training Loss 0.8383\t Accuracy 0.8624\n",
      "Epoch [48][100]\t Batch [150][550]\t Training Loss 0.8457\t Accuracy 0.8578\n",
      "Epoch [48][100]\t Batch [200][550]\t Training Loss 0.8499\t Accuracy 0.8574\n",
      "Epoch [48][100]\t Batch [250][550]\t Training Loss 0.8480\t Accuracy 0.8583\n",
      "Epoch [48][100]\t Batch [300][550]\t Training Loss 0.8479\t Accuracy 0.8583\n",
      "Epoch [48][100]\t Batch [350][550]\t Training Loss 0.8538\t Accuracy 0.8572\n",
      "Epoch [48][100]\t Batch [400][550]\t Training Loss 0.8538\t Accuracy 0.8574\n",
      "Epoch [48][100]\t Batch [450][550]\t Training Loss 0.8554\t Accuracy 0.8572\n",
      "Epoch [48][100]\t Batch [500][550]\t Training Loss 0.8561\t Accuracy 0.8570\n",
      "\n",
      "Epoch [48]\t Average training loss 0.8558\t Average training accuracy 0.8568\n",
      "Epoch [48]\t Average validation loss 0.7882\t Average validation accuracy 0.8940\n",
      "\n",
      "Epoch [49][100]\t Batch [0][550]\t Training Loss 0.8251\t Accuracy 0.8700\n",
      "Epoch [49][100]\t Batch [50][550]\t Training Loss 0.8343\t Accuracy 0.8637\n",
      "Epoch [49][100]\t Batch [100][550]\t Training Loss 0.8386\t Accuracy 0.8620\n",
      "Epoch [49][100]\t Batch [150][550]\t Training Loss 0.8461\t Accuracy 0.8574\n",
      "Epoch [49][100]\t Batch [200][550]\t Training Loss 0.8502\t Accuracy 0.8571\n",
      "Epoch [49][100]\t Batch [250][550]\t Training Loss 0.8483\t Accuracy 0.8580\n",
      "Epoch [49][100]\t Batch [300][550]\t Training Loss 0.8482\t Accuracy 0.8580\n",
      "Epoch [49][100]\t Batch [350][550]\t Training Loss 0.8541\t Accuracy 0.8570\n",
      "Epoch [49][100]\t Batch [400][550]\t Training Loss 0.8541\t Accuracy 0.8571\n",
      "Epoch [49][100]\t Batch [450][550]\t Training Loss 0.8557\t Accuracy 0.8569\n",
      "Epoch [49][100]\t Batch [500][550]\t Training Loss 0.8564\t Accuracy 0.8568\n",
      "\n",
      "Epoch [49]\t Average training loss 0.8561\t Average training accuracy 0.8566\n",
      "Epoch [49]\t Average validation loss 0.7886\t Average validation accuracy 0.8940\n",
      "\n",
      "Epoch [50][100]\t Batch [0][550]\t Training Loss 0.8255\t Accuracy 0.8700\n",
      "Epoch [50][100]\t Batch [50][550]\t Training Loss 0.8347\t Accuracy 0.8637\n",
      "Epoch [50][100]\t Batch [100][550]\t Training Loss 0.8389\t Accuracy 0.8621\n",
      "Epoch [50][100]\t Batch [150][550]\t Training Loss 0.8464\t Accuracy 0.8574\n",
      "Epoch [50][100]\t Batch [200][550]\t Training Loss 0.8505\t Accuracy 0.8571\n",
      "Epoch [50][100]\t Batch [250][550]\t Training Loss 0.8486\t Accuracy 0.8580\n",
      "Epoch [50][100]\t Batch [300][550]\t Training Loss 0.8485\t Accuracy 0.8579\n",
      "Epoch [50][100]\t Batch [350][550]\t Training Loss 0.8544\t Accuracy 0.8570\n",
      "Epoch [50][100]\t Batch [400][550]\t Training Loss 0.8544\t Accuracy 0.8572\n",
      "Epoch [50][100]\t Batch [450][550]\t Training Loss 0.8560\t Accuracy 0.8570\n",
      "Epoch [50][100]\t Batch [500][550]\t Training Loss 0.8567\t Accuracy 0.8568\n",
      "\n",
      "Epoch [50]\t Average training loss 0.8564\t Average training accuracy 0.8566\n",
      "Epoch [50]\t Average validation loss 0.7890\t Average validation accuracy 0.8940\n",
      "\n",
      "Epoch [51][100]\t Batch [0][550]\t Training Loss 0.8260\t Accuracy 0.8700\n",
      "Epoch [51][100]\t Batch [50][550]\t Training Loss 0.8350\t Accuracy 0.8637\n",
      "Epoch [51][100]\t Batch [100][550]\t Training Loss 0.8392\t Accuracy 0.8621\n",
      "Epoch [51][100]\t Batch [150][550]\t Training Loss 0.8467\t Accuracy 0.8574\n",
      "Epoch [51][100]\t Batch [200][550]\t Training Loss 0.8508\t Accuracy 0.8571\n",
      "Epoch [51][100]\t Batch [250][550]\t Training Loss 0.8489\t Accuracy 0.8580\n",
      "Epoch [51][100]\t Batch [300][550]\t Training Loss 0.8488\t Accuracy 0.8579\n",
      "Epoch [51][100]\t Batch [350][550]\t Training Loss 0.8547\t Accuracy 0.8570\n",
      "Epoch [51][100]\t Batch [400][550]\t Training Loss 0.8547\t Accuracy 0.8571\n",
      "Epoch [51][100]\t Batch [450][550]\t Training Loss 0.8563\t Accuracy 0.8569\n",
      "Epoch [51][100]\t Batch [500][550]\t Training Loss 0.8570\t Accuracy 0.8568\n",
      "\n",
      "Epoch [51]\t Average training loss 0.8567\t Average training accuracy 0.8565\n",
      "Epoch [51]\t Average validation loss 0.7894\t Average validation accuracy 0.8940\n",
      "\n",
      "Epoch [52][100]\t Batch [0][550]\t Training Loss 0.8264\t Accuracy 0.8700\n",
      "Epoch [52][100]\t Batch [50][550]\t Training Loss 0.8353\t Accuracy 0.8635\n",
      "Epoch [52][100]\t Batch [100][550]\t Training Loss 0.8395\t Accuracy 0.8619\n",
      "Epoch [52][100]\t Batch [150][550]\t Training Loss 0.8470\t Accuracy 0.8572\n",
      "Epoch [52][100]\t Batch [200][550]\t Training Loss 0.8511\t Accuracy 0.8569\n",
      "Epoch [52][100]\t Batch [250][550]\t Training Loss 0.8492\t Accuracy 0.8578\n",
      "Epoch [52][100]\t Batch [300][550]\t Training Loss 0.8490\t Accuracy 0.8577\n",
      "Epoch [52][100]\t Batch [350][550]\t Training Loss 0.8549\t Accuracy 0.8568\n",
      "Epoch [52][100]\t Batch [400][550]\t Training Loss 0.8550\t Accuracy 0.8570\n",
      "Epoch [52][100]\t Batch [450][550]\t Training Loss 0.8565\t Accuracy 0.8568\n",
      "Epoch [52][100]\t Batch [500][550]\t Training Loss 0.8572\t Accuracy 0.8567\n",
      "\n",
      "Epoch [52]\t Average training loss 0.8569\t Average training accuracy 0.8564\n",
      "Epoch [52]\t Average validation loss 0.7897\t Average validation accuracy 0.8940\n",
      "\n",
      "Epoch [53][100]\t Batch [0][550]\t Training Loss 0.8268\t Accuracy 0.8700\n",
      "Epoch [53][100]\t Batch [50][550]\t Training Loss 0.8356\t Accuracy 0.8635\n",
      "Epoch [53][100]\t Batch [100][550]\t Training Loss 0.8398\t Accuracy 0.8616\n",
      "Epoch [53][100]\t Batch [150][550]\t Training Loss 0.8472\t Accuracy 0.8570\n",
      "Epoch [53][100]\t Batch [200][550]\t Training Loss 0.8514\t Accuracy 0.8568\n",
      "Epoch [53][100]\t Batch [250][550]\t Training Loss 0.8494\t Accuracy 0.8577\n",
      "Epoch [53][100]\t Batch [300][550]\t Training Loss 0.8493\t Accuracy 0.8576\n",
      "Epoch [53][100]\t Batch [350][550]\t Training Loss 0.8552\t Accuracy 0.8568\n",
      "Epoch [53][100]\t Batch [400][550]\t Training Loss 0.8552\t Accuracy 0.8569\n",
      "Epoch [53][100]\t Batch [450][550]\t Training Loss 0.8568\t Accuracy 0.8567\n",
      "Epoch [53][100]\t Batch [500][550]\t Training Loss 0.8575\t Accuracy 0.8566\n",
      "\n",
      "Epoch [53]\t Average training loss 0.8572\t Average training accuracy 0.8563\n",
      "Epoch [53]\t Average validation loss 0.7900\t Average validation accuracy 0.8936\n",
      "\n",
      "Epoch [54][100]\t Batch [0][550]\t Training Loss 0.8272\t Accuracy 0.8700\n",
      "Epoch [54][100]\t Batch [50][550]\t Training Loss 0.8358\t Accuracy 0.8639\n",
      "Epoch [54][100]\t Batch [100][550]\t Training Loss 0.8400\t Accuracy 0.8618\n",
      "Epoch [54][100]\t Batch [150][550]\t Training Loss 0.8475\t Accuracy 0.8570\n",
      "Epoch [54][100]\t Batch [200][550]\t Training Loss 0.8516\t Accuracy 0.8569\n",
      "Epoch [54][100]\t Batch [250][550]\t Training Loss 0.8496\t Accuracy 0.8577\n",
      "Epoch [54][100]\t Batch [300][550]\t Training Loss 0.8495\t Accuracy 0.8576\n",
      "Epoch [54][100]\t Batch [350][550]\t Training Loss 0.8554\t Accuracy 0.8568\n",
      "Epoch [54][100]\t Batch [400][550]\t Training Loss 0.8554\t Accuracy 0.8569\n",
      "Epoch [54][100]\t Batch [450][550]\t Training Loss 0.8570\t Accuracy 0.8567\n",
      "Epoch [54][100]\t Batch [500][550]\t Training Loss 0.8577\t Accuracy 0.8565\n",
      "\n",
      "Epoch [54]\t Average training loss 0.8574\t Average training accuracy 0.8563\n",
      "Epoch [54]\t Average validation loss 0.7903\t Average validation accuracy 0.8934\n",
      "\n",
      "Epoch [55][100]\t Batch [0][550]\t Training Loss 0.8276\t Accuracy 0.8700\n",
      "Epoch [55][100]\t Batch [50][550]\t Training Loss 0.8361\t Accuracy 0.8639\n",
      "Epoch [55][100]\t Batch [100][550]\t Training Loss 0.8403\t Accuracy 0.8620\n",
      "Epoch [55][100]\t Batch [150][550]\t Training Loss 0.8477\t Accuracy 0.8571\n",
      "Epoch [55][100]\t Batch [200][550]\t Training Loss 0.8518\t Accuracy 0.8568\n",
      "Epoch [55][100]\t Batch [250][550]\t Training Loss 0.8498\t Accuracy 0.8576\n",
      "Epoch [55][100]\t Batch [300][550]\t Training Loss 0.8497\t Accuracy 0.8575\n",
      "Epoch [55][100]\t Batch [350][550]\t Training Loss 0.8556\t Accuracy 0.8568\n",
      "Epoch [55][100]\t Batch [400][550]\t Training Loss 0.8556\t Accuracy 0.8568\n",
      "Epoch [55][100]\t Batch [450][550]\t Training Loss 0.8572\t Accuracy 0.8567\n",
      "Epoch [55][100]\t Batch [500][550]\t Training Loss 0.8579\t Accuracy 0.8565\n",
      "\n",
      "Epoch [55]\t Average training loss 0.8576\t Average training accuracy 0.8562\n",
      "Epoch [55]\t Average validation loss 0.7905\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [56][100]\t Batch [0][550]\t Training Loss 0.8279\t Accuracy 0.8700\n",
      "Epoch [56][100]\t Batch [50][550]\t Training Loss 0.8363\t Accuracy 0.8639\n",
      "Epoch [56][100]\t Batch [100][550]\t Training Loss 0.8405\t Accuracy 0.8622\n",
      "Epoch [56][100]\t Batch [150][550]\t Training Loss 0.8479\t Accuracy 0.8572\n",
      "Epoch [56][100]\t Batch [200][550]\t Training Loss 0.8520\t Accuracy 0.8568\n",
      "Epoch [56][100]\t Batch [250][550]\t Training Loss 0.8500\t Accuracy 0.8577\n",
      "Epoch [56][100]\t Batch [300][550]\t Training Loss 0.8499\t Accuracy 0.8576\n",
      "Epoch [56][100]\t Batch [350][550]\t Training Loss 0.8558\t Accuracy 0.8568\n",
      "Epoch [56][100]\t Batch [400][550]\t Training Loss 0.8558\t Accuracy 0.8569\n",
      "Epoch [56][100]\t Batch [450][550]\t Training Loss 0.8574\t Accuracy 0.8567\n",
      "Epoch [56][100]\t Batch [500][550]\t Training Loss 0.8581\t Accuracy 0.8565\n",
      "\n",
      "Epoch [56]\t Average training loss 0.8578\t Average training accuracy 0.8563\n",
      "Epoch [56]\t Average validation loss 0.7908\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [57][100]\t Batch [0][550]\t Training Loss 0.8282\t Accuracy 0.8700\n",
      "Epoch [57][100]\t Batch [50][550]\t Training Loss 0.8365\t Accuracy 0.8641\n",
      "Epoch [57][100]\t Batch [100][550]\t Training Loss 0.8407\t Accuracy 0.8624\n",
      "Epoch [57][100]\t Batch [150][550]\t Training Loss 0.8481\t Accuracy 0.8572\n",
      "Epoch [57][100]\t Batch [200][550]\t Training Loss 0.8522\t Accuracy 0.8567\n",
      "Epoch [57][100]\t Batch [250][550]\t Training Loss 0.8502\t Accuracy 0.8576\n",
      "Epoch [57][100]\t Batch [300][550]\t Training Loss 0.8501\t Accuracy 0.8575\n",
      "Epoch [57][100]\t Batch [350][550]\t Training Loss 0.8560\t Accuracy 0.8567\n",
      "Epoch [57][100]\t Batch [400][550]\t Training Loss 0.8560\t Accuracy 0.8568\n",
      "Epoch [57][100]\t Batch [450][550]\t Training Loss 0.8576\t Accuracy 0.8566\n",
      "Epoch [57][100]\t Batch [500][550]\t Training Loss 0.8582\t Accuracy 0.8564\n",
      "\n",
      "Epoch [57]\t Average training loss 0.8579\t Average training accuracy 0.8561\n",
      "Epoch [57]\t Average validation loss 0.7910\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [58][100]\t Batch [0][550]\t Training Loss 0.8285\t Accuracy 0.8700\n",
      "Epoch [58][100]\t Batch [50][550]\t Training Loss 0.8367\t Accuracy 0.8643\n",
      "Epoch [58][100]\t Batch [100][550]\t Training Loss 0.8409\t Accuracy 0.8625\n",
      "Epoch [58][100]\t Batch [150][550]\t Training Loss 0.8483\t Accuracy 0.8571\n",
      "Epoch [58][100]\t Batch [200][550]\t Training Loss 0.8524\t Accuracy 0.8565\n",
      "Epoch [58][100]\t Batch [250][550]\t Training Loss 0.8504\t Accuracy 0.8574\n",
      "Epoch [58][100]\t Batch [300][550]\t Training Loss 0.8503\t Accuracy 0.8573\n",
      "Epoch [58][100]\t Batch [350][550]\t Training Loss 0.8561\t Accuracy 0.8566\n",
      "Epoch [58][100]\t Batch [400][550]\t Training Loss 0.8561\t Accuracy 0.8567\n",
      "Epoch [58][100]\t Batch [450][550]\t Training Loss 0.8577\t Accuracy 0.8565\n",
      "Epoch [58][100]\t Batch [500][550]\t Training Loss 0.8584\t Accuracy 0.8563\n",
      "\n",
      "Epoch [58]\t Average training loss 0.8581\t Average training accuracy 0.8560\n",
      "Epoch [58]\t Average validation loss 0.7912\t Average validation accuracy 0.8930\n",
      "\n",
      "Epoch [59][100]\t Batch [0][550]\t Training Loss 0.8288\t Accuracy 0.8700\n",
      "Epoch [59][100]\t Batch [50][550]\t Training Loss 0.8369\t Accuracy 0.8643\n",
      "Epoch [59][100]\t Batch [100][550]\t Training Loss 0.8411\t Accuracy 0.8625\n",
      "Epoch [59][100]\t Batch [150][550]\t Training Loss 0.8484\t Accuracy 0.8569\n",
      "Epoch [59][100]\t Batch [200][550]\t Training Loss 0.8525\t Accuracy 0.8563\n",
      "Epoch [59][100]\t Batch [250][550]\t Training Loss 0.8505\t Accuracy 0.8573\n",
      "Epoch [59][100]\t Batch [300][550]\t Training Loss 0.8504\t Accuracy 0.8573\n",
      "Epoch [59][100]\t Batch [350][550]\t Training Loss 0.8563\t Accuracy 0.8566\n",
      "Epoch [59][100]\t Batch [400][550]\t Training Loss 0.8563\t Accuracy 0.8567\n",
      "Epoch [59][100]\t Batch [450][550]\t Training Loss 0.8579\t Accuracy 0.8565\n",
      "Epoch [59][100]\t Batch [500][550]\t Training Loss 0.8585\t Accuracy 0.8563\n",
      "\n",
      "Epoch [59]\t Average training loss 0.8583\t Average training accuracy 0.8560\n",
      "Epoch [59]\t Average validation loss 0.7914\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [60][100]\t Batch [0][550]\t Training Loss 0.8290\t Accuracy 0.8700\n",
      "Epoch [60][100]\t Batch [50][550]\t Training Loss 0.8371\t Accuracy 0.8641\n",
      "Epoch [60][100]\t Batch [100][550]\t Training Loss 0.8412\t Accuracy 0.8623\n",
      "Epoch [60][100]\t Batch [150][550]\t Training Loss 0.8486\t Accuracy 0.8567\n",
      "Epoch [60][100]\t Batch [200][550]\t Training Loss 0.8527\t Accuracy 0.8561\n",
      "Epoch [60][100]\t Batch [250][550]\t Training Loss 0.8507\t Accuracy 0.8571\n",
      "Epoch [60][100]\t Batch [300][550]\t Training Loss 0.8506\t Accuracy 0.8572\n",
      "Epoch [60][100]\t Batch [350][550]\t Training Loss 0.8564\t Accuracy 0.8565\n",
      "Epoch [60][100]\t Batch [400][550]\t Training Loss 0.8564\t Accuracy 0.8565\n",
      "Epoch [60][100]\t Batch [450][550]\t Training Loss 0.8580\t Accuracy 0.8563\n",
      "Epoch [60][100]\t Batch [500][550]\t Training Loss 0.8587\t Accuracy 0.8562\n",
      "\n",
      "Epoch [60]\t Average training loss 0.8584\t Average training accuracy 0.8559\n",
      "Epoch [60]\t Average validation loss 0.7916\t Average validation accuracy 0.8934\n",
      "\n",
      "Epoch [61][100]\t Batch [0][550]\t Training Loss 0.8293\t Accuracy 0.8700\n",
      "Epoch [61][100]\t Batch [50][550]\t Training Loss 0.8372\t Accuracy 0.8635\n",
      "Epoch [61][100]\t Batch [100][550]\t Training Loss 0.8414\t Accuracy 0.8617\n",
      "Epoch [61][100]\t Batch [150][550]\t Training Loss 0.8487\t Accuracy 0.8563\n",
      "Epoch [61][100]\t Batch [200][550]\t Training Loss 0.8528\t Accuracy 0.8558\n",
      "Epoch [61][100]\t Batch [250][550]\t Training Loss 0.8508\t Accuracy 0.8567\n",
      "Epoch [61][100]\t Batch [300][550]\t Training Loss 0.8507\t Accuracy 0.8567\n",
      "Epoch [61][100]\t Batch [350][550]\t Training Loss 0.8566\t Accuracy 0.8561\n",
      "Epoch [61][100]\t Batch [400][550]\t Training Loss 0.8566\t Accuracy 0.8562\n",
      "Epoch [61][100]\t Batch [450][550]\t Training Loss 0.8581\t Accuracy 0.8561\n",
      "Epoch [61][100]\t Batch [500][550]\t Training Loss 0.8588\t Accuracy 0.8559\n",
      "\n",
      "Epoch [61]\t Average training loss 0.8585\t Average training accuracy 0.8556\n",
      "Epoch [61]\t Average validation loss 0.7918\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [62][100]\t Batch [0][550]\t Training Loss 0.8295\t Accuracy 0.8700\n",
      "Epoch [62][100]\t Batch [50][550]\t Training Loss 0.8374\t Accuracy 0.8633\n",
      "Epoch [62][100]\t Batch [100][550]\t Training Loss 0.8415\t Accuracy 0.8615\n",
      "Epoch [62][100]\t Batch [150][550]\t Training Loss 0.8489\t Accuracy 0.8560\n",
      "Epoch [62][100]\t Batch [200][550]\t Training Loss 0.8530\t Accuracy 0.8554\n",
      "Epoch [62][100]\t Batch [250][550]\t Training Loss 0.8510\t Accuracy 0.8563\n",
      "Epoch [62][100]\t Batch [300][550]\t Training Loss 0.8508\t Accuracy 0.8564\n",
      "Epoch [62][100]\t Batch [350][550]\t Training Loss 0.8567\t Accuracy 0.8559\n",
      "Epoch [62][100]\t Batch [400][550]\t Training Loss 0.8567\t Accuracy 0.8561\n",
      "Epoch [62][100]\t Batch [450][550]\t Training Loss 0.8583\t Accuracy 0.8560\n",
      "Epoch [62][100]\t Batch [500][550]\t Training Loss 0.8589\t Accuracy 0.8558\n",
      "\n",
      "Epoch [62]\t Average training loss 0.8587\t Average training accuracy 0.8555\n",
      "Epoch [62]\t Average validation loss 0.7920\t Average validation accuracy 0.8936\n",
      "\n",
      "Epoch [63][100]\t Batch [0][550]\t Training Loss 0.8297\t Accuracy 0.8700\n",
      "Epoch [63][100]\t Batch [50][550]\t Training Loss 0.8375\t Accuracy 0.8631\n",
      "Epoch [63][100]\t Batch [100][550]\t Training Loss 0.8417\t Accuracy 0.8611\n",
      "Epoch [63][100]\t Batch [150][550]\t Training Loss 0.8490\t Accuracy 0.8556\n",
      "Epoch [63][100]\t Batch [200][550]\t Training Loss 0.8531\t Accuracy 0.8552\n",
      "Epoch [63][100]\t Batch [250][550]\t Training Loss 0.8511\t Accuracy 0.8562\n",
      "Epoch [63][100]\t Batch [300][550]\t Training Loss 0.8510\t Accuracy 0.8563\n",
      "Epoch [63][100]\t Batch [350][550]\t Training Loss 0.8568\t Accuracy 0.8557\n",
      "Epoch [63][100]\t Batch [400][550]\t Training Loss 0.8568\t Accuracy 0.8560\n",
      "Epoch [63][100]\t Batch [450][550]\t Training Loss 0.8584\t Accuracy 0.8558\n",
      "Epoch [63][100]\t Batch [500][550]\t Training Loss 0.8590\t Accuracy 0.8556\n",
      "\n",
      "Epoch [63]\t Average training loss 0.8588\t Average training accuracy 0.8553\n",
      "Epoch [63]\t Average validation loss 0.7922\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [64][100]\t Batch [0][550]\t Training Loss 0.8299\t Accuracy 0.8700\n",
      "Epoch [64][100]\t Batch [50][550]\t Training Loss 0.8377\t Accuracy 0.8629\n",
      "Epoch [64][100]\t Batch [100][550]\t Training Loss 0.8418\t Accuracy 0.8610\n",
      "Epoch [64][100]\t Batch [150][550]\t Training Loss 0.8491\t Accuracy 0.8554\n",
      "Epoch [64][100]\t Batch [200][550]\t Training Loss 0.8532\t Accuracy 0.8550\n",
      "Epoch [64][100]\t Batch [250][550]\t Training Loss 0.8512\t Accuracy 0.8561\n",
      "Epoch [64][100]\t Batch [300][550]\t Training Loss 0.8511\t Accuracy 0.8561\n",
      "Epoch [64][100]\t Batch [350][550]\t Training Loss 0.8569\t Accuracy 0.8556\n",
      "Epoch [64][100]\t Batch [400][550]\t Training Loss 0.8569\t Accuracy 0.8559\n",
      "Epoch [64][100]\t Batch [450][550]\t Training Loss 0.8585\t Accuracy 0.8557\n",
      "Epoch [64][100]\t Batch [500][550]\t Training Loss 0.8592\t Accuracy 0.8555\n",
      "\n",
      "Epoch [64]\t Average training loss 0.8589\t Average training accuracy 0.8552\n",
      "Epoch [64]\t Average validation loss 0.7923\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [65][100]\t Batch [0][550]\t Training Loss 0.8301\t Accuracy 0.8700\n",
      "Epoch [65][100]\t Batch [50][550]\t Training Loss 0.8378\t Accuracy 0.8629\n",
      "Epoch [65][100]\t Batch [100][550]\t Training Loss 0.8419\t Accuracy 0.8611\n",
      "Epoch [65][100]\t Batch [150][550]\t Training Loss 0.8493\t Accuracy 0.8554\n",
      "Epoch [65][100]\t Batch [200][550]\t Training Loss 0.8533\t Accuracy 0.8550\n",
      "Epoch [65][100]\t Batch [250][550]\t Training Loss 0.8513\t Accuracy 0.8560\n",
      "Epoch [65][100]\t Batch [300][550]\t Training Loss 0.8512\t Accuracy 0.8560\n",
      "Epoch [65][100]\t Batch [350][550]\t Training Loss 0.8570\t Accuracy 0.8554\n",
      "Epoch [65][100]\t Batch [400][550]\t Training Loss 0.8570\t Accuracy 0.8557\n",
      "Epoch [65][100]\t Batch [450][550]\t Training Loss 0.8586\t Accuracy 0.8555\n",
      "Epoch [65][100]\t Batch [500][550]\t Training Loss 0.8593\t Accuracy 0.8554\n",
      "\n",
      "Epoch [65]\t Average training loss 0.8590\t Average training accuracy 0.8551\n",
      "Epoch [65]\t Average validation loss 0.7925\t Average validation accuracy 0.8930\n",
      "\n",
      "Epoch [66][100]\t Batch [0][550]\t Training Loss 0.8303\t Accuracy 0.8700\n",
      "Epoch [66][100]\t Batch [50][550]\t Training Loss 0.8379\t Accuracy 0.8629\n",
      "Epoch [66][100]\t Batch [100][550]\t Training Loss 0.8420\t Accuracy 0.8610\n",
      "Epoch [66][100]\t Batch [150][550]\t Training Loss 0.8494\t Accuracy 0.8554\n",
      "Epoch [66][100]\t Batch [200][550]\t Training Loss 0.8534\t Accuracy 0.8549\n",
      "Epoch [66][100]\t Batch [250][550]\t Training Loss 0.8514\t Accuracy 0.8559\n",
      "Epoch [66][100]\t Batch [300][550]\t Training Loss 0.8513\t Accuracy 0.8559\n",
      "Epoch [66][100]\t Batch [350][550]\t Training Loss 0.8571\t Accuracy 0.8553\n",
      "Epoch [66][100]\t Batch [400][550]\t Training Loss 0.8571\t Accuracy 0.8556\n",
      "Epoch [66][100]\t Batch [450][550]\t Training Loss 0.8587\t Accuracy 0.8555\n",
      "Epoch [66][100]\t Batch [500][550]\t Training Loss 0.8593\t Accuracy 0.8553\n",
      "\n",
      "Epoch [66]\t Average training loss 0.8591\t Average training accuracy 0.8550\n",
      "Epoch [66]\t Average validation loss 0.7926\t Average validation accuracy 0.8926\n",
      "\n",
      "Epoch [67][100]\t Batch [0][550]\t Training Loss 0.8305\t Accuracy 0.8700\n",
      "Epoch [67][100]\t Batch [50][550]\t Training Loss 0.8380\t Accuracy 0.8631\n",
      "Epoch [67][100]\t Batch [100][550]\t Training Loss 0.8421\t Accuracy 0.8611\n",
      "Epoch [67][100]\t Batch [150][550]\t Training Loss 0.8495\t Accuracy 0.8554\n",
      "Epoch [67][100]\t Batch [200][550]\t Training Loss 0.8535\t Accuracy 0.8549\n",
      "Epoch [67][100]\t Batch [250][550]\t Training Loss 0.8515\t Accuracy 0.8559\n",
      "Epoch [67][100]\t Batch [300][550]\t Training Loss 0.8514\t Accuracy 0.8559\n",
      "Epoch [67][100]\t Batch [350][550]\t Training Loss 0.8572\t Accuracy 0.8553\n",
      "Epoch [67][100]\t Batch [400][550]\t Training Loss 0.8572\t Accuracy 0.8556\n",
      "Epoch [67][100]\t Batch [450][550]\t Training Loss 0.8588\t Accuracy 0.8555\n",
      "Epoch [67][100]\t Batch [500][550]\t Training Loss 0.8594\t Accuracy 0.8553\n",
      "\n",
      "Epoch [67]\t Average training loss 0.8592\t Average training accuracy 0.8550\n",
      "Epoch [67]\t Average validation loss 0.7927\t Average validation accuracy 0.8924\n",
      "\n",
      "Epoch [68][100]\t Batch [0][550]\t Training Loss 0.8306\t Accuracy 0.8700\n",
      "Epoch [68][100]\t Batch [50][550]\t Training Loss 0.8381\t Accuracy 0.8633\n",
      "Epoch [68][100]\t Batch [100][550]\t Training Loss 0.8422\t Accuracy 0.8613\n",
      "Epoch [68][100]\t Batch [150][550]\t Training Loss 0.8496\t Accuracy 0.8555\n",
      "Epoch [68][100]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8550\n",
      "Epoch [68][100]\t Batch [250][550]\t Training Loss 0.8516\t Accuracy 0.8559\n",
      "Epoch [68][100]\t Batch [300][550]\t Training Loss 0.8515\t Accuracy 0.8559\n",
      "Epoch [68][100]\t Batch [350][550]\t Training Loss 0.8573\t Accuracy 0.8554\n",
      "Epoch [68][100]\t Batch [400][550]\t Training Loss 0.8573\t Accuracy 0.8556\n",
      "Epoch [68][100]\t Batch [450][550]\t Training Loss 0.8589\t Accuracy 0.8554\n",
      "Epoch [68][100]\t Batch [500][550]\t Training Loss 0.8595\t Accuracy 0.8552\n",
      "\n",
      "Epoch [68]\t Average training loss 0.8593\t Average training accuracy 0.8549\n",
      "Epoch [68]\t Average validation loss 0.7928\t Average validation accuracy 0.8922\n",
      "\n",
      "Epoch [69][100]\t Batch [0][550]\t Training Loss 0.8308\t Accuracy 0.8600\n",
      "Epoch [69][100]\t Batch [50][550]\t Training Loss 0.8382\t Accuracy 0.8633\n",
      "Epoch [69][100]\t Batch [100][550]\t Training Loss 0.8423\t Accuracy 0.8613\n",
      "Epoch [69][100]\t Batch [150][550]\t Training Loss 0.8497\t Accuracy 0.8556\n",
      "Epoch [69][100]\t Batch [200][550]\t Training Loss 0.8537\t Accuracy 0.8550\n",
      "Epoch [69][100]\t Batch [250][550]\t Training Loss 0.8517\t Accuracy 0.8558\n",
      "Epoch [69][100]\t Batch [300][550]\t Training Loss 0.8515\t Accuracy 0.8559\n",
      "Epoch [69][100]\t Batch [350][550]\t Training Loss 0.8574\t Accuracy 0.8554\n",
      "Epoch [69][100]\t Batch [400][550]\t Training Loss 0.8574\t Accuracy 0.8556\n",
      "Epoch [69][100]\t Batch [450][550]\t Training Loss 0.8589\t Accuracy 0.8554\n",
      "Epoch [69][100]\t Batch [500][550]\t Training Loss 0.8596\t Accuracy 0.8552\n",
      "\n",
      "Epoch [69]\t Average training loss 0.8593\t Average training accuracy 0.8549\n",
      "Epoch [69]\t Average validation loss 0.7929\t Average validation accuracy 0.8920\n",
      "\n",
      "Epoch [70][100]\t Batch [0][550]\t Training Loss 0.8309\t Accuracy 0.8600\n",
      "Epoch [70][100]\t Batch [50][550]\t Training Loss 0.8383\t Accuracy 0.8635\n",
      "Epoch [70][100]\t Batch [100][550]\t Training Loss 0.8424\t Accuracy 0.8613\n",
      "Epoch [70][100]\t Batch [150][550]\t Training Loss 0.8497\t Accuracy 0.8556\n",
      "Epoch [70][100]\t Batch [200][550]\t Training Loss 0.8538\t Accuracy 0.8549\n",
      "Epoch [70][100]\t Batch [250][550]\t Training Loss 0.8517\t Accuracy 0.8557\n",
      "Epoch [70][100]\t Batch [300][550]\t Training Loss 0.8516\t Accuracy 0.8557\n",
      "Epoch [70][100]\t Batch [350][550]\t Training Loss 0.8574\t Accuracy 0.8552\n",
      "Epoch [70][100]\t Batch [400][550]\t Training Loss 0.8575\t Accuracy 0.8554\n",
      "Epoch [70][100]\t Batch [450][550]\t Training Loss 0.8590\t Accuracy 0.8553\n",
      "Epoch [70][100]\t Batch [500][550]\t Training Loss 0.8597\t Accuracy 0.8551\n",
      "\n",
      "Epoch [70]\t Average training loss 0.8594\t Average training accuracy 0.8547\n",
      "Epoch [70]\t Average validation loss 0.7930\t Average validation accuracy 0.8920\n",
      "\n",
      "Epoch [71][100]\t Batch [0][550]\t Training Loss 0.8311\t Accuracy 0.8600\n",
      "Epoch [71][100]\t Batch [50][550]\t Training Loss 0.8384\t Accuracy 0.8631\n",
      "Epoch [71][100]\t Batch [100][550]\t Training Loss 0.8425\t Accuracy 0.8613\n",
      "Epoch [71][100]\t Batch [150][550]\t Training Loss 0.8498\t Accuracy 0.8557\n",
      "Epoch [71][100]\t Batch [200][550]\t Training Loss 0.8539\t Accuracy 0.8550\n",
      "Epoch [71][100]\t Batch [250][550]\t Training Loss 0.8518\t Accuracy 0.8557\n",
      "Epoch [71][100]\t Batch [300][550]\t Training Loss 0.8517\t Accuracy 0.8556\n",
      "Epoch [71][100]\t Batch [350][550]\t Training Loss 0.8575\t Accuracy 0.8551\n",
      "Epoch [71][100]\t Batch [400][550]\t Training Loss 0.8575\t Accuracy 0.8553\n",
      "Epoch [71][100]\t Batch [450][550]\t Training Loss 0.8591\t Accuracy 0.8552\n",
      "Epoch [71][100]\t Batch [500][550]\t Training Loss 0.8597\t Accuracy 0.8550\n",
      "\n",
      "Epoch [71]\t Average training loss 0.8595\t Average training accuracy 0.8546\n",
      "Epoch [71]\t Average validation loss 0.7931\t Average validation accuracy 0.8922\n",
      "\n",
      "Epoch [72][100]\t Batch [0][550]\t Training Loss 0.8312\t Accuracy 0.8600\n",
      "Epoch [72][100]\t Batch [50][550]\t Training Loss 0.8385\t Accuracy 0.8627\n",
      "Epoch [72][100]\t Batch [100][550]\t Training Loss 0.8426\t Accuracy 0.8612\n",
      "Epoch [72][100]\t Batch [150][550]\t Training Loss 0.8499\t Accuracy 0.8555\n",
      "Epoch [72][100]\t Batch [200][550]\t Training Loss 0.8539\t Accuracy 0.8549\n",
      "Epoch [72][100]\t Batch [250][550]\t Training Loss 0.8519\t Accuracy 0.8554\n",
      "Epoch [72][100]\t Batch [300][550]\t Training Loss 0.8517\t Accuracy 0.8553\n",
      "Epoch [72][100]\t Batch [350][550]\t Training Loss 0.8576\t Accuracy 0.8548\n",
      "Epoch [72][100]\t Batch [400][550]\t Training Loss 0.8576\t Accuracy 0.8551\n",
      "Epoch [72][100]\t Batch [450][550]\t Training Loss 0.8591\t Accuracy 0.8550\n",
      "Epoch [72][100]\t Batch [500][550]\t Training Loss 0.8598\t Accuracy 0.8548\n",
      "\n",
      "Epoch [72]\t Average training loss 0.8595\t Average training accuracy 0.8544\n",
      "Epoch [72]\t Average validation loss 0.7932\t Average validation accuracy 0.8922\n",
      "\n",
      "Epoch [73][100]\t Batch [0][550]\t Training Loss 0.8313\t Accuracy 0.8600\n",
      "Epoch [73][100]\t Batch [50][550]\t Training Loss 0.8386\t Accuracy 0.8624\n",
      "Epoch [73][100]\t Batch [100][550]\t Training Loss 0.8427\t Accuracy 0.8610\n",
      "Epoch [73][100]\t Batch [150][550]\t Training Loss 0.8500\t Accuracy 0.8553\n",
      "Epoch [73][100]\t Batch [200][550]\t Training Loss 0.8540\t Accuracy 0.8547\n",
      "Epoch [73][100]\t Batch [250][550]\t Training Loss 0.8519\t Accuracy 0.8553\n",
      "Epoch [73][100]\t Batch [300][550]\t Training Loss 0.8518\t Accuracy 0.8551\n",
      "Epoch [73][100]\t Batch [350][550]\t Training Loss 0.8576\t Accuracy 0.8547\n",
      "Epoch [73][100]\t Batch [400][550]\t Training Loss 0.8576\t Accuracy 0.8549\n",
      "Epoch [73][100]\t Batch [450][550]\t Training Loss 0.8592\t Accuracy 0.8548\n",
      "Epoch [73][100]\t Batch [500][550]\t Training Loss 0.8598\t Accuracy 0.8546\n",
      "\n",
      "Epoch [73]\t Average training loss 0.8596\t Average training accuracy 0.8542\n",
      "Epoch [73]\t Average validation loss 0.7933\t Average validation accuracy 0.8922\n",
      "\n",
      "Epoch [74][100]\t Batch [0][550]\t Training Loss 0.8314\t Accuracy 0.8600\n",
      "Epoch [74][100]\t Batch [50][550]\t Training Loss 0.8387\t Accuracy 0.8624\n",
      "Epoch [74][100]\t Batch [100][550]\t Training Loss 0.8427\t Accuracy 0.8609\n",
      "Epoch [74][100]\t Batch [150][550]\t Training Loss 0.8500\t Accuracy 0.8552\n",
      "Epoch [74][100]\t Batch [200][550]\t Training Loss 0.8541\t Accuracy 0.8546\n",
      "Epoch [74][100]\t Batch [250][550]\t Training Loss 0.8520\t Accuracy 0.8552\n",
      "Epoch [74][100]\t Batch [300][550]\t Training Loss 0.8519\t Accuracy 0.8551\n",
      "Epoch [74][100]\t Batch [350][550]\t Training Loss 0.8577\t Accuracy 0.8546\n",
      "Epoch [74][100]\t Batch [400][550]\t Training Loss 0.8577\t Accuracy 0.8548\n",
      "Epoch [74][100]\t Batch [450][550]\t Training Loss 0.8593\t Accuracy 0.8547\n",
      "Epoch [74][100]\t Batch [500][550]\t Training Loss 0.8599\t Accuracy 0.8545\n",
      "\n",
      "Epoch [74]\t Average training loss 0.8596\t Average training accuracy 0.8541\n",
      "Epoch [74]\t Average validation loss 0.7934\t Average validation accuracy 0.8920\n",
      "\n",
      "Epoch [75][100]\t Batch [0][550]\t Training Loss 0.8315\t Accuracy 0.8600\n",
      "Epoch [75][100]\t Batch [50][550]\t Training Loss 0.8387\t Accuracy 0.8624\n",
      "Epoch [75][100]\t Batch [100][550]\t Training Loss 0.8428\t Accuracy 0.8608\n",
      "Epoch [75][100]\t Batch [150][550]\t Training Loss 0.8501\t Accuracy 0.8552\n",
      "Epoch [75][100]\t Batch [200][550]\t Training Loss 0.8541\t Accuracy 0.8546\n",
      "Epoch [75][100]\t Batch [250][550]\t Training Loss 0.8521\t Accuracy 0.8551\n",
      "Epoch [75][100]\t Batch [300][550]\t Training Loss 0.8519\t Accuracy 0.8550\n",
      "Epoch [75][100]\t Batch [350][550]\t Training Loss 0.8577\t Accuracy 0.8546\n",
      "Epoch [75][100]\t Batch [400][550]\t Training Loss 0.8578\t Accuracy 0.8548\n",
      "Epoch [75][100]\t Batch [450][550]\t Training Loss 0.8593\t Accuracy 0.8547\n",
      "Epoch [75][100]\t Batch [500][550]\t Training Loss 0.8599\t Accuracy 0.8544\n",
      "\n",
      "Epoch [75]\t Average training loss 0.8597\t Average training accuracy 0.8540\n",
      "Epoch [75]\t Average validation loss 0.7935\t Average validation accuracy 0.8920\n",
      "\n",
      "Epoch [76][100]\t Batch [0][550]\t Training Loss 0.8316\t Accuracy 0.8600\n",
      "Epoch [76][100]\t Batch [50][550]\t Training Loss 0.8388\t Accuracy 0.8624\n",
      "Epoch [76][100]\t Batch [100][550]\t Training Loss 0.8429\t Accuracy 0.8605\n",
      "Epoch [76][100]\t Batch [150][550]\t Training Loss 0.8501\t Accuracy 0.8550\n",
      "Epoch [76][100]\t Batch [200][550]\t Training Loss 0.8542\t Accuracy 0.8544\n",
      "Epoch [76][100]\t Batch [250][550]\t Training Loss 0.8521\t Accuracy 0.8549\n",
      "Epoch [76][100]\t Batch [300][550]\t Training Loss 0.8520\t Accuracy 0.8549\n",
      "Epoch [76][100]\t Batch [350][550]\t Training Loss 0.8578\t Accuracy 0.8544\n",
      "Epoch [76][100]\t Batch [400][550]\t Training Loss 0.8578\t Accuracy 0.8546\n",
      "Epoch [76][100]\t Batch [450][550]\t Training Loss 0.8594\t Accuracy 0.8545\n",
      "Epoch [76][100]\t Batch [500][550]\t Training Loss 0.8600\t Accuracy 0.8543\n",
      "\n",
      "Epoch [76]\t Average training loss 0.8597\t Average training accuracy 0.8539\n",
      "Epoch [76]\t Average validation loss 0.7935\t Average validation accuracy 0.8918\n",
      "\n",
      "Epoch [77][100]\t Batch [0][550]\t Training Loss 0.8317\t Accuracy 0.8600\n",
      "Epoch [77][100]\t Batch [50][550]\t Training Loss 0.8388\t Accuracy 0.8624\n",
      "Epoch [77][100]\t Batch [100][550]\t Training Loss 0.8429\t Accuracy 0.8604\n",
      "Epoch [77][100]\t Batch [150][550]\t Training Loss 0.8502\t Accuracy 0.8548\n",
      "Epoch [77][100]\t Batch [200][550]\t Training Loss 0.8542\t Accuracy 0.8542\n",
      "Epoch [77][100]\t Batch [250][550]\t Training Loss 0.8521\t Accuracy 0.8548\n",
      "Epoch [77][100]\t Batch [300][550]\t Training Loss 0.8520\t Accuracy 0.8548\n",
      "Epoch [77][100]\t Batch [350][550]\t Training Loss 0.8578\t Accuracy 0.8544\n",
      "Epoch [77][100]\t Batch [400][550]\t Training Loss 0.8578\t Accuracy 0.8546\n",
      "Epoch [77][100]\t Batch [450][550]\t Training Loss 0.8594\t Accuracy 0.8545\n",
      "Epoch [77][100]\t Batch [500][550]\t Training Loss 0.8600\t Accuracy 0.8542\n",
      "\n",
      "Epoch [77]\t Average training loss 0.8598\t Average training accuracy 0.8538\n",
      "Epoch [77]\t Average validation loss 0.7936\t Average validation accuracy 0.8916\n",
      "\n",
      "Epoch [78][100]\t Batch [0][550]\t Training Loss 0.8318\t Accuracy 0.8600\n",
      "Epoch [78][100]\t Batch [50][550]\t Training Loss 0.8389\t Accuracy 0.8624\n",
      "Epoch [78][100]\t Batch [100][550]\t Training Loss 0.8430\t Accuracy 0.8604\n",
      "Epoch [78][100]\t Batch [150][550]\t Training Loss 0.8502\t Accuracy 0.8548\n",
      "Epoch [78][100]\t Batch [200][550]\t Training Loss 0.8543\t Accuracy 0.8542\n",
      "Epoch [78][100]\t Batch [250][550]\t Training Loss 0.8522\t Accuracy 0.8548\n",
      "Epoch [78][100]\t Batch [300][550]\t Training Loss 0.8521\t Accuracy 0.8548\n",
      "Epoch [78][100]\t Batch [350][550]\t Training Loss 0.8579\t Accuracy 0.8544\n",
      "Epoch [78][100]\t Batch [400][550]\t Training Loss 0.8579\t Accuracy 0.8545\n",
      "Epoch [78][100]\t Batch [450][550]\t Training Loss 0.8594\t Accuracy 0.8544\n",
      "Epoch [78][100]\t Batch [500][550]\t Training Loss 0.8601\t Accuracy 0.8541\n",
      "\n",
      "Epoch [78]\t Average training loss 0.8598\t Average training accuracy 0.8538\n",
      "Epoch [78]\t Average validation loss 0.7937\t Average validation accuracy 0.8916\n",
      "\n",
      "Epoch [79][100]\t Batch [0][550]\t Training Loss 0.8319\t Accuracy 0.8600\n",
      "Epoch [79][100]\t Batch [50][550]\t Training Loss 0.8390\t Accuracy 0.8620\n",
      "Epoch [79][100]\t Batch [100][550]\t Training Loss 0.8430\t Accuracy 0.8602\n",
      "Epoch [79][100]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8546\n",
      "Epoch [79][100]\t Batch [200][550]\t Training Loss 0.8543\t Accuracy 0.8541\n",
      "Epoch [79][100]\t Batch [250][550]\t Training Loss 0.8522\t Accuracy 0.8547\n",
      "Epoch [79][100]\t Batch [300][550]\t Training Loss 0.8521\t Accuracy 0.8547\n",
      "Epoch [79][100]\t Batch [350][550]\t Training Loss 0.8579\t Accuracy 0.8542\n",
      "Epoch [79][100]\t Batch [400][550]\t Training Loss 0.8579\t Accuracy 0.8544\n",
      "Epoch [79][100]\t Batch [450][550]\t Training Loss 0.8595\t Accuracy 0.8543\n",
      "Epoch [79][100]\t Batch [500][550]\t Training Loss 0.8601\t Accuracy 0.8540\n",
      "\n",
      "Epoch [79]\t Average training loss 0.8599\t Average training accuracy 0.8537\n",
      "Epoch [79]\t Average validation loss 0.7937\t Average validation accuracy 0.8914\n",
      "\n",
      "Epoch [80][100]\t Batch [0][550]\t Training Loss 0.8320\t Accuracy 0.8600\n",
      "Epoch [80][100]\t Batch [50][550]\t Training Loss 0.8390\t Accuracy 0.8618\n",
      "Epoch [80][100]\t Batch [100][550]\t Training Loss 0.8431\t Accuracy 0.8601\n",
      "Epoch [80][100]\t Batch [150][550]\t Training Loss 0.8503\t Accuracy 0.8545\n",
      "Epoch [80][100]\t Batch [200][550]\t Training Loss 0.8544\t Accuracy 0.8540\n",
      "Epoch [80][100]\t Batch [250][550]\t Training Loss 0.8523\t Accuracy 0.8546\n",
      "Epoch [80][100]\t Batch [300][550]\t Training Loss 0.8521\t Accuracy 0.8546\n",
      "Epoch [80][100]\t Batch [350][550]\t Training Loss 0.8579\t Accuracy 0.8542\n",
      "Epoch [80][100]\t Batch [400][550]\t Training Loss 0.8580\t Accuracy 0.8544\n",
      "Epoch [80][100]\t Batch [450][550]\t Training Loss 0.8595\t Accuracy 0.8542\n",
      "Epoch [80][100]\t Batch [500][550]\t Training Loss 0.8601\t Accuracy 0.8539\n",
      "\n",
      "Epoch [80]\t Average training loss 0.8599\t Average training accuracy 0.8536\n",
      "Epoch [80]\t Average validation loss 0.7938\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [81][100]\t Batch [0][550]\t Training Loss 0.8321\t Accuracy 0.8600\n",
      "Epoch [81][100]\t Batch [50][550]\t Training Loss 0.8390\t Accuracy 0.8614\n",
      "Epoch [81][100]\t Batch [100][550]\t Training Loss 0.8431\t Accuracy 0.8599\n",
      "Epoch [81][100]\t Batch [150][550]\t Training Loss 0.8504\t Accuracy 0.8543\n",
      "Epoch [81][100]\t Batch [200][550]\t Training Loss 0.8544\t Accuracy 0.8538\n",
      "Epoch [81][100]\t Batch [250][550]\t Training Loss 0.8523\t Accuracy 0.8545\n",
      "Epoch [81][100]\t Batch [300][550]\t Training Loss 0.8522\t Accuracy 0.8545\n",
      "Epoch [81][100]\t Batch [350][550]\t Training Loss 0.8580\t Accuracy 0.8540\n",
      "Epoch [81][100]\t Batch [400][550]\t Training Loss 0.8580\t Accuracy 0.8542\n",
      "Epoch [81][100]\t Batch [450][550]\t Training Loss 0.8595\t Accuracy 0.8541\n",
      "Epoch [81][100]\t Batch [500][550]\t Training Loss 0.8602\t Accuracy 0.8538\n",
      "\n",
      "Epoch [81]\t Average training loss 0.8599\t Average training accuracy 0.8535\n",
      "Epoch [81]\t Average validation loss 0.7938\t Average validation accuracy 0.8908\n",
      "\n",
      "Epoch [82][100]\t Batch [0][550]\t Training Loss 0.8321\t Accuracy 0.8600\n",
      "Epoch [82][100]\t Batch [50][550]\t Training Loss 0.8391\t Accuracy 0.8614\n",
      "Epoch [82][100]\t Batch [100][550]\t Training Loss 0.8431\t Accuracy 0.8599\n",
      "Epoch [82][100]\t Batch [150][550]\t Training Loss 0.8504\t Accuracy 0.8543\n",
      "Epoch [82][100]\t Batch [200][550]\t Training Loss 0.8544\t Accuracy 0.8537\n",
      "Epoch [82][100]\t Batch [250][550]\t Training Loss 0.8523\t Accuracy 0.8543\n",
      "Epoch [82][100]\t Batch [300][550]\t Training Loss 0.8522\t Accuracy 0.8545\n",
      "Epoch [82][100]\t Batch [350][550]\t Training Loss 0.8580\t Accuracy 0.8540\n",
      "Epoch [82][100]\t Batch [400][550]\t Training Loss 0.8580\t Accuracy 0.8541\n",
      "Epoch [82][100]\t Batch [450][550]\t Training Loss 0.8596\t Accuracy 0.8540\n",
      "Epoch [82][100]\t Batch [500][550]\t Training Loss 0.8602\t Accuracy 0.8537\n",
      "\n",
      "Epoch [82]\t Average training loss 0.8599\t Average training accuracy 0.8534\n",
      "Epoch [82]\t Average validation loss 0.7939\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [83][100]\t Batch [0][550]\t Training Loss 0.8322\t Accuracy 0.8600\n",
      "Epoch [83][100]\t Batch [50][550]\t Training Loss 0.8391\t Accuracy 0.8616\n",
      "Epoch [83][100]\t Batch [100][550]\t Training Loss 0.8432\t Accuracy 0.8601\n",
      "Epoch [83][100]\t Batch [150][550]\t Training Loss 0.8504\t Accuracy 0.8545\n",
      "Epoch [83][100]\t Batch [200][550]\t Training Loss 0.8545\t Accuracy 0.8539\n",
      "Epoch [83][100]\t Batch [250][550]\t Training Loss 0.8524\t Accuracy 0.8545\n",
      "Epoch [83][100]\t Batch [300][550]\t Training Loss 0.8522\t Accuracy 0.8546\n",
      "Epoch [83][100]\t Batch [350][550]\t Training Loss 0.8580\t Accuracy 0.8540\n",
      "Epoch [83][100]\t Batch [400][550]\t Training Loss 0.8580\t Accuracy 0.8542\n",
      "Epoch [83][100]\t Batch [450][550]\t Training Loss 0.8596\t Accuracy 0.8541\n",
      "Epoch [83][100]\t Batch [500][550]\t Training Loss 0.8602\t Accuracy 0.8537\n",
      "\n",
      "Epoch [83]\t Average training loss 0.8600\t Average training accuracy 0.8534\n",
      "Epoch [83]\t Average validation loss 0.7939\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [84][100]\t Batch [0][550]\t Training Loss 0.8323\t Accuracy 0.8600\n",
      "Epoch [84][100]\t Batch [50][550]\t Training Loss 0.8392\t Accuracy 0.8614\n",
      "Epoch [84][100]\t Batch [100][550]\t Training Loss 0.8432\t Accuracy 0.8599\n",
      "Epoch [84][100]\t Batch [150][550]\t Training Loss 0.8505\t Accuracy 0.8544\n",
      "Epoch [84][100]\t Batch [200][550]\t Training Loss 0.8545\t Accuracy 0.8538\n",
      "Epoch [84][100]\t Batch [250][550]\t Training Loss 0.8524\t Accuracy 0.8544\n",
      "Epoch [84][100]\t Batch [300][550]\t Training Loss 0.8522\t Accuracy 0.8545\n",
      "Epoch [84][100]\t Batch [350][550]\t Training Loss 0.8580\t Accuracy 0.8540\n",
      "Epoch [84][100]\t Batch [400][550]\t Training Loss 0.8581\t Accuracy 0.8542\n",
      "Epoch [84][100]\t Batch [450][550]\t Training Loss 0.8596\t Accuracy 0.8541\n",
      "Epoch [84][100]\t Batch [500][550]\t Training Loss 0.8602\t Accuracy 0.8537\n",
      "\n",
      "Epoch [84]\t Average training loss 0.8600\t Average training accuracy 0.8533\n",
      "Epoch [84]\t Average validation loss 0.7939\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [85][100]\t Batch [0][550]\t Training Loss 0.8323\t Accuracy 0.8600\n",
      "Epoch [85][100]\t Batch [50][550]\t Training Loss 0.8392\t Accuracy 0.8610\n",
      "Epoch [85][100]\t Batch [100][550]\t Training Loss 0.8432\t Accuracy 0.8597\n",
      "Epoch [85][100]\t Batch [150][550]\t Training Loss 0.8505\t Accuracy 0.8542\n",
      "Epoch [85][100]\t Batch [200][550]\t Training Loss 0.8545\t Accuracy 0.8537\n",
      "Epoch [85][100]\t Batch [250][550]\t Training Loss 0.8524\t Accuracy 0.8543\n",
      "Epoch [85][100]\t Batch [300][550]\t Training Loss 0.8523\t Accuracy 0.8544\n",
      "Epoch [85][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8539\n",
      "Epoch [85][100]\t Batch [400][550]\t Training Loss 0.8581\t Accuracy 0.8541\n",
      "Epoch [85][100]\t Batch [450][550]\t Training Loss 0.8596\t Accuracy 0.8540\n",
      "Epoch [85][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8537\n",
      "\n",
      "Epoch [85]\t Average training loss 0.8600\t Average training accuracy 0.8532\n",
      "Epoch [85]\t Average validation loss 0.7940\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [86][100]\t Batch [0][550]\t Training Loss 0.8324\t Accuracy 0.8600\n",
      "Epoch [86][100]\t Batch [50][550]\t Training Loss 0.8392\t Accuracy 0.8608\n",
      "Epoch [86][100]\t Batch [100][550]\t Training Loss 0.8433\t Accuracy 0.8596\n",
      "Epoch [86][100]\t Batch [150][550]\t Training Loss 0.8505\t Accuracy 0.8541\n",
      "Epoch [86][100]\t Batch [200][550]\t Training Loss 0.8545\t Accuracy 0.8536\n",
      "Epoch [86][100]\t Batch [250][550]\t Training Loss 0.8524\t Accuracy 0.8542\n",
      "Epoch [86][100]\t Batch [300][550]\t Training Loss 0.8523\t Accuracy 0.8543\n",
      "Epoch [86][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8538\n",
      "Epoch [86][100]\t Batch [400][550]\t Training Loss 0.8581\t Accuracy 0.8540\n",
      "Epoch [86][100]\t Batch [450][550]\t Training Loss 0.8596\t Accuracy 0.8540\n",
      "Epoch [86][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8536\n",
      "\n",
      "Epoch [86]\t Average training loss 0.8600\t Average training accuracy 0.8531\n",
      "Epoch [86]\t Average validation loss 0.7940\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [87][100]\t Batch [0][550]\t Training Loss 0.8324\t Accuracy 0.8600\n",
      "Epoch [87][100]\t Batch [50][550]\t Training Loss 0.8393\t Accuracy 0.8604\n",
      "Epoch [87][100]\t Batch [100][550]\t Training Loss 0.8433\t Accuracy 0.8595\n",
      "Epoch [87][100]\t Batch [150][550]\t Training Loss 0.8505\t Accuracy 0.8540\n",
      "Epoch [87][100]\t Batch [200][550]\t Training Loss 0.8546\t Accuracy 0.8535\n",
      "Epoch [87][100]\t Batch [250][550]\t Training Loss 0.8524\t Accuracy 0.8541\n",
      "Epoch [87][100]\t Batch [300][550]\t Training Loss 0.8523\t Accuracy 0.8542\n",
      "Epoch [87][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8538\n",
      "Epoch [87][100]\t Batch [400][550]\t Training Loss 0.8581\t Accuracy 0.8539\n",
      "Epoch [87][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8539\n",
      "Epoch [87][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8536\n",
      "\n",
      "Epoch [87]\t Average training loss 0.8601\t Average training accuracy 0.8531\n",
      "Epoch [87]\t Average validation loss 0.7940\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [88][100]\t Batch [0][550]\t Training Loss 0.8325\t Accuracy 0.8600\n",
      "Epoch [88][100]\t Batch [50][550]\t Training Loss 0.8393\t Accuracy 0.8604\n",
      "Epoch [88][100]\t Batch [100][550]\t Training Loss 0.8433\t Accuracy 0.8595\n",
      "Epoch [88][100]\t Batch [150][550]\t Training Loss 0.8505\t Accuracy 0.8540\n",
      "Epoch [88][100]\t Batch [200][550]\t Training Loss 0.8546\t Accuracy 0.8535\n",
      "Epoch [88][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8541\n",
      "Epoch [88][100]\t Batch [300][550]\t Training Loss 0.8523\t Accuracy 0.8542\n",
      "Epoch [88][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8537\n",
      "Epoch [88][100]\t Batch [400][550]\t Training Loss 0.8581\t Accuracy 0.8539\n",
      "Epoch [88][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8539\n",
      "Epoch [88][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8536\n",
      "\n",
      "Epoch [88]\t Average training loss 0.8601\t Average training accuracy 0.8531\n",
      "Epoch [88]\t Average validation loss 0.7940\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [89][100]\t Batch [0][550]\t Training Loss 0.8325\t Accuracy 0.8600\n",
      "Epoch [89][100]\t Batch [50][550]\t Training Loss 0.8393\t Accuracy 0.8604\n",
      "Epoch [89][100]\t Batch [100][550]\t Training Loss 0.8433\t Accuracy 0.8595\n",
      "Epoch [89][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8539\n",
      "Epoch [89][100]\t Batch [200][550]\t Training Loss 0.8546\t Accuracy 0.8534\n",
      "Epoch [89][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8540\n",
      "Epoch [89][100]\t Batch [300][550]\t Training Loss 0.8523\t Accuracy 0.8541\n",
      "Epoch [89][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8537\n",
      "Epoch [89][100]\t Batch [400][550]\t Training Loss 0.8581\t Accuracy 0.8539\n",
      "Epoch [89][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8539\n",
      "Epoch [89][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8536\n",
      "\n",
      "Epoch [89]\t Average training loss 0.8601\t Average training accuracy 0.8531\n",
      "Epoch [89]\t Average validation loss 0.7941\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [90][100]\t Batch [0][550]\t Training Loss 0.8326\t Accuracy 0.8600\n",
      "Epoch [90][100]\t Batch [50][550]\t Training Loss 0.8393\t Accuracy 0.8604\n",
      "Epoch [90][100]\t Batch [100][550]\t Training Loss 0.8433\t Accuracy 0.8595\n",
      "Epoch [90][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8538\n",
      "Epoch [90][100]\t Batch [200][550]\t Training Loss 0.8546\t Accuracy 0.8533\n",
      "Epoch [90][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8539\n",
      "Epoch [90][100]\t Batch [300][550]\t Training Loss 0.8523\t Accuracy 0.8540\n",
      "Epoch [90][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8536\n",
      "Epoch [90][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8538\n",
      "Epoch [90][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8538\n",
      "Epoch [90][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8535\n",
      "\n",
      "Epoch [90]\t Average training loss 0.8601\t Average training accuracy 0.8530\n",
      "Epoch [90]\t Average validation loss 0.7941\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [91][100]\t Batch [0][550]\t Training Loss 0.8326\t Accuracy 0.8600\n",
      "Epoch [91][100]\t Batch [50][550]\t Training Loss 0.8393\t Accuracy 0.8602\n",
      "Epoch [91][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8594\n",
      "Epoch [91][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8537\n",
      "Epoch [91][100]\t Batch [200][550]\t Training Loss 0.8546\t Accuracy 0.8532\n",
      "Epoch [91][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8538\n",
      "Epoch [91][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8540\n",
      "Epoch [91][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8535\n",
      "Epoch [91][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8536\n",
      "Epoch [91][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8536\n",
      "Epoch [91][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8534\n",
      "\n",
      "Epoch [91]\t Average training loss 0.8601\t Average training accuracy 0.8529\n",
      "Epoch [91]\t Average validation loss 0.7941\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [92][100]\t Batch [0][550]\t Training Loss 0.8326\t Accuracy 0.8600\n",
      "Epoch [92][100]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8602\n",
      "Epoch [92][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8594\n",
      "Epoch [92][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8537\n",
      "Epoch [92][100]\t Batch [200][550]\t Training Loss 0.8546\t Accuracy 0.8532\n",
      "Epoch [92][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8537\n",
      "Epoch [92][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8539\n",
      "Epoch [92][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8534\n",
      "Epoch [92][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8535\n",
      "Epoch [92][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8536\n",
      "Epoch [92][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8533\n",
      "\n",
      "Epoch [92]\t Average training loss 0.8601\t Average training accuracy 0.8528\n",
      "Epoch [92]\t Average validation loss 0.7941\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [93][100]\t Batch [0][550]\t Training Loss 0.8327\t Accuracy 0.8600\n",
      "Epoch [93][100]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8602\n",
      "Epoch [93][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8594\n",
      "Epoch [93][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8538\n",
      "Epoch [93][100]\t Batch [200][550]\t Training Loss 0.8547\t Accuracy 0.8532\n",
      "Epoch [93][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8537\n",
      "Epoch [93][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8539\n",
      "Epoch [93][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8534\n",
      "Epoch [93][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8536\n",
      "Epoch [93][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8536\n",
      "Epoch [93][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8533\n",
      "\n",
      "Epoch [93]\t Average training loss 0.8601\t Average training accuracy 0.8528\n",
      "Epoch [93]\t Average validation loss 0.7941\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [94][100]\t Batch [0][550]\t Training Loss 0.8327\t Accuracy 0.8600\n",
      "Epoch [94][100]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8602\n",
      "Epoch [94][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8595\n",
      "Epoch [94][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8538\n",
      "Epoch [94][100]\t Batch [200][550]\t Training Loss 0.8547\t Accuracy 0.8533\n",
      "Epoch [94][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8538\n",
      "Epoch [94][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8540\n",
      "Epoch [94][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8534\n",
      "Epoch [94][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8536\n",
      "Epoch [94][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8536\n",
      "Epoch [94][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8534\n",
      "\n",
      "Epoch [94]\t Average training loss 0.8601\t Average training accuracy 0.8528\n",
      "Epoch [94]\t Average validation loss 0.7942\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [95][100]\t Batch [0][550]\t Training Loss 0.8327\t Accuracy 0.8600\n",
      "Epoch [95][100]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8600\n",
      "Epoch [95][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8594\n",
      "Epoch [95][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8536\n",
      "Epoch [95][100]\t Batch [200][550]\t Training Loss 0.8547\t Accuracy 0.8532\n",
      "Epoch [95][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8536\n",
      "Epoch [95][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8538\n",
      "Epoch [95][100]\t Batch [350][550]\t Training Loss 0.8582\t Accuracy 0.8533\n",
      "Epoch [95][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8535\n",
      "Epoch [95][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8535\n",
      "Epoch [95][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8532\n",
      "\n",
      "Epoch [95]\t Average training loss 0.8601\t Average training accuracy 0.8527\n",
      "Epoch [95]\t Average validation loss 0.7942\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [96][100]\t Batch [0][550]\t Training Loss 0.8328\t Accuracy 0.8600\n",
      "Epoch [96][100]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8600\n",
      "Epoch [96][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8593\n",
      "Epoch [96][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8535\n",
      "Epoch [96][100]\t Batch [200][550]\t Training Loss 0.8547\t Accuracy 0.8531\n",
      "Epoch [96][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8535\n",
      "Epoch [96][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8537\n",
      "Epoch [96][100]\t Batch [350][550]\t Training Loss 0.8582\t Accuracy 0.8532\n",
      "Epoch [96][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8534\n",
      "Epoch [96][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8534\n",
      "Epoch [96][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8532\n",
      "\n",
      "Epoch [96]\t Average training loss 0.8601\t Average training accuracy 0.8526\n",
      "Epoch [96]\t Average validation loss 0.7942\t Average validation accuracy 0.8910\n",
      "\n",
      "Epoch [97][100]\t Batch [0][550]\t Training Loss 0.8328\t Accuracy 0.8600\n",
      "Epoch [97][100]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8598\n",
      "Epoch [97][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8592\n",
      "Epoch [97][100]\t Batch [150][550]\t Training Loss 0.8506\t Accuracy 0.8534\n",
      "Epoch [97][100]\t Batch [200][550]\t Training Loss 0.8547\t Accuracy 0.8530\n",
      "Epoch [97][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8534\n",
      "Epoch [97][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8537\n",
      "Epoch [97][100]\t Batch [350][550]\t Training Loss 0.8582\t Accuracy 0.8532\n",
      "Epoch [97][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8534\n",
      "Epoch [97][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8533\n",
      "Epoch [97][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8531\n",
      "\n",
      "Epoch [97]\t Average training loss 0.8601\t Average training accuracy 0.8526\n",
      "Epoch [97]\t Average validation loss 0.7942\t Average validation accuracy 0.8906\n",
      "\n",
      "Epoch [98][100]\t Batch [0][550]\t Training Loss 0.8328\t Accuracy 0.8600\n",
      "Epoch [98][100]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8598\n",
      "Epoch [98][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8590\n",
      "Epoch [98][100]\t Batch [150][550]\t Training Loss 0.8507\t Accuracy 0.8532\n",
      "Epoch [98][100]\t Batch [200][550]\t Training Loss 0.8547\t Accuracy 0.8528\n",
      "Epoch [98][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8533\n",
      "Epoch [98][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8536\n",
      "Epoch [98][100]\t Batch [350][550]\t Training Loss 0.8582\t Accuracy 0.8531\n",
      "Epoch [98][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8533\n",
      "Epoch [98][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8532\n",
      "Epoch [98][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8530\n",
      "\n",
      "Epoch [98]\t Average training loss 0.8601\t Average training accuracy 0.8525\n",
      "Epoch [98]\t Average validation loss 0.7942\t Average validation accuracy 0.8904\n",
      "\n",
      "Epoch [99][100]\t Batch [0][550]\t Training Loss 0.8328\t Accuracy 0.8600\n",
      "Epoch [99][100]\t Batch [50][550]\t Training Loss 0.8394\t Accuracy 0.8596\n",
      "Epoch [99][100]\t Batch [100][550]\t Training Loss 0.8434\t Accuracy 0.8588\n",
      "Epoch [99][100]\t Batch [150][550]\t Training Loss 0.8507\t Accuracy 0.8531\n",
      "Epoch [99][100]\t Batch [200][550]\t Training Loss 0.8547\t Accuracy 0.8526\n",
      "Epoch [99][100]\t Batch [250][550]\t Training Loss 0.8525\t Accuracy 0.8532\n",
      "Epoch [99][100]\t Batch [300][550]\t Training Loss 0.8524\t Accuracy 0.8535\n",
      "Epoch [99][100]\t Batch [350][550]\t Training Loss 0.8581\t Accuracy 0.8530\n",
      "Epoch [99][100]\t Batch [400][550]\t Training Loss 0.8582\t Accuracy 0.8532\n",
      "Epoch [99][100]\t Batch [450][550]\t Training Loss 0.8597\t Accuracy 0.8531\n",
      "Epoch [99][100]\t Batch [500][550]\t Training Loss 0.8603\t Accuracy 0.8529\n",
      "\n",
      "Epoch [99]\t Average training loss 0.8601\t Average training accuracy 0.8524\n",
      "Epoch [99]\t Average validation loss 0.7942\t Average validation accuracy 0.8902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max epochs\n",
    "reluMLP, relu_loss, relu_acc = train(model=reluMLP, criterion=criterion, optimizer=SGD(0.001, 0.1), dataset=data_train, max_epoch=100, batch_size=100, disp_freq=disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_epoch = 100\n",
    "plot_graph({\n",
    "    \"x\": { \"max_epoch\": [i for i in range(max_epoch)]}, \n",
    "    \"y\": [{ \"loss\": relu_loss }, \n",
    "          { \"accuracy\": relu_acc }]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/maffinnn/Desktop/DL Lab/Lab2/mlp/solver.py:13: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 16:37:22.673392: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-11 16:37:22.674000: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-03-11 16:37:22.900439: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][200]\t Batch [0][550]\t Training Loss 2.5871\t Accuracy 0.1100\n",
      "Epoch [0][200]\t Batch [50][550]\t Training Loss 2.5545\t Accuracy 0.1210\n",
      "Epoch [0][200]\t Batch [100][550]\t Training Loss 2.4717\t Accuracy 0.1324\n",
      "Epoch [0][200]\t Batch [150][550]\t Training Loss 2.4144\t Accuracy 0.1500\n",
      "Epoch [0][200]\t Batch [200][550]\t Training Loss 2.3677\t Accuracy 0.1646\n",
      "Epoch [0][200]\t Batch [250][550]\t Training Loss 2.3288\t Accuracy 0.1814\n",
      "Epoch [0][200]\t Batch [300][550]\t Training Loss 2.2958\t Accuracy 0.1971\n",
      "Epoch [0][200]\t Batch [350][550]\t Training Loss 2.2700\t Accuracy 0.2118\n",
      "Epoch [0][200]\t Batch [400][550]\t Training Loss 2.2480\t Accuracy 0.2284\n",
      "Epoch [0][200]\t Batch [450][550]\t Training Loss 2.2293\t Accuracy 0.2467\n",
      "Epoch [0][200]\t Batch [500][550]\t Training Loss 2.2135\t Accuracy 0.2638\n",
      "\n",
      "Epoch [0]\t Average training loss 2.1999\t Average training accuracy 0.2814\n",
      "Epoch [0]\t Average validation loss 2.0498\t Average validation accuracy 0.4980\n",
      "\n",
      "Epoch [1][200]\t Batch [0][550]\t Training Loss 2.0309\t Accuracy 0.5900\n",
      "Epoch [1][200]\t Batch [50][550]\t Training Loss 2.0589\t Accuracy 0.4808\n",
      "Epoch [1][200]\t Batch [100][550]\t Training Loss 2.0569\t Accuracy 0.4965\n",
      "Epoch [1][200]\t Batch [150][550]\t Training Loss 2.0593\t Accuracy 0.5042\n",
      "Epoch [1][200]\t Batch [200][550]\t Training Loss 2.0621\t Accuracy 0.5101\n",
      "Epoch [1][200]\t Batch [250][550]\t Training Loss 2.0632\t Accuracy 0.5210\n",
      "Epoch [1][200]\t Batch [300][550]\t Training Loss 2.0642\t Accuracy 0.5302\n",
      "Epoch [1][200]\t Batch [350][550]\t Training Loss 2.0675\t Accuracy 0.5364\n",
      "Epoch [1][200]\t Batch [400][550]\t Training Loss 2.0698\t Accuracy 0.5438\n",
      "Epoch [1][200]\t Batch [450][550]\t Training Loss 2.0726\t Accuracy 0.5504\n",
      "Epoch [1][200]\t Batch [500][550]\t Training Loss 2.0753\t Accuracy 0.5571\n",
      "\n",
      "Epoch [1]\t Average training loss 2.0778\t Average training accuracy 0.5631\n",
      "Epoch [1]\t Average validation loss 2.0997\t Average validation accuracy 0.6652\n",
      "\n",
      "Epoch [2][200]\t Batch [0][550]\t Training Loss 2.0895\t Accuracy 0.7300\n",
      "Epoch [2][200]\t Batch [50][550]\t Training Loss 2.1090\t Accuracy 0.6363\n",
      "Epoch [2][200]\t Batch [100][550]\t Training Loss 2.1116\t Accuracy 0.6422\n",
      "Epoch [2][200]\t Batch [150][550]\t Training Loss 2.1163\t Accuracy 0.6382\n",
      "Epoch [2][200]\t Batch [200][550]\t Training Loss 2.1213\t Accuracy 0.6339\n",
      "Epoch [2][200]\t Batch [250][550]\t Training Loss 2.1247\t Accuracy 0.6351\n",
      "Epoch [2][200]\t Batch [300][550]\t Training Loss 2.1278\t Accuracy 0.6376\n",
      "Epoch [2][200]\t Batch [350][550]\t Training Loss 2.1322\t Accuracy 0.6382\n",
      "Epoch [2][200]\t Batch [400][550]\t Training Loss 2.1356\t Accuracy 0.6396\n",
      "Epoch [2][200]\t Batch [450][550]\t Training Loss 2.1393\t Accuracy 0.6407\n",
      "Epoch [2][200]\t Batch [500][550]\t Training Loss 2.1427\t Accuracy 0.6422\n",
      "\n",
      "Epoch [2]\t Average training loss 2.1458\t Average training accuracy 0.6435\n",
      "Epoch [2]\t Average validation loss 2.1761\t Average validation accuracy 0.6932\n",
      "\n",
      "Epoch [3][200]\t Batch [0][550]\t Training Loss 2.1703\t Accuracy 0.7600\n",
      "Epoch [3][200]\t Batch [50][550]\t Training Loss 2.1825\t Accuracy 0.6539\n",
      "Epoch [3][200]\t Batch [100][550]\t Training Loss 2.1847\t Accuracy 0.6595\n",
      "Epoch [3][200]\t Batch [150][550]\t Training Loss 2.1882\t Accuracy 0.6511\n",
      "Epoch [3][200]\t Batch [200][550]\t Training Loss 2.1917\t Accuracy 0.6443\n",
      "Epoch [3][200]\t Batch [250][550]\t Training Loss 2.1944\t Accuracy 0.6429\n",
      "Epoch [3][200]\t Batch [300][550]\t Training Loss 2.1967\t Accuracy 0.6431\n",
      "Epoch [3][200]\t Batch [350][550]\t Training Loss 2.1998\t Accuracy 0.6419\n",
      "Epoch [3][200]\t Batch [400][550]\t Training Loss 2.2023\t Accuracy 0.6409\n",
      "Epoch [3][200]\t Batch [450][550]\t Training Loss 2.2049\t Accuracy 0.6403\n",
      "Epoch [3][200]\t Batch [500][550]\t Training Loss 2.2073\t Accuracy 0.6394\n",
      "\n",
      "Epoch [3]\t Average training loss 2.2094\t Average training accuracy 0.6392\n",
      "Epoch [3]\t Average validation loss 2.2310\t Average validation accuracy 0.6668\n",
      "\n",
      "Epoch [4][200]\t Batch [0][550]\t Training Loss 2.2277\t Accuracy 0.7200\n",
      "Epoch [4][200]\t Batch [50][550]\t Training Loss 2.2348\t Accuracy 0.6288\n",
      "Epoch [4][200]\t Batch [100][550]\t Training Loss 2.2362\t Accuracy 0.6347\n",
      "Epoch [4][200]\t Batch [150][550]\t Training Loss 2.2383\t Accuracy 0.6268\n",
      "Epoch [4][200]\t Batch [200][550]\t Training Loss 2.2404\t Accuracy 0.6197\n",
      "Epoch [4][200]\t Batch [250][550]\t Training Loss 2.2420\t Accuracy 0.6169\n",
      "Epoch [4][200]\t Batch [300][550]\t Training Loss 2.2435\t Accuracy 0.6149\n",
      "Epoch [4][200]\t Batch [350][550]\t Training Loss 2.2454\t Accuracy 0.6117\n",
      "Epoch [4][200]\t Batch [400][550]\t Training Loss 2.2468\t Accuracy 0.6096\n",
      "Epoch [4][200]\t Batch [450][550]\t Training Loss 2.2484\t Accuracy 0.6076\n",
      "Epoch [4][200]\t Batch [500][550]\t Training Loss 2.2498\t Accuracy 0.6050\n",
      "\n",
      "Epoch [4]\t Average training loss 2.2511\t Average training accuracy 0.6033\n",
      "Epoch [4]\t Average validation loss 2.2640\t Average validation accuracy 0.6134\n",
      "\n",
      "Epoch [5][200]\t Batch [0][550]\t Training Loss 2.2619\t Accuracy 0.6900\n",
      "Epoch [5][200]\t Batch [50][550]\t Training Loss 2.2660\t Accuracy 0.5782\n",
      "Epoch [5][200]\t Batch [100][550]\t Training Loss 2.2668\t Accuracy 0.5801\n",
      "Epoch [5][200]\t Batch [150][550]\t Training Loss 2.2679\t Accuracy 0.5687\n",
      "Epoch [5][200]\t Batch [200][550]\t Training Loss 2.2691\t Accuracy 0.5604\n",
      "Epoch [5][200]\t Batch [250][550]\t Training Loss 2.2701\t Accuracy 0.5561\n",
      "Epoch [5][200]\t Batch [300][550]\t Training Loss 2.2709\t Accuracy 0.5522\n",
      "Epoch [5][200]\t Batch [350][550]\t Training Loss 2.2719\t Accuracy 0.5448\n",
      "Epoch [5][200]\t Batch [400][550]\t Training Loss 2.2727\t Accuracy 0.5407\n",
      "Epoch [5][200]\t Batch [450][550]\t Training Loss 2.2736\t Accuracy 0.5361\n",
      "Epoch [5][200]\t Batch [500][550]\t Training Loss 2.2743\t Accuracy 0.5310\n",
      "\n",
      "Epoch [5]\t Average training loss 2.2751\t Average training accuracy 0.5268\n",
      "Epoch [5]\t Average validation loss 2.2822\t Average validation accuracy 0.5006\n",
      "\n",
      "Epoch [6][200]\t Batch [0][550]\t Training Loss 2.2806\t Accuracy 0.5600\n",
      "Epoch [6][200]\t Batch [50][550]\t Training Loss 2.2832\t Accuracy 0.4727\n",
      "Epoch [6][200]\t Batch [100][550]\t Training Loss 2.2836\t Accuracy 0.4697\n",
      "Epoch [6][200]\t Batch [150][550]\t Training Loss 2.2842\t Accuracy 0.4577\n",
      "Epoch [6][200]\t Batch [200][550]\t Training Loss 2.2848\t Accuracy 0.4473\n",
      "Epoch [6][200]\t Batch [250][550]\t Training Loss 2.2853\t Accuracy 0.4408\n",
      "Epoch [6][200]\t Batch [300][550]\t Training Loss 2.2858\t Accuracy 0.4339\n",
      "Epoch [6][200]\t Batch [350][550]\t Training Loss 2.2863\t Accuracy 0.4247\n",
      "Epoch [6][200]\t Batch [400][550]\t Training Loss 2.2868\t Accuracy 0.4179\n",
      "Epoch [6][200]\t Batch [450][550]\t Training Loss 2.2872\t Accuracy 0.4115\n",
      "Epoch [6][200]\t Batch [500][550]\t Training Loss 2.2876\t Accuracy 0.4051\n",
      "\n",
      "Epoch [6]\t Average training loss 2.2880\t Average training accuracy 0.3987\n",
      "Epoch [6]\t Average validation loss 2.2918\t Average validation accuracy 0.3458\n",
      "\n",
      "Epoch [7][200]\t Batch [0][550]\t Training Loss 2.2906\t Accuracy 0.4100\n",
      "Epoch [7][200]\t Batch [50][550]\t Training Loss 2.2923\t Accuracy 0.3429\n",
      "Epoch [7][200]\t Batch [100][550]\t Training Loss 2.2925\t Accuracy 0.3381\n",
      "Epoch [7][200]\t Batch [150][550]\t Training Loss 2.2928\t Accuracy 0.3313\n",
      "Epoch [7][200]\t Batch [200][550]\t Training Loss 2.2932\t Accuracy 0.3266\n",
      "Epoch [7][200]\t Batch [250][550]\t Training Loss 2.2934\t Accuracy 0.3216\n",
      "Epoch [7][200]\t Batch [300][550]\t Training Loss 2.2937\t Accuracy 0.3172\n",
      "Epoch [7][200]\t Batch [350][550]\t Training Loss 2.2939\t Accuracy 0.3123\n",
      "Epoch [7][200]\t Batch [400][550]\t Training Loss 2.2942\t Accuracy 0.3077\n",
      "Epoch [7][200]\t Batch [450][550]\t Training Loss 2.2944\t Accuracy 0.3043\n",
      "Epoch [7][200]\t Batch [500][550]\t Training Loss 2.2946\t Accuracy 0.3007\n",
      "\n",
      "Epoch [7]\t Average training loss 2.2948\t Average training accuracy 0.2975\n",
      "Epoch [7]\t Average validation loss 2.2969\t Average validation accuracy 0.2760\n",
      "\n",
      "Epoch [8][200]\t Batch [0][550]\t Training Loss 2.2958\t Accuracy 0.3100\n",
      "Epoch [8][200]\t Batch [50][550]\t Training Loss 2.2971\t Accuracy 0.2720\n",
      "Epoch [8][200]\t Batch [100][550]\t Training Loss 2.2971\t Accuracy 0.2716\n",
      "Epoch [8][200]\t Batch [150][550]\t Training Loss 2.2973\t Accuracy 0.2683\n",
      "Epoch [8][200]\t Batch [200][550]\t Training Loss 2.2975\t Accuracy 0.2640\n",
      "Epoch [8][200]\t Batch [250][550]\t Training Loss 2.2976\t Accuracy 0.2593\n",
      "Epoch [8][200]\t Batch [300][550]\t Training Loss 2.2978\t Accuracy 0.2539\n",
      "Epoch [8][200]\t Batch [350][550]\t Training Loss 2.2979\t Accuracy 0.2476\n",
      "Epoch [8][200]\t Batch [400][550]\t Training Loss 2.2980\t Accuracy 0.2417\n",
      "Epoch [8][200]\t Batch [450][550]\t Training Loss 2.2982\t Accuracy 0.2351\n",
      "Epoch [8][200]\t Batch [500][550]\t Training Loss 2.2983\t Accuracy 0.2289\n",
      "\n",
      "Epoch [8]\t Average training loss 2.2984\t Average training accuracy 0.2236\n",
      "Epoch [8]\t Average validation loss 2.2995\t Average validation accuracy 0.1666\n",
      "\n",
      "Epoch [9][200]\t Batch [0][550]\t Training Loss 2.2986\t Accuracy 0.2400\n",
      "Epoch [9][200]\t Batch [50][550]\t Training Loss 2.2995\t Accuracy 0.1700\n",
      "Epoch [9][200]\t Batch [100][550]\t Training Loss 2.2996\t Accuracy 0.1689\n",
      "Epoch [9][200]\t Batch [150][550]\t Training Loss 2.2997\t Accuracy 0.1632\n",
      "Epoch [9][200]\t Batch [200][550]\t Training Loss 2.2998\t Accuracy 0.1571\n",
      "Epoch [9][200]\t Batch [250][550]\t Training Loss 2.2998\t Accuracy 0.1524\n",
      "Epoch [9][200]\t Batch [300][550]\t Training Loss 2.2999\t Accuracy 0.1482\n",
      "Epoch [9][200]\t Batch [350][550]\t Training Loss 2.3000\t Accuracy 0.1439\n",
      "Epoch [9][200]\t Batch [400][550]\t Training Loss 2.3001\t Accuracy 0.1403\n",
      "Epoch [9][200]\t Batch [450][550]\t Training Loss 2.3001\t Accuracy 0.1373\n",
      "Epoch [9][200]\t Batch [500][550]\t Training Loss 2.3002\t Accuracy 0.1346\n",
      "\n",
      "Epoch [9]\t Average training loss 2.3002\t Average training accuracy 0.1321\n",
      "Epoch [9]\t Average validation loss 2.3009\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [10][200]\t Batch [0][550]\t Training Loss 2.3000\t Accuracy 0.1400\n",
      "Epoch [10][200]\t Batch [50][550]\t Training Loss 2.3008\t Accuracy 0.1125\n",
      "Epoch [10][200]\t Batch [100][550]\t Training Loss 2.3008\t Accuracy 0.1127\n",
      "Epoch [10][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [10][200]\t Batch [200][550]\t Training Loss 2.3010\t Accuracy 0.1142\n",
      "Epoch [10][200]\t Batch [250][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [350][550]\t Training Loss 2.3011\t Accuracy 0.1143\n",
      "Epoch [10][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [10][200]\t Batch [450][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [500][550]\t Training Loss 2.3012\t Accuracy 0.1136\n",
      "\n",
      "Epoch [10]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [10]\t Average validation loss 2.3016\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [11][200]\t Batch [0][550]\t Training Loss 2.3008\t Accuracy 0.1400\n",
      "Epoch [11][200]\t Batch [50][550]\t Training Loss 2.3015\t Accuracy 0.1125\n",
      "Epoch [11][200]\t Batch [100][550]\t Training Loss 2.3015\t Accuracy 0.1127\n",
      "Epoch [11][200]\t Batch [150][550]\t Training Loss 2.3016\t Accuracy 0.1128\n",
      "Epoch [11][200]\t Batch [200][550]\t Training Loss 2.3016\t Accuracy 0.1142\n",
      "Epoch [11][200]\t Batch [250][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [300][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [350][550]\t Training Loss 2.3017\t Accuracy 0.1143\n",
      "Epoch [11][200]\t Batch [400][550]\t Training Loss 2.3017\t Accuracy 0.1141\n",
      "Epoch [11][200]\t Batch [450][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [500][550]\t Training Loss 2.3017\t Accuracy 0.1136\n",
      "\n",
      "Epoch [11]\t Average training loss 2.3017\t Average training accuracy 0.1129\n",
      "Epoch [11]\t Average validation loss 2.3020\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [12][200]\t Batch [0][550]\t Training Loss 2.3012\t Accuracy 0.1400\n",
      "Epoch [12][200]\t Batch [50][550]\t Training Loss 2.3019\t Accuracy 0.1125\n",
      "Epoch [12][200]\t Batch [100][550]\t Training Loss 2.3019\t Accuracy 0.1127\n",
      "Epoch [12][200]\t Batch [150][550]\t Training Loss 2.3019\t Accuracy 0.1128\n",
      "Epoch [12][200]\t Batch [200][550]\t Training Loss 2.3019\t Accuracy 0.1142\n",
      "Epoch [12][200]\t Batch [250][550]\t Training Loss 2.3019\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [300][550]\t Training Loss 2.3020\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [350][550]\t Training Loss 2.3020\t Accuracy 0.1143\n",
      "Epoch [12][200]\t Batch [400][550]\t Training Loss 2.3020\t Accuracy 0.1141\n",
      "Epoch [12][200]\t Batch [450][550]\t Training Loss 2.3020\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [500][550]\t Training Loss 2.3020\t Accuracy 0.1136\n",
      "\n",
      "Epoch [12]\t Average training loss 2.3020\t Average training accuracy 0.1129\n",
      "Epoch [12]\t Average validation loss 2.3022\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [13][200]\t Batch [0][550]\t Training Loss 2.3014\t Accuracy 0.1400\n",
      "Epoch [13][200]\t Batch [50][550]\t Training Loss 2.3021\t Accuracy 0.1125\n",
      "Epoch [13][200]\t Batch [100][550]\t Training Loss 2.3021\t Accuracy 0.1127\n",
      "Epoch [13][200]\t Batch [150][550]\t Training Loss 2.3021\t Accuracy 0.1128\n",
      "Epoch [13][200]\t Batch [200][550]\t Training Loss 2.3021\t Accuracy 0.1142\n",
      "Epoch [13][200]\t Batch [250][550]\t Training Loss 2.3021\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [300][550]\t Training Loss 2.3021\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [350][550]\t Training Loss 2.3021\t Accuracy 0.1143\n",
      "Epoch [13][200]\t Batch [400][550]\t Training Loss 2.3021\t Accuracy 0.1141\n",
      "Epoch [13][200]\t Batch [450][550]\t Training Loss 2.3021\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [500][550]\t Training Loss 2.3021\t Accuracy 0.1136\n",
      "\n",
      "Epoch [13]\t Average training loss 2.3022\t Average training accuracy 0.1129\n",
      "Epoch [13]\t Average validation loss 2.3023\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [14][200]\t Batch [0][550]\t Training Loss 2.3016\t Accuracy 0.1400\n",
      "Epoch [14][200]\t Batch [50][550]\t Training Loss 2.3022\t Accuracy 0.1125\n",
      "Epoch [14][200]\t Batch [100][550]\t Training Loss 2.3022\t Accuracy 0.1127\n",
      "Epoch [14][200]\t Batch [150][550]\t Training Loss 2.3022\t Accuracy 0.1128\n",
      "Epoch [14][200]\t Batch [200][550]\t Training Loss 2.3022\t Accuracy 0.1142\n",
      "Epoch [14][200]\t Batch [250][550]\t Training Loss 2.3022\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [300][550]\t Training Loss 2.3022\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [350][550]\t Training Loss 2.3022\t Accuracy 0.1143\n",
      "Epoch [14][200]\t Batch [400][550]\t Training Loss 2.3022\t Accuracy 0.1141\n",
      "Epoch [14][200]\t Batch [450][550]\t Training Loss 2.3022\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [500][550]\t Training Loss 2.3022\t Accuracy 0.1136\n",
      "\n",
      "Epoch [14]\t Average training loss 2.3022\t Average training accuracy 0.1129\n",
      "Epoch [14]\t Average validation loss 2.3023\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [15][200]\t Batch [0][550]\t Training Loss 2.3016\t Accuracy 0.1400\n",
      "Epoch [15][200]\t Batch [50][550]\t Training Loss 2.3022\t Accuracy 0.1125\n",
      "Epoch [15][200]\t Batch [100][550]\t Training Loss 2.3022\t Accuracy 0.1127\n",
      "Epoch [15][200]\t Batch [150][550]\t Training Loss 2.3022\t Accuracy 0.1128\n",
      "Epoch [15][200]\t Batch [200][550]\t Training Loss 2.3022\t Accuracy 0.1142\n",
      "Epoch [15][200]\t Batch [250][550]\t Training Loss 2.3022\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [15][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [15][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [15]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [15]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [16][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [16][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [16][200]\t Batch [100][550]\t Training Loss 2.3022\t Accuracy 0.1127\n",
      "Epoch [16][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [16][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [16][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [16][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [16][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [16]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [16]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [17][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [17][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [17][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [17][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [17][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [17][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [17][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [17][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [17]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [17]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [18][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [18][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [18][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [18][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [18][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [18][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [18][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [18][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [18]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [18]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [19][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [19][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [19][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [19][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [19][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [19][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [19][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [19][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [19]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [19]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [20][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [20][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [20][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [20][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [20][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [20][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [20][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [20][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [20]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [20]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [21][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [21][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [21][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [21][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [21][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [21][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [21][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [21][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [21]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [21]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [22][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [22][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [22][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [22][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [22][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [22][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [22][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [22][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [22]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [22]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [23][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [23][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [23][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [23][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [23][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [23][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [23][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [23][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [23]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [23]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [24][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [24][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [24][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [24][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [24][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [24][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [24][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [24][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [24]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [24]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [25][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [25][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [25][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [25][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [25][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [25][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [25][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [25][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [25]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [25]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [26][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [26][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [26][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [26][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [26][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [26][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [26][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [26][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [26]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [26]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [27][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [27][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [27][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [27][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [27][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [27][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [27][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [27][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [27]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [27]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [28][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [28][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [28][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [28][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [28][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [28][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [28][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [28][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [28]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [28]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [29][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [29][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [29][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [29][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [29][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [29][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [29][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [29][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [29]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [29]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [30][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [30][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [30][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [30][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [30][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [30][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [30][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [30][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [30]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [30]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [31][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [31][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [31][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [31][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [31][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [31][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [31][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [31][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [31]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [31]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [32][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [32][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [32][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [32][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [32][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [32][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [32][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [32][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [32]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [32]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [33][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [33][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [33][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [33][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [33][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [33][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [33][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [33][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [33]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [33]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [34][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [34][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [34][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [34][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [34][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [34][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [34][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [34][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [34]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [34]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [35][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [35][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [35][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [35][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [35][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [35][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [35][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [35][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [35]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [35]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [36][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [36][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [36][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [36][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [36][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [36][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [36][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [36][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [36]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [36]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [37][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [37][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [37][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [37][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [37][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [37][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [37][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [37][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [37]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [37]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [38][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [38][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [38][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [38][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [38][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [38][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [38][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [38][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [38][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [38][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [38][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [38]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [38]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [39][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [39][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [39][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [39][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [39][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [39][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [39][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [39][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [39][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [39][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [39][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [39]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [39]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [40][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [40][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [40][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [40][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [40][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [40][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [40][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [40][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [40][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [40][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [40][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [40]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [40]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [41][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [41][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [41][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [41][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [41][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [41][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [41][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [41][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [41][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [41][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [41][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [41]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [41]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [42][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [42][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [42][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [42][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [42][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [42][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [42][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [42][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [42][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [42][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [42][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [42]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [42]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [43][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [43][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [43][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [43][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [43][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [43][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [43][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [43][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [43][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [43][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [43][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [43]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [43]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [44][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [44][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [44][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [44][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [44][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [44][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [44][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [44][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [44][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [44][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [44][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [44]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [44]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [45][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [45][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [45][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [45][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [45][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [45][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [45][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [45][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [45][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [45][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [45][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [45]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [45]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [46][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [46][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [46][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [46][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [46][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [46][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [46][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [46][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [46][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [46][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [46][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [46]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [46]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [47][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [47][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [47][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [47][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [47][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [47][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [47][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [47][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [47][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [47][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [47][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [47]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [47]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [48][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [48][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [48][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [48][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [48][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [48][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [48][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [48][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [48][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [48][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [48][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [48]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [48]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [49][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [49][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [49][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [49][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [49][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [49][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [49][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [49][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [49][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [49][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [49][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [49]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [49]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [50][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [50][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [50][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [50][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [50][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [50][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [50][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [50][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [50][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [50][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [50][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [50]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [50]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [51][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [51][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [51][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [51][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [51][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [51][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [51][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [51][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [51][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [51][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [51][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [51]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [51]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [52][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [52][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [52][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [52][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [52][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [52][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [52][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [52][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [52][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [52][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [52][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [52]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [52]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [53][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [53][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [53][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [53][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [53][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [53][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [53][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [53][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [53][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [53][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [53][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [53]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [53]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [54][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [54][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [54][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [54][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [54][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [54][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [54][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [54][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [54][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [54][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [54][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [54]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [54]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [55][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [55][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [55][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [55][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [55][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [55][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [55][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [55][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [55][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [55][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [55][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [55]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [55]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [56][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [56][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [56][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [56][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [56][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [56][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [56][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [56][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [56][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [56][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [56][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [56]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [56]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [57][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [57][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [57][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [57][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [57][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [57][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [57][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [57][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [57][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [57][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [57][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [57]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [57]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [58][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [58][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [58][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [58][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [58][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [58][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [58][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [58][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [58][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [58][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [58][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [58]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [58]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [59][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [59][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [59][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [59][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [59][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [59][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [59][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [59][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [59][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [59][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [59][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [59]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [59]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [60][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [60][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [60][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [60][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [60][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [60][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [60][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [60][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [60][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [60][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [60][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [60]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [60]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [61][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [61][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [61][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [61][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [61][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [61][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [61][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [61][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [61][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [61][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [61][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [61]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [61]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [62][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [62][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [62][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [62][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [62][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [62][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [62][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [62][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [62][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [62][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [62][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [62]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [62]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [63][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [63][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [63][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [63][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [63][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [63][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [63][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [63][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [63][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [63][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [63][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [63]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [63]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [64][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [64][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [64][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [64][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [64][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [64][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [64][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [64][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [64][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [64][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [64][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [64]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [64]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [65][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [65][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [65][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [65][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [65][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [65][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [65][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [65][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [65][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [65][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [65][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [65]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [65]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [66][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [66][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [66][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [66][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [66][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [66][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [66][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [66][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [66][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [66][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [66][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [66]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [66]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [67][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [67][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [67][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [67][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [67][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [67][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [67][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [67][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [67][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [67][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [67][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [67]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [67]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [68][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [68][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [68][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [68][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [68][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [68][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [68][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [68][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [68][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [68][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [68][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [68]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [68]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [69][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [69][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [69][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [69][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [69][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [69][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [69][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [69][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [69][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [69][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [69][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [69]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [69]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [70][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [70][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [70][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [70][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [70][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [70][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [70][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [70][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [70][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [70][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [70][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [70]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [70]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [71][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [71][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [71][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [71][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [71][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [71][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [71][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [71][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [71][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [71][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [71][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [71]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [71]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [72][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [72][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [72][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [72][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [72][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [72][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [72][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [72][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [72][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [72][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [72][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [72]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [72]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [73][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [73][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [73][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [73][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [73][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [73][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [73][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [73][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [73][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [73][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [73][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [73]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [73]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [74][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [74][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [74][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [74][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [74][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [74][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [74][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [74][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [74][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [74][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [74][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [74]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [74]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [75][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [75][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [75][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [75][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [75][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [75][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [75][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [75][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [75][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [75][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [75][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [75]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [75]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [76][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [76][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [76][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [76][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [76][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [76][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [76][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [76][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [76][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [76][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [76][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [76]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [76]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [77][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [77][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [77][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [77][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [77][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [77][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [77][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [77][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [77][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [77][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [77][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [77]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [77]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [78][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [78][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [78][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [78][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [78][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [78][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [78][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [78][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [78][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [78][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [78][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [78]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [78]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [79][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [79][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [79][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [79][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [79][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [79][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [79][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [79][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [79][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [79][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [79][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [79]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [79]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [80][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [80][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [80][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [80][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [80][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [80][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [80][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [80][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [80][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [80][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [80][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [80]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [80]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [81][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [81][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [81][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [81][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [81][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [81][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [81][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [81][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [81][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [81][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [81][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [81]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [81]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [82][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [82][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [82][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [82][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [82][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [82][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [82][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [82][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [82][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [82][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [82][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [82]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [82]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [83][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [83][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [83][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [83][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [83][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [83][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [83][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [83][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [83][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [83][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [83][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [83]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [83]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [84][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [84][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [84][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [84][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [84][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [84][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [84][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [84][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [84][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [84][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [84][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [84]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [84]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [85][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [85][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [85][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [85][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [85][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [85][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [85][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [85][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [85][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [85][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [85][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [85]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [85]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [86][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [86][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [86][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [86][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [86][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [86][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [86][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [86][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [86][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [86][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [86][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [86]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [86]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [87][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [87][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [87][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [87][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [87][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [87][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [87][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [87][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [87][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [87][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [87][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [87]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [87]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [88][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [88][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [88][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [88][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [88][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [88][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [88][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [88][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [88][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [88][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [88][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [88]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [88]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [89][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [89][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [89][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [89][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [89][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [89][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [89][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [89][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [89][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [89][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [89][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [89]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [89]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [90][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [90][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [90][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [90][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [90][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [90][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [90][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [90][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [90][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [90][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [90][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [90]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [90]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [91][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [91][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [91][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [91][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [91][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [91][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [91][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [91][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [91][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [91][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [91][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [91]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [91]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [92][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [92][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [92][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [92][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [92][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [92][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [92][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [92][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [92][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [92][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [92][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [92]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [92]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [93][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [93][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [93][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [93][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [93][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [93][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [93][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [93][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [93][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [93][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [93][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [93]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [93]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [94][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [94][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [94][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [94][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [94][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [94][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [94][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [94][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [94][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [94][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [94][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [94]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [94]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [95][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [95][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [95][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [95][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [95][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [95][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [95][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [95][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [95][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [95][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [95][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [95]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [95]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [96][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [96][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [96][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [96][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [96][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [96][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [96][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [96][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [96][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [96][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [96][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [96]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [96]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [97][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [97][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [97][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [97][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [97][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [97][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [97][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [97][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [97][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [97][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [97][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [97]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [97]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [98][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [98][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [98][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [98][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [98][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [98][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [98][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [98][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [98][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [98][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [98][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [98]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [98]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [99][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [99][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [99][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [99][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [99][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [99][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [99][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [99][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [99][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [99][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [99][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [99]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [99]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [100][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [100][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [100][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [100][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [100][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [100][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [100][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [100][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [100][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [100][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [100][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [100]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [100]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [101][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [101][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [101][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [101][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [101][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [101][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [101][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [101][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [101][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [101][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [101][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [101]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [101]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [102][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [102][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [102][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [102][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [102][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [102][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [102][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [102][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [102][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [102][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [102][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [102]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [102]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [103][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [103][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [103][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [103][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [103][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [103][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [103][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [103][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [103][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [103][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [103][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [103]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [103]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [104][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [104][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [104][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [104][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [104][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [104][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [104][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [104][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [104][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [104][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [104][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [104]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [104]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [105][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [105][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [105][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [105][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [105][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [105][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [105][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [105][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [105][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [105][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [105][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [105]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [105]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [106][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [106][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [106][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [106][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [106][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [106][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [106][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [106][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [106][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [106][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [106][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [106]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [106]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [107][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [107][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [107][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [107][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [107][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [107][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [107][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [107][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [107][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [107][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [107][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [107]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [107]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [108][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [108][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [108][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [108][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [108][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [108][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [108][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [108][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [108][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [108][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [108][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [108]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [108]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [109][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [109][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [109][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [109][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [109][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [109][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [109][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [109][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [109][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [109][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [109][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [109]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [109]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [110][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [110][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [110][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [110][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [110][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [110][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [110][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [110][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [110][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [110][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [110][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [110]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [110]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [111][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [111][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [111][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [111][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [111][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [111][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [111][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [111][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [111][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [111][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [111][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [111]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [111]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [112][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [112][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [112][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [112][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [112][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [112][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [112][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [112][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [112][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [112][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [112][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [112]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [112]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [113][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [113][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [113][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [113][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [113][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [113][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [113][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [113][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [113][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [113][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [113][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [113]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [113]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [114][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [114][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [114][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [114][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [114][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [114][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [114][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [114][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [114][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [114][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [114][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [114]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [114]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [115][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [115][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [115][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [115][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [115][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [115][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [115][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [115][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [115][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [115][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [115][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [115]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [115]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [116][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [116][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [116][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [116][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [116][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [116][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [116][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [116][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [116][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [116][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [116][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [116]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [116]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [117][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [117][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [117][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [117][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [117][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [117][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [117][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [117][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [117][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [117][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [117][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [117]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [117]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [118][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [118][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [118][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [118][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [118][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [118][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [118][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [118][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [118][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [118][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [118][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [118]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [118]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [119][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [119][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [119][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [119][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [119][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [119][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [119][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [119][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [119][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [119][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [119][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [119]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [119]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [120][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [120][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [120][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [120][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [120][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [120][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [120][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [120][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [120][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [120][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [120][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [120]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [120]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [121][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [121][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [121][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [121][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [121][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [121][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [121][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [121][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [121][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [121][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [121][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [121]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [121]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [122][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [122][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [122][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [122][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [122][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [122][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [122][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [122][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [122][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [122][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [122][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [122]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [122]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [123][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [123][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [123][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [123][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [123][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [123][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [123][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [123][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [123][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [123][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [123][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [123]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [123]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [124][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [124][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [124][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [124][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [124][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [124][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [124][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [124][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [124][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [124][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [124][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [124]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [124]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [125][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [125][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [125][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [125][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [125][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [125][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [125][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [125][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [125][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [125][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [125][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [125]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [125]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [126][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [126][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [126][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [126][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [126][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [126][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [126][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [126][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [126][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [126][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [126][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [126]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [126]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [127][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [127][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [127][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [127][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [127][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [127][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [127][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [127][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [127][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [127][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [127][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [127]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [127]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [128][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [128][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [128][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [128][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [128][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [128][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [128][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [128][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [128][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [128][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [128][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [128]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [128]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [129][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [129][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [129][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [129][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [129][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [129][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [129][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [129][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [129][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [129][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [129][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [129]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [129]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [130][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [130][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [130][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [130][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [130][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [130][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [130][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [130][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [130][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [130][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [130][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [130]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [130]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [131][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [131][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [131][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [131][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [131][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [131][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [131][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [131][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [131][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [131][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [131][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [131]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [131]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [132][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [132][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [132][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [132][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [132][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [132][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [132][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [132][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [132][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [132][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [132][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [132]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [132]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [133][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [133][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [133][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [133][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [133][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [133][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [133][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [133][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [133][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [133][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [133][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [133]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [133]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [134][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [134][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [134][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [134][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [134][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [134][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [134][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [134][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [134][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [134][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [134][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [134]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [134]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [135][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [135][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [135][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [135][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [135][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [135][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [135][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [135][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [135][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [135][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [135][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [135]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [135]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [136][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [136][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [136][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [136][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [136][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [136][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [136][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [136][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [136][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [136][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [136][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [136]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [136]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [137][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [137][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [137][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [137][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [137][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [137][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [137][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [137][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [137][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [137][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [137][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [137]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [137]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [138][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [138][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [138][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [138][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [138][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [138][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [138][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [138][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [138][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [138][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [138][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [138]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [138]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [139][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [139][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [139][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [139][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [139][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [139][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [139][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [139][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [139][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [139][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [139][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [139]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [139]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [140][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [140][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [140][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [140][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [140][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [140][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [140][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [140][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [140][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [140][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [140][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [140]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [140]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [141][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [141][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [141][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [141][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [141][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [141][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [141][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [141][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [141][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [141][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [141][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [141]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [141]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [142][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [142][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [142][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [142][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [142][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [142][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [142][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [142][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [142][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [142][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [142][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [142]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [142]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [143][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [143][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [143][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [143][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [143][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [143][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [143][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [143][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [143][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [143][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [143][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [143]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [143]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [144][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [144][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [144][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [144][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [144][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [144][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [144][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [144][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [144][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [144][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [144][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [144]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [144]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [145][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [145][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [145][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [145][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [145][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [145][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [145][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [145][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [145][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [145][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [145][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [145]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [145]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [146][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [146][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [146][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [146][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [146][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [146][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [146][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [146][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [146][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [146][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [146][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [146]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [146]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [147][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [147][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [147][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [147][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [147][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [147][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [147][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [147][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [147][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [147][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [147][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [147]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [147]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [148][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [148][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [148][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [148][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [148][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [148][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [148][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [148][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [148][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [148][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [148][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [148]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [148]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [149][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [149][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [149][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [149][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [149][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [149][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [149][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [149][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [149][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [149][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [149][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [149]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [149]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [150][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [150][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [150][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [150][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [150][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [150][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [150][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [150][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [150][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [150][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [150][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [150]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [150]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [151][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [151][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [151][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [151][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [151][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [151][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [151][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [151][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [151][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [151][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [151][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [151]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [151]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [152][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [152][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [152][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [152][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [152][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [152][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [152][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [152][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [152][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [152][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [152][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [152]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [152]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [153][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [153][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [153][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [153][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [153][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [153][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [153][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [153][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [153][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [153][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [153][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [153]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [153]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [154][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [154][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [154][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [154][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [154][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [154][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [154][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [154][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [154][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [154][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [154][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [154]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [154]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [155][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [155][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [155][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [155][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [155][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [155][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [155][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [155][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [155][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [155][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [155][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [155]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [155]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [156][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [156][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [156][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [156][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [156][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [156][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [156][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [156][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [156][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [156][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [156][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [156]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [156]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [157][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [157][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [157][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [157][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [157][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [157][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [157][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [157][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [157][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [157][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [157][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [157]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [157]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [158][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [158][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [158][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [158][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [158][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [158][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [158][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [158][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [158][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [158][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [158][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [158]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [158]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [159][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [159][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [159][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [159][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [159][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [159][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [159][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [159][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [159][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [159][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [159][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [159]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [159]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [160][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [160][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [160][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [160][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [160][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [160][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [160][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [160][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [160][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [160][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [160][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [160]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [160]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [161][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [161][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [161][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [161][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [161][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [161][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [161][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [161][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [161][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [161][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [161][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [161]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [161]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [162][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [162][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [162][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [162][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [162][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [162][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [162][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [162][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [162][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [162][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [162][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [162]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [162]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [163][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [163][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [163][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [163][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [163][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [163][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [163][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [163][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [163][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [163][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [163][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [163]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [163]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [164][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [164][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [164][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [164][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [164][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [164][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [164][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [164][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [164][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [164][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [164][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [164]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [164]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [165][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [165][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [165][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [165][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [165][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [165][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [165][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [165][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [165][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [165][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [165][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [165]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [165]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [166][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [166][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [166][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [166][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [166][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [166][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [166][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [166][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [166][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [166][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [166][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [166]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [166]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [167][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [167][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [167][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [167][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [167][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [167][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [167][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [167][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [167][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [167][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [167][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [167]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [167]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [168][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [168][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [168][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [168][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [168][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [168][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [168][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [168][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [168][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [168][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [168][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [168]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [168]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [169][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [169][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [169][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [169][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [169][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [169][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [169][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [169][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [169][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [169][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [169][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [169]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [169]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [170][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [170][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [170][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [170][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [170][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [170][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [170][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [170][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [170][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [170][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [170][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [170]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [170]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [171][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [171][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [171][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [171][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [171][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [171][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [171][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [171][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [171][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [171][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [171][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [171]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [171]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [172][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [172][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [172][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [172][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [172][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [172][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [172][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [172][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [172][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [172][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [172][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [172]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [172]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [173][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [173][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [173][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [173][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [173][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [173][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [173][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [173][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [173][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [173][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [173][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [173]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [173]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [174][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [174][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [174][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [174][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [174][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [174][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [174][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [174][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [174][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [174][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [174][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [174]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [174]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [175][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [175][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [175][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [175][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [175][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [175][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [175][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [175][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [175][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [175][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [175][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [175]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [175]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [176][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [176][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [176][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [176][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [176][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [176][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [176][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [176][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [176][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [176][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [176][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [176]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [176]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [177][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [177][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [177][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [177][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [177][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [177][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [177][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [177][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [177][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [177][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [177][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [177]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [177]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [178][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [178][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [178][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [178][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [178][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [178][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [178][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [178][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [178][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [178][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [178][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [178]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [178]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [179][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [179][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [179][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [179][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [179][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [179][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [179][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [179][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [179][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [179][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [179][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [179]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [179]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [180][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [180][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [180][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [180][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [180][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [180][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [180][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [180][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [180][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [180][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [180][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [180]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [180]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [181][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [181][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [181][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [181][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [181][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [181][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [181][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [181][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [181][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [181][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [181][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [181]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [181]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [182][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [182][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [182][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [182][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [182][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [182][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [182][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [182][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [182][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [182][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [182][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [182]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [182]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [183][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [183][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [183][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [183][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [183][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [183][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [183][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [183][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [183][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [183][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [183][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [183]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [183]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [184][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [184][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [184][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [184][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [184][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [184][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [184][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [184][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [184][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [184][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [184][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [184]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [184]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [185][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [185][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [185][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [185][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [185][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [185][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [185][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [185][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [185][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [185][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [185][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [185]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [185]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [186][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [186][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [186][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [186][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [186][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [186][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [186][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [186][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [186][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [186][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [186][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [186]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [186]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [187][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [187][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [187][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [187][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [187][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [187][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [187][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [187][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [187][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [187][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [187][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [187]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [187]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [188][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [188][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [188][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [188][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [188][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [188][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [188][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [188][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [188][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [188][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [188][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [188]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [188]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [189][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [189][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [189][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [189][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [189][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [189][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [189][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [189][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [189][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [189][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [189][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [189]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [189]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [190][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [190][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [190][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [190][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [190][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [190][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [190][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [190][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [190][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [190][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [190][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [190]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [190]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [191][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [191][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [191][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [191][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [191][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [191][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [191][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [191][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [191][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [191][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [191][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [191]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [191]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [192][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [192][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [192][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [192][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [192][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [192][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [192][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [192][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [192][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [192][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [192][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [192]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [192]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [193][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [193][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [193][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [193][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [193][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [193][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [193][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [193][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [193][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [193][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [193][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [193]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [193]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [194][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [194][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [194][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [194][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [194][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [194][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [194][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [194][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [194][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [194][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [194][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [194]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [194]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [195][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [195][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [195][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [195][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [195][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [195][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [195][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [195][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [195][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [195][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [195][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [195]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [195]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [196][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [196][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [196][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [196][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [196][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [196][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [196][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [196][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [196][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [196][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [196][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [196]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [196]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [197][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [197][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [197][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [197][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [197][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [197][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [197][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [197][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [197][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [197][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [197][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [197]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [197]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [198][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [198][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [198][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [198][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [198][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [198][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [198][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [198][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [198][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [198][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [198][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [198]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [198]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [199][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [199][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [199][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [199][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [199][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [199][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [199][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [199][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [199][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [199][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [199][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [199]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [199]\t Average validation loss 2.3024\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [0][200]\t Batch [0][550]\t Training Loss 2.3017\t Accuracy 0.1400\n",
      "Epoch [0][200]\t Batch [50][550]\t Training Loss 2.3023\t Accuracy 0.1125\n",
      "Epoch [0][200]\t Batch [100][550]\t Training Loss 2.3023\t Accuracy 0.1127\n",
      "Epoch [0][200]\t Batch [150][550]\t Training Loss 2.3023\t Accuracy 0.1128\n",
      "Epoch [0][200]\t Batch [200][550]\t Training Loss 2.3023\t Accuracy 0.1142\n",
      "Epoch [0][200]\t Batch [250][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [0][200]\t Batch [300][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [0][200]\t Batch [350][550]\t Training Loss 2.3023\t Accuracy 0.1143\n",
      "Epoch [0][200]\t Batch [400][550]\t Training Loss 2.3023\t Accuracy 0.1141\n",
      "Epoch [0][200]\t Batch [450][550]\t Training Loss 2.3023\t Accuracy 0.1140\n",
      "Epoch [0][200]\t Batch [500][550]\t Training Loss 2.3023\t Accuracy 0.1136\n",
      "\n",
      "Epoch [0]\t Average training loss 2.3023\t Average training accuracy 0.1129\n",
      "Epoch [0]\t Average validation loss 2.3023\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [1][200]\t Batch [0][550]\t Training Loss 2.3013\t Accuracy 0.1400\n",
      "Epoch [1][200]\t Batch [50][550]\t Training Loss 2.3022\t Accuracy 0.1125\n",
      "Epoch [1][200]\t Batch [100][550]\t Training Loss 2.3021\t Accuracy 0.1127\n",
      "Epoch [1][200]\t Batch [150][550]\t Training Loss 2.3022\t Accuracy 0.1128\n",
      "Epoch [1][200]\t Batch [200][550]\t Training Loss 2.3021\t Accuracy 0.1142\n",
      "Epoch [1][200]\t Batch [250][550]\t Training Loss 2.3022\t Accuracy 0.1140\n",
      "Epoch [1][200]\t Batch [300][550]\t Training Loss 2.3022\t Accuracy 0.1140\n",
      "Epoch [1][200]\t Batch [350][550]\t Training Loss 2.3022\t Accuracy 0.1143\n",
      "Epoch [1][200]\t Batch [400][550]\t Training Loss 2.3022\t Accuracy 0.1141\n",
      "Epoch [1][200]\t Batch [450][550]\t Training Loss 2.3022\t Accuracy 0.1140\n",
      "Epoch [1][200]\t Batch [500][550]\t Training Loss 2.3022\t Accuracy 0.1136\n",
      "\n",
      "Epoch [1]\t Average training loss 2.3022\t Average training accuracy 0.1129\n",
      "Epoch [1]\t Average validation loss 2.3022\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [2][200]\t Batch [0][550]\t Training Loss 2.3009\t Accuracy 0.1400\n",
      "Epoch [2][200]\t Batch [50][550]\t Training Loss 2.3021\t Accuracy 0.1125\n",
      "Epoch [2][200]\t Batch [100][550]\t Training Loss 2.3020\t Accuracy 0.1127\n",
      "Epoch [2][200]\t Batch [150][550]\t Training Loss 2.3020\t Accuracy 0.1128\n",
      "Epoch [2][200]\t Batch [200][550]\t Training Loss 2.3020\t Accuracy 0.1142\n",
      "Epoch [2][200]\t Batch [250][550]\t Training Loss 2.3020\t Accuracy 0.1140\n",
      "Epoch [2][200]\t Batch [300][550]\t Training Loss 2.3021\t Accuracy 0.1140\n",
      "Epoch [2][200]\t Batch [350][550]\t Training Loss 2.3021\t Accuracy 0.1143\n",
      "Epoch [2][200]\t Batch [400][550]\t Training Loss 2.3021\t Accuracy 0.1141\n",
      "Epoch [2][200]\t Batch [450][550]\t Training Loss 2.3021\t Accuracy 0.1140\n",
      "Epoch [2][200]\t Batch [500][550]\t Training Loss 2.3021\t Accuracy 0.1136\n",
      "\n",
      "Epoch [2]\t Average training loss 2.3021\t Average training accuracy 0.1129\n",
      "Epoch [2]\t Average validation loss 2.3022\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [3][200]\t Batch [0][550]\t Training Loss 2.3006\t Accuracy 0.1400\n",
      "Epoch [3][200]\t Batch [50][550]\t Training Loss 2.3020\t Accuracy 0.1125\n",
      "Epoch [3][200]\t Batch [100][550]\t Training Loss 2.3019\t Accuracy 0.1127\n",
      "Epoch [3][200]\t Batch [150][550]\t Training Loss 2.3020\t Accuracy 0.1128\n",
      "Epoch [3][200]\t Batch [200][550]\t Training Loss 2.3019\t Accuracy 0.1142\n",
      "Epoch [3][200]\t Batch [250][550]\t Training Loss 2.3020\t Accuracy 0.1140\n",
      "Epoch [3][200]\t Batch [300][550]\t Training Loss 2.3020\t Accuracy 0.1140\n",
      "Epoch [3][200]\t Batch [350][550]\t Training Loss 2.3020\t Accuracy 0.1143\n",
      "Epoch [3][200]\t Batch [400][550]\t Training Loss 2.3020\t Accuracy 0.1141\n",
      "Epoch [3][200]\t Batch [450][550]\t Training Loss 2.3020\t Accuracy 0.1140\n",
      "Epoch [3][200]\t Batch [500][550]\t Training Loss 2.3020\t Accuracy 0.1136\n",
      "\n",
      "Epoch [3]\t Average training loss 2.3020\t Average training accuracy 0.1129\n",
      "Epoch [3]\t Average validation loss 2.3021\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [4][200]\t Batch [0][550]\t Training Loss 2.3003\t Accuracy 0.1400\n",
      "Epoch [4][200]\t Batch [50][550]\t Training Loss 2.3019\t Accuracy 0.1125\n",
      "Epoch [4][200]\t Batch [100][550]\t Training Loss 2.3018\t Accuracy 0.1127\n",
      "Epoch [4][200]\t Batch [150][550]\t Training Loss 2.3019\t Accuracy 0.1128\n",
      "Epoch [4][200]\t Batch [200][550]\t Training Loss 2.3019\t Accuracy 0.1142\n",
      "Epoch [4][200]\t Batch [250][550]\t Training Loss 2.3019\t Accuracy 0.1140\n",
      "Epoch [4][200]\t Batch [300][550]\t Training Loss 2.3019\t Accuracy 0.1140\n",
      "Epoch [4][200]\t Batch [350][550]\t Training Loss 2.3019\t Accuracy 0.1143\n",
      "Epoch [4][200]\t Batch [400][550]\t Training Loss 2.3019\t Accuracy 0.1141\n",
      "Epoch [4][200]\t Batch [450][550]\t Training Loss 2.3019\t Accuracy 0.1140\n",
      "Epoch [4][200]\t Batch [500][550]\t Training Loss 2.3019\t Accuracy 0.1136\n",
      "\n",
      "Epoch [4]\t Average training loss 2.3019\t Average training accuracy 0.1129\n",
      "Epoch [4]\t Average validation loss 2.3021\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [5][200]\t Batch [0][550]\t Training Loss 2.3000\t Accuracy 0.1400\n",
      "Epoch [5][200]\t Batch [50][550]\t Training Loss 2.3018\t Accuracy 0.1125\n",
      "Epoch [5][200]\t Batch [100][550]\t Training Loss 2.3018\t Accuracy 0.1127\n",
      "Epoch [5][200]\t Batch [150][550]\t Training Loss 2.3018\t Accuracy 0.1128\n",
      "Epoch [5][200]\t Batch [200][550]\t Training Loss 2.3018\t Accuracy 0.1142\n",
      "Epoch [5][200]\t Batch [250][550]\t Training Loss 2.3018\t Accuracy 0.1140\n",
      "Epoch [5][200]\t Batch [300][550]\t Training Loss 2.3018\t Accuracy 0.1140\n",
      "Epoch [5][200]\t Batch [350][550]\t Training Loss 2.3018\t Accuracy 0.1143\n",
      "Epoch [5][200]\t Batch [400][550]\t Training Loss 2.3018\t Accuracy 0.1141\n",
      "Epoch [5][200]\t Batch [450][550]\t Training Loss 2.3018\t Accuracy 0.1140\n",
      "Epoch [5][200]\t Batch [500][550]\t Training Loss 2.3019\t Accuracy 0.1136\n",
      "\n",
      "Epoch [5]\t Average training loss 2.3019\t Average training accuracy 0.1129\n",
      "Epoch [5]\t Average validation loss 2.3021\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [6][200]\t Batch [0][550]\t Training Loss 2.2998\t Accuracy 0.1400\n",
      "Epoch [6][200]\t Batch [50][550]\t Training Loss 2.3018\t Accuracy 0.1125\n",
      "Epoch [6][200]\t Batch [100][550]\t Training Loss 2.3017\t Accuracy 0.1127\n",
      "Epoch [6][200]\t Batch [150][550]\t Training Loss 2.3017\t Accuracy 0.1128\n",
      "Epoch [6][200]\t Batch [200][550]\t Training Loss 2.3017\t Accuracy 0.1142\n",
      "Epoch [6][200]\t Batch [250][550]\t Training Loss 2.3018\t Accuracy 0.1140\n",
      "Epoch [6][200]\t Batch [300][550]\t Training Loss 2.3018\t Accuracy 0.1140\n",
      "Epoch [6][200]\t Batch [350][550]\t Training Loss 2.3018\t Accuracy 0.1143\n",
      "Epoch [6][200]\t Batch [400][550]\t Training Loss 2.3018\t Accuracy 0.1141\n",
      "Epoch [6][200]\t Batch [450][550]\t Training Loss 2.3018\t Accuracy 0.1140\n",
      "Epoch [6][200]\t Batch [500][550]\t Training Loss 2.3018\t Accuracy 0.1136\n",
      "\n",
      "Epoch [6]\t Average training loss 2.3018\t Average training accuracy 0.1129\n",
      "Epoch [6]\t Average validation loss 2.3020\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [7][200]\t Batch [0][550]\t Training Loss 2.2996\t Accuracy 0.1400\n",
      "Epoch [7][200]\t Batch [50][550]\t Training Loss 2.3017\t Accuracy 0.1125\n",
      "Epoch [7][200]\t Batch [100][550]\t Training Loss 2.3016\t Accuracy 0.1127\n",
      "Epoch [7][200]\t Batch [150][550]\t Training Loss 2.3017\t Accuracy 0.1128\n",
      "Epoch [7][200]\t Batch [200][550]\t Training Loss 2.3017\t Accuracy 0.1142\n",
      "Epoch [7][200]\t Batch [250][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [7][200]\t Batch [300][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [7][200]\t Batch [350][550]\t Training Loss 2.3017\t Accuracy 0.1143\n",
      "Epoch [7][200]\t Batch [400][550]\t Training Loss 2.3017\t Accuracy 0.1141\n",
      "Epoch [7][200]\t Batch [450][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [7][200]\t Batch [500][550]\t Training Loss 2.3018\t Accuracy 0.1136\n",
      "\n",
      "Epoch [7]\t Average training loss 2.3018\t Average training accuracy 0.1129\n",
      "Epoch [7]\t Average validation loss 2.3020\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [8][200]\t Batch [0][550]\t Training Loss 2.2994\t Accuracy 0.1400\n",
      "Epoch [8][200]\t Batch [50][550]\t Training Loss 2.3017\t Accuracy 0.1125\n",
      "Epoch [8][200]\t Batch [100][550]\t Training Loss 2.3016\t Accuracy 0.1127\n",
      "Epoch [8][200]\t Batch [150][550]\t Training Loss 2.3016\t Accuracy 0.1128\n",
      "Epoch [8][200]\t Batch [200][550]\t Training Loss 2.3016\t Accuracy 0.1142\n",
      "Epoch [8][200]\t Batch [250][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [8][200]\t Batch [300][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [8][200]\t Batch [350][550]\t Training Loss 2.3017\t Accuracy 0.1143\n",
      "Epoch [8][200]\t Batch [400][550]\t Training Loss 2.3017\t Accuracy 0.1141\n",
      "Epoch [8][200]\t Batch [450][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [8][200]\t Batch [500][550]\t Training Loss 2.3017\t Accuracy 0.1136\n",
      "\n",
      "Epoch [8]\t Average training loss 2.3017\t Average training accuracy 0.1129\n",
      "Epoch [8]\t Average validation loss 2.3020\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [9][200]\t Batch [0][550]\t Training Loss 2.2992\t Accuracy 0.1400\n",
      "Epoch [9][200]\t Batch [50][550]\t Training Loss 2.3016\t Accuracy 0.1125\n",
      "Epoch [9][200]\t Batch [100][550]\t Training Loss 2.3015\t Accuracy 0.1127\n",
      "Epoch [9][200]\t Batch [150][550]\t Training Loss 2.3016\t Accuracy 0.1128\n",
      "Epoch [9][200]\t Batch [200][550]\t Training Loss 2.3016\t Accuracy 0.1142\n",
      "Epoch [9][200]\t Batch [250][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [9][200]\t Batch [300][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [9][200]\t Batch [350][550]\t Training Loss 2.3016\t Accuracy 0.1143\n",
      "Epoch [9][200]\t Batch [400][550]\t Training Loss 2.3017\t Accuracy 0.1141\n",
      "Epoch [9][200]\t Batch [450][550]\t Training Loss 2.3017\t Accuracy 0.1140\n",
      "Epoch [9][200]\t Batch [500][550]\t Training Loss 2.3017\t Accuracy 0.1136\n",
      "\n",
      "Epoch [9]\t Average training loss 2.3017\t Average training accuracy 0.1129\n",
      "Epoch [9]\t Average validation loss 2.3020\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [10][200]\t Batch [0][550]\t Training Loss 2.2991\t Accuracy 0.1400\n",
      "Epoch [10][200]\t Batch [50][550]\t Training Loss 2.3016\t Accuracy 0.1125\n",
      "Epoch [10][200]\t Batch [100][550]\t Training Loss 2.3015\t Accuracy 0.1127\n",
      "Epoch [10][200]\t Batch [150][550]\t Training Loss 2.3016\t Accuracy 0.1128\n",
      "Epoch [10][200]\t Batch [200][550]\t Training Loss 2.3015\t Accuracy 0.1142\n",
      "Epoch [10][200]\t Batch [250][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [300][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [350][550]\t Training Loss 2.3016\t Accuracy 0.1143\n",
      "Epoch [10][200]\t Batch [400][550]\t Training Loss 2.3016\t Accuracy 0.1141\n",
      "Epoch [10][200]\t Batch [450][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [500][550]\t Training Loss 2.3017\t Accuracy 0.1136\n",
      "\n",
      "Epoch [10]\t Average training loss 2.3017\t Average training accuracy 0.1129\n",
      "Epoch [10]\t Average validation loss 2.3020\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [11][200]\t Batch [0][550]\t Training Loss 2.2990\t Accuracy 0.1400\n",
      "Epoch [11][200]\t Batch [50][550]\t Training Loss 2.3016\t Accuracy 0.1125\n",
      "Epoch [11][200]\t Batch [100][550]\t Training Loss 2.3015\t Accuracy 0.1127\n",
      "Epoch [11][200]\t Batch [150][550]\t Training Loss 2.3015\t Accuracy 0.1128\n",
      "Epoch [11][200]\t Batch [200][550]\t Training Loss 2.3015\t Accuracy 0.1142\n",
      "Epoch [11][200]\t Batch [250][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [300][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [350][550]\t Training Loss 2.3016\t Accuracy 0.1143\n",
      "Epoch [11][200]\t Batch [400][550]\t Training Loss 2.3016\t Accuracy 0.1141\n",
      "Epoch [11][200]\t Batch [450][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [500][550]\t Training Loss 2.3016\t Accuracy 0.1136\n",
      "\n",
      "Epoch [11]\t Average training loss 2.3017\t Average training accuracy 0.1129\n",
      "Epoch [11]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [12][200]\t Batch [0][550]\t Training Loss 2.2989\t Accuracy 0.1400\n",
      "Epoch [12][200]\t Batch [50][550]\t Training Loss 2.3015\t Accuracy 0.1125\n",
      "Epoch [12][200]\t Batch [100][550]\t Training Loss 2.3014\t Accuracy 0.1127\n",
      "Epoch [12][200]\t Batch [150][550]\t Training Loss 2.3015\t Accuracy 0.1128\n",
      "Epoch [12][200]\t Batch [200][550]\t Training Loss 2.3015\t Accuracy 0.1142\n",
      "Epoch [12][200]\t Batch [250][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [300][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [350][550]\t Training Loss 2.3016\t Accuracy 0.1143\n",
      "Epoch [12][200]\t Batch [400][550]\t Training Loss 2.3016\t Accuracy 0.1141\n",
      "Epoch [12][200]\t Batch [450][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [500][550]\t Training Loss 2.3016\t Accuracy 0.1136\n",
      "\n",
      "Epoch [12]\t Average training loss 2.3016\t Average training accuracy 0.1129\n",
      "Epoch [12]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [13][200]\t Batch [0][550]\t Training Loss 2.2988\t Accuracy 0.1400\n",
      "Epoch [13][200]\t Batch [50][550]\t Training Loss 2.3015\t Accuracy 0.1125\n",
      "Epoch [13][200]\t Batch [100][550]\t Training Loss 2.3014\t Accuracy 0.1127\n",
      "Epoch [13][200]\t Batch [150][550]\t Training Loss 2.3015\t Accuracy 0.1128\n",
      "Epoch [13][200]\t Batch [200][550]\t Training Loss 2.3015\t Accuracy 0.1142\n",
      "Epoch [13][200]\t Batch [250][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [300][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [350][550]\t Training Loss 2.3015\t Accuracy 0.1143\n",
      "Epoch [13][200]\t Batch [400][550]\t Training Loss 2.3016\t Accuracy 0.1141\n",
      "Epoch [13][200]\t Batch [450][550]\t Training Loss 2.3016\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [500][550]\t Training Loss 2.3016\t Accuracy 0.1136\n",
      "\n",
      "Epoch [13]\t Average training loss 2.3016\t Average training accuracy 0.1129\n",
      "Epoch [13]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [14][200]\t Batch [0][550]\t Training Loss 2.2987\t Accuracy 0.1400\n",
      "Epoch [14][200]\t Batch [50][550]\t Training Loss 2.3015\t Accuracy 0.1125\n",
      "Epoch [14][200]\t Batch [100][550]\t Training Loss 2.3014\t Accuracy 0.1127\n",
      "Epoch [14][200]\t Batch [150][550]\t Training Loss 2.3015\t Accuracy 0.1128\n",
      "Epoch [14][200]\t Batch [200][550]\t Training Loss 2.3014\t Accuracy 0.1142\n",
      "Epoch [14][200]\t Batch [250][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [300][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [350][550]\t Training Loss 2.3015\t Accuracy 0.1143\n",
      "Epoch [14][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [14][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [500][550]\t Training Loss 2.3016\t Accuracy 0.1136\n",
      "\n",
      "Epoch [14]\t Average training loss 2.3016\t Average training accuracy 0.1129\n",
      "Epoch [14]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [15][200]\t Batch [0][550]\t Training Loss 2.2986\t Accuracy 0.1400\n",
      "Epoch [15][200]\t Batch [50][550]\t Training Loss 2.3015\t Accuracy 0.1125\n",
      "Epoch [15][200]\t Batch [100][550]\t Training Loss 2.3014\t Accuracy 0.1127\n",
      "Epoch [15][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [15][200]\t Batch [200][550]\t Training Loss 2.3014\t Accuracy 0.1142\n",
      "Epoch [15][200]\t Batch [250][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [300][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [350][550]\t Training Loss 2.3015\t Accuracy 0.1143\n",
      "Epoch [15][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [15][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [500][550]\t Training Loss 2.3016\t Accuracy 0.1136\n",
      "\n",
      "Epoch [15]\t Average training loss 2.3016\t Average training accuracy 0.1129\n",
      "Epoch [15]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [16][200]\t Batch [0][550]\t Training Loss 2.2985\t Accuracy 0.1400\n",
      "Epoch [16][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [16][200]\t Batch [100][550]\t Training Loss 2.3014\t Accuracy 0.1127\n",
      "Epoch [16][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [16][200]\t Batch [200][550]\t Training Loss 2.3014\t Accuracy 0.1142\n",
      "Epoch [16][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [300][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [350][550]\t Training Loss 2.3015\t Accuracy 0.1143\n",
      "Epoch [16][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [16][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [16]\t Average training loss 2.3016\t Average training accuracy 0.1129\n",
      "Epoch [16]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [17][200]\t Batch [0][550]\t Training Loss 2.2984\t Accuracy 0.1400\n",
      "Epoch [17][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [17][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [17][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [17][200]\t Batch [200][550]\t Training Loss 2.3014\t Accuracy 0.1142\n",
      "Epoch [17][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [300][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [350][550]\t Training Loss 2.3015\t Accuracy 0.1143\n",
      "Epoch [17][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [17][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [17]\t Average training loss 2.3016\t Average training accuracy 0.1129\n",
      "Epoch [17]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [18][200]\t Batch [0][550]\t Training Loss 2.2984\t Accuracy 0.1400\n",
      "Epoch [18][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [18][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [18][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [18][200]\t Batch [200][550]\t Training Loss 2.3014\t Accuracy 0.1142\n",
      "Epoch [18][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [300][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [350][550]\t Training Loss 2.3015\t Accuracy 0.1143\n",
      "Epoch [18][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [18][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [18]\t Average training loss 2.3016\t Average training accuracy 0.1129\n",
      "Epoch [18]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [19][200]\t Batch [0][550]\t Training Loss 2.2983\t Accuracy 0.1400\n",
      "Epoch [19][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [19][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [19][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [19][200]\t Batch [200][550]\t Training Loss 2.3014\t Accuracy 0.1142\n",
      "Epoch [19][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [300][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [19][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [19][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [19]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [19]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [20][200]\t Batch [0][550]\t Training Loss 2.2983\t Accuracy 0.1400\n",
      "Epoch [20][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [20][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [20][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [20][200]\t Batch [200][550]\t Training Loss 2.3014\t Accuracy 0.1142\n",
      "Epoch [20][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [300][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [20][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [20][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [20]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [20]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [21][200]\t Batch [0][550]\t Training Loss 2.2982\t Accuracy 0.1400\n",
      "Epoch [21][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [21][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [21][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [21][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [21][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [300][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [21][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [21][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [21]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [21]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [22][200]\t Batch [0][550]\t Training Loss 2.2982\t Accuracy 0.1400\n",
      "Epoch [22][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [22][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [22][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [22][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [22][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [22][200]\t Batch [400][550]\t Training Loss 2.3015\t Accuracy 0.1141\n",
      "Epoch [22][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [22]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [22]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [23][200]\t Batch [0][550]\t Training Loss 2.2982\t Accuracy 0.1400\n",
      "Epoch [23][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [23][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [23][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [23][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [23][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [23][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [23][200]\t Batch [450][550]\t Training Loss 2.3015\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [23]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [23]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [24][200]\t Batch [0][550]\t Training Loss 2.2981\t Accuracy 0.1400\n",
      "Epoch [24][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [24][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [24][200]\t Batch [150][550]\t Training Loss 2.3014\t Accuracy 0.1128\n",
      "Epoch [24][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [24][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [24][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [24][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [24]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [24]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [25][200]\t Batch [0][550]\t Training Loss 2.2981\t Accuracy 0.1400\n",
      "Epoch [25][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [25][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [25][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [25][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [25][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [25][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [25][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [25]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [25]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [26][200]\t Batch [0][550]\t Training Loss 2.2981\t Accuracy 0.1400\n",
      "Epoch [26][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [26][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [26][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [26][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [26][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [26][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [26][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [26]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [26]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [27][200]\t Batch [0][550]\t Training Loss 2.2981\t Accuracy 0.1400\n",
      "Epoch [27][200]\t Batch [50][550]\t Training Loss 2.3014\t Accuracy 0.1125\n",
      "Epoch [27][200]\t Batch [100][550]\t Training Loss 2.3013\t Accuracy 0.1127\n",
      "Epoch [27][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [27][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [27][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [27][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [27][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [27]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [27]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [28][200]\t Batch [0][550]\t Training Loss 2.2981\t Accuracy 0.1400\n",
      "Epoch [28][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [28][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [28][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [28][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [28][200]\t Batch [250][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [28][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [28][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [28]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [28]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [29][200]\t Batch [0][550]\t Training Loss 2.2980\t Accuracy 0.1400\n",
      "Epoch [29][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [29][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [29][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [29][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [29][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [29][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [29][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [500][550]\t Training Loss 2.3015\t Accuracy 0.1136\n",
      "\n",
      "Epoch [29]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [29]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [30][200]\t Batch [0][550]\t Training Loss 2.2980\t Accuracy 0.1400\n",
      "Epoch [30][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [30][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [30][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [30][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [30][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [30][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [30][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [30]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [30]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [31][200]\t Batch [0][550]\t Training Loss 2.2980\t Accuracy 0.1400\n",
      "Epoch [31][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [31][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [31][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [31][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [31][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [31][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [31][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [31]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [31]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [32][200]\t Batch [0][550]\t Training Loss 2.2980\t Accuracy 0.1400\n",
      "Epoch [32][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [32][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [32][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [32][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [32][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [32][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [32][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [32]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [32]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [33][200]\t Batch [0][550]\t Training Loss 2.2980\t Accuracy 0.1400\n",
      "Epoch [33][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [33][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [33][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [33][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [33][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [33][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [33][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [33]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [33]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [34][200]\t Batch [0][550]\t Training Loss 2.2980\t Accuracy 0.1400\n",
      "Epoch [34][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [34][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [34][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [34][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [34][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [34][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [34][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [34]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [34]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [35][200]\t Batch [0][550]\t Training Loss 2.2980\t Accuracy 0.1400\n",
      "Epoch [35][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [35][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [35][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [35][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [35][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [35][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [35][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [35]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [35]\t Average validation loss 2.3019\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [36][200]\t Batch [0][550]\t Training Loss 2.2980\t Accuracy 0.1400\n",
      "Epoch [36][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [36][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [36][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [36][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [36][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [36][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [36][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [36]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [36]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [37][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [37][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [37][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [37][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [37][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [37][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [37][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [37][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [37]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [37]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [38][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [38][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [38][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [38][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [38][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [38][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [38][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [38][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [38][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [38][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [38][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [38]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [38]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [39][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [39][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [39][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [39][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [39][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [39][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [39][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [39][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [39][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [39][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [39][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [39]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [39]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [40][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [40][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [40][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [40][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [40][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [40][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [40][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [40][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [40][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [40][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [40][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [40]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [40]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [41][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [41][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [41][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [41][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [41][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [41][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [41][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [41][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [41][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [41][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [41][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [41]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [41]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [42][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [42][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [42][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [42][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [42][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [42][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [42][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [42][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [42][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [42][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [42][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [42]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [42]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [43][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [43][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [43][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [43][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [43][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [43][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [43][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [43][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [43][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [43][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [43][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [43]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [43]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [44][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [44][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [44][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [44][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [44][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [44][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [44][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [44][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [44][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [44][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [44][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [44]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [44]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [45][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [45][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [45][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [45][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [45][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [45][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [45][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [45][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [45][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [45][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [45][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [45]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [45]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [46][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [46][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [46][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [46][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [46][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [46][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [46][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [46][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [46][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [46][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [46][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [46]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [46]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [47][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [47][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [47][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [47][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [47][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [47][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [47][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [47][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [47][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [47][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [47][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [47]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [47]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [48][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [48][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [48][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [48][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [48][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [48][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [48][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [48][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [48][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [48][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [48][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [48]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [48]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [49][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [49][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [49][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [49][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [49][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [49][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [49][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [49][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [49][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [49][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [49][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [49]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [49]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [50][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [50][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [50][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [50][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [50][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [50][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [50][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [50][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [50][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [50][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [50][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [50]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [50]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [51][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [51][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [51][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [51][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [51][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [51][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [51][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [51][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [51][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [51][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [51][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [51]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [51]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [52][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [52][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [52][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [52][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [52][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [52][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [52][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [52][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [52][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [52][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [52][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [52]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [52]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [53][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [53][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [53][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [53][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [53][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [53][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [53][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [53][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [53][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [53][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [53][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [53]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [53]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [54][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [54][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [54][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [54][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [54][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [54][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [54][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [54][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [54][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [54][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [54][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [54]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [54]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [55][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [55][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [55][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [55][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [55][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [55][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [55][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [55][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [55][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [55][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [55][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [55]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [55]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [56][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [56][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [56][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [56][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [56][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [56][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [56][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [56][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [56][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [56][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [56][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [56]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [56]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [57][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [57][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [57][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [57][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [57][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [57][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [57][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [57][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [57][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [57][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [57][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [57]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [57]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [58][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [58][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [58][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [58][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [58][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [58][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [58][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [58][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [58][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [58][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [58][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [58]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [58]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [59][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [59][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [59][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [59][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [59][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [59][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [59][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [59][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [59][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [59][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [59][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [59]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [59]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [60][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [60][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [60][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [60][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [60][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [60][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [60][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [60][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [60][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [60][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [60][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [60]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [60]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [61][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [61][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [61][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [61][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [61][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [61][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [61][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [61][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [61][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [61][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [61][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [61]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [61]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [62][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [62][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [62][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [62][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [62][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [62][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [62][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [62][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [62][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [62][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [62][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [62]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [62]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [63][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [63][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [63][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [63][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [63][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [63][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [63][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [63][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [63][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [63][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [63][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [63]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [63]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [64][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [64][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [64][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [64][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [64][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [64][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [64][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [64][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [64][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [64][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [64][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [64]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [64]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [65][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [65][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [65][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [65][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [65][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [65][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [65][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [65][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [65][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [65][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [65][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [65]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [65]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [66][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [66][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [66][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [66][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [66][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [66][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [66][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [66][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [66][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [66][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [66][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [66]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [66]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [67][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [67][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [67][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [67][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [67][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [67][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [67][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [67][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [67][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [67][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [67][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [67]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [67]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [68][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [68][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [68][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [68][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [68][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [68][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [68][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [68][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [68][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [68][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [68][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [68]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [68]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [69][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [69][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [69][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [69][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [69][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [69][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [69][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [69][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [69][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [69][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [69][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [69]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [69]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [70][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [70][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [70][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [70][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [70][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [70][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [70][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [70][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [70][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [70][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [70][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [70]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [70]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [71][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [71][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [71][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [71][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [71][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [71][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [71][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [71][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [71][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [71][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [71][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [71]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [71]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [72][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [72][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [72][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [72][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [72][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [72][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [72][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [72][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [72][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [72][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [72][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [72]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [72]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [73][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [73][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [73][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [73][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [73][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [73][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [73][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [73][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [73][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [73][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [73][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [73]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [73]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [74][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [74][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [74][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [74][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [74][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [74][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [74][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [74][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [74][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [74][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [74][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [74]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [74]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [75][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [75][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [75][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [75][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [75][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [75][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [75][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [75][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [75][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [75][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [75][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [75]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [75]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [76][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [76][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [76][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [76][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [76][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [76][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [76][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [76][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [76][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [76][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [76][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [76]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [76]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [77][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [77][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [77][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [77][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [77][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [77][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [77][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [77][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [77][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [77][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [77][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [77]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [77]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [78][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [78][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [78][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [78][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [78][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [78][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [78][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [78][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [78][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [78][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [78][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [78]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [78]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [79][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [79][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [79][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [79][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [79][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [79][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [79][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [79][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [79][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [79][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [79][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [79]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [79]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [80][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [80][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [80][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [80][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [80][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [80][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [80][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [80][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [80][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [80][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [80][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [80]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [80]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [81][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [81][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [81][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [81][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [81][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [81][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [81][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [81][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [81][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [81][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [81][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [81]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [81]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [82][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [82][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [82][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [82][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [82][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [82][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [82][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [82][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [82][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [82][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [82][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [82]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [82]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [83][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [83][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [83][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [83][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [83][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [83][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [83][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [83][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [83][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [83][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [83][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [83]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [83]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [84][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [84][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [84][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [84][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [84][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [84][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [84][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [84][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [84][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [84][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [84][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [84]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [84]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [85][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [85][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [85][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [85][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [85][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [85][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [85][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [85][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [85][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [85][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [85][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [85]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [85]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [86][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [86][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [86][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [86][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [86][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [86][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [86][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [86][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [86][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [86][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [86][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [86]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [86]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [87][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [87][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [87][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [87][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [87][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [87][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [87][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [87][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [87][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [87][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [87][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [87]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [87]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [88][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [88][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [88][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [88][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [88][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [88][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [88][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [88][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [88][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [88][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [88][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [88]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [88]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [89][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [89][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [89][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [89][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [89][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [89][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [89][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [89][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [89][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [89][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [89][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [89]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [89]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [90][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [90][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [90][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [90][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [90][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [90][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [90][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [90][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [90][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [90][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [90][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [90]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [90]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [91][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [91][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [91][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [91][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [91][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [91][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [91][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [91][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [91][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [91][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [91][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [91]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [91]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [92][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [92][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [92][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [92][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [92][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [92][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [92][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [92][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [92][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [92][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [92][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [92]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [92]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [93][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [93][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [93][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [93][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [93][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [93][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [93][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [93][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [93][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [93][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [93][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [93]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [93]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [94][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [94][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [94][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [94][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [94][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [94][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [94][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [94][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [94][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [94][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [94][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [94]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [94]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [95][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [95][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [95][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [95][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [95][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [95][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [95][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [95][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [95][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [95][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [95][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [95]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [95]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [96][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [96][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [96][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [96][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [96][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [96][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [96][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [96][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [96][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [96][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [96][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [96]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [96]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [97][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [97][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [97][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [97][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [97][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [97][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [97][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [97][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [97][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [97][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [97][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [97]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [97]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [98][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [98][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [98][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [98][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [98][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [98][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [98][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [98][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [98][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [98][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [98][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [98]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [98]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [99][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [99][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [99][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [99][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [99][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [99][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [99][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [99][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [99][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [99][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [99][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [99]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [99]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [100][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [100][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [100][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [100][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [100][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [100][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [100][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [100][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [100][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [100][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [100][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [100]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [100]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [101][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [101][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [101][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [101][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [101][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [101][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [101][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [101][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [101][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [101][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [101][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [101]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [101]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [102][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [102][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [102][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [102][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [102][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [102][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [102][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [102][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [102][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [102][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [102][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [102]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [102]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [103][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [103][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [103][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [103][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [103][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [103][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [103][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [103][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [103][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [103][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [103][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [103]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [103]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [104][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [104][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [104][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [104][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [104][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [104][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [104][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [104][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [104][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [104][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [104][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [104]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [104]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [105][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [105][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [105][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [105][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [105][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [105][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [105][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [105][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [105][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [105][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [105][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [105]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [105]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [106][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [106][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [106][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [106][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [106][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [106][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [106][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [106][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [106][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [106][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [106][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [106]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [106]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [107][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [107][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [107][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [107][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [107][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [107][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [107][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [107][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [107][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [107][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [107][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [107]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [107]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [108][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [108][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [108][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [108][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [108][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [108][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [108][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [108][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [108][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [108][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [108][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [108]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [108]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [109][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [109][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [109][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [109][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [109][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [109][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [109][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [109][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [109][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [109][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [109][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [109]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [109]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [110][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [110][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [110][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [110][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [110][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [110][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [110][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [110][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [110][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [110][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [110][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [110]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [110]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [111][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [111][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [111][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [111][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [111][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [111][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [111][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [111][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [111][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [111][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [111][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [111]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [111]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [112][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [112][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [112][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [112][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [112][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [112][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [112][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [112][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [112][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [112][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [112][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [112]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [112]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [113][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [113][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [113][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [113][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [113][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [113][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [113][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [113][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [113][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [113][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [113][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [113]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [113]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [114][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [114][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [114][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [114][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [114][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [114][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [114][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [114][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [114][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [114][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [114][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [114]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [114]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [115][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [115][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [115][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [115][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [115][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [115][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [115][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [115][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [115][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [115][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [115][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [115]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [115]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [116][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [116][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [116][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [116][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [116][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [116][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [116][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [116][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [116][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [116][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [116][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [116]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [116]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [117][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [117][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [117][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [117][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [117][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [117][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [117][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [117][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [117][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [117][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [117][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [117]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [117]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [118][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [118][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [118][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [118][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [118][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [118][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [118][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [118][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [118][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [118][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [118][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [118]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [118]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [119][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [119][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [119][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [119][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [119][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [119][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [119][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [119][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [119][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [119][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [119][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [119]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [119]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [120][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [120][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [120][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [120][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [120][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [120][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [120][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [120][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [120][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [120][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [120][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [120]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [120]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [121][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [121][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [121][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [121][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [121][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [121][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [121][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [121][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [121][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [121][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [121][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [121]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [121]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [122][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [122][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [122][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [122][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [122][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [122][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [122][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [122][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [122][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [122][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [122][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [122]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [122]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [123][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [123][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [123][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [123][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [123][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [123][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [123][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [123][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [123][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [123][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [123][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [123]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [123]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [124][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [124][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [124][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [124][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [124][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [124][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [124][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [124][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [124][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [124][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [124][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [124]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [124]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [125][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [125][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [125][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [125][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [125][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [125][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [125][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [125][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [125][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [125][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [125][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [125]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [125]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [126][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [126][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [126][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [126][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [126][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [126][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [126][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [126][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [126][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [126][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [126][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [126]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [126]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [127][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [127][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [127][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [127][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [127][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [127][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [127][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [127][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [127][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [127][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [127][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [127]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [127]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [128][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [128][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [128][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [128][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [128][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [128][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [128][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [128][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [128][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [128][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [128][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [128]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [128]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [129][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [129][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [129][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [129][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [129][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [129][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [129][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [129][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [129][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [129][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [129][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [129]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [129]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [130][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [130][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [130][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [130][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [130][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [130][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [130][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [130][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [130][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [130][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [130][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [130]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [130]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [131][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [131][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [131][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [131][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [131][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [131][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [131][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [131][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [131][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [131][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [131][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [131]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [131]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [132][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [132][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [132][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [132][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [132][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [132][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [132][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [132][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [132][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [132][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [132][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [132]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [132]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [133][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [133][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [133][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [133][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [133][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [133][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [133][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [133][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [133][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [133][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [133][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [133]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [133]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [134][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [134][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [134][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [134][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [134][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [134][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [134][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [134][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [134][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [134][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [134][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [134]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [134]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [135][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [135][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [135][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [135][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [135][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [135][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [135][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [135][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [135][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [135][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [135][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [135]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [135]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [136][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [136][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [136][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [136][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [136][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [136][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [136][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [136][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [136][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [136][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [136][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [136]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [136]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [137][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [137][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [137][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [137][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [137][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [137][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [137][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [137][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [137][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [137][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [137][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [137]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [137]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [138][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [138][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [138][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [138][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [138][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [138][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [138][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [138][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [138][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [138][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [138][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [138]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [138]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [139][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [139][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [139][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [139][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [139][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [139][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [139][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [139][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [139][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [139][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [139][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [139]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [139]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [140][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [140][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [140][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [140][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [140][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [140][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [140][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [140][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [140][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [140][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [140][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [140]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [140]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [141][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [141][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [141][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [141][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [141][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [141][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [141][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [141][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [141][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [141][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [141][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [141]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [141]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [142][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [142][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [142][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [142][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [142][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [142][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [142][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [142][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [142][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [142][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [142][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [142]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [142]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [143][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [143][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [143][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [143][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [143][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [143][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [143][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [143][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [143][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [143][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [143][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [143]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [143]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [144][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [144][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [144][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [144][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [144][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [144][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [144][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [144][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [144][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [144][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [144][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [144]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [144]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [145][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [145][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [145][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [145][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [145][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [145][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [145][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [145][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [145][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [145][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [145][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [145]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [145]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [146][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [146][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [146][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [146][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [146][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [146][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [146][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [146][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [146][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [146][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [146][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [146]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [146]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [147][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [147][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [147][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [147][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [147][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [147][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [147][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [147][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [147][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [147][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [147][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [147]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [147]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [148][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [148][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [148][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [148][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [148][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [148][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [148][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [148][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [148][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [148][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [148][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [148]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [148]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [149][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [149][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [149][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [149][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [149][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [149][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [149][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [149][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [149][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [149][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [149][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [149]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [149]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [150][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [150][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [150][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [150][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [150][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [150][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [150][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [150][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [150][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [150][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [150][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [150]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [150]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [151][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [151][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [151][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [151][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [151][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [151][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [151][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [151][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [151][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [151][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [151][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [151]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [151]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [152][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [152][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [152][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [152][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [152][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [152][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [152][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [152][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [152][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [152][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [152][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [152]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [152]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [153][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [153][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [153][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [153][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [153][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [153][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [153][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [153][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [153][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [153][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [153][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [153]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [153]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [154][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [154][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [154][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [154][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [154][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [154][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [154][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [154][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [154][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [154][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [154][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [154]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [154]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [155][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [155][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [155][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [155][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [155][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [155][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [155][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [155][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [155][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [155][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [155][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [155]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [155]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [156][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [156][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [156][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [156][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [156][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [156][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [156][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [156][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [156][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [156][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [156][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [156]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [156]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [157][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [157][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [157][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [157][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [157][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [157][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [157][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [157][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [157][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [157][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [157][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [157]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [157]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [158][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [158][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [158][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [158][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [158][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [158][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [158][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [158][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [158][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [158][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [158][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [158]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [158]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [159][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [159][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [159][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [159][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [159][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [159][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [159][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [159][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [159][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [159][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [159][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [159]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [159]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [160][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [160][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [160][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [160][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [160][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [160][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [160][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [160][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [160][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [160][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [160][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [160]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [160]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [161][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [161][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [161][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [161][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [161][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [161][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [161][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [161][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [161][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [161][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [161][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [161]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [161]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [162][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [162][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [162][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [162][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [162][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [162][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [162][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [162][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [162][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [162][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [162][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [162]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [162]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [163][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [163][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [163][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [163][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [163][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [163][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [163][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [163][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [163][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [163][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [163][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [163]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [163]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [164][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [164][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [164][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [164][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [164][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [164][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [164][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [164][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [164][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [164][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [164][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [164]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [164]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [165][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [165][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [165][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [165][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [165][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [165][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [165][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [165][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [165][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [165][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [165][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [165]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [165]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [166][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [166][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [166][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [166][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [166][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [166][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [166][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [166][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [166][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [166][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [166][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [166]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [166]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [167][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [167][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [167][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [167][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [167][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [167][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [167][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [167][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [167][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [167][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [167][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [167]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [167]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [168][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [168][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [168][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [168][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [168][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [168][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [168][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [168][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [168][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [168][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [168][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [168]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [168]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [169][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [169][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [169][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [169][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [169][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [169][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [169][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [169][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [169][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [169][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [169][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [169]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [169]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [170][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [170][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [170][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [170][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [170][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [170][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [170][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [170][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [170][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [170][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [170][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [170]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [170]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [171][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [171][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [171][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [171][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [171][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [171][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [171][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [171][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [171][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [171][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [171][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [171]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [171]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [172][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [172][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [172][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [172][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [172][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [172][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [172][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [172][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [172][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [172][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [172][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [172]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [172]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [173][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [173][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [173][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [173][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [173][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [173][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [173][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [173][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [173][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [173][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [173][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [173]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [173]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [174][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [174][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [174][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [174][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [174][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [174][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [174][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [174][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [174][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [174][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [174][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [174]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [174]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [175][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [175][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [175][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [175][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [175][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [175][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [175][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [175][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [175][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [175][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [175][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [175]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [175]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [176][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [176][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [176][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [176][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [176][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [176][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [176][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [176][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [176][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [176][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [176][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [176]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [176]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [177][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [177][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [177][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [177][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [177][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [177][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [177][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [177][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [177][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [177][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [177][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [177]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [177]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [178][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [178][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [178][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [178][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [178][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [178][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [178][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [178][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [178][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [178][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [178][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [178]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [178]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [179][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [179][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [179][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [179][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [179][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [179][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [179][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [179][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [179][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [179][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [179][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [179]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [179]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [180][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [180][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [180][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [180][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [180][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [180][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [180][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [180][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [180][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [180][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [180][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [180]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [180]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [181][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [181][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [181][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [181][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [181][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [181][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [181][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [181][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [181][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [181][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [181][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [181]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [181]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [182][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [182][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [182][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [182][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [182][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [182][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [182][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [182][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [182][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [182][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [182][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [182]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [182]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [183][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [183][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [183][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [183][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [183][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [183][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [183][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [183][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [183][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [183][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [183][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [183]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [183]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [184][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [184][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [184][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [184][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [184][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [184][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [184][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [184][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [184][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [184][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [184][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [184]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [184]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [185][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [185][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [185][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [185][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [185][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [185][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [185][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [185][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [185][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [185][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [185][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [185]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [185]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [186][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [186][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [186][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [186][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [186][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [186][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [186][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [186][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [186][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [186][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [186][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [186]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [186]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [187][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [187][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [187][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [187][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [187][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [187][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [187][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [187][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [187][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [187][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [187][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [187]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [187]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [188][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [188][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [188][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [188][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [188][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [188][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [188][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [188][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [188][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [188][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [188][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [188]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [188]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [189][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [189][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [189][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [189][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [189][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [189][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [189][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [189][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [189][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [189][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [189][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [189]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [189]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [190][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [190][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [190][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [190][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [190][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [190][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [190][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [190][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [190][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [190][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [190][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [190]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [190]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [191][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [191][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [191][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [191][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [191][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [191][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [191][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [191][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [191][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [191][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [191][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [191]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [191]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [192][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [192][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [192][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [192][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [192][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [192][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [192][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [192][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [192][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [192][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [192][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [192]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [192]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [193][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [193][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [193][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [193][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [193][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [193][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [193][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [193][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [193][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [193][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [193][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [193]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [193]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [194][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [194][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [194][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [194][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [194][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [194][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [194][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [194][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [194][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [194][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [194][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [194]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [194]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [195][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [195][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [195][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [195][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [195][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [195][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [195][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [195][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [195][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [195][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [195][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [195]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [195]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [196][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [196][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [196][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [196][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [196][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [196][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [196][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [196][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [196][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [196][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [196][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [196]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [196]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [197][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [197][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [197][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [197][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [197][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [197][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [197][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [197][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [197][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [197][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [197][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [197]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [197]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [198][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [198][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [198][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [198][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [198][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [198][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [198][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [198][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [198][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [198][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [198][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [198]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [198]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [199][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [199][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [199][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [199][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [199][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [199][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [199][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [199][200]\t Batch [350][550]\t Training Loss 2.3014\t Accuracy 0.1143\n",
      "Epoch [199][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [199][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [199][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [199]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [199]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [0][200]\t Batch [0][550]\t Training Loss 2.2979\t Accuracy 0.1400\n",
      "Epoch [0][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [0][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [0][200]\t Batch [150][550]\t Training Loss 2.3013\t Accuracy 0.1128\n",
      "Epoch [0][200]\t Batch [200][550]\t Training Loss 2.3013\t Accuracy 0.1142\n",
      "Epoch [0][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [0][200]\t Batch [300][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [0][200]\t Batch [350][550]\t Training Loss 2.3013\t Accuracy 0.1143\n",
      "Epoch [0][200]\t Batch [400][550]\t Training Loss 2.3014\t Accuracy 0.1141\n",
      "Epoch [0][200]\t Batch [450][550]\t Training Loss 2.3014\t Accuracy 0.1140\n",
      "Epoch [0][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [0]\t Average training loss 2.3015\t Average training accuracy 0.1129\n",
      "Epoch [0]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [1][200]\t Batch [0][550]\t Training Loss 2.2977\t Accuracy 0.1400\n",
      "Epoch [1][200]\t Batch [50][550]\t Training Loss 2.3013\t Accuracy 0.1125\n",
      "Epoch [1][200]\t Batch [100][550]\t Training Loss 2.3012\t Accuracy 0.1127\n",
      "Epoch [1][200]\t Batch [150][550]\t Training Loss 2.3012\t Accuracy 0.1128\n",
      "Epoch [1][200]\t Batch [200][550]\t Training Loss 2.3012\t Accuracy 0.1142\n",
      "Epoch [1][200]\t Batch [250][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [1][200]\t Batch [300][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [1][200]\t Batch [350][550]\t Training Loss 2.3013\t Accuracy 0.1143\n",
      "Epoch [1][200]\t Batch [400][550]\t Training Loss 2.3013\t Accuracy 0.1141\n",
      "Epoch [1][200]\t Batch [450][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [1][200]\t Batch [500][550]\t Training Loss 2.3014\t Accuracy 0.1136\n",
      "\n",
      "Epoch [1]\t Average training loss 2.3014\t Average training accuracy 0.1129\n",
      "Epoch [1]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [2][200]\t Batch [0][550]\t Training Loss 2.2974\t Accuracy 0.1400\n",
      "Epoch [2][200]\t Batch [50][550]\t Training Loss 2.3012\t Accuracy 0.1125\n",
      "Epoch [2][200]\t Batch [100][550]\t Training Loss 2.3011\t Accuracy 0.1127\n",
      "Epoch [2][200]\t Batch [150][550]\t Training Loss 2.3012\t Accuracy 0.1128\n",
      "Epoch [2][200]\t Batch [200][550]\t Training Loss 2.3012\t Accuracy 0.1142\n",
      "Epoch [2][200]\t Batch [250][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [2][200]\t Batch [300][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [2][200]\t Batch [350][550]\t Training Loss 2.3013\t Accuracy 0.1143\n",
      "Epoch [2][200]\t Batch [400][550]\t Training Loss 2.3013\t Accuracy 0.1141\n",
      "Epoch [2][200]\t Batch [450][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [2][200]\t Batch [500][550]\t Training Loss 2.3013\t Accuracy 0.1136\n",
      "\n",
      "Epoch [2]\t Average training loss 2.3014\t Average training accuracy 0.1129\n",
      "Epoch [2]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [3][200]\t Batch [0][550]\t Training Loss 2.2972\t Accuracy 0.1400\n",
      "Epoch [3][200]\t Batch [50][550]\t Training Loss 2.3012\t Accuracy 0.1125\n",
      "Epoch [3][200]\t Batch [100][550]\t Training Loss 2.3011\t Accuracy 0.1127\n",
      "Epoch [3][200]\t Batch [150][550]\t Training Loss 2.3012\t Accuracy 0.1128\n",
      "Epoch [3][200]\t Batch [200][550]\t Training Loss 2.3011\t Accuracy 0.1142\n",
      "Epoch [3][200]\t Batch [250][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [3][200]\t Batch [300][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [3][200]\t Batch [350][550]\t Training Loss 2.3012\t Accuracy 0.1143\n",
      "Epoch [3][200]\t Batch [400][550]\t Training Loss 2.3013\t Accuracy 0.1141\n",
      "Epoch [3][200]\t Batch [450][550]\t Training Loss 2.3013\t Accuracy 0.1140\n",
      "Epoch [3][200]\t Batch [500][550]\t Training Loss 2.3013\t Accuracy 0.1136\n",
      "\n",
      "Epoch [3]\t Average training loss 2.3014\t Average training accuracy 0.1129\n",
      "Epoch [3]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [4][200]\t Batch [0][550]\t Training Loss 2.2971\t Accuracy 0.1400\n",
      "Epoch [4][200]\t Batch [50][550]\t Training Loss 2.3011\t Accuracy 0.1125\n",
      "Epoch [4][200]\t Batch [100][550]\t Training Loss 2.3010\t Accuracy 0.1127\n",
      "Epoch [4][200]\t Batch [150][550]\t Training Loss 2.3011\t Accuracy 0.1128\n",
      "Epoch [4][200]\t Batch [200][550]\t Training Loss 2.3011\t Accuracy 0.1142\n",
      "Epoch [4][200]\t Batch [250][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [4][200]\t Batch [300][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [4][200]\t Batch [350][550]\t Training Loss 2.3012\t Accuracy 0.1143\n",
      "Epoch [4][200]\t Batch [400][550]\t Training Loss 2.3012\t Accuracy 0.1141\n",
      "Epoch [4][200]\t Batch [450][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [4][200]\t Batch [500][550]\t Training Loss 2.3013\t Accuracy 0.1136\n",
      "\n",
      "Epoch [4]\t Average training loss 2.3013\t Average training accuracy 0.1129\n",
      "Epoch [4]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [5][200]\t Batch [0][550]\t Training Loss 2.2969\t Accuracy 0.1400\n",
      "Epoch [5][200]\t Batch [50][550]\t Training Loss 2.3011\t Accuracy 0.1125\n",
      "Epoch [5][200]\t Batch [100][550]\t Training Loss 2.3010\t Accuracy 0.1127\n",
      "Epoch [5][200]\t Batch [150][550]\t Training Loss 2.3011\t Accuracy 0.1128\n",
      "Epoch [5][200]\t Batch [200][550]\t Training Loss 2.3011\t Accuracy 0.1142\n",
      "Epoch [5][200]\t Batch [250][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [5][200]\t Batch [300][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [5][200]\t Batch [350][550]\t Training Loss 2.3012\t Accuracy 0.1143\n",
      "Epoch [5][200]\t Batch [400][550]\t Training Loss 2.3012\t Accuracy 0.1141\n",
      "Epoch [5][200]\t Batch [450][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [5][200]\t Batch [500][550]\t Training Loss 2.3013\t Accuracy 0.1136\n",
      "\n",
      "Epoch [5]\t Average training loss 2.3013\t Average training accuracy 0.1129\n",
      "Epoch [5]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [6][200]\t Batch [0][550]\t Training Loss 2.2967\t Accuracy 0.1400\n",
      "Epoch [6][200]\t Batch [50][550]\t Training Loss 2.3011\t Accuracy 0.1125\n",
      "Epoch [6][200]\t Batch [100][550]\t Training Loss 2.3010\t Accuracy 0.1127\n",
      "Epoch [6][200]\t Batch [150][550]\t Training Loss 2.3011\t Accuracy 0.1128\n",
      "Epoch [6][200]\t Batch [200][550]\t Training Loss 2.3010\t Accuracy 0.1142\n",
      "Epoch [6][200]\t Batch [250][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [6][200]\t Batch [300][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [6][200]\t Batch [350][550]\t Training Loss 2.3011\t Accuracy 0.1143\n",
      "Epoch [6][200]\t Batch [400][550]\t Training Loss 2.3012\t Accuracy 0.1141\n",
      "Epoch [6][200]\t Batch [450][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [6][200]\t Batch [500][550]\t Training Loss 2.3012\t Accuracy 0.1136\n",
      "\n",
      "Epoch [6]\t Average training loss 2.3013\t Average training accuracy 0.1129\n",
      "Epoch [6]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [7][200]\t Batch [0][550]\t Training Loss 2.2966\t Accuracy 0.1400\n",
      "Epoch [7][200]\t Batch [50][550]\t Training Loss 2.3011\t Accuracy 0.1125\n",
      "Epoch [7][200]\t Batch [100][550]\t Training Loss 2.3009\t Accuracy 0.1127\n",
      "Epoch [7][200]\t Batch [150][550]\t Training Loss 2.3010\t Accuracy 0.1128\n",
      "Epoch [7][200]\t Batch [200][550]\t Training Loss 2.3010\t Accuracy 0.1142\n",
      "Epoch [7][200]\t Batch [250][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [7][200]\t Batch [300][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [7][200]\t Batch [350][550]\t Training Loss 2.3011\t Accuracy 0.1143\n",
      "Epoch [7][200]\t Batch [400][550]\t Training Loss 2.3012\t Accuracy 0.1141\n",
      "Epoch [7][200]\t Batch [450][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [7][200]\t Batch [500][550]\t Training Loss 2.3012\t Accuracy 0.1136\n",
      "\n",
      "Epoch [7]\t Average training loss 2.3013\t Average training accuracy 0.1129\n",
      "Epoch [7]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [8][200]\t Batch [0][550]\t Training Loss 2.2964\t Accuracy 0.1400\n",
      "Epoch [8][200]\t Batch [50][550]\t Training Loss 2.3010\t Accuracy 0.1125\n",
      "Epoch [8][200]\t Batch [100][550]\t Training Loss 2.3009\t Accuracy 0.1127\n",
      "Epoch [8][200]\t Batch [150][550]\t Training Loss 2.3010\t Accuracy 0.1128\n",
      "Epoch [8][200]\t Batch [200][550]\t Training Loss 2.3010\t Accuracy 0.1142\n",
      "Epoch [8][200]\t Batch [250][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [8][200]\t Batch [300][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [8][200]\t Batch [350][550]\t Training Loss 2.3011\t Accuracy 0.1143\n",
      "Epoch [8][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [8][200]\t Batch [450][550]\t Training Loss 2.3012\t Accuracy 0.1140\n",
      "Epoch [8][200]\t Batch [500][550]\t Training Loss 2.3012\t Accuracy 0.1136\n",
      "\n",
      "Epoch [8]\t Average training loss 2.3013\t Average training accuracy 0.1129\n",
      "Epoch [8]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [9][200]\t Batch [0][550]\t Training Loss 2.2963\t Accuracy 0.1400\n",
      "Epoch [9][200]\t Batch [50][550]\t Training Loss 2.3010\t Accuracy 0.1125\n",
      "Epoch [9][200]\t Batch [100][550]\t Training Loss 2.3009\t Accuracy 0.1127\n",
      "Epoch [9][200]\t Batch [150][550]\t Training Loss 2.3010\t Accuracy 0.1128\n",
      "Epoch [9][200]\t Batch [200][550]\t Training Loss 2.3010\t Accuracy 0.1142\n",
      "Epoch [9][200]\t Batch [250][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [9][200]\t Batch [300][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [9][200]\t Batch [350][550]\t Training Loss 2.3011\t Accuracy 0.1143\n",
      "Epoch [9][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [9][200]\t Batch [450][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [9][200]\t Batch [500][550]\t Training Loss 2.3012\t Accuracy 0.1136\n",
      "\n",
      "Epoch [9]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [9]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [10][200]\t Batch [0][550]\t Training Loss 2.2961\t Accuracy 0.1400\n",
      "Epoch [10][200]\t Batch [50][550]\t Training Loss 2.3010\t Accuracy 0.1125\n",
      "Epoch [10][200]\t Batch [100][550]\t Training Loss 2.3009\t Accuracy 0.1127\n",
      "Epoch [10][200]\t Batch [150][550]\t Training Loss 2.3010\t Accuracy 0.1128\n",
      "Epoch [10][200]\t Batch [200][550]\t Training Loss 2.3009\t Accuracy 0.1142\n",
      "Epoch [10][200]\t Batch [250][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [300][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [350][550]\t Training Loss 2.3011\t Accuracy 0.1143\n",
      "Epoch [10][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [10][200]\t Batch [450][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [10][200]\t Batch [500][550]\t Training Loss 2.3012\t Accuracy 0.1136\n",
      "\n",
      "Epoch [10]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [10]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [11][200]\t Batch [0][550]\t Training Loss 2.2960\t Accuracy 0.1400\n",
      "Epoch [11][200]\t Batch [50][550]\t Training Loss 2.3010\t Accuracy 0.1125\n",
      "Epoch [11][200]\t Batch [100][550]\t Training Loss 2.3008\t Accuracy 0.1127\n",
      "Epoch [11][200]\t Batch [150][550]\t Training Loss 2.3010\t Accuracy 0.1128\n",
      "Epoch [11][200]\t Batch [200][550]\t Training Loss 2.3009\t Accuracy 0.1142\n",
      "Epoch [11][200]\t Batch [250][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [300][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [350][550]\t Training Loss 2.3011\t Accuracy 0.1143\n",
      "Epoch [11][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [11][200]\t Batch [450][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [11][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [11]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [11]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [12][200]\t Batch [0][550]\t Training Loss 2.2959\t Accuracy 0.1400\n",
      "Epoch [12][200]\t Batch [50][550]\t Training Loss 2.3010\t Accuracy 0.1125\n",
      "Epoch [12][200]\t Batch [100][550]\t Training Loss 2.3008\t Accuracy 0.1127\n",
      "Epoch [12][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [12][200]\t Batch [200][550]\t Training Loss 2.3009\t Accuracy 0.1142\n",
      "Epoch [12][200]\t Batch [250][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [300][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [12][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [12][200]\t Batch [450][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [12][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [12]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [12]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [13][200]\t Batch [0][550]\t Training Loss 2.2958\t Accuracy 0.1400\n",
      "Epoch [13][200]\t Batch [50][550]\t Training Loss 2.3010\t Accuracy 0.1125\n",
      "Epoch [13][200]\t Batch [100][550]\t Training Loss 2.3008\t Accuracy 0.1127\n",
      "Epoch [13][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [13][200]\t Batch [200][550]\t Training Loss 2.3009\t Accuracy 0.1142\n",
      "Epoch [13][200]\t Batch [250][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [300][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [13][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [13][200]\t Batch [450][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [13][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [13]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [13]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [14][200]\t Batch [0][550]\t Training Loss 2.2957\t Accuracy 0.1400\n",
      "Epoch [14][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [14][200]\t Batch [100][550]\t Training Loss 2.3008\t Accuracy 0.1127\n",
      "Epoch [14][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [14][200]\t Batch [200][550]\t Training Loss 2.3009\t Accuracy 0.1142\n",
      "Epoch [14][200]\t Batch [250][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [300][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [14][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [14][200]\t Batch [450][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [14][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [14]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [14]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [15][200]\t Batch [0][550]\t Training Loss 2.2956\t Accuracy 0.1400\n",
      "Epoch [15][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [15][200]\t Batch [100][550]\t Training Loss 2.3008\t Accuracy 0.1127\n",
      "Epoch [15][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [15][200]\t Batch [200][550]\t Training Loss 2.3009\t Accuracy 0.1142\n",
      "Epoch [15][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [300][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [15][200]\t Batch [400][550]\t Training Loss 2.3011\t Accuracy 0.1141\n",
      "Epoch [15][200]\t Batch [450][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [15][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [15]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [15]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [16][200]\t Batch [0][550]\t Training Loss 2.2955\t Accuracy 0.1400\n",
      "Epoch [16][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [16][200]\t Batch [100][550]\t Training Loss 2.3008\t Accuracy 0.1127\n",
      "Epoch [16][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [16][200]\t Batch [200][550]\t Training Loss 2.3009\t Accuracy 0.1142\n",
      "Epoch [16][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [16][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [16][200]\t Batch [450][550]\t Training Loss 2.3011\t Accuracy 0.1140\n",
      "Epoch [16][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [16]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [16]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [17][200]\t Batch [0][550]\t Training Loss 2.2954\t Accuracy 0.1400\n",
      "Epoch [17][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [17][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [17][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [17][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [17][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [17][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [17][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [17][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [17]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [17]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [18][200]\t Batch [0][550]\t Training Loss 2.2953\t Accuracy 0.1400\n",
      "Epoch [18][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [18][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [18][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [18][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [18][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [18][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [18][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [18][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [18]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [18]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [19][200]\t Batch [0][550]\t Training Loss 2.2953\t Accuracy 0.1400\n",
      "Epoch [19][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [19][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [19][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [19][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [19][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [19][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [19][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [19][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [19]\t Average training loss 2.3012\t Average training accuracy 0.1129\n",
      "Epoch [19]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [20][200]\t Batch [0][550]\t Training Loss 2.2952\t Accuracy 0.1400\n",
      "Epoch [20][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [20][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [20][200]\t Batch [150][550]\t Training Loss 2.3009\t Accuracy 0.1128\n",
      "Epoch [20][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [20][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [20][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [20][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [20][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [20]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [20]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [21][200]\t Batch [0][550]\t Training Loss 2.2951\t Accuracy 0.1400\n",
      "Epoch [21][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [21][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [21][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [21][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [21][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [21][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [21][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [21][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [21]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [21]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [22][200]\t Batch [0][550]\t Training Loss 2.2951\t Accuracy 0.1400\n",
      "Epoch [22][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [22][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [22][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [22][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [22][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [22][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [22][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [22][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [22]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [22]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [23][200]\t Batch [0][550]\t Training Loss 2.2950\t Accuracy 0.1400\n",
      "Epoch [23][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [23][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [23][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [23][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [23][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [350][550]\t Training Loss 2.3010\t Accuracy 0.1143\n",
      "Epoch [23][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [23][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [23][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [23]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [23]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [24][200]\t Batch [0][550]\t Training Loss 2.2949\t Accuracy 0.1400\n",
      "Epoch [24][200]\t Batch [50][550]\t Training Loss 2.3009\t Accuracy 0.1125\n",
      "Epoch [24][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [24][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [24][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [24][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [350][550]\t Training Loss 2.3009\t Accuracy 0.1143\n",
      "Epoch [24][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [24][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [24][200]\t Batch [500][550]\t Training Loss 2.3011\t Accuracy 0.1136\n",
      "\n",
      "Epoch [24]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [24]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [25][200]\t Batch [0][550]\t Training Loss 2.2949\t Accuracy 0.1400\n",
      "Epoch [25][200]\t Batch [50][550]\t Training Loss 2.3008\t Accuracy 0.1125\n",
      "Epoch [25][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [25][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [25][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [25][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [350][550]\t Training Loss 2.3009\t Accuracy 0.1143\n",
      "Epoch [25][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [25][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [25][200]\t Batch [500][550]\t Training Loss 2.3010\t Accuracy 0.1136\n",
      "\n",
      "Epoch [25]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [25]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [26][200]\t Batch [0][550]\t Training Loss 2.2948\t Accuracy 0.1400\n",
      "Epoch [26][200]\t Batch [50][550]\t Training Loss 2.3008\t Accuracy 0.1125\n",
      "Epoch [26][200]\t Batch [100][550]\t Training Loss 2.3007\t Accuracy 0.1127\n",
      "Epoch [26][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [26][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [26][200]\t Batch [250][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [350][550]\t Training Loss 2.3009\t Accuracy 0.1143\n",
      "Epoch [26][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [26][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [26][200]\t Batch [500][550]\t Training Loss 2.3010\t Accuracy 0.1136\n",
      "\n",
      "Epoch [26]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [26]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [27][200]\t Batch [0][550]\t Training Loss 2.2948\t Accuracy 0.1400\n",
      "Epoch [27][200]\t Batch [50][550]\t Training Loss 2.3008\t Accuracy 0.1125\n",
      "Epoch [27][200]\t Batch [100][550]\t Training Loss 2.3006\t Accuracy 0.1127\n",
      "Epoch [27][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [27][200]\t Batch [200][550]\t Training Loss 2.3008\t Accuracy 0.1142\n",
      "Epoch [27][200]\t Batch [250][550]\t Training Loss 2.3008\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [350][550]\t Training Loss 2.3009\t Accuracy 0.1143\n",
      "Epoch [27][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [27][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [27][200]\t Batch [500][550]\t Training Loss 2.3010\t Accuracy 0.1136\n",
      "\n",
      "Epoch [27]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [27]\t Average validation loss 2.3018\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [28][200]\t Batch [0][550]\t Training Loss 2.2947\t Accuracy 0.1400\n",
      "Epoch [28][200]\t Batch [50][550]\t Training Loss 2.3008\t Accuracy 0.1125\n",
      "Epoch [28][200]\t Batch [100][550]\t Training Loss 2.3006\t Accuracy 0.1127\n",
      "Epoch [28][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [28][200]\t Batch [200][550]\t Training Loss 2.3007\t Accuracy 0.1142\n",
      "Epoch [28][200]\t Batch [250][550]\t Training Loss 2.3008\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [300][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [350][550]\t Training Loss 2.3009\t Accuracy 0.1143\n",
      "Epoch [28][200]\t Batch [400][550]\t Training Loss 2.3010\t Accuracy 0.1141\n",
      "Epoch [28][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [28][200]\t Batch [500][550]\t Training Loss 2.3010\t Accuracy 0.1136\n",
      "\n",
      "Epoch [28]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [28]\t Average validation loss 2.3017\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [29][200]\t Batch [0][550]\t Training Loss 2.2947\t Accuracy 0.1400\n",
      "Epoch [29][200]\t Batch [50][550]\t Training Loss 2.3008\t Accuracy 0.1125\n",
      "Epoch [29][200]\t Batch [100][550]\t Training Loss 2.3006\t Accuracy 0.1127\n",
      "Epoch [29][200]\t Batch [150][550]\t Training Loss 2.3008\t Accuracy 0.1128\n",
      "Epoch [29][200]\t Batch [200][550]\t Training Loss 2.3007\t Accuracy 0.1142\n",
      "Epoch [29][200]\t Batch [250][550]\t Training Loss 2.3008\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [300][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [350][550]\t Training Loss 2.3009\t Accuracy 0.1143\n",
      "Epoch [29][200]\t Batch [400][550]\t Training Loss 2.3009\t Accuracy 0.1141\n",
      "Epoch [29][200]\t Batch [450][550]\t Training Loss 2.3010\t Accuracy 0.1140\n",
      "Epoch [29][200]\t Batch [500][550]\t Training Loss 2.3010\t Accuracy 0.1136\n",
      "\n",
      "Epoch [29]\t Average training loss 2.3011\t Average training accuracy 0.1129\n",
      "Epoch [29]\t Average validation loss 2.3017\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [30][200]\t Batch [0][550]\t Training Loss 2.2946\t Accuracy 0.1400\n",
      "Epoch [30][200]\t Batch [50][550]\t Training Loss 2.3008\t Accuracy 0.1125\n",
      "Epoch [30][200]\t Batch [100][550]\t Training Loss 2.3006\t Accuracy 0.1127\n",
      "Epoch [30][200]\t Batch [150][550]\t Training Loss 2.3007\t Accuracy 0.1128\n",
      "Epoch [30][200]\t Batch [200][550]\t Training Loss 2.3007\t Accuracy 0.1142\n",
      "Epoch [30][200]\t Batch [250][550]\t Training Loss 2.3008\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [300][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [350][550]\t Training Loss 2.3009\t Accuracy 0.1143\n",
      "Epoch [30][200]\t Batch [400][550]\t Training Loss 2.3009\t Accuracy 0.1141\n",
      "Epoch [30][200]\t Batch [450][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [30][200]\t Batch [500][550]\t Training Loss 2.3010\t Accuracy 0.1136\n",
      "\n",
      "Epoch [30]\t Average training loss 2.3010\t Average training accuracy 0.1129\n",
      "Epoch [30]\t Average validation loss 2.3017\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [31][200]\t Batch [0][550]\t Training Loss 2.2945\t Accuracy 0.1400\n",
      "Epoch [31][200]\t Batch [50][550]\t Training Loss 2.3007\t Accuracy 0.1125\n",
      "Epoch [31][200]\t Batch [100][550]\t Training Loss 2.3005\t Accuracy 0.1127\n",
      "Epoch [31][200]\t Batch [150][550]\t Training Loss 2.3007\t Accuracy 0.1128\n",
      "Epoch [31][200]\t Batch [200][550]\t Training Loss 2.3006\t Accuracy 0.1142\n",
      "Epoch [31][200]\t Batch [250][550]\t Training Loss 2.3007\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [300][550]\t Training Loss 2.3008\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [350][550]\t Training Loss 2.3008\t Accuracy 0.1143\n",
      "Epoch [31][200]\t Batch [400][550]\t Training Loss 2.3008\t Accuracy 0.1141\n",
      "Epoch [31][200]\t Batch [450][550]\t Training Loss 2.3009\t Accuracy 0.1140\n",
      "Epoch [31][200]\t Batch [500][550]\t Training Loss 2.3009\t Accuracy 0.1136\n",
      "\n",
      "Epoch [31]\t Average training loss 2.3010\t Average training accuracy 0.1129\n",
      "Epoch [31]\t Average validation loss 2.3016\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [32][200]\t Batch [0][550]\t Training Loss 2.2943\t Accuracy 0.1400\n",
      "Epoch [32][200]\t Batch [50][550]\t Training Loss 2.3006\t Accuracy 0.1125\n",
      "Epoch [32][200]\t Batch [100][550]\t Training Loss 2.3004\t Accuracy 0.1127\n",
      "Epoch [32][200]\t Batch [150][550]\t Training Loss 2.3006\t Accuracy 0.1128\n",
      "Epoch [32][200]\t Batch [200][550]\t Training Loss 2.3005\t Accuracy 0.1142\n",
      "Epoch [32][200]\t Batch [250][550]\t Training Loss 2.3006\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [300][550]\t Training Loss 2.3007\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [350][550]\t Training Loss 2.3007\t Accuracy 0.1143\n",
      "Epoch [32][200]\t Batch [400][550]\t Training Loss 2.3007\t Accuracy 0.1141\n",
      "Epoch [32][200]\t Batch [450][550]\t Training Loss 2.3007\t Accuracy 0.1140\n",
      "Epoch [32][200]\t Batch [500][550]\t Training Loss 2.3008\t Accuracy 0.1136\n",
      "\n",
      "Epoch [32]\t Average training loss 2.3009\t Average training accuracy 0.1129\n",
      "Epoch [32]\t Average validation loss 2.3014\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [33][200]\t Batch [0][550]\t Training Loss 2.2941\t Accuracy 0.1400\n",
      "Epoch [33][200]\t Batch [50][550]\t Training Loss 2.3005\t Accuracy 0.1125\n",
      "Epoch [33][200]\t Batch [100][550]\t Training Loss 2.3003\t Accuracy 0.1127\n",
      "Epoch [33][200]\t Batch [150][550]\t Training Loss 2.3004\t Accuracy 0.1128\n",
      "Epoch [33][200]\t Batch [200][550]\t Training Loss 2.3004\t Accuracy 0.1142\n",
      "Epoch [33][200]\t Batch [250][550]\t Training Loss 2.3005\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [300][550]\t Training Loss 2.3006\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [350][550]\t Training Loss 2.3005\t Accuracy 0.1143\n",
      "Epoch [33][200]\t Batch [400][550]\t Training Loss 2.3006\t Accuracy 0.1141\n",
      "Epoch [33][200]\t Batch [450][550]\t Training Loss 2.3006\t Accuracy 0.1140\n",
      "Epoch [33][200]\t Batch [500][550]\t Training Loss 2.3006\t Accuracy 0.1136\n",
      "\n",
      "Epoch [33]\t Average training loss 2.3007\t Average training accuracy 0.1129\n",
      "Epoch [33]\t Average validation loss 2.3012\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [34][200]\t Batch [0][550]\t Training Loss 2.2938\t Accuracy 0.1400\n",
      "Epoch [34][200]\t Batch [50][550]\t Training Loss 2.3002\t Accuracy 0.1125\n",
      "Epoch [34][200]\t Batch [100][550]\t Training Loss 2.3000\t Accuracy 0.1127\n",
      "Epoch [34][200]\t Batch [150][550]\t Training Loss 2.3002\t Accuracy 0.1128\n",
      "Epoch [34][200]\t Batch [200][550]\t Training Loss 2.3001\t Accuracy 0.1142\n",
      "Epoch [34][200]\t Batch [250][550]\t Training Loss 2.3002\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [300][550]\t Training Loss 2.3003\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [350][550]\t Training Loss 2.3002\t Accuracy 0.1143\n",
      "Epoch [34][200]\t Batch [400][550]\t Training Loss 2.3003\t Accuracy 0.1141\n",
      "Epoch [34][200]\t Batch [450][550]\t Training Loss 2.3003\t Accuracy 0.1140\n",
      "Epoch [34][200]\t Batch [500][550]\t Training Loss 2.3003\t Accuracy 0.1136\n",
      "\n",
      "Epoch [34]\t Average training loss 2.3003\t Average training accuracy 0.1129\n",
      "Epoch [34]\t Average validation loss 2.3008\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [35][200]\t Batch [0][550]\t Training Loss 2.2932\t Accuracy 0.1400\n",
      "Epoch [35][200]\t Batch [50][550]\t Training Loss 2.2998\t Accuracy 0.1125\n",
      "Epoch [35][200]\t Batch [100][550]\t Training Loss 2.2995\t Accuracy 0.1127\n",
      "Epoch [35][200]\t Batch [150][550]\t Training Loss 2.2997\t Accuracy 0.1128\n",
      "Epoch [35][200]\t Batch [200][550]\t Training Loss 2.2996\t Accuracy 0.1142\n",
      "Epoch [35][200]\t Batch [250][550]\t Training Loss 2.2997\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [300][550]\t Training Loss 2.2998\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [350][550]\t Training Loss 2.2997\t Accuracy 0.1143\n",
      "Epoch [35][200]\t Batch [400][550]\t Training Loss 2.2997\t Accuracy 0.1141\n",
      "Epoch [35][200]\t Batch [450][550]\t Training Loss 2.2997\t Accuracy 0.1140\n",
      "Epoch [35][200]\t Batch [500][550]\t Training Loss 2.2997\t Accuracy 0.1136\n",
      "\n",
      "Epoch [35]\t Average training loss 2.2998\t Average training accuracy 0.1129\n",
      "Epoch [35]\t Average validation loss 2.3000\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [36][200]\t Batch [0][550]\t Training Loss 2.2922\t Accuracy 0.1400\n",
      "Epoch [36][200]\t Batch [50][550]\t Training Loss 2.2990\t Accuracy 0.1125\n",
      "Epoch [36][200]\t Batch [100][550]\t Training Loss 2.2987\t Accuracy 0.1127\n",
      "Epoch [36][200]\t Batch [150][550]\t Training Loss 2.2989\t Accuracy 0.1128\n",
      "Epoch [36][200]\t Batch [200][550]\t Training Loss 2.2988\t Accuracy 0.1142\n",
      "Epoch [36][200]\t Batch [250][550]\t Training Loss 2.2988\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [300][550]\t Training Loss 2.2989\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [350][550]\t Training Loss 2.2988\t Accuracy 0.1143\n",
      "Epoch [36][200]\t Batch [400][550]\t Training Loss 2.2988\t Accuracy 0.1141\n",
      "Epoch [36][200]\t Batch [450][550]\t Training Loss 2.2988\t Accuracy 0.1140\n",
      "Epoch [36][200]\t Batch [500][550]\t Training Loss 2.2988\t Accuracy 0.1136\n",
      "\n",
      "Epoch [36]\t Average training loss 2.2988\t Average training accuracy 0.1129\n",
      "Epoch [36]\t Average validation loss 2.2987\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [37][200]\t Batch [0][550]\t Training Loss 2.2905\t Accuracy 0.1400\n",
      "Epoch [37][200]\t Batch [50][550]\t Training Loss 2.2977\t Accuracy 0.1125\n",
      "Epoch [37][200]\t Batch [100][550]\t Training Loss 2.2973\t Accuracy 0.1127\n",
      "Epoch [37][200]\t Batch [150][550]\t Training Loss 2.2975\t Accuracy 0.1128\n",
      "Epoch [37][200]\t Batch [200][550]\t Training Loss 2.2974\t Accuracy 0.1142\n",
      "Epoch [37][200]\t Batch [250][550]\t Training Loss 2.2974\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [300][550]\t Training Loss 2.2974\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [350][550]\t Training Loss 2.2973\t Accuracy 0.1143\n",
      "Epoch [37][200]\t Batch [400][550]\t Training Loss 2.2972\t Accuracy 0.1141\n",
      "Epoch [37][200]\t Batch [450][550]\t Training Loss 2.2972\t Accuracy 0.1140\n",
      "Epoch [37][200]\t Batch [500][550]\t Training Loss 2.2971\t Accuracy 0.1136\n",
      "\n",
      "Epoch [37]\t Average training loss 2.2971\t Average training accuracy 0.1129\n",
      "Epoch [37]\t Average validation loss 2.2965\t Average validation accuracy 0.1060\n",
      "\n",
      "Epoch [38][200]\t Batch [0][550]\t Training Loss 2.2877\t Accuracy 0.1400\n",
      "Epoch [38][200]\t Batch [50][550]\t Training Loss 2.2955\t Accuracy 0.1125\n",
      "Epoch [38][200]\t Batch [100][550]\t Training Loss 2.2950\t Accuracy 0.1128\n",
      "Epoch [38][200]\t Batch [150][550]\t Training Loss 2.2950\t Accuracy 0.1132\n",
      "Epoch [38][200]\t Batch [200][550]\t Training Loss 2.2949\t Accuracy 0.1153\n",
      "Epoch [38][200]\t Batch [250][550]\t Training Loss 2.2949\t Accuracy 0.1160\n",
      "Epoch [38][200]\t Batch [300][550]\t Training Loss 2.2948\t Accuracy 0.1177\n",
      "Epoch [38][200]\t Batch [350][550]\t Training Loss 2.2947\t Accuracy 0.1189\n",
      "Epoch [38][200]\t Batch [400][550]\t Training Loss 2.2946\t Accuracy 0.1209\n",
      "Epoch [38][200]\t Batch [450][550]\t Training Loss 2.2944\t Accuracy 0.1222\n",
      "Epoch [38][200]\t Batch [500][550]\t Training Loss 2.2943\t Accuracy 0.1239\n",
      "\n",
      "Epoch [38]\t Average training loss 2.2942\t Average training accuracy 0.1261\n",
      "Epoch [38]\t Average validation loss 2.2927\t Average validation accuracy 0.1528\n",
      "\n",
      "Epoch [39][200]\t Batch [0][550]\t Training Loss 2.2828\t Accuracy 0.2300\n",
      "Epoch [39][200]\t Batch [50][550]\t Training Loss 2.2917\t Accuracy 0.1639\n",
      "Epoch [39][200]\t Batch [100][550]\t Training Loss 2.2910\t Accuracy 0.1676\n",
      "Epoch [39][200]\t Batch [150][550]\t Training Loss 2.2910\t Accuracy 0.1695\n",
      "Epoch [39][200]\t Batch [200][550]\t Training Loss 2.2908\t Accuracy 0.1721\n",
      "Epoch [39][200]\t Batch [250][550]\t Training Loss 2.2906\t Accuracy 0.1748\n",
      "Epoch [39][200]\t Batch [300][550]\t Training Loss 2.2905\t Accuracy 0.1767\n",
      "Epoch [39][200]\t Batch [350][550]\t Training Loss 2.2903\t Accuracy 0.1787\n",
      "Epoch [39][200]\t Batch [400][550]\t Training Loss 2.2900\t Accuracy 0.1804\n",
      "Epoch [39][200]\t Batch [450][550]\t Training Loss 2.2898\t Accuracy 0.1820\n",
      "Epoch [39][200]\t Batch [500][550]\t Training Loss 2.2895\t Accuracy 0.1832\n",
      "\n",
      "Epoch [39]\t Average training loss 2.2893\t Average training accuracy 0.1843\n",
      "Epoch [39]\t Average validation loss 2.2864\t Average validation accuracy 0.1950\n",
      "\n",
      "Epoch [40][200]\t Batch [0][550]\t Training Loss 2.2748\t Accuracy 0.2600\n",
      "Epoch [40][200]\t Batch [50][550]\t Training Loss 2.2854\t Accuracy 0.2002\n",
      "Epoch [40][200]\t Batch [100][550]\t Training Loss 2.2843\t Accuracy 0.2048\n",
      "Epoch [40][200]\t Batch [150][550]\t Training Loss 2.2843\t Accuracy 0.2050\n",
      "Epoch [40][200]\t Batch [200][550]\t Training Loss 2.2840\t Accuracy 0.2062\n",
      "Epoch [40][200]\t Batch [250][550]\t Training Loss 2.2837\t Accuracy 0.2062\n",
      "Epoch [40][200]\t Batch [300][550]\t Training Loss 2.2833\t Accuracy 0.2063\n",
      "Epoch [40][200]\t Batch [350][550]\t Training Loss 2.2831\t Accuracy 0.2064\n",
      "Epoch [40][200]\t Batch [400][550]\t Training Loss 2.2826\t Accuracy 0.2065\n",
      "Epoch [40][200]\t Batch [450][550]\t Training Loss 2.2823\t Accuracy 0.2068\n",
      "Epoch [40][200]\t Batch [500][550]\t Training Loss 2.2818\t Accuracy 0.2070\n",
      "\n",
      "Epoch [40]\t Average training loss 2.2814\t Average training accuracy 0.2068\n",
      "Epoch [40]\t Average validation loss 2.2763\t Average validation accuracy 0.2016\n",
      "\n",
      "Epoch [41][200]\t Batch [0][550]\t Training Loss 2.2619\t Accuracy 0.2700\n",
      "Epoch [41][200]\t Batch [50][550]\t Training Loss 2.2753\t Accuracy 0.2067\n",
      "Epoch [41][200]\t Batch [100][550]\t Training Loss 2.2737\t Accuracy 0.2108\n",
      "Epoch [41][200]\t Batch [150][550]\t Training Loss 2.2735\t Accuracy 0.2105\n",
      "Epoch [41][200]\t Batch [200][550]\t Training Loss 2.2732\t Accuracy 0.2117\n",
      "Epoch [41][200]\t Batch [250][550]\t Training Loss 2.2726\t Accuracy 0.2109\n",
      "Epoch [41][200]\t Batch [300][550]\t Training Loss 2.2720\t Accuracy 0.2108\n",
      "Epoch [41][200]\t Batch [350][550]\t Training Loss 2.2716\t Accuracy 0.2103\n",
      "Epoch [41][200]\t Batch [400][550]\t Training Loss 2.2710\t Accuracy 0.2102\n",
      "Epoch [41][200]\t Batch [450][550]\t Training Loss 2.2704\t Accuracy 0.2103\n",
      "Epoch [41][200]\t Batch [500][550]\t Training Loss 2.2697\t Accuracy 0.2102\n",
      "\n",
      "Epoch [41]\t Average training loss 2.2690\t Average training accuracy 0.2098\n",
      "Epoch [41]\t Average validation loss 2.2609\t Average validation accuracy 0.2026\n",
      "\n",
      "Epoch [42][200]\t Batch [0][550]\t Training Loss 2.2424\t Accuracy 0.2500\n",
      "Epoch [42][200]\t Batch [50][550]\t Training Loss 2.2601\t Accuracy 0.2067\n",
      "Epoch [42][200]\t Batch [100][550]\t Training Loss 2.2577\t Accuracy 0.2105\n",
      "Epoch [42][200]\t Batch [150][550]\t Training Loss 2.2574\t Accuracy 0.2103\n",
      "Epoch [42][200]\t Batch [200][550]\t Training Loss 2.2569\t Accuracy 0.2114\n",
      "Epoch [42][200]\t Batch [250][550]\t Training Loss 2.2561\t Accuracy 0.2104\n",
      "Epoch [42][200]\t Batch [300][550]\t Training Loss 2.2552\t Accuracy 0.2103\n",
      "Epoch [42][200]\t Batch [350][550]\t Training Loss 2.2547\t Accuracy 0.2097\n",
      "Epoch [42][200]\t Batch [400][550]\t Training Loss 2.2537\t Accuracy 0.2096\n",
      "Epoch [42][200]\t Batch [450][550]\t Training Loss 2.2529\t Accuracy 0.2097\n",
      "Epoch [42][200]\t Batch [500][550]\t Training Loss 2.2519\t Accuracy 0.2096\n",
      "\n",
      "Epoch [42]\t Average training loss 2.2509\t Average training accuracy 0.2091\n",
      "Epoch [42]\t Average validation loss 2.2395\t Average validation accuracy 0.2014\n",
      "\n",
      "Epoch [43][200]\t Batch [0][550]\t Training Loss 2.2152\t Accuracy 0.2500\n",
      "Epoch [43][200]\t Batch [50][550]\t Training Loss 2.2389\t Accuracy 0.2065\n",
      "Epoch [43][200]\t Batch [100][550]\t Training Loss 2.2355\t Accuracy 0.2098\n",
      "Epoch [43][200]\t Batch [150][550]\t Training Loss 2.2353\t Accuracy 0.2092\n",
      "Epoch [43][200]\t Batch [200][550]\t Training Loss 2.2348\t Accuracy 0.2104\n",
      "Epoch [43][200]\t Batch [250][550]\t Training Loss 2.2337\t Accuracy 0.2096\n",
      "Epoch [43][200]\t Batch [300][550]\t Training Loss 2.2325\t Accuracy 0.2096\n",
      "Epoch [43][200]\t Batch [350][550]\t Training Loss 2.2320\t Accuracy 0.2090\n",
      "Epoch [43][200]\t Batch [400][550]\t Training Loss 2.2307\t Accuracy 0.2088\n",
      "Epoch [43][200]\t Batch [450][550]\t Training Loss 2.2298\t Accuracy 0.2089\n",
      "Epoch [43][200]\t Batch [500][550]\t Training Loss 2.2285\t Accuracy 0.2089\n",
      "\n",
      "Epoch [43]\t Average training loss 2.2272\t Average training accuracy 0.2083\n",
      "Epoch [43]\t Average validation loss 2.2132\t Average validation accuracy 0.2008\n",
      "\n",
      "Epoch [44][200]\t Batch [0][550]\t Training Loss 2.1819\t Accuracy 0.2500\n",
      "Epoch [44][200]\t Batch [50][550]\t Training Loss 2.2131\t Accuracy 0.2057\n",
      "Epoch [44][200]\t Batch [100][550]\t Training Loss 2.2088\t Accuracy 0.2089\n",
      "Epoch [44][200]\t Batch [150][550]\t Training Loss 2.2087\t Accuracy 0.2079\n",
      "Epoch [44][200]\t Batch [200][550]\t Training Loss 2.2082\t Accuracy 0.2094\n",
      "Epoch [44][200]\t Batch [250][550]\t Training Loss 2.2071\t Accuracy 0.2086\n",
      "Epoch [44][200]\t Batch [300][550]\t Training Loss 2.2057\t Accuracy 0.2086\n",
      "Epoch [44][200]\t Batch [350][550]\t Training Loss 2.2054\t Accuracy 0.2080\n",
      "Epoch [44][200]\t Batch [400][550]\t Training Loss 2.2040\t Accuracy 0.2078\n",
      "Epoch [44][200]\t Batch [450][550]\t Training Loss 2.2031\t Accuracy 0.2080\n",
      "Epoch [44][200]\t Batch [500][550]\t Training Loss 2.2016\t Accuracy 0.2081\n",
      "\n",
      "Epoch [44]\t Average training loss 2.2003\t Average training accuracy 0.2075\n",
      "Epoch [44]\t Average validation loss 2.1855\t Average validation accuracy 0.2008\n",
      "\n",
      "Epoch [45][200]\t Batch [0][550]\t Training Loss 2.1473\t Accuracy 0.2500\n",
      "Epoch [45][200]\t Batch [50][550]\t Training Loss 2.1861\t Accuracy 0.2053\n",
      "Epoch [45][200]\t Batch [100][550]\t Training Loss 2.1812\t Accuracy 0.2084\n",
      "Epoch [45][200]\t Batch [150][550]\t Training Loss 2.1814\t Accuracy 0.2074\n",
      "Epoch [45][200]\t Batch [200][550]\t Training Loss 2.1811\t Accuracy 0.2090\n",
      "Epoch [45][200]\t Batch [250][550]\t Training Loss 2.1800\t Accuracy 0.2083\n",
      "Epoch [45][200]\t Batch [300][550]\t Training Loss 2.1787\t Accuracy 0.2083\n",
      "Epoch [45][200]\t Batch [350][550]\t Training Loss 2.1787\t Accuracy 0.2076\n",
      "Epoch [45][200]\t Batch [400][550]\t Training Loss 2.1774\t Accuracy 0.2076\n",
      "Epoch [45][200]\t Batch [450][550]\t Training Loss 2.1765\t Accuracy 0.2078\n",
      "Epoch [45][200]\t Batch [500][550]\t Training Loss 2.1752\t Accuracy 0.2079\n",
      "\n",
      "Epoch [45]\t Average training loss 2.1739\t Average training accuracy 0.2074\n",
      "Epoch [45]\t Average validation loss 2.1601\t Average validation accuracy 0.2008\n",
      "\n",
      "Epoch [46][200]\t Batch [0][550]\t Training Loss 2.1169\t Accuracy 0.2500\n",
      "Epoch [46][200]\t Batch [50][550]\t Training Loss 2.1615\t Accuracy 0.2053\n",
      "Epoch [46][200]\t Batch [100][550]\t Training Loss 2.1562\t Accuracy 0.2086\n",
      "Epoch [46][200]\t Batch [150][550]\t Training Loss 2.1567\t Accuracy 0.2075\n",
      "Epoch [46][200]\t Batch [200][550]\t Training Loss 2.1566\t Accuracy 0.2091\n",
      "Epoch [46][200]\t Batch [250][550]\t Training Loss 2.1558\t Accuracy 0.2084\n",
      "Epoch [46][200]\t Batch [300][550]\t Training Loss 2.1546\t Accuracy 0.2084\n",
      "Epoch [46][200]\t Batch [350][550]\t Training Loss 2.1549\t Accuracy 0.2078\n",
      "Epoch [46][200]\t Batch [400][550]\t Training Loss 2.1537\t Accuracy 0.2078\n",
      "Epoch [46][200]\t Batch [450][550]\t Training Loss 2.1530\t Accuracy 0.2080\n",
      "Epoch [46][200]\t Batch [500][550]\t Training Loss 2.1517\t Accuracy 0.2081\n",
      "\n",
      "Epoch [46]\t Average training loss 2.1506\t Average training accuracy 0.2075\n",
      "Epoch [46]\t Average validation loss 2.1383\t Average validation accuracy 0.2008\n",
      "\n",
      "Epoch [47][200]\t Batch [0][550]\t Training Loss 2.0924\t Accuracy 0.2500\n",
      "Epoch [47][200]\t Batch [50][550]\t Training Loss 2.1405\t Accuracy 0.2055\n",
      "Epoch [47][200]\t Batch [100][550]\t Training Loss 2.1349\t Accuracy 0.2091\n",
      "Epoch [47][200]\t Batch [150][550]\t Training Loss 2.1356\t Accuracy 0.2081\n",
      "Epoch [47][200]\t Batch [200][550]\t Training Loss 2.1357\t Accuracy 0.2096\n",
      "Epoch [47][200]\t Batch [250][550]\t Training Loss 2.1350\t Accuracy 0.2088\n",
      "Epoch [47][200]\t Batch [300][550]\t Training Loss 2.1341\t Accuracy 0.2088\n",
      "Epoch [47][200]\t Batch [350][550]\t Training Loss 2.1345\t Accuracy 0.2083\n",
      "Epoch [47][200]\t Batch [400][550]\t Training Loss 2.1334\t Accuracy 0.2082\n",
      "Epoch [47][200]\t Batch [450][550]\t Training Loss 2.1328\t Accuracy 0.2085\n",
      "Epoch [47][200]\t Batch [500][550]\t Training Loss 2.1317\t Accuracy 0.2085\n",
      "\n",
      "Epoch [47]\t Average training loss 2.1306\t Average training accuracy 0.2080\n",
      "Epoch [47]\t Average validation loss 2.1195\t Average validation accuracy 0.2018\n",
      "\n",
      "Epoch [48][200]\t Batch [0][550]\t Training Loss 2.0732\t Accuracy 0.2500\n",
      "Epoch [48][200]\t Batch [50][550]\t Training Loss 2.1224\t Accuracy 0.2067\n",
      "Epoch [48][200]\t Batch [100][550]\t Training Loss 2.1166\t Accuracy 0.2111\n",
      "Epoch [48][200]\t Batch [150][550]\t Training Loss 2.1175\t Accuracy 0.2099\n",
      "Epoch [48][200]\t Batch [200][550]\t Training Loss 2.1177\t Accuracy 0.2113\n",
      "Epoch [48][200]\t Batch [250][550]\t Training Loss 2.1171\t Accuracy 0.2104\n",
      "Epoch [48][200]\t Batch [300][550]\t Training Loss 2.1162\t Accuracy 0.2103\n",
      "Epoch [48][200]\t Batch [350][550]\t Training Loss 2.1168\t Accuracy 0.2098\n",
      "Epoch [48][200]\t Batch [400][550]\t Training Loss 2.1158\t Accuracy 0.2096\n",
      "Epoch [48][200]\t Batch [450][550]\t Training Loss 2.1153\t Accuracy 0.2098\n",
      "Epoch [48][200]\t Batch [500][550]\t Training Loss 2.1142\t Accuracy 0.2100\n",
      "\n",
      "Epoch [48]\t Average training loss 2.1132\t Average training accuracy 0.2094\n",
      "Epoch [48]\t Average validation loss 2.1026\t Average validation accuracy 0.2042\n",
      "\n",
      "Epoch [49][200]\t Batch [0][550]\t Training Loss 2.0578\t Accuracy 0.2500\n",
      "Epoch [49][200]\t Batch [50][550]\t Training Loss 2.1062\t Accuracy 0.2082\n",
      "Epoch [49][200]\t Batch [100][550]\t Training Loss 2.1004\t Accuracy 0.2125\n",
      "Epoch [49][200]\t Batch [150][550]\t Training Loss 2.1014\t Accuracy 0.2113\n",
      "Epoch [49][200]\t Batch [200][550]\t Training Loss 2.1015\t Accuracy 0.2124\n",
      "Epoch [49][200]\t Batch [250][550]\t Training Loss 2.1011\t Accuracy 0.2114\n",
      "Epoch [49][200]\t Batch [300][550]\t Training Loss 2.1003\t Accuracy 0.2114\n",
      "Epoch [49][200]\t Batch [350][550]\t Training Loss 2.1010\t Accuracy 0.2110\n",
      "Epoch [49][200]\t Batch [400][550]\t Training Loss 2.1000\t Accuracy 0.2109\n",
      "Epoch [49][200]\t Batch [450][550]\t Training Loss 2.0995\t Accuracy 0.2112\n",
      "Epoch [49][200]\t Batch [500][550]\t Training Loss 2.0985\t Accuracy 0.2115\n",
      "\n",
      "Epoch [49]\t Average training loss 2.0975\t Average training accuracy 0.2111\n",
      "Epoch [49]\t Average validation loss 2.0871\t Average validation accuracy 0.2064\n",
      "\n",
      "Epoch [50][200]\t Batch [0][550]\t Training Loss 2.0449\t Accuracy 0.2500\n",
      "Epoch [50][200]\t Batch [50][550]\t Training Loss 2.0914\t Accuracy 0.2100\n",
      "Epoch [50][200]\t Batch [100][550]\t Training Loss 2.0856\t Accuracy 0.2138\n",
      "Epoch [50][200]\t Batch [150][550]\t Training Loss 2.0866\t Accuracy 0.2130\n",
      "Epoch [50][200]\t Batch [200][550]\t Training Loss 2.0868\t Accuracy 0.2138\n",
      "Epoch [50][200]\t Batch [250][550]\t Training Loss 2.0863\t Accuracy 0.2130\n",
      "Epoch [50][200]\t Batch [300][550]\t Training Loss 2.0855\t Accuracy 0.2130\n",
      "Epoch [50][200]\t Batch [350][550]\t Training Loss 2.0863\t Accuracy 0.2126\n",
      "Epoch [50][200]\t Batch [400][550]\t Training Loss 2.0854\t Accuracy 0.2125\n",
      "Epoch [50][200]\t Batch [450][550]\t Training Loss 2.0849\t Accuracy 0.2127\n",
      "Epoch [50][200]\t Batch [500][550]\t Training Loss 2.0839\t Accuracy 0.2129\n",
      "\n",
      "Epoch [50]\t Average training loss 2.0829\t Average training accuracy 0.2125\n",
      "Epoch [50]\t Average validation loss 2.0726\t Average validation accuracy 0.2078\n",
      "\n",
      "Epoch [51][200]\t Batch [0][550]\t Training Loss 2.0337\t Accuracy 0.2500\n",
      "Epoch [51][200]\t Batch [50][550]\t Training Loss 2.0775\t Accuracy 0.2110\n",
      "Epoch [51][200]\t Batch [100][550]\t Training Loss 2.0718\t Accuracy 0.2153\n",
      "Epoch [51][200]\t Batch [150][550]\t Training Loss 2.0729\t Accuracy 0.2148\n",
      "Epoch [51][200]\t Batch [200][550]\t Training Loss 2.0730\t Accuracy 0.2158\n",
      "Epoch [51][200]\t Batch [250][550]\t Training Loss 2.0725\t Accuracy 0.2149\n",
      "Epoch [51][200]\t Batch [300][550]\t Training Loss 2.0718\t Accuracy 0.2150\n",
      "Epoch [51][200]\t Batch [350][550]\t Training Loss 2.0726\t Accuracy 0.2145\n",
      "Epoch [51][200]\t Batch [400][550]\t Training Loss 2.0717\t Accuracy 0.2144\n",
      "Epoch [51][200]\t Batch [450][550]\t Training Loss 2.0713\t Accuracy 0.2145\n",
      "Epoch [51][200]\t Batch [500][550]\t Training Loss 2.0702\t Accuracy 0.2147\n",
      "\n",
      "Epoch [51]\t Average training loss 2.0693\t Average training accuracy 0.2143\n",
      "Epoch [51]\t Average validation loss 2.0588\t Average validation accuracy 0.2104\n",
      "\n",
      "Epoch [52][200]\t Batch [0][550]\t Training Loss 2.0237\t Accuracy 0.2500\n",
      "Epoch [52][200]\t Batch [50][550]\t Training Loss 2.0644\t Accuracy 0.2124\n",
      "Epoch [52][200]\t Batch [100][550]\t Training Loss 2.0587\t Accuracy 0.2166\n",
      "Epoch [52][200]\t Batch [150][550]\t Training Loss 2.0599\t Accuracy 0.2164\n",
      "Epoch [52][200]\t Batch [200][550]\t Training Loss 2.0600\t Accuracy 0.2173\n",
      "Epoch [52][200]\t Batch [250][550]\t Training Loss 2.0596\t Accuracy 0.2165\n",
      "Epoch [52][200]\t Batch [300][550]\t Training Loss 2.0588\t Accuracy 0.2167\n",
      "Epoch [52][200]\t Batch [350][550]\t Training Loss 2.0597\t Accuracy 0.2161\n",
      "Epoch [52][200]\t Batch [400][550]\t Training Loss 2.0588\t Accuracy 0.2162\n",
      "Epoch [52][200]\t Batch [450][550]\t Training Loss 2.0584\t Accuracy 0.2164\n",
      "Epoch [52][200]\t Batch [500][550]\t Training Loss 2.0574\t Accuracy 0.2165\n",
      "\n",
      "Epoch [52]\t Average training loss 2.0564\t Average training accuracy 0.2162\n",
      "Epoch [52]\t Average validation loss 2.0457\t Average validation accuracy 0.2124\n",
      "\n",
      "Epoch [53][200]\t Batch [0][550]\t Training Loss 2.0146\t Accuracy 0.2500\n",
      "Epoch [53][200]\t Batch [50][550]\t Training Loss 2.0520\t Accuracy 0.2129\n",
      "Epoch [53][200]\t Batch [100][550]\t Training Loss 2.0464\t Accuracy 0.2175\n",
      "Epoch [53][200]\t Batch [150][550]\t Training Loss 2.0476\t Accuracy 0.2177\n",
      "Epoch [53][200]\t Batch [200][550]\t Training Loss 2.0477\t Accuracy 0.2189\n",
      "Epoch [53][200]\t Batch [250][550]\t Training Loss 2.0473\t Accuracy 0.2181\n",
      "Epoch [53][200]\t Batch [300][550]\t Training Loss 2.0465\t Accuracy 0.2182\n",
      "Epoch [53][200]\t Batch [350][550]\t Training Loss 2.0475\t Accuracy 0.2175\n",
      "Epoch [53][200]\t Batch [400][550]\t Training Loss 2.0466\t Accuracy 0.2176\n",
      "Epoch [53][200]\t Batch [450][550]\t Training Loss 2.0462\t Accuracy 0.2178\n",
      "Epoch [53][200]\t Batch [500][550]\t Training Loss 2.0452\t Accuracy 0.2181\n",
      "\n",
      "Epoch [53]\t Average training loss 2.0442\t Average training accuracy 0.2177\n",
      "Epoch [53]\t Average validation loss 2.0332\t Average validation accuracy 0.2128\n",
      "\n",
      "Epoch [54][200]\t Batch [0][550]\t Training Loss 2.0062\t Accuracy 0.2500\n",
      "Epoch [54][200]\t Batch [50][550]\t Training Loss 2.0402\t Accuracy 0.2147\n",
      "Epoch [54][200]\t Batch [100][550]\t Training Loss 2.0347\t Accuracy 0.2196\n",
      "Epoch [54][200]\t Batch [150][550]\t Training Loss 2.0360\t Accuracy 0.2199\n",
      "Epoch [54][200]\t Batch [200][550]\t Training Loss 2.0361\t Accuracy 0.2209\n",
      "Epoch [54][200]\t Batch [250][550]\t Training Loss 2.0356\t Accuracy 0.2202\n",
      "Epoch [54][200]\t Batch [300][550]\t Training Loss 2.0349\t Accuracy 0.2202\n",
      "Epoch [54][200]\t Batch [350][550]\t Training Loss 2.0359\t Accuracy 0.2195\n",
      "Epoch [54][200]\t Batch [400][550]\t Training Loss 2.0350\t Accuracy 0.2196\n",
      "Epoch [54][200]\t Batch [450][550]\t Training Loss 2.0346\t Accuracy 0.2198\n",
      "Epoch [54][200]\t Batch [500][550]\t Training Loss 2.0336\t Accuracy 0.2201\n",
      "\n",
      "Epoch [54]\t Average training loss 2.0327\t Average training accuracy 0.2197\n",
      "Epoch [54]\t Average validation loss 2.0214\t Average validation accuracy 0.2140\n",
      "\n",
      "Epoch [55][200]\t Batch [0][550]\t Training Loss 1.9985\t Accuracy 0.2500\n",
      "Epoch [55][200]\t Batch [50][550]\t Training Loss 2.0290\t Accuracy 0.2159\n",
      "Epoch [55][200]\t Batch [100][550]\t Training Loss 2.0236\t Accuracy 0.2208\n",
      "Epoch [55][200]\t Batch [150][550]\t Training Loss 2.0249\t Accuracy 0.2213\n",
      "Epoch [55][200]\t Batch [200][550]\t Training Loss 2.0250\t Accuracy 0.2224\n",
      "Epoch [55][200]\t Batch [250][550]\t Training Loss 2.0246\t Accuracy 0.2218\n",
      "Epoch [55][200]\t Batch [300][550]\t Training Loss 2.0238\t Accuracy 0.2221\n",
      "Epoch [55][200]\t Batch [350][550]\t Training Loss 2.0250\t Accuracy 0.2214\n",
      "Epoch [55][200]\t Batch [400][550]\t Training Loss 2.0240\t Accuracy 0.2212\n",
      "Epoch [55][200]\t Batch [450][550]\t Training Loss 2.0236\t Accuracy 0.2216\n",
      "Epoch [55][200]\t Batch [500][550]\t Training Loss 2.0226\t Accuracy 0.2218\n",
      "\n",
      "Epoch [55]\t Average training loss 2.0217\t Average training accuracy 0.2213\n",
      "Epoch [55]\t Average validation loss 2.0101\t Average validation accuracy 0.2152\n",
      "\n",
      "Epoch [56][200]\t Batch [0][550]\t Training Loss 1.9911\t Accuracy 0.2500\n",
      "Epoch [56][200]\t Batch [50][550]\t Training Loss 2.0183\t Accuracy 0.2175\n",
      "Epoch [56][200]\t Batch [100][550]\t Training Loss 2.0130\t Accuracy 0.2231\n",
      "Epoch [56][200]\t Batch [150][550]\t Training Loss 2.0144\t Accuracy 0.2234\n",
      "Epoch [56][200]\t Batch [200][550]\t Training Loss 2.0145\t Accuracy 0.2242\n",
      "Epoch [56][200]\t Batch [250][550]\t Training Loss 2.0141\t Accuracy 0.2236\n",
      "Epoch [56][200]\t Batch [300][550]\t Training Loss 2.0133\t Accuracy 0.2239\n",
      "Epoch [56][200]\t Batch [350][550]\t Training Loss 2.0145\t Accuracy 0.2231\n",
      "Epoch [56][200]\t Batch [400][550]\t Training Loss 2.0136\t Accuracy 0.2229\n",
      "Epoch [56][200]\t Batch [450][550]\t Training Loss 2.0132\t Accuracy 0.2232\n",
      "Epoch [56][200]\t Batch [500][550]\t Training Loss 2.0122\t Accuracy 0.2234\n",
      "\n",
      "Epoch [56]\t Average training loss 2.0113\t Average training accuracy 0.2229\n",
      "Epoch [56]\t Average validation loss 1.9993\t Average validation accuracy 0.2166\n",
      "\n",
      "Epoch [57][200]\t Batch [0][550]\t Training Loss 1.9841\t Accuracy 0.2500\n",
      "Epoch [57][200]\t Batch [50][550]\t Training Loss 2.0081\t Accuracy 0.2198\n",
      "Epoch [57][200]\t Batch [100][550]\t Training Loss 2.0029\t Accuracy 0.2253\n",
      "Epoch [57][200]\t Batch [150][550]\t Training Loss 2.0044\t Accuracy 0.2256\n",
      "Epoch [57][200]\t Batch [200][550]\t Training Loss 2.0045\t Accuracy 0.2262\n",
      "Epoch [57][200]\t Batch [250][550]\t Training Loss 2.0041\t Accuracy 0.2254\n",
      "Epoch [57][200]\t Batch [300][550]\t Training Loss 2.0034\t Accuracy 0.2255\n",
      "Epoch [57][200]\t Batch [350][550]\t Training Loss 2.0046\t Accuracy 0.2247\n",
      "Epoch [57][200]\t Batch [400][550]\t Training Loss 2.0037\t Accuracy 0.2244\n",
      "Epoch [57][200]\t Batch [450][550]\t Training Loss 2.0032\t Accuracy 0.2247\n",
      "Epoch [57][200]\t Batch [500][550]\t Training Loss 2.0023\t Accuracy 0.2248\n",
      "\n",
      "Epoch [57]\t Average training loss 2.0014\t Average training accuracy 0.2244\n",
      "Epoch [57]\t Average validation loss 1.9891\t Average validation accuracy 0.2190\n",
      "\n",
      "Epoch [58][200]\t Batch [0][550]\t Training Loss 1.9774\t Accuracy 0.2500\n",
      "Epoch [58][200]\t Batch [50][550]\t Training Loss 1.9984\t Accuracy 0.2212\n",
      "Epoch [58][200]\t Batch [100][550]\t Training Loss 1.9932\t Accuracy 0.2265\n",
      "Epoch [58][200]\t Batch [150][550]\t Training Loss 1.9948\t Accuracy 0.2267\n",
      "Epoch [58][200]\t Batch [200][550]\t Training Loss 1.9950\t Accuracy 0.2271\n",
      "Epoch [58][200]\t Batch [250][550]\t Training Loss 1.9946\t Accuracy 0.2262\n",
      "Epoch [58][200]\t Batch [300][550]\t Training Loss 1.9939\t Accuracy 0.2264\n",
      "Epoch [58][200]\t Batch [350][550]\t Training Loss 1.9951\t Accuracy 0.2257\n",
      "Epoch [58][200]\t Batch [400][550]\t Training Loss 1.9942\t Accuracy 0.2254\n",
      "Epoch [58][200]\t Batch [450][550]\t Training Loss 1.9937\t Accuracy 0.2257\n",
      "Epoch [58][200]\t Batch [500][550]\t Training Loss 1.9928\t Accuracy 0.2259\n",
      "\n",
      "Epoch [58]\t Average training loss 1.9919\t Average training accuracy 0.2255\n",
      "Epoch [58]\t Average validation loss 1.9793\t Average validation accuracy 0.2206\n",
      "\n",
      "Epoch [59][200]\t Batch [0][550]\t Training Loss 1.9710\t Accuracy 0.2500\n",
      "Epoch [59][200]\t Batch [50][550]\t Training Loss 1.9891\t Accuracy 0.2216\n",
      "Epoch [59][200]\t Batch [100][550]\t Training Loss 1.9840\t Accuracy 0.2269\n",
      "Epoch [59][200]\t Batch [150][550]\t Training Loss 1.9857\t Accuracy 0.2277\n",
      "Epoch [59][200]\t Batch [200][550]\t Training Loss 1.9859\t Accuracy 0.2283\n",
      "Epoch [59][200]\t Batch [250][550]\t Training Loss 1.9855\t Accuracy 0.2273\n",
      "Epoch [59][200]\t Batch [300][550]\t Training Loss 1.9848\t Accuracy 0.2276\n",
      "Epoch [59][200]\t Batch [350][550]\t Training Loss 1.9861\t Accuracy 0.2270\n",
      "Epoch [59][200]\t Batch [400][550]\t Training Loss 1.9851\t Accuracy 0.2267\n",
      "Epoch [59][200]\t Batch [450][550]\t Training Loss 1.9847\t Accuracy 0.2271\n",
      "Epoch [59][200]\t Batch [500][550]\t Training Loss 1.9838\t Accuracy 0.2271\n",
      "\n",
      "Epoch [59]\t Average training loss 1.9829\t Average training accuracy 0.2268\n",
      "Epoch [59]\t Average validation loss 1.9699\t Average validation accuracy 0.2230\n",
      "\n",
      "Epoch [60][200]\t Batch [0][550]\t Training Loss 1.9649\t Accuracy 0.2500\n",
      "Epoch [60][200]\t Batch [50][550]\t Training Loss 1.9802\t Accuracy 0.2220\n",
      "Epoch [60][200]\t Batch [100][550]\t Training Loss 1.9751\t Accuracy 0.2275\n",
      "Epoch [60][200]\t Batch [150][550]\t Training Loss 1.9770\t Accuracy 0.2287\n",
      "Epoch [60][200]\t Batch [200][550]\t Training Loss 1.9772\t Accuracy 0.2296\n",
      "Epoch [60][200]\t Batch [250][550]\t Training Loss 1.9768\t Accuracy 0.2289\n",
      "Epoch [60][200]\t Batch [300][550]\t Training Loss 1.9761\t Accuracy 0.2290\n",
      "Epoch [60][200]\t Batch [350][550]\t Training Loss 1.9774\t Accuracy 0.2285\n",
      "Epoch [60][200]\t Batch [400][550]\t Training Loss 1.9765\t Accuracy 0.2283\n",
      "Epoch [60][200]\t Batch [450][550]\t Training Loss 1.9760\t Accuracy 0.2285\n",
      "Epoch [60][200]\t Batch [500][550]\t Training Loss 1.9751\t Accuracy 0.2286\n",
      "\n",
      "Epoch [60]\t Average training loss 1.9742\t Average training accuracy 0.2282\n",
      "Epoch [60]\t Average validation loss 1.9610\t Average validation accuracy 0.2258\n",
      "\n",
      "Epoch [61][200]\t Batch [0][550]\t Training Loss 1.9590\t Accuracy 0.2500\n",
      "Epoch [61][200]\t Batch [50][550]\t Training Loss 1.9716\t Accuracy 0.2235\n",
      "Epoch [61][200]\t Batch [100][550]\t Training Loss 1.9666\t Accuracy 0.2295\n",
      "Epoch [61][200]\t Batch [150][550]\t Training Loss 1.9686\t Accuracy 0.2307\n",
      "Epoch [61][200]\t Batch [200][550]\t Training Loss 1.9688\t Accuracy 0.2314\n",
      "Epoch [61][200]\t Batch [250][550]\t Training Loss 1.9685\t Accuracy 0.2306\n",
      "Epoch [61][200]\t Batch [300][550]\t Training Loss 1.9678\t Accuracy 0.2307\n",
      "Epoch [61][200]\t Batch [350][550]\t Training Loss 1.9691\t Accuracy 0.2301\n",
      "Epoch [61][200]\t Batch [400][550]\t Training Loss 1.9682\t Accuracy 0.2298\n",
      "Epoch [61][200]\t Batch [450][550]\t Training Loss 1.9678\t Accuracy 0.2300\n",
      "Epoch [61][200]\t Batch [500][550]\t Training Loss 1.9669\t Accuracy 0.2300\n",
      "\n",
      "Epoch [61]\t Average training loss 1.9660\t Average training accuracy 0.2296\n",
      "Epoch [61]\t Average validation loss 1.9524\t Average validation accuracy 0.2270\n",
      "\n",
      "Epoch [62][200]\t Batch [0][550]\t Training Loss 1.9533\t Accuracy 0.2500\n",
      "Epoch [62][200]\t Batch [50][550]\t Training Loss 1.9634\t Accuracy 0.2255\n",
      "Epoch [62][200]\t Batch [100][550]\t Training Loss 1.9585\t Accuracy 0.2314\n",
      "Epoch [62][200]\t Batch [150][550]\t Training Loss 1.9606\t Accuracy 0.2325\n",
      "Epoch [62][200]\t Batch [200][550]\t Training Loss 1.9608\t Accuracy 0.2333\n",
      "Epoch [62][200]\t Batch [250][550]\t Training Loss 1.9605\t Accuracy 0.2325\n",
      "Epoch [62][200]\t Batch [300][550]\t Training Loss 1.9599\t Accuracy 0.2326\n",
      "Epoch [62][200]\t Batch [350][550]\t Training Loss 1.9612\t Accuracy 0.2320\n",
      "Epoch [62][200]\t Batch [400][550]\t Training Loss 1.9602\t Accuracy 0.2315\n",
      "Epoch [62][200]\t Batch [450][550]\t Training Loss 1.9598\t Accuracy 0.2317\n",
      "Epoch [62][200]\t Batch [500][550]\t Training Loss 1.9589\t Accuracy 0.2316\n",
      "\n",
      "Epoch [62]\t Average training loss 1.9580\t Average training accuracy 0.2313\n",
      "Epoch [62]\t Average validation loss 1.9441\t Average validation accuracy 0.2278\n",
      "\n",
      "Epoch [63][200]\t Batch [0][550]\t Training Loss 1.9478\t Accuracy 0.2500\n",
      "Epoch [63][200]\t Batch [50][550]\t Training Loss 1.9555\t Accuracy 0.2275\n",
      "Epoch [63][200]\t Batch [100][550]\t Training Loss 1.9507\t Accuracy 0.2335\n",
      "Epoch [63][200]\t Batch [150][550]\t Training Loss 1.9528\t Accuracy 0.2346\n",
      "Epoch [63][200]\t Batch [200][550]\t Training Loss 1.9532\t Accuracy 0.2354\n",
      "Epoch [63][200]\t Batch [250][550]\t Training Loss 1.9529\t Accuracy 0.2343\n",
      "Epoch [63][200]\t Batch [300][550]\t Training Loss 1.9522\t Accuracy 0.2345\n",
      "Epoch [63][200]\t Batch [350][550]\t Training Loss 1.9536\t Accuracy 0.2335\n",
      "Epoch [63][200]\t Batch [400][550]\t Training Loss 1.9526\t Accuracy 0.2329\n",
      "Epoch [63][200]\t Batch [450][550]\t Training Loss 1.9522\t Accuracy 0.2331\n",
      "Epoch [63][200]\t Batch [500][550]\t Training Loss 1.9513\t Accuracy 0.2329\n",
      "\n",
      "Epoch [63]\t Average training loss 1.9504\t Average training accuracy 0.2327\n",
      "Epoch [63]\t Average validation loss 1.9362\t Average validation accuracy 0.2296\n",
      "\n",
      "Epoch [64][200]\t Batch [0][550]\t Training Loss 1.9425\t Accuracy 0.2500\n",
      "Epoch [64][200]\t Batch [50][550]\t Training Loss 1.9479\t Accuracy 0.2292\n",
      "Epoch [64][200]\t Batch [100][550]\t Training Loss 1.9431\t Accuracy 0.2356\n",
      "Epoch [64][200]\t Batch [150][550]\t Training Loss 1.9454\t Accuracy 0.2366\n",
      "Epoch [64][200]\t Batch [200][550]\t Training Loss 1.9458\t Accuracy 0.2375\n",
      "Epoch [64][200]\t Batch [250][550]\t Training Loss 1.9455\t Accuracy 0.2362\n",
      "Epoch [64][200]\t Batch [300][550]\t Training Loss 1.9449\t Accuracy 0.2365\n",
      "Epoch [64][200]\t Batch [350][550]\t Training Loss 1.9463\t Accuracy 0.2356\n",
      "Epoch [64][200]\t Batch [400][550]\t Training Loss 1.9453\t Accuracy 0.2348\n",
      "Epoch [64][200]\t Batch [450][550]\t Training Loss 1.9449\t Accuracy 0.2348\n",
      "Epoch [64][200]\t Batch [500][550]\t Training Loss 1.9440\t Accuracy 0.2346\n",
      "\n",
      "Epoch [64]\t Average training loss 1.9431\t Average training accuracy 0.2342\n",
      "Epoch [64]\t Average validation loss 1.9286\t Average validation accuracy 0.2310\n",
      "\n",
      "Epoch [65][200]\t Batch [0][550]\t Training Loss 1.9373\t Accuracy 0.2500\n",
      "Epoch [65][200]\t Batch [50][550]\t Training Loss 1.9406\t Accuracy 0.2316\n",
      "Epoch [65][200]\t Batch [100][550]\t Training Loss 1.9359\t Accuracy 0.2375\n",
      "Epoch [65][200]\t Batch [150][550]\t Training Loss 1.9383\t Accuracy 0.2381\n",
      "Epoch [65][200]\t Batch [200][550]\t Training Loss 1.9387\t Accuracy 0.2391\n",
      "Epoch [65][200]\t Batch [250][550]\t Training Loss 1.9385\t Accuracy 0.2379\n",
      "Epoch [65][200]\t Batch [300][550]\t Training Loss 1.9379\t Accuracy 0.2378\n",
      "Epoch [65][200]\t Batch [350][550]\t Training Loss 1.9393\t Accuracy 0.2370\n",
      "Epoch [65][200]\t Batch [400][550]\t Training Loss 1.9383\t Accuracy 0.2363\n",
      "Epoch [65][200]\t Batch [450][550]\t Training Loss 1.9378\t Accuracy 0.2363\n",
      "Epoch [65][200]\t Batch [500][550]\t Training Loss 1.9370\t Accuracy 0.2361\n",
      "\n",
      "Epoch [65]\t Average training loss 1.9361\t Average training accuracy 0.2358\n",
      "Epoch [65]\t Average validation loss 1.9213\t Average validation accuracy 0.2342\n",
      "\n",
      "Epoch [66][200]\t Batch [0][550]\t Training Loss 1.9324\t Accuracy 0.2500\n",
      "Epoch [66][200]\t Batch [50][550]\t Training Loss 1.9336\t Accuracy 0.2318\n",
      "Epoch [66][200]\t Batch [100][550]\t Training Loss 1.9289\t Accuracy 0.2386\n",
      "Epoch [66][200]\t Batch [150][550]\t Training Loss 1.9315\t Accuracy 0.2391\n",
      "Epoch [66][200]\t Batch [200][550]\t Training Loss 1.9319\t Accuracy 0.2407\n",
      "Epoch [66][200]\t Batch [250][550]\t Training Loss 1.9317\t Accuracy 0.2399\n",
      "Epoch [66][200]\t Batch [300][550]\t Training Loss 1.9311\t Accuracy 0.2397\n",
      "Epoch [66][200]\t Batch [350][550]\t Training Loss 1.9325\t Accuracy 0.2391\n",
      "Epoch [66][200]\t Batch [400][550]\t Training Loss 1.9315\t Accuracy 0.2383\n",
      "Epoch [66][200]\t Batch [450][550]\t Training Loss 1.9311\t Accuracy 0.2384\n",
      "Epoch [66][200]\t Batch [500][550]\t Training Loss 1.9302\t Accuracy 0.2383\n",
      "\n",
      "Epoch [66]\t Average training loss 1.9293\t Average training accuracy 0.2380\n",
      "Epoch [66]\t Average validation loss 1.9143\t Average validation accuracy 0.2352\n",
      "\n",
      "Epoch [67][200]\t Batch [0][550]\t Training Loss 1.9276\t Accuracy 0.2500\n",
      "Epoch [67][200]\t Batch [50][550]\t Training Loss 1.9268\t Accuracy 0.2316\n",
      "Epoch [67][200]\t Batch [100][550]\t Training Loss 1.9222\t Accuracy 0.2390\n",
      "Epoch [67][200]\t Batch [150][550]\t Training Loss 1.9249\t Accuracy 0.2396\n",
      "Epoch [67][200]\t Batch [200][550]\t Training Loss 1.9253\t Accuracy 0.2415\n",
      "Epoch [67][200]\t Batch [250][550]\t Training Loss 1.9252\t Accuracy 0.2410\n",
      "Epoch [67][200]\t Batch [300][550]\t Training Loss 1.9246\t Accuracy 0.2408\n",
      "Epoch [67][200]\t Batch [350][550]\t Training Loss 1.9260\t Accuracy 0.2401\n",
      "Epoch [67][200]\t Batch [400][550]\t Training Loss 1.9250\t Accuracy 0.2392\n",
      "Epoch [67][200]\t Batch [450][550]\t Training Loss 1.9245\t Accuracy 0.2395\n",
      "Epoch [67][200]\t Batch [500][550]\t Training Loss 1.9237\t Accuracy 0.2394\n",
      "\n",
      "Epoch [67]\t Average training loss 1.9228\t Average training accuracy 0.2392\n",
      "Epoch [67]\t Average validation loss 1.9075\t Average validation accuracy 0.2352\n",
      "\n",
      "Epoch [68][200]\t Batch [0][550]\t Training Loss 1.9230\t Accuracy 0.2500\n",
      "Epoch [68][200]\t Batch [50][550]\t Training Loss 1.9202\t Accuracy 0.2316\n",
      "Epoch [68][200]\t Batch [100][550]\t Training Loss 1.9158\t Accuracy 0.2401\n",
      "Epoch [68][200]\t Batch [150][550]\t Training Loss 1.9185\t Accuracy 0.2406\n",
      "Epoch [68][200]\t Batch [200][550]\t Training Loss 1.9190\t Accuracy 0.2427\n",
      "Epoch [68][200]\t Batch [250][550]\t Training Loss 1.9189\t Accuracy 0.2421\n",
      "Epoch [68][200]\t Batch [300][550]\t Training Loss 1.9183\t Accuracy 0.2420\n",
      "Epoch [68][200]\t Batch [350][550]\t Training Loss 1.9197\t Accuracy 0.2410\n",
      "Epoch [68][200]\t Batch [400][550]\t Training Loss 1.9187\t Accuracy 0.2401\n",
      "Epoch [68][200]\t Batch [450][550]\t Training Loss 1.9183\t Accuracy 0.2405\n",
      "Epoch [68][200]\t Batch [500][550]\t Training Loss 1.9174\t Accuracy 0.2405\n",
      "\n",
      "Epoch [68]\t Average training loss 1.9166\t Average training accuracy 0.2405\n",
      "Epoch [68]\t Average validation loss 1.9010\t Average validation accuracy 0.2374\n",
      "\n",
      "Epoch [69][200]\t Batch [0][550]\t Training Loss 1.9185\t Accuracy 0.2500\n",
      "Epoch [69][200]\t Batch [50][550]\t Training Loss 1.9139\t Accuracy 0.2314\n",
      "Epoch [69][200]\t Batch [100][550]\t Training Loss 1.9095\t Accuracy 0.2411\n",
      "Epoch [69][200]\t Batch [150][550]\t Training Loss 1.9124\t Accuracy 0.2415\n",
      "Epoch [69][200]\t Batch [200][550]\t Training Loss 1.9129\t Accuracy 0.2441\n",
      "Epoch [69][200]\t Batch [250][550]\t Training Loss 1.9128\t Accuracy 0.2435\n",
      "Epoch [69][200]\t Batch [300][550]\t Training Loss 1.9123\t Accuracy 0.2435\n",
      "Epoch [69][200]\t Batch [350][550]\t Training Loss 1.9137\t Accuracy 0.2426\n",
      "Epoch [69][200]\t Batch [400][550]\t Training Loss 1.9127\t Accuracy 0.2419\n",
      "Epoch [69][200]\t Batch [450][550]\t Training Loss 1.9122\t Accuracy 0.2422\n",
      "Epoch [69][200]\t Batch [500][550]\t Training Loss 1.9114\t Accuracy 0.2423\n",
      "\n",
      "Epoch [69]\t Average training loss 1.9105\t Average training accuracy 0.2423\n",
      "Epoch [69]\t Average validation loss 1.8947\t Average validation accuracy 0.2384\n",
      "\n",
      "Epoch [70][200]\t Batch [0][550]\t Training Loss 1.9142\t Accuracy 0.2500\n",
      "Epoch [70][200]\t Batch [50][550]\t Training Loss 1.9078\t Accuracy 0.2333\n",
      "Epoch [70][200]\t Batch [100][550]\t Training Loss 1.9035\t Accuracy 0.2435\n",
      "Epoch [70][200]\t Batch [150][550]\t Training Loss 1.9065\t Accuracy 0.2430\n",
      "Epoch [70][200]\t Batch [200][550]\t Training Loss 1.9070\t Accuracy 0.2454\n",
      "Epoch [70][200]\t Batch [250][550]\t Training Loss 1.9070\t Accuracy 0.2447\n",
      "Epoch [70][200]\t Batch [300][550]\t Training Loss 1.9065\t Accuracy 0.2446\n",
      "Epoch [70][200]\t Batch [350][550]\t Training Loss 1.9079\t Accuracy 0.2439\n",
      "Epoch [70][200]\t Batch [400][550]\t Training Loss 1.9068\t Accuracy 0.2431\n",
      "Epoch [70][200]\t Batch [450][550]\t Training Loss 1.9064\t Accuracy 0.2435\n",
      "Epoch [70][200]\t Batch [500][550]\t Training Loss 1.9056\t Accuracy 0.2435\n",
      "\n",
      "Epoch [70]\t Average training loss 1.9047\t Average training accuracy 0.2437\n",
      "Epoch [70]\t Average validation loss 1.8886\t Average validation accuracy 0.2398\n",
      "\n",
      "Epoch [71][200]\t Batch [0][550]\t Training Loss 1.9099\t Accuracy 0.2600\n",
      "Epoch [71][200]\t Batch [50][550]\t Training Loss 1.9019\t Accuracy 0.2359\n",
      "Epoch [71][200]\t Batch [100][550]\t Training Loss 1.8977\t Accuracy 0.2458\n",
      "Epoch [71][200]\t Batch [150][550]\t Training Loss 1.9008\t Accuracy 0.2450\n",
      "Epoch [71][200]\t Batch [200][550]\t Training Loss 1.9014\t Accuracy 0.2474\n",
      "Epoch [71][200]\t Batch [250][550]\t Training Loss 1.9014\t Accuracy 0.2465\n",
      "Epoch [71][200]\t Batch [300][550]\t Training Loss 1.9008\t Accuracy 0.2464\n",
      "Epoch [71][200]\t Batch [350][550]\t Training Loss 1.9023\t Accuracy 0.2456\n",
      "Epoch [71][200]\t Batch [400][550]\t Training Loss 1.9012\t Accuracy 0.2447\n",
      "Epoch [71][200]\t Batch [450][550]\t Training Loss 1.9008\t Accuracy 0.2451\n",
      "Epoch [71][200]\t Batch [500][550]\t Training Loss 1.8999\t Accuracy 0.2450\n",
      "\n",
      "Epoch [71]\t Average training loss 1.8991\t Average training accuracy 0.2454\n",
      "Epoch [71]\t Average validation loss 1.8827\t Average validation accuracy 0.2416\n",
      "\n",
      "Epoch [72][200]\t Batch [0][550]\t Training Loss 1.9058\t Accuracy 0.2700\n",
      "Epoch [72][200]\t Batch [50][550]\t Training Loss 1.8962\t Accuracy 0.2369\n",
      "Epoch [72][200]\t Batch [100][550]\t Training Loss 1.8921\t Accuracy 0.2468\n",
      "Epoch [72][200]\t Batch [150][550]\t Training Loss 1.8953\t Accuracy 0.2464\n",
      "Epoch [72][200]\t Batch [200][550]\t Training Loss 1.8959\t Accuracy 0.2488\n",
      "Epoch [72][200]\t Batch [250][550]\t Training Loss 1.8959\t Accuracy 0.2478\n",
      "Epoch [72][200]\t Batch [300][550]\t Training Loss 1.8954\t Accuracy 0.2477\n",
      "Epoch [72][200]\t Batch [350][550]\t Training Loss 1.8968\t Accuracy 0.2471\n",
      "Epoch [72][200]\t Batch [400][550]\t Training Loss 1.8958\t Accuracy 0.2460\n",
      "Epoch [72][200]\t Batch [450][550]\t Training Loss 1.8953\t Accuracy 0.2465\n",
      "Epoch [72][200]\t Batch [500][550]\t Training Loss 1.8945\t Accuracy 0.2464\n",
      "\n",
      "Epoch [72]\t Average training loss 1.8937\t Average training accuracy 0.2467\n",
      "Epoch [72]\t Average validation loss 1.8770\t Average validation accuracy 0.2418\n",
      "\n",
      "Epoch [73][200]\t Batch [0][550]\t Training Loss 1.9019\t Accuracy 0.2700\n",
      "Epoch [73][200]\t Batch [50][550]\t Training Loss 1.8907\t Accuracy 0.2378\n",
      "Epoch [73][200]\t Batch [100][550]\t Training Loss 1.8867\t Accuracy 0.2481\n",
      "Epoch [73][200]\t Batch [150][550]\t Training Loss 1.8900\t Accuracy 0.2477\n",
      "Epoch [73][200]\t Batch [200][550]\t Training Loss 1.8906\t Accuracy 0.2502\n",
      "Epoch [73][200]\t Batch [250][550]\t Training Loss 1.8906\t Accuracy 0.2492\n",
      "Epoch [73][200]\t Batch [300][550]\t Training Loss 1.8901\t Accuracy 0.2489\n",
      "Epoch [73][200]\t Batch [350][550]\t Training Loss 1.8916\t Accuracy 0.2481\n",
      "Epoch [73][200]\t Batch [400][550]\t Training Loss 1.8905\t Accuracy 0.2470\n",
      "Epoch [73][200]\t Batch [450][550]\t Training Loss 1.8900\t Accuracy 0.2474\n",
      "Epoch [73][200]\t Batch [500][550]\t Training Loss 1.8892\t Accuracy 0.2473\n",
      "\n",
      "Epoch [73]\t Average training loss 1.8884\t Average training accuracy 0.2476\n",
      "Epoch [73]\t Average validation loss 1.8715\t Average validation accuracy 0.2446\n",
      "\n",
      "Epoch [74][200]\t Batch [0][550]\t Training Loss 1.8980\t Accuracy 0.2700\n",
      "Epoch [74][200]\t Batch [50][550]\t Training Loss 1.8854\t Accuracy 0.2400\n",
      "Epoch [74][200]\t Batch [100][550]\t Training Loss 1.8814\t Accuracy 0.2495\n",
      "Epoch [74][200]\t Batch [150][550]\t Training Loss 1.8848\t Accuracy 0.2490\n",
      "Epoch [74][200]\t Batch [200][550]\t Training Loss 1.8854\t Accuracy 0.2516\n",
      "Epoch [74][200]\t Batch [250][550]\t Training Loss 1.8855\t Accuracy 0.2506\n",
      "Epoch [74][200]\t Batch [300][550]\t Training Loss 1.8850\t Accuracy 0.2503\n",
      "Epoch [74][200]\t Batch [350][550]\t Training Loss 1.8865\t Accuracy 0.2495\n",
      "Epoch [74][200]\t Batch [400][550]\t Training Loss 1.8854\t Accuracy 0.2484\n",
      "Epoch [74][200]\t Batch [450][550]\t Training Loss 1.8849\t Accuracy 0.2488\n",
      "Epoch [74][200]\t Batch [500][550]\t Training Loss 1.8841\t Accuracy 0.2487\n",
      "\n",
      "Epoch [74]\t Average training loss 1.8833\t Average training accuracy 0.2490\n",
      "Epoch [74]\t Average validation loss 1.8661\t Average validation accuracy 0.2446\n",
      "\n",
      "Epoch [75][200]\t Batch [0][550]\t Training Loss 1.8943\t Accuracy 0.2700\n",
      "Epoch [75][200]\t Batch [50][550]\t Training Loss 1.8802\t Accuracy 0.2404\n",
      "Epoch [75][200]\t Batch [100][550]\t Training Loss 1.8763\t Accuracy 0.2501\n",
      "Epoch [75][200]\t Batch [150][550]\t Training Loss 1.8798\t Accuracy 0.2497\n",
      "Epoch [75][200]\t Batch [200][550]\t Training Loss 1.8804\t Accuracy 0.2526\n",
      "Epoch [75][200]\t Batch [250][550]\t Training Loss 1.8805\t Accuracy 0.2516\n",
      "Epoch [75][200]\t Batch [300][550]\t Training Loss 1.8801\t Accuracy 0.2513\n",
      "Epoch [75][200]\t Batch [350][550]\t Training Loss 1.8815\t Accuracy 0.2508\n",
      "Epoch [75][200]\t Batch [400][550]\t Training Loss 1.8804\t Accuracy 0.2496\n",
      "Epoch [75][200]\t Batch [450][550]\t Training Loss 1.8799\t Accuracy 0.2500\n",
      "Epoch [75][200]\t Batch [500][550]\t Training Loss 1.8791\t Accuracy 0.2499\n",
      "\n",
      "Epoch [75]\t Average training loss 1.8783\t Average training accuracy 0.2501\n",
      "Epoch [75]\t Average validation loss 1.8609\t Average validation accuracy 0.2454\n",
      "\n",
      "Epoch [76][200]\t Batch [0][550]\t Training Loss 1.8906\t Accuracy 0.2800\n",
      "Epoch [76][200]\t Batch [50][550]\t Training Loss 1.8751\t Accuracy 0.2429\n",
      "Epoch [76][200]\t Batch [100][550]\t Training Loss 1.8713\t Accuracy 0.2522\n",
      "Epoch [76][200]\t Batch [150][550]\t Training Loss 1.8748\t Accuracy 0.2518\n",
      "Epoch [76][200]\t Batch [200][550]\t Training Loss 1.8755\t Accuracy 0.2543\n",
      "Epoch [76][200]\t Batch [250][550]\t Training Loss 1.8756\t Accuracy 0.2530\n",
      "Epoch [76][200]\t Batch [300][550]\t Training Loss 1.8752\t Accuracy 0.2526\n",
      "Epoch [76][200]\t Batch [350][550]\t Training Loss 1.8766\t Accuracy 0.2521\n",
      "Epoch [76][200]\t Batch [400][550]\t Training Loss 1.8755\t Accuracy 0.2508\n",
      "Epoch [76][200]\t Batch [450][550]\t Training Loss 1.8750\t Accuracy 0.2512\n",
      "Epoch [76][200]\t Batch [500][550]\t Training Loss 1.8743\t Accuracy 0.2511\n",
      "\n",
      "Epoch [76]\t Average training loss 1.8734\t Average training accuracy 0.2513\n",
      "Epoch [76]\t Average validation loss 1.8557\t Average validation accuracy 0.2470\n",
      "\n",
      "Epoch [77][200]\t Batch [0][550]\t Training Loss 1.8870\t Accuracy 0.2800\n",
      "Epoch [77][200]\t Batch [50][550]\t Training Loss 1.8700\t Accuracy 0.2435\n",
      "Epoch [77][200]\t Batch [100][550]\t Training Loss 1.8663\t Accuracy 0.2526\n",
      "Epoch [77][200]\t Batch [150][550]\t Training Loss 1.8700\t Accuracy 0.2525\n",
      "Epoch [77][200]\t Batch [200][550]\t Training Loss 1.8707\t Accuracy 0.2556\n",
      "Epoch [77][200]\t Batch [250][550]\t Training Loss 1.8708\t Accuracy 0.2543\n",
      "Epoch [77][200]\t Batch [300][550]\t Training Loss 1.8704\t Accuracy 0.2538\n",
      "Epoch [77][200]\t Batch [350][550]\t Training Loss 1.8718\t Accuracy 0.2534\n",
      "Epoch [77][200]\t Batch [400][550]\t Training Loss 1.8707\t Accuracy 0.2521\n",
      "Epoch [77][200]\t Batch [450][550]\t Training Loss 1.8702\t Accuracy 0.2526\n",
      "Epoch [77][200]\t Batch [500][550]\t Training Loss 1.8694\t Accuracy 0.2526\n",
      "\n",
      "Epoch [77]\t Average training loss 1.8686\t Average training accuracy 0.2527\n",
      "Epoch [77]\t Average validation loss 1.8506\t Average validation accuracy 0.2482\n",
      "\n",
      "Epoch [78][200]\t Batch [0][550]\t Training Loss 1.8833\t Accuracy 0.2800\n",
      "Epoch [78][200]\t Batch [50][550]\t Training Loss 1.8650\t Accuracy 0.2439\n",
      "Epoch [78][200]\t Batch [100][550]\t Training Loss 1.8614\t Accuracy 0.2525\n",
      "Epoch [78][200]\t Batch [150][550]\t Training Loss 1.8651\t Accuracy 0.2528\n",
      "Epoch [78][200]\t Batch [200][550]\t Training Loss 1.8658\t Accuracy 0.2560\n",
      "Epoch [78][200]\t Batch [250][550]\t Training Loss 1.8660\t Accuracy 0.2551\n",
      "Epoch [78][200]\t Batch [300][550]\t Training Loss 1.8656\t Accuracy 0.2544\n",
      "Epoch [78][200]\t Batch [350][550]\t Training Loss 1.8670\t Accuracy 0.2540\n",
      "Epoch [78][200]\t Batch [400][550]\t Training Loss 1.8659\t Accuracy 0.2529\n",
      "Epoch [78][200]\t Batch [450][550]\t Training Loss 1.8654\t Accuracy 0.2533\n",
      "Epoch [78][200]\t Batch [500][550]\t Training Loss 1.8646\t Accuracy 0.2533\n",
      "\n",
      "Epoch [78]\t Average training loss 1.8638\t Average training accuracy 0.2534\n",
      "Epoch [78]\t Average validation loss 1.8454\t Average validation accuracy 0.2502\n",
      "\n",
      "Epoch [79][200]\t Batch [0][550]\t Training Loss 1.8796\t Accuracy 0.2800\n",
      "Epoch [79][200]\t Batch [50][550]\t Training Loss 1.8599\t Accuracy 0.2457\n",
      "Epoch [79][200]\t Batch [100][550]\t Training Loss 1.8564\t Accuracy 0.2541\n",
      "Epoch [79][200]\t Batch [150][550]\t Training Loss 1.8602\t Accuracy 0.2544\n",
      "Epoch [79][200]\t Batch [200][550]\t Training Loss 1.8609\t Accuracy 0.2580\n",
      "Epoch [79][200]\t Batch [250][550]\t Training Loss 1.8611\t Accuracy 0.2570\n",
      "Epoch [79][200]\t Batch [300][550]\t Training Loss 1.8607\t Accuracy 0.2562\n",
      "Epoch [79][200]\t Batch [350][550]\t Training Loss 1.8621\t Accuracy 0.2558\n",
      "Epoch [79][200]\t Batch [400][550]\t Training Loss 1.8609\t Accuracy 0.2547\n",
      "Epoch [79][200]\t Batch [450][550]\t Training Loss 1.8604\t Accuracy 0.2549\n",
      "Epoch [79][200]\t Batch [500][550]\t Training Loss 1.8596\t Accuracy 0.2548\n",
      "\n",
      "Epoch [79]\t Average training loss 1.8588\t Average training accuracy 0.2551\n",
      "Epoch [79]\t Average validation loss 1.8399\t Average validation accuracy 0.2524\n",
      "\n",
      "Epoch [80][200]\t Batch [0][550]\t Training Loss 1.8756\t Accuracy 0.2800\n",
      "Epoch [80][200]\t Batch [50][550]\t Training Loss 1.8546\t Accuracy 0.2467\n",
      "Epoch [80][200]\t Batch [100][550]\t Training Loss 1.8511\t Accuracy 0.2550\n",
      "Epoch [80][200]\t Batch [150][550]\t Training Loss 1.8550\t Accuracy 0.2558\n",
      "Epoch [80][200]\t Batch [200][550]\t Training Loss 1.8557\t Accuracy 0.2594\n",
      "Epoch [80][200]\t Batch [250][550]\t Training Loss 1.8559\t Accuracy 0.2582\n",
      "Epoch [80][200]\t Batch [300][550]\t Training Loss 1.8554\t Accuracy 0.2574\n",
      "Epoch [80][200]\t Batch [350][550]\t Training Loss 1.8568\t Accuracy 0.2570\n",
      "Epoch [80][200]\t Batch [400][550]\t Training Loss 1.8557\t Accuracy 0.2560\n",
      "Epoch [80][200]\t Batch [450][550]\t Training Loss 1.8551\t Accuracy 0.2561\n",
      "Epoch [80][200]\t Batch [500][550]\t Training Loss 1.8543\t Accuracy 0.2561\n",
      "\n",
      "Epoch [80]\t Average training loss 1.8534\t Average training accuracy 0.2562\n",
      "Epoch [80]\t Average validation loss 1.8340\t Average validation accuracy 0.2542\n",
      "\n",
      "Epoch [81][200]\t Batch [0][550]\t Training Loss 1.8711\t Accuracy 0.2800\n",
      "Epoch [81][200]\t Batch [50][550]\t Training Loss 1.8487\t Accuracy 0.2480\n",
      "Epoch [81][200]\t Batch [100][550]\t Training Loss 1.8453\t Accuracy 0.2564\n",
      "Epoch [81][200]\t Batch [150][550]\t Training Loss 1.8492\t Accuracy 0.2570\n",
      "Epoch [81][200]\t Batch [200][550]\t Training Loss 1.8499\t Accuracy 0.2609\n",
      "Epoch [81][200]\t Batch [250][550]\t Training Loss 1.8501\t Accuracy 0.2596\n",
      "Epoch [81][200]\t Batch [300][550]\t Training Loss 1.8497\t Accuracy 0.2587\n",
      "Epoch [81][200]\t Batch [350][550]\t Training Loss 1.8510\t Accuracy 0.2585\n",
      "Epoch [81][200]\t Batch [400][550]\t Training Loss 1.8498\t Accuracy 0.2574\n",
      "Epoch [81][200]\t Batch [450][550]\t Training Loss 1.8492\t Accuracy 0.2576\n",
      "Epoch [81][200]\t Batch [500][550]\t Training Loss 1.8484\t Accuracy 0.2575\n",
      "\n",
      "Epoch [81]\t Average training loss 1.8474\t Average training accuracy 0.2576\n",
      "Epoch [81]\t Average validation loss 1.8272\t Average validation accuracy 0.2568\n",
      "\n",
      "Epoch [82][200]\t Batch [0][550]\t Training Loss 1.8659\t Accuracy 0.2800\n",
      "Epoch [82][200]\t Batch [50][550]\t Training Loss 1.8419\t Accuracy 0.2500\n",
      "Epoch [82][200]\t Batch [100][550]\t Training Loss 1.8386\t Accuracy 0.2588\n",
      "Epoch [82][200]\t Batch [150][550]\t Training Loss 1.8426\t Accuracy 0.2595\n",
      "Epoch [82][200]\t Batch [200][550]\t Training Loss 1.8433\t Accuracy 0.2634\n",
      "Epoch [82][200]\t Batch [250][550]\t Training Loss 1.8434\t Accuracy 0.2624\n",
      "Epoch [82][200]\t Batch [300][550]\t Training Loss 1.8429\t Accuracy 0.2611\n",
      "Epoch [82][200]\t Batch [350][550]\t Training Loss 1.8442\t Accuracy 0.2607\n",
      "Epoch [82][200]\t Batch [400][550]\t Training Loss 1.8429\t Accuracy 0.2597\n",
      "Epoch [82][200]\t Batch [450][550]\t Training Loss 1.8422\t Accuracy 0.2598\n",
      "Epoch [82][200]\t Batch [500][550]\t Training Loss 1.8413\t Accuracy 0.2597\n",
      "\n",
      "Epoch [82]\t Average training loss 1.8404\t Average training accuracy 0.2599\n",
      "Epoch [82]\t Average validation loss 1.8189\t Average validation accuracy 0.2598\n",
      "\n",
      "Epoch [83][200]\t Batch [0][550]\t Training Loss 1.8594\t Accuracy 0.2900\n",
      "Epoch [83][200]\t Batch [50][550]\t Training Loss 1.8336\t Accuracy 0.2537\n",
      "Epoch [83][200]\t Batch [100][550]\t Training Loss 1.8305\t Accuracy 0.2630\n",
      "Epoch [83][200]\t Batch [150][550]\t Training Loss 1.8344\t Accuracy 0.2639\n",
      "Epoch [83][200]\t Batch [200][550]\t Training Loss 1.8351\t Accuracy 0.2679\n",
      "Epoch [83][200]\t Batch [250][550]\t Training Loss 1.8352\t Accuracy 0.2675\n",
      "Epoch [83][200]\t Batch [300][550]\t Training Loss 1.8346\t Accuracy 0.2670\n",
      "Epoch [83][200]\t Batch [350][550]\t Training Loss 1.8357\t Accuracy 0.2680\n",
      "Epoch [83][200]\t Batch [400][550]\t Training Loss 1.8344\t Accuracy 0.2684\n",
      "Epoch [83][200]\t Batch [450][550]\t Training Loss 1.8336\t Accuracy 0.2702\n",
      "Epoch [83][200]\t Batch [500][550]\t Training Loss 1.8326\t Accuracy 0.2717\n",
      "\n",
      "Epoch [83]\t Average training loss 1.8315\t Average training accuracy 0.2735\n",
      "Epoch [83]\t Average validation loss 1.8083\t Average validation accuracy 0.3012\n",
      "\n",
      "Epoch [84][200]\t Batch [0][550]\t Training Loss 1.8510\t Accuracy 0.3200\n",
      "Epoch [84][200]\t Batch [50][550]\t Training Loss 1.8231\t Accuracy 0.2949\n",
      "Epoch [84][200]\t Batch [100][550]\t Training Loss 1.8201\t Accuracy 0.3056\n",
      "Epoch [84][200]\t Batch [150][550]\t Training Loss 1.8239\t Accuracy 0.3087\n",
      "Epoch [84][200]\t Batch [200][550]\t Training Loss 1.8245\t Accuracy 0.3144\n",
      "Epoch [84][200]\t Batch [250][550]\t Training Loss 1.8245\t Accuracy 0.3159\n",
      "Epoch [84][200]\t Batch [300][550]\t Training Loss 1.8238\t Accuracy 0.3163\n",
      "Epoch [84][200]\t Batch [350][550]\t Training Loss 1.8248\t Accuracy 0.3190\n",
      "Epoch [84][200]\t Batch [400][550]\t Training Loss 1.8233\t Accuracy 0.3194\n",
      "Epoch [84][200]\t Batch [450][550]\t Training Loss 1.8224\t Accuracy 0.3216\n",
      "Epoch [84][200]\t Batch [500][550]\t Training Loss 1.8214\t Accuracy 0.3232\n",
      "\n",
      "Epoch [84]\t Average training loss 1.8201\t Average training accuracy 0.3253\n",
      "Epoch [84]\t Average validation loss 1.7946\t Average validation accuracy 0.3584\n",
      "\n",
      "Epoch [85][200]\t Batch [0][550]\t Training Loss 1.8399\t Accuracy 0.3700\n",
      "Epoch [85][200]\t Batch [50][550]\t Training Loss 1.8093\t Accuracy 0.3443\n",
      "Epoch [85][200]\t Batch [100][550]\t Training Loss 1.8066\t Accuracy 0.3517\n",
      "Epoch [85][200]\t Batch [150][550]\t Training Loss 1.8103\t Accuracy 0.3517\n",
      "Epoch [85][200]\t Batch [200][550]\t Training Loss 1.8109\t Accuracy 0.3558\n",
      "Epoch [85][200]\t Batch [250][550]\t Training Loss 1.8107\t Accuracy 0.3553\n",
      "Epoch [85][200]\t Batch [300][550]\t Training Loss 1.8098\t Accuracy 0.3536\n",
      "Epoch [85][200]\t Batch [350][550]\t Training Loss 1.8107\t Accuracy 0.3547\n",
      "Epoch [85][200]\t Batch [400][550]\t Training Loss 1.8090\t Accuracy 0.3537\n",
      "Epoch [85][200]\t Batch [450][550]\t Training Loss 1.8079\t Accuracy 0.3546\n",
      "Epoch [85][200]\t Batch [500][550]\t Training Loss 1.8068\t Accuracy 0.3543\n",
      "\n",
      "Epoch [85]\t Average training loss 1.8054\t Average training accuracy 0.3550\n",
      "Epoch [85]\t Average validation loss 1.7770\t Average validation accuracy 0.3730\n",
      "\n",
      "Epoch [86][200]\t Batch [0][550]\t Training Loss 1.8254\t Accuracy 0.4000\n",
      "Epoch [86][200]\t Batch [50][550]\t Training Loss 1.7917\t Accuracy 0.3596\n",
      "Epoch [86][200]\t Batch [100][550]\t Training Loss 1.7893\t Accuracy 0.3639\n",
      "Epoch [86][200]\t Batch [150][550]\t Training Loss 1.7929\t Accuracy 0.3632\n",
      "Epoch [86][200]\t Batch [200][550]\t Training Loss 1.7935\t Accuracy 0.3679\n",
      "Epoch [86][200]\t Batch [250][550]\t Training Loss 1.7932\t Accuracy 0.3672\n",
      "Epoch [86][200]\t Batch [300][550]\t Training Loss 1.7921\t Accuracy 0.3670\n",
      "Epoch [86][200]\t Batch [350][550]\t Training Loss 1.7928\t Accuracy 0.3690\n",
      "Epoch [86][200]\t Batch [400][550]\t Training Loss 1.7910\t Accuracy 0.3679\n",
      "Epoch [86][200]\t Batch [450][550]\t Training Loss 1.7898\t Accuracy 0.3690\n",
      "Epoch [86][200]\t Batch [500][550]\t Training Loss 1.7884\t Accuracy 0.3691\n",
      "\n",
      "Epoch [86]\t Average training loss 1.7869\t Average training accuracy 0.3704\n",
      "Epoch [86]\t Average validation loss 1.7556\t Average validation accuracy 0.3838\n",
      "\n",
      "Epoch [87][200]\t Batch [0][550]\t Training Loss 1.8075\t Accuracy 0.4100\n",
      "Epoch [87][200]\t Batch [50][550]\t Training Loss 1.7703\t Accuracy 0.3745\n",
      "Epoch [87][200]\t Batch [100][550]\t Training Loss 1.7685\t Accuracy 0.3767\n",
      "Epoch [87][200]\t Batch [150][550]\t Training Loss 1.7720\t Accuracy 0.3746\n",
      "Epoch [87][200]\t Batch [200][550]\t Training Loss 1.7726\t Accuracy 0.3794\n",
      "Epoch [87][200]\t Batch [250][550]\t Training Loss 1.7723\t Accuracy 0.3762\n",
      "Epoch [87][200]\t Batch [300][550]\t Training Loss 1.7710\t Accuracy 0.3758\n",
      "Epoch [87][200]\t Batch [350][550]\t Training Loss 1.7715\t Accuracy 0.3772\n",
      "Epoch [87][200]\t Batch [400][550]\t Training Loss 1.7696\t Accuracy 0.3762\n",
      "Epoch [87][200]\t Batch [450][550]\t Training Loss 1.7683\t Accuracy 0.3769\n",
      "Epoch [87][200]\t Batch [500][550]\t Training Loss 1.7669\t Accuracy 0.3754\n",
      "\n",
      "Epoch [87]\t Average training loss 1.7654\t Average training accuracy 0.3753\n",
      "Epoch [87]\t Average validation loss 1.7315\t Average validation accuracy 0.3704\n",
      "\n",
      "Epoch [88][200]\t Batch [0][550]\t Training Loss 1.7870\t Accuracy 0.4000\n",
      "Epoch [88][200]\t Batch [50][550]\t Training Loss 1.7463\t Accuracy 0.3669\n",
      "Epoch [88][200]\t Batch [100][550]\t Training Loss 1.7453\t Accuracy 0.3693\n",
      "Epoch [88][200]\t Batch [150][550]\t Training Loss 1.7488\t Accuracy 0.3697\n",
      "Epoch [88][200]\t Batch [200][550]\t Training Loss 1.7496\t Accuracy 0.3735\n",
      "Epoch [88][200]\t Batch [250][550]\t Training Loss 1.7492\t Accuracy 0.3706\n",
      "Epoch [88][200]\t Batch [300][550]\t Training Loss 1.7479\t Accuracy 0.3702\n",
      "Epoch [88][200]\t Batch [350][550]\t Training Loss 1.7483\t Accuracy 0.3711\n",
      "Epoch [88][200]\t Batch [400][550]\t Training Loss 1.7464\t Accuracy 0.3705\n",
      "Epoch [88][200]\t Batch [450][550]\t Training Loss 1.7451\t Accuracy 0.3712\n",
      "Epoch [88][200]\t Batch [500][550]\t Training Loss 1.7437\t Accuracy 0.3694\n",
      "\n",
      "Epoch [88]\t Average training loss 1.7421\t Average training accuracy 0.3692\n",
      "Epoch [88]\t Average validation loss 1.7067\t Average validation accuracy 0.3664\n",
      "\n",
      "Epoch [89][200]\t Batch [0][550]\t Training Loss 1.7655\t Accuracy 0.3900\n",
      "Epoch [89][200]\t Batch [50][550]\t Training Loss 1.7216\t Accuracy 0.3633\n",
      "Epoch [89][200]\t Batch [100][550]\t Training Loss 1.7216\t Accuracy 0.3668\n",
      "Epoch [89][200]\t Batch [150][550]\t Training Loss 1.7252\t Accuracy 0.3674\n",
      "Epoch [89][200]\t Batch [200][550]\t Training Loss 1.7262\t Accuracy 0.3713\n",
      "Epoch [89][200]\t Batch [250][550]\t Training Loss 1.7259\t Accuracy 0.3689\n",
      "Epoch [89][200]\t Batch [300][550]\t Training Loss 1.7246\t Accuracy 0.3688\n",
      "Epoch [89][200]\t Batch [350][550]\t Training Loss 1.7250\t Accuracy 0.3701\n",
      "Epoch [89][200]\t Batch [400][550]\t Training Loss 1.7232\t Accuracy 0.3691\n",
      "Epoch [89][200]\t Batch [450][550]\t Training Loss 1.7219\t Accuracy 0.3698\n",
      "Epoch [89][200]\t Batch [500][550]\t Training Loss 1.7206\t Accuracy 0.3684\n",
      "\n",
      "Epoch [89]\t Average training loss 1.7191\t Average training accuracy 0.3685\n",
      "Epoch [89]\t Average validation loss 1.6828\t Average validation accuracy 0.3676\n",
      "\n",
      "Epoch [90][200]\t Batch [0][550]\t Training Loss 1.7448\t Accuracy 0.4300\n",
      "Epoch [90][200]\t Batch [50][550]\t Training Loss 1.6980\t Accuracy 0.3682\n",
      "Epoch [90][200]\t Batch [100][550]\t Training Loss 1.6990\t Accuracy 0.3698\n",
      "Epoch [90][200]\t Batch [150][550]\t Training Loss 1.7029\t Accuracy 0.3691\n",
      "Epoch [90][200]\t Batch [200][550]\t Training Loss 1.7041\t Accuracy 0.3729\n",
      "Epoch [90][200]\t Batch [250][550]\t Training Loss 1.7039\t Accuracy 0.3712\n",
      "Epoch [90][200]\t Batch [300][550]\t Training Loss 1.7026\t Accuracy 0.3710\n",
      "Epoch [90][200]\t Batch [350][550]\t Training Loss 1.7030\t Accuracy 0.3725\n",
      "Epoch [90][200]\t Batch [400][550]\t Training Loss 1.7013\t Accuracy 0.3716\n",
      "Epoch [90][200]\t Batch [450][550]\t Training Loss 1.7002\t Accuracy 0.3721\n",
      "Epoch [90][200]\t Batch [500][550]\t Training Loss 1.6990\t Accuracy 0.3710\n",
      "\n",
      "Epoch [90]\t Average training loss 1.6976\t Average training accuracy 0.3711\n",
      "Epoch [90]\t Average validation loss 1.6610\t Average validation accuracy 0.3710\n",
      "\n",
      "Epoch [91][200]\t Batch [0][550]\t Training Loss 1.7255\t Accuracy 0.4500\n",
      "Epoch [91][200]\t Batch [50][550]\t Training Loss 1.6765\t Accuracy 0.3716\n",
      "Epoch [91][200]\t Batch [100][550]\t Training Loss 1.6784\t Accuracy 0.3730\n",
      "Epoch [91][200]\t Batch [150][550]\t Training Loss 1.6826\t Accuracy 0.3717\n",
      "Epoch [91][200]\t Batch [200][550]\t Training Loss 1.6839\t Accuracy 0.3750\n",
      "Epoch [91][200]\t Batch [250][550]\t Training Loss 1.6839\t Accuracy 0.3733\n",
      "Epoch [91][200]\t Batch [300][550]\t Training Loss 1.6826\t Accuracy 0.3734\n",
      "Epoch [91][200]\t Batch [350][550]\t Training Loss 1.6831\t Accuracy 0.3750\n",
      "Epoch [91][200]\t Batch [400][550]\t Training Loss 1.6816\t Accuracy 0.3741\n",
      "Epoch [91][200]\t Batch [450][550]\t Training Loss 1.6805\t Accuracy 0.3748\n",
      "Epoch [91][200]\t Batch [500][550]\t Training Loss 1.6795\t Accuracy 0.3739\n",
      "\n",
      "Epoch [91]\t Average training loss 1.6782\t Average training accuracy 0.3740\n",
      "Epoch [91]\t Average validation loss 1.6414\t Average validation accuracy 0.3726\n",
      "\n",
      "Epoch [92][200]\t Batch [0][550]\t Training Loss 1.7079\t Accuracy 0.4500\n",
      "Epoch [92][200]\t Batch [50][550]\t Training Loss 1.6572\t Accuracy 0.3743\n",
      "Epoch [92][200]\t Batch [100][550]\t Training Loss 1.6599\t Accuracy 0.3761\n",
      "Epoch [92][200]\t Batch [150][550]\t Training Loss 1.6643\t Accuracy 0.3753\n",
      "Epoch [92][200]\t Batch [200][550]\t Training Loss 1.6658\t Accuracy 0.3797\n",
      "Epoch [92][200]\t Batch [250][550]\t Training Loss 1.6658\t Accuracy 0.3778\n",
      "Epoch [92][200]\t Batch [300][550]\t Training Loss 1.6647\t Accuracy 0.3777\n",
      "Epoch [92][200]\t Batch [350][550]\t Training Loss 1.6652\t Accuracy 0.3791\n",
      "Epoch [92][200]\t Batch [400][550]\t Training Loss 1.6638\t Accuracy 0.3786\n",
      "Epoch [92][200]\t Batch [450][550]\t Training Loss 1.6628\t Accuracy 0.3792\n",
      "Epoch [92][200]\t Batch [500][550]\t Training Loss 1.6618\t Accuracy 0.3781\n",
      "\n",
      "Epoch [92]\t Average training loss 1.6606\t Average training accuracy 0.3783\n",
      "Epoch [92]\t Average validation loss 1.6236\t Average validation accuracy 0.3768\n",
      "\n",
      "Epoch [93][200]\t Batch [0][550]\t Training Loss 1.6916\t Accuracy 0.4500\n",
      "Epoch [93][200]\t Batch [50][550]\t Training Loss 1.6398\t Accuracy 0.3776\n",
      "Epoch [93][200]\t Batch [100][550]\t Training Loss 1.6431\t Accuracy 0.3817\n",
      "Epoch [93][200]\t Batch [150][550]\t Training Loss 1.6477\t Accuracy 0.3811\n",
      "Epoch [93][200]\t Batch [200][550]\t Training Loss 1.6493\t Accuracy 0.3859\n",
      "Epoch [93][200]\t Batch [250][550]\t Training Loss 1.6495\t Accuracy 0.3837\n",
      "Epoch [93][200]\t Batch [300][550]\t Training Loss 1.6483\t Accuracy 0.3833\n",
      "Epoch [93][200]\t Batch [350][550]\t Training Loss 1.6488\t Accuracy 0.3846\n",
      "Epoch [93][200]\t Batch [400][550]\t Training Loss 1.6475\t Accuracy 0.3841\n",
      "Epoch [93][200]\t Batch [450][550]\t Training Loss 1.6466\t Accuracy 0.3847\n",
      "Epoch [93][200]\t Batch [500][550]\t Training Loss 1.6457\t Accuracy 0.3833\n",
      "\n",
      "Epoch [93]\t Average training loss 1.6446\t Average training accuracy 0.3834\n",
      "Epoch [93]\t Average validation loss 1.6073\t Average validation accuracy 0.3800\n",
      "\n",
      "Epoch [94][200]\t Batch [0][550]\t Training Loss 1.6767\t Accuracy 0.4800\n",
      "Epoch [94][200]\t Batch [50][550]\t Training Loss 1.6239\t Accuracy 0.3841\n",
      "Epoch [94][200]\t Batch [100][550]\t Training Loss 1.6277\t Accuracy 0.3878\n",
      "Epoch [94][200]\t Batch [150][550]\t Training Loss 1.6325\t Accuracy 0.3863\n",
      "Epoch [94][200]\t Batch [200][550]\t Training Loss 1.6342\t Accuracy 0.3910\n",
      "Epoch [94][200]\t Batch [250][550]\t Training Loss 1.6343\t Accuracy 0.3893\n",
      "Epoch [94][200]\t Batch [300][550]\t Training Loss 1.6332\t Accuracy 0.3885\n",
      "Epoch [94][200]\t Batch [350][550]\t Training Loss 1.6337\t Accuracy 0.3900\n",
      "Epoch [94][200]\t Batch [400][550]\t Training Loss 1.6325\t Accuracy 0.3894\n",
      "Epoch [94][200]\t Batch [450][550]\t Training Loss 1.6317\t Accuracy 0.3901\n",
      "Epoch [94][200]\t Batch [500][550]\t Training Loss 1.6308\t Accuracy 0.3888\n",
      "\n",
      "Epoch [94]\t Average training loss 1.6298\t Average training accuracy 0.3889\n",
      "Epoch [94]\t Average validation loss 1.5920\t Average validation accuracy 0.3876\n",
      "\n",
      "Epoch [95][200]\t Batch [0][550]\t Training Loss 1.6626\t Accuracy 0.4700\n",
      "Epoch [95][200]\t Batch [50][550]\t Training Loss 1.6090\t Accuracy 0.3884\n",
      "Epoch [95][200]\t Batch [100][550]\t Training Loss 1.6131\t Accuracy 0.3926\n",
      "Epoch [95][200]\t Batch [150][550]\t Training Loss 1.6182\t Accuracy 0.3915\n",
      "Epoch [95][200]\t Batch [200][550]\t Training Loss 1.6199\t Accuracy 0.3965\n",
      "Epoch [95][200]\t Batch [250][550]\t Training Loss 1.6201\t Accuracy 0.3949\n",
      "Epoch [95][200]\t Batch [300][550]\t Training Loss 1.6189\t Accuracy 0.3945\n",
      "Epoch [95][200]\t Batch [350][550]\t Training Loss 1.6194\t Accuracy 0.3960\n",
      "Epoch [95][200]\t Batch [400][550]\t Training Loss 1.6183\t Accuracy 0.3954\n",
      "Epoch [95][200]\t Batch [450][550]\t Training Loss 1.6175\t Accuracy 0.3959\n",
      "Epoch [95][200]\t Batch [500][550]\t Training Loss 1.6167\t Accuracy 0.3949\n",
      "\n",
      "Epoch [95]\t Average training loss 1.6157\t Average training accuracy 0.3951\n",
      "Epoch [95]\t Average validation loss 1.5774\t Average validation accuracy 0.3932\n",
      "\n",
      "Epoch [96][200]\t Batch [0][550]\t Training Loss 1.6488\t Accuracy 0.4800\n",
      "Epoch [96][200]\t Batch [50][550]\t Training Loss 1.5947\t Accuracy 0.3939\n",
      "Epoch [96][200]\t Batch [100][550]\t Training Loss 1.5992\t Accuracy 0.3983\n",
      "Epoch [96][200]\t Batch [150][550]\t Training Loss 1.6044\t Accuracy 0.3977\n",
      "Epoch [96][200]\t Batch [200][550]\t Training Loss 1.6061\t Accuracy 0.4025\n",
      "Epoch [96][200]\t Batch [250][550]\t Training Loss 1.6063\t Accuracy 0.4009\n",
      "Epoch [96][200]\t Batch [300][550]\t Training Loss 1.6050\t Accuracy 0.4006\n",
      "Epoch [96][200]\t Batch [350][550]\t Training Loss 1.6056\t Accuracy 0.4023\n",
      "Epoch [96][200]\t Batch [400][550]\t Training Loss 1.6046\t Accuracy 0.4017\n",
      "Epoch [96][200]\t Batch [450][550]\t Training Loss 1.6038\t Accuracy 0.4022\n",
      "Epoch [96][200]\t Batch [500][550]\t Training Loss 1.6030\t Accuracy 0.4012\n",
      "\n",
      "Epoch [96]\t Average training loss 1.6021\t Average training accuracy 0.4019\n",
      "Epoch [96]\t Average validation loss 1.5630\t Average validation accuracy 0.4006\n",
      "\n",
      "Epoch [97][200]\t Batch [0][550]\t Training Loss 1.6349\t Accuracy 0.4900\n",
      "Epoch [97][200]\t Batch [50][550]\t Training Loss 1.5808\t Accuracy 0.4024\n",
      "Epoch [97][200]\t Batch [100][550]\t Training Loss 1.5854\t Accuracy 0.4081\n",
      "Epoch [97][200]\t Batch [150][550]\t Training Loss 1.5908\t Accuracy 0.4056\n",
      "Epoch [97][200]\t Batch [200][550]\t Training Loss 1.5925\t Accuracy 0.4104\n",
      "Epoch [97][200]\t Batch [250][550]\t Training Loss 1.5927\t Accuracy 0.4086\n",
      "Epoch [97][200]\t Batch [300][550]\t Training Loss 1.5913\t Accuracy 0.4090\n",
      "Epoch [97][200]\t Batch [350][550]\t Training Loss 1.5920\t Accuracy 0.4103\n",
      "Epoch [97][200]\t Batch [400][550]\t Training Loss 1.5909\t Accuracy 0.4097\n",
      "Epoch [97][200]\t Batch [450][550]\t Training Loss 1.5902\t Accuracy 0.4101\n",
      "Epoch [97][200]\t Batch [500][550]\t Training Loss 1.5893\t Accuracy 0.4093\n",
      "\n",
      "Epoch [97]\t Average training loss 1.5884\t Average training accuracy 0.4101\n",
      "Epoch [97]\t Average validation loss 1.5484\t Average validation accuracy 0.4140\n",
      "\n",
      "Epoch [98][200]\t Batch [0][550]\t Training Loss 1.6207\t Accuracy 0.4800\n",
      "Epoch [98][200]\t Batch [50][550]\t Training Loss 1.5667\t Accuracy 0.4106\n",
      "Epoch [98][200]\t Batch [100][550]\t Training Loss 1.5714\t Accuracy 0.4180\n",
      "Epoch [98][200]\t Batch [150][550]\t Training Loss 1.5770\t Accuracy 0.4150\n",
      "Epoch [98][200]\t Batch [200][550]\t Training Loss 1.5787\t Accuracy 0.4193\n",
      "Epoch [98][200]\t Batch [250][550]\t Training Loss 1.5788\t Accuracy 0.4182\n",
      "Epoch [98][200]\t Batch [300][550]\t Training Loss 1.5773\t Accuracy 0.4191\n",
      "Epoch [98][200]\t Batch [350][550]\t Training Loss 1.5780\t Accuracy 0.4203\n",
      "Epoch [98][200]\t Batch [400][550]\t Training Loss 1.5769\t Accuracy 0.4197\n",
      "Epoch [98][200]\t Batch [450][550]\t Training Loss 1.5763\t Accuracy 0.4203\n",
      "Epoch [98][200]\t Batch [500][550]\t Training Loss 1.5754\t Accuracy 0.4199\n",
      "\n",
      "Epoch [98]\t Average training loss 1.5745\t Average training accuracy 0.4209\n",
      "Epoch [98]\t Average validation loss 1.5332\t Average validation accuracy 0.4236\n",
      "\n",
      "Epoch [99][200]\t Batch [0][550]\t Training Loss 1.6056\t Accuracy 0.4900\n",
      "Epoch [99][200]\t Batch [50][550]\t Training Loss 1.5520\t Accuracy 0.4235\n",
      "Epoch [99][200]\t Batch [100][550]\t Training Loss 1.5567\t Accuracy 0.4284\n",
      "Epoch [99][200]\t Batch [150][550]\t Training Loss 1.5625\t Accuracy 0.4253\n",
      "Epoch [99][200]\t Batch [200][550]\t Training Loss 1.5641\t Accuracy 0.4291\n",
      "Epoch [99][200]\t Batch [250][550]\t Training Loss 1.5642\t Accuracy 0.4284\n",
      "Epoch [99][200]\t Batch [300][550]\t Training Loss 1.5626\t Accuracy 0.4298\n",
      "Epoch [99][200]\t Batch [350][550]\t Training Loss 1.5633\t Accuracy 0.4308\n",
      "Epoch [99][200]\t Batch [400][550]\t Training Loss 1.5622\t Accuracy 0.4300\n",
      "Epoch [99][200]\t Batch [450][550]\t Training Loss 1.5615\t Accuracy 0.4307\n",
      "Epoch [99][200]\t Batch [500][550]\t Training Loss 1.5605\t Accuracy 0.4306\n",
      "\n",
      "Epoch [99]\t Average training loss 1.5596\t Average training accuracy 0.4316\n",
      "Epoch [99]\t Average validation loss 1.5168\t Average validation accuracy 0.4378\n",
      "\n",
      "Epoch [100][200]\t Batch [0][550]\t Training Loss 1.5892\t Accuracy 0.5000\n",
      "Epoch [100][200]\t Batch [50][550]\t Training Loss 1.5362\t Accuracy 0.4335\n",
      "Epoch [100][200]\t Batch [100][550]\t Training Loss 1.5409\t Accuracy 0.4389\n",
      "Epoch [100][200]\t Batch [150][550]\t Training Loss 1.5467\t Accuracy 0.4352\n",
      "Epoch [100][200]\t Batch [200][550]\t Training Loss 1.5483\t Accuracy 0.4393\n",
      "Epoch [100][200]\t Batch [250][550]\t Training Loss 1.5483\t Accuracy 0.4395\n",
      "Epoch [100][200]\t Batch [300][550]\t Training Loss 1.5465\t Accuracy 0.4409\n",
      "Epoch [100][200]\t Batch [350][550]\t Training Loss 1.5471\t Accuracy 0.4418\n",
      "Epoch [100][200]\t Batch [400][550]\t Training Loss 1.5460\t Accuracy 0.4411\n",
      "Epoch [100][200]\t Batch [450][550]\t Training Loss 1.5453\t Accuracy 0.4419\n",
      "Epoch [100][200]\t Batch [500][550]\t Training Loss 1.5442\t Accuracy 0.4420\n",
      "\n",
      "Epoch [100]\t Average training loss 1.5433\t Average training accuracy 0.4432\n",
      "Epoch [100]\t Average validation loss 1.4983\t Average validation accuracy 0.4552\n",
      "\n",
      "Epoch [101][200]\t Batch [0][550]\t Training Loss 1.5706\t Accuracy 0.5100\n",
      "Epoch [101][200]\t Batch [50][550]\t Training Loss 1.5185\t Accuracy 0.4476\n",
      "Epoch [101][200]\t Batch [100][550]\t Training Loss 1.5229\t Accuracy 0.4520\n",
      "Epoch [101][200]\t Batch [150][550]\t Training Loss 1.5288\t Accuracy 0.4483\n",
      "Epoch [101][200]\t Batch [200][550]\t Training Loss 1.5303\t Accuracy 0.4522\n",
      "Epoch [101][200]\t Batch [250][550]\t Training Loss 1.5302\t Accuracy 0.4530\n",
      "Epoch [101][200]\t Batch [300][550]\t Training Loss 1.5282\t Accuracy 0.4545\n",
      "Epoch [101][200]\t Batch [350][550]\t Training Loss 1.5288\t Accuracy 0.4557\n",
      "Epoch [101][200]\t Batch [400][550]\t Training Loss 1.5275\t Accuracy 0.4559\n",
      "Epoch [101][200]\t Batch [450][550]\t Training Loss 1.5268\t Accuracy 0.4571\n",
      "Epoch [101][200]\t Batch [500][550]\t Training Loss 1.5255\t Accuracy 0.4573\n",
      "\n",
      "Epoch [101]\t Average training loss 1.5244\t Average training accuracy 0.4590\n",
      "Epoch [101]\t Average validation loss 1.4766\t Average validation accuracy 0.4750\n",
      "\n",
      "Epoch [102][200]\t Batch [0][550]\t Training Loss 1.5489\t Accuracy 0.5500\n",
      "Epoch [102][200]\t Batch [50][550]\t Training Loss 1.4977\t Accuracy 0.4682\n",
      "Epoch [102][200]\t Batch [100][550]\t Training Loss 1.5018\t Accuracy 0.4697\n",
      "Epoch [102][200]\t Batch [150][550]\t Training Loss 1.5076\t Accuracy 0.4665\n",
      "Epoch [102][200]\t Batch [200][550]\t Training Loss 1.5091\t Accuracy 0.4711\n",
      "Epoch [102][200]\t Batch [250][550]\t Training Loss 1.5087\t Accuracy 0.4725\n",
      "Epoch [102][200]\t Batch [300][550]\t Training Loss 1.5064\t Accuracy 0.4744\n",
      "Epoch [102][200]\t Batch [350][550]\t Training Loss 1.5070\t Accuracy 0.4760\n",
      "Epoch [102][200]\t Batch [400][550]\t Training Loss 1.5056\t Accuracy 0.4769\n",
      "Epoch [102][200]\t Batch [450][550]\t Training Loss 1.5046\t Accuracy 0.4786\n",
      "Epoch [102][200]\t Batch [500][550]\t Training Loss 1.5031\t Accuracy 0.4793\n",
      "\n",
      "Epoch [102]\t Average training loss 1.5019\t Average training accuracy 0.4817\n",
      "Epoch [102]\t Average validation loss 1.4503\t Average validation accuracy 0.5030\n",
      "\n",
      "Epoch [103][200]\t Batch [0][550]\t Training Loss 1.5227\t Accuracy 0.5500\n",
      "Epoch [103][200]\t Batch [50][550]\t Training Loss 1.4724\t Accuracy 0.5027\n",
      "Epoch [103][200]\t Batch [100][550]\t Training Loss 1.4760\t Accuracy 0.4985\n",
      "Epoch [103][200]\t Batch [150][550]\t Training Loss 1.4819\t Accuracy 0.4953\n",
      "Epoch [103][200]\t Batch [200][550]\t Training Loss 1.4832\t Accuracy 0.4995\n",
      "Epoch [103][200]\t Batch [250][550]\t Training Loss 1.4825\t Accuracy 0.5004\n",
      "Epoch [103][200]\t Batch [300][550]\t Training Loss 1.4798\t Accuracy 0.5019\n",
      "Epoch [103][200]\t Batch [350][550]\t Training Loss 1.4803\t Accuracy 0.5027\n",
      "Epoch [103][200]\t Batch [400][550]\t Training Loss 1.4786\t Accuracy 0.5041\n",
      "Epoch [103][200]\t Batch [450][550]\t Training Loss 1.4775\t Accuracy 0.5063\n",
      "Epoch [103][200]\t Batch [500][550]\t Training Loss 1.4756\t Accuracy 0.5072\n",
      "\n",
      "Epoch [103]\t Average training loss 1.4742\t Average training accuracy 0.5098\n",
      "Epoch [103]\t Average validation loss 1.4177\t Average validation accuracy 0.5340\n",
      "\n",
      "Epoch [104][200]\t Batch [0][550]\t Training Loss 1.4909\t Accuracy 0.5400\n",
      "Epoch [104][200]\t Batch [50][550]\t Training Loss 1.4410\t Accuracy 0.5322\n",
      "Epoch [104][200]\t Batch [100][550]\t Training Loss 1.4441\t Accuracy 0.5294\n",
      "Epoch [104][200]\t Batch [150][550]\t Training Loss 1.4498\t Accuracy 0.5275\n",
      "Epoch [104][200]\t Batch [200][550]\t Training Loss 1.4509\t Accuracy 0.5302\n",
      "Epoch [104][200]\t Batch [250][550]\t Training Loss 1.4498\t Accuracy 0.5307\n",
      "Epoch [104][200]\t Batch [300][550]\t Training Loss 1.4467\t Accuracy 0.5319\n",
      "Epoch [104][200]\t Batch [350][550]\t Training Loss 1.4470\t Accuracy 0.5330\n",
      "Epoch [104][200]\t Batch [400][550]\t Training Loss 1.4451\t Accuracy 0.5347\n",
      "Epoch [104][200]\t Batch [450][550]\t Training Loss 1.4437\t Accuracy 0.5369\n",
      "Epoch [104][200]\t Batch [500][550]\t Training Loss 1.4414\t Accuracy 0.5373\n",
      "\n",
      "Epoch [104]\t Average training loss 1.4397\t Average training accuracy 0.5398\n",
      "Epoch [104]\t Average validation loss 1.3774\t Average validation accuracy 0.5666\n",
      "\n",
      "Epoch [105][200]\t Batch [0][550]\t Training Loss 1.4519\t Accuracy 0.5500\n",
      "Epoch [105][200]\t Batch [50][550]\t Training Loss 1.4022\t Accuracy 0.5606\n",
      "Epoch [105][200]\t Batch [100][550]\t Training Loss 1.4046\t Accuracy 0.5578\n",
      "Epoch [105][200]\t Batch [150][550]\t Training Loss 1.4100\t Accuracy 0.5556\n",
      "Epoch [105][200]\t Batch [200][550]\t Training Loss 1.4110\t Accuracy 0.5584\n",
      "Epoch [105][200]\t Batch [250][550]\t Training Loss 1.4094\t Accuracy 0.5578\n",
      "Epoch [105][200]\t Batch [300][550]\t Training Loss 1.4058\t Accuracy 0.5583\n",
      "Epoch [105][200]\t Batch [350][550]\t Training Loss 1.4060\t Accuracy 0.5592\n",
      "Epoch [105][200]\t Batch [400][550]\t Training Loss 1.4037\t Accuracy 0.5603\n",
      "Epoch [105][200]\t Batch [450][550]\t Training Loss 1.4020\t Accuracy 0.5629\n",
      "Epoch [105][200]\t Batch [500][550]\t Training Loss 1.3993\t Accuracy 0.5632\n",
      "\n",
      "Epoch [105]\t Average training loss 1.3972\t Average training accuracy 0.5654\n",
      "Epoch [105]\t Average validation loss 1.3289\t Average validation accuracy 0.5924\n",
      "\n",
      "Epoch [106][200]\t Batch [0][550]\t Training Loss 1.4053\t Accuracy 0.5700\n",
      "Epoch [106][200]\t Batch [50][550]\t Training Loss 1.3554\t Accuracy 0.5835\n",
      "Epoch [106][200]\t Batch [100][550]\t Training Loss 1.3572\t Accuracy 0.5838\n",
      "Epoch [106][200]\t Batch [150][550]\t Training Loss 1.3623\t Accuracy 0.5805\n",
      "Epoch [106][200]\t Batch [200][550]\t Training Loss 1.3631\t Accuracy 0.5842\n",
      "Epoch [106][200]\t Batch [250][550]\t Training Loss 1.3610\t Accuracy 0.5837\n",
      "Epoch [106][200]\t Batch [300][550]\t Training Loss 1.3570\t Accuracy 0.5831\n",
      "Epoch [106][200]\t Batch [350][550]\t Training Loss 1.3572\t Accuracy 0.5846\n",
      "Epoch [106][200]\t Batch [400][550]\t Training Loss 1.3546\t Accuracy 0.5857\n",
      "Epoch [106][200]\t Batch [450][550]\t Training Loss 1.3526\t Accuracy 0.5876\n",
      "Epoch [106][200]\t Batch [500][550]\t Training Loss 1.3496\t Accuracy 0.5878\n",
      "\n",
      "Epoch [106]\t Average training loss 1.3472\t Average training accuracy 0.5897\n",
      "Epoch [106]\t Average validation loss 1.2734\t Average validation accuracy 0.6144\n",
      "\n",
      "Epoch [107][200]\t Batch [0][550]\t Training Loss 1.3529\t Accuracy 0.6200\n",
      "Epoch [107][200]\t Batch [50][550]\t Training Loss 1.3020\t Accuracy 0.6090\n",
      "Epoch [107][200]\t Batch [100][550]\t Training Loss 1.3033\t Accuracy 0.6053\n",
      "Epoch [107][200]\t Batch [150][550]\t Training Loss 1.3083\t Accuracy 0.6015\n",
      "Epoch [107][200]\t Batch [200][550]\t Training Loss 1.3090\t Accuracy 0.6045\n",
      "Epoch [107][200]\t Batch [250][550]\t Training Loss 1.3066\t Accuracy 0.6034\n",
      "Epoch [107][200]\t Batch [300][550]\t Training Loss 1.3023\t Accuracy 0.6019\n",
      "Epoch [107][200]\t Batch [350][550]\t Training Loss 1.3027\t Accuracy 0.6026\n",
      "Epoch [107][200]\t Batch [400][550]\t Training Loss 1.2998\t Accuracy 0.6037\n",
      "Epoch [107][200]\t Batch [450][550]\t Training Loss 1.2978\t Accuracy 0.6050\n",
      "Epoch [107][200]\t Batch [500][550]\t Training Loss 1.2945\t Accuracy 0.6050\n",
      "\n",
      "Epoch [107]\t Average training loss 1.2921\t Average training accuracy 0.6067\n",
      "Epoch [107]\t Average validation loss 1.2145\t Average validation accuracy 0.6332\n",
      "\n",
      "Epoch [108][200]\t Batch [0][550]\t Training Loss 1.2981\t Accuracy 0.6200\n",
      "Epoch [108][200]\t Batch [50][550]\t Training Loss 1.2455\t Accuracy 0.6216\n",
      "Epoch [108][200]\t Batch [100][550]\t Training Loss 1.2466\t Accuracy 0.6170\n",
      "Epoch [108][200]\t Batch [150][550]\t Training Loss 1.2517\t Accuracy 0.6144\n",
      "Epoch [108][200]\t Batch [200][550]\t Training Loss 1.2524\t Accuracy 0.6171\n",
      "Epoch [108][200]\t Batch [250][550]\t Training Loss 1.2498\t Accuracy 0.6169\n",
      "Epoch [108][200]\t Batch [300][550]\t Training Loss 1.2456\t Accuracy 0.6158\n",
      "Epoch [108][200]\t Batch [350][550]\t Training Loss 1.2463\t Accuracy 0.6164\n",
      "Epoch [108][200]\t Batch [400][550]\t Training Loss 1.2434\t Accuracy 0.6176\n",
      "Epoch [108][200]\t Batch [450][550]\t Training Loss 1.2415\t Accuracy 0.6186\n",
      "Epoch [108][200]\t Batch [500][550]\t Training Loss 1.2381\t Accuracy 0.6186\n",
      "\n",
      "Epoch [108]\t Average training loss 1.2358\t Average training accuracy 0.6201\n",
      "Epoch [108]\t Average validation loss 1.1566\t Average validation accuracy 0.6504\n",
      "\n",
      "Epoch [109][200]\t Batch [0][550]\t Training Loss 1.2454\t Accuracy 0.6200\n",
      "Epoch [109][200]\t Batch [50][550]\t Training Loss 1.1901\t Accuracy 0.6284\n",
      "Epoch [109][200]\t Batch [100][550]\t Training Loss 1.1914\t Accuracy 0.6266\n",
      "Epoch [109][200]\t Batch [150][550]\t Training Loss 1.1968\t Accuracy 0.6250\n",
      "Epoch [109][200]\t Batch [200][550]\t Training Loss 1.1977\t Accuracy 0.6278\n",
      "Epoch [109][200]\t Batch [250][550]\t Training Loss 1.1951\t Accuracy 0.6282\n",
      "Epoch [109][200]\t Batch [300][550]\t Training Loss 1.1910\t Accuracy 0.6273\n",
      "Epoch [109][200]\t Batch [350][550]\t Training Loss 1.1921\t Accuracy 0.6279\n",
      "Epoch [109][200]\t Batch [400][550]\t Training Loss 1.1894\t Accuracy 0.6289\n",
      "Epoch [109][200]\t Batch [450][550]\t Training Loss 1.1877\t Accuracy 0.6295\n",
      "Epoch [109][200]\t Batch [500][550]\t Training Loss 1.1844\t Accuracy 0.6298\n",
      "\n",
      "Epoch [109]\t Average training loss 1.1823\t Average training accuracy 0.6313\n",
      "Epoch [109]\t Average validation loss 1.1031\t Average validation accuracy 0.6606\n",
      "\n",
      "Epoch [110][200]\t Batch [0][550]\t Training Loss 1.1977\t Accuracy 0.6400\n",
      "Epoch [110][200]\t Batch [50][550]\t Training Loss 1.1392\t Accuracy 0.6418\n",
      "Epoch [110][200]\t Batch [100][550]\t Training Loss 1.1407\t Accuracy 0.6394\n",
      "Epoch [110][200]\t Batch [150][550]\t Training Loss 1.1466\t Accuracy 0.6375\n",
      "Epoch [110][200]\t Batch [200][550]\t Training Loss 1.1478\t Accuracy 0.6399\n",
      "Epoch [110][200]\t Batch [250][550]\t Training Loss 1.1453\t Accuracy 0.6397\n",
      "Epoch [110][200]\t Batch [300][550]\t Training Loss 1.1415\t Accuracy 0.6391\n",
      "Epoch [110][200]\t Batch [350][550]\t Training Loss 1.1430\t Accuracy 0.6395\n",
      "Epoch [110][200]\t Batch [400][550]\t Training Loss 1.1406\t Accuracy 0.6401\n",
      "Epoch [110][200]\t Batch [450][550]\t Training Loss 1.1391\t Accuracy 0.6405\n",
      "Epoch [110][200]\t Batch [500][550]\t Training Loss 1.1360\t Accuracy 0.6408\n",
      "\n",
      "Epoch [110]\t Average training loss 1.1342\t Average training accuracy 0.6421\n",
      "Epoch [110]\t Average validation loss 1.0555\t Average validation accuracy 0.6712\n",
      "\n",
      "Epoch [111][200]\t Batch [0][550]\t Training Loss 1.1559\t Accuracy 0.6500\n",
      "Epoch [111][200]\t Batch [50][550]\t Training Loss 1.0941\t Accuracy 0.6522\n",
      "Epoch [111][200]\t Batch [100][550]\t Training Loss 1.0961\t Accuracy 0.6490\n",
      "Epoch [111][200]\t Batch [150][550]\t Training Loss 1.1025\t Accuracy 0.6477\n",
      "Epoch [111][200]\t Batch [200][550]\t Training Loss 1.1039\t Accuracy 0.6498\n",
      "Epoch [111][200]\t Batch [250][550]\t Training Loss 1.1016\t Accuracy 0.6489\n",
      "Epoch [111][200]\t Batch [300][550]\t Training Loss 1.0981\t Accuracy 0.6486\n",
      "Epoch [111][200]\t Batch [350][550]\t Training Loss 1.1000\t Accuracy 0.6495\n",
      "Epoch [111][200]\t Batch [400][550]\t Training Loss 1.0978\t Accuracy 0.6500\n",
      "Epoch [111][200]\t Batch [450][550]\t Training Loss 1.0965\t Accuracy 0.6501\n",
      "Epoch [111][200]\t Batch [500][550]\t Training Loss 1.0937\t Accuracy 0.6502\n",
      "\n",
      "Epoch [111]\t Average training loss 1.0921\t Average training accuracy 0.6517\n",
      "Epoch [111]\t Average validation loss 1.0141\t Average validation accuracy 0.6822\n",
      "\n",
      "Epoch [112][200]\t Batch [0][550]\t Training Loss 1.1199\t Accuracy 0.6500\n",
      "Epoch [112][200]\t Batch [50][550]\t Training Loss 1.0549\t Accuracy 0.6624\n",
      "Epoch [112][200]\t Batch [100][550]\t Training Loss 1.0574\t Accuracy 0.6597\n",
      "Epoch [112][200]\t Batch [150][550]\t Training Loss 1.0644\t Accuracy 0.6580\n",
      "Epoch [112][200]\t Batch [200][550]\t Training Loss 1.0659\t Accuracy 0.6593\n",
      "Epoch [112][200]\t Batch [250][550]\t Training Loss 1.0638\t Accuracy 0.6584\n",
      "Epoch [112][200]\t Batch [300][550]\t Training Loss 1.0607\t Accuracy 0.6579\n",
      "Epoch [112][200]\t Batch [350][550]\t Training Loss 1.0628\t Accuracy 0.6589\n",
      "Epoch [112][200]\t Batch [400][550]\t Training Loss 1.0608\t Accuracy 0.6594\n",
      "Epoch [112][200]\t Batch [450][550]\t Training Loss 1.0597\t Accuracy 0.6595\n",
      "Epoch [112][200]\t Batch [500][550]\t Training Loss 1.0571\t Accuracy 0.6595\n",
      "\n",
      "Epoch [112]\t Average training loss 1.0558\t Average training accuracy 0.6610\n",
      "Epoch [112]\t Average validation loss 0.9783\t Average validation accuracy 0.6904\n",
      "\n",
      "Epoch [113][200]\t Batch [0][550]\t Training Loss 1.0889\t Accuracy 0.6600\n",
      "Epoch [113][200]\t Batch [50][550]\t Training Loss 1.0210\t Accuracy 0.6712\n",
      "Epoch [113][200]\t Batch [100][550]\t Training Loss 1.0241\t Accuracy 0.6677\n",
      "Epoch [113][200]\t Batch [150][550]\t Training Loss 1.0316\t Accuracy 0.6660\n",
      "Epoch [113][200]\t Batch [200][550]\t Training Loss 1.0332\t Accuracy 0.6672\n",
      "Epoch [113][200]\t Batch [250][550]\t Training Loss 1.0312\t Accuracy 0.6668\n",
      "Epoch [113][200]\t Batch [300][550]\t Training Loss 1.0284\t Accuracy 0.6662\n",
      "Epoch [113][200]\t Batch [350][550]\t Training Loss 1.0307\t Accuracy 0.6675\n",
      "Epoch [113][200]\t Batch [400][550]\t Training Loss 1.0290\t Accuracy 0.6678\n",
      "Epoch [113][200]\t Batch [450][550]\t Training Loss 1.0280\t Accuracy 0.6680\n",
      "Epoch [113][200]\t Batch [500][550]\t Training Loss 1.0256\t Accuracy 0.6680\n",
      "\n",
      "Epoch [113]\t Average training loss 1.0244\t Average training accuracy 0.6695\n",
      "Epoch [113]\t Average validation loss 0.9471\t Average validation accuracy 0.6984\n",
      "\n",
      "Epoch [114][200]\t Batch [0][550]\t Training Loss 1.0619\t Accuracy 0.6700\n",
      "Epoch [114][200]\t Batch [50][550]\t Training Loss 0.9916\t Accuracy 0.6806\n",
      "Epoch [114][200]\t Batch [100][550]\t Training Loss 0.9952\t Accuracy 0.6784\n",
      "Epoch [114][200]\t Batch [150][550]\t Training Loss 1.0031\t Accuracy 0.6756\n",
      "Epoch [114][200]\t Batch [200][550]\t Training Loss 1.0048\t Accuracy 0.6767\n",
      "Epoch [114][200]\t Batch [250][550]\t Training Loss 1.0030\t Accuracy 0.6767\n",
      "Epoch [114][200]\t Batch [300][550]\t Training Loss 1.0004\t Accuracy 0.6754\n",
      "Epoch [114][200]\t Batch [350][550]\t Training Loss 1.0029\t Accuracy 0.6763\n",
      "Epoch [114][200]\t Batch [400][550]\t Training Loss 1.0014\t Accuracy 0.6763\n",
      "Epoch [114][200]\t Batch [450][550]\t Training Loss 1.0005\t Accuracy 0.6765\n",
      "Epoch [114][200]\t Batch [500][550]\t Training Loss 0.9982\t Accuracy 0.6765\n",
      "\n",
      "Epoch [114]\t Average training loss 0.9972\t Average training accuracy 0.6775\n",
      "Epoch [114]\t Average validation loss 0.9199\t Average validation accuracy 0.7076\n",
      "\n",
      "Epoch [115][200]\t Batch [0][550]\t Training Loss 1.0381\t Accuracy 0.6800\n",
      "Epoch [115][200]\t Batch [50][550]\t Training Loss 0.9658\t Accuracy 0.6880\n",
      "Epoch [115][200]\t Batch [100][550]\t Training Loss 0.9699\t Accuracy 0.6849\n",
      "Epoch [115][200]\t Batch [150][550]\t Training Loss 0.9784\t Accuracy 0.6823\n",
      "Epoch [115][200]\t Batch [200][550]\t Training Loss 0.9801\t Accuracy 0.6830\n",
      "Epoch [115][200]\t Batch [250][550]\t Training Loss 0.9784\t Accuracy 0.6832\n",
      "Epoch [115][200]\t Batch [300][550]\t Training Loss 0.9761\t Accuracy 0.6823\n",
      "Epoch [115][200]\t Batch [350][550]\t Training Loss 0.9787\t Accuracy 0.6830\n",
      "Epoch [115][200]\t Batch [400][550]\t Training Loss 0.9773\t Accuracy 0.6832\n",
      "Epoch [115][200]\t Batch [450][550]\t Training Loss 0.9765\t Accuracy 0.6836\n",
      "Epoch [115][200]\t Batch [500][550]\t Training Loss 0.9743\t Accuracy 0.6836\n",
      "\n",
      "Epoch [115]\t Average training loss 0.9735\t Average training accuracy 0.6845\n",
      "Epoch [115]\t Average validation loss 0.8958\t Average validation accuracy 0.7124\n",
      "\n",
      "Epoch [116][200]\t Batch [0][550]\t Training Loss 1.0171\t Accuracy 0.6800\n",
      "Epoch [116][200]\t Batch [50][550]\t Training Loss 0.9430\t Accuracy 0.6947\n",
      "Epoch [116][200]\t Batch [100][550]\t Training Loss 0.9477\t Accuracy 0.6901\n",
      "Epoch [116][200]\t Batch [150][550]\t Training Loss 0.9566\t Accuracy 0.6874\n",
      "Epoch [116][200]\t Batch [200][550]\t Training Loss 0.9584\t Accuracy 0.6884\n",
      "Epoch [116][200]\t Batch [250][550]\t Training Loss 0.9567\t Accuracy 0.6889\n",
      "Epoch [116][200]\t Batch [300][550]\t Training Loss 0.9546\t Accuracy 0.6883\n",
      "Epoch [116][200]\t Batch [350][550]\t Training Loss 0.9573\t Accuracy 0.6890\n",
      "Epoch [116][200]\t Batch [400][550]\t Training Loss 0.9560\t Accuracy 0.6889\n",
      "Epoch [116][200]\t Batch [450][550]\t Training Loss 0.9553\t Accuracy 0.6895\n",
      "Epoch [116][200]\t Batch [500][550]\t Training Loss 0.9533\t Accuracy 0.6898\n",
      "\n",
      "Epoch [116]\t Average training loss 0.9525\t Average training accuracy 0.6906\n",
      "Epoch [116]\t Average validation loss 0.8744\t Average validation accuracy 0.7218\n",
      "\n",
      "Epoch [117][200]\t Batch [0][550]\t Training Loss 0.9985\t Accuracy 0.6800\n",
      "Epoch [117][200]\t Batch [50][550]\t Training Loss 0.9227\t Accuracy 0.6984\n",
      "Epoch [117][200]\t Batch [100][550]\t Training Loss 0.9280\t Accuracy 0.6964\n",
      "Epoch [117][200]\t Batch [150][550]\t Training Loss 0.9373\t Accuracy 0.6939\n",
      "Epoch [117][200]\t Batch [200][550]\t Training Loss 0.9390\t Accuracy 0.6946\n",
      "Epoch [117][200]\t Batch [250][550]\t Training Loss 0.9375\t Accuracy 0.6949\n",
      "Epoch [117][200]\t Batch [300][550]\t Training Loss 0.9356\t Accuracy 0.6946\n",
      "Epoch [117][200]\t Batch [350][550]\t Training Loss 0.9383\t Accuracy 0.6952\n",
      "Epoch [117][200]\t Batch [400][550]\t Training Loss 0.9371\t Accuracy 0.6948\n",
      "Epoch [117][200]\t Batch [450][550]\t Training Loss 0.9363\t Accuracy 0.6954\n",
      "Epoch [117][200]\t Batch [500][550]\t Training Loss 0.9345\t Accuracy 0.6956\n",
      "\n",
      "Epoch [117]\t Average training loss 0.9338\t Average training accuracy 0.6963\n",
      "Epoch [117]\t Average validation loss 0.8551\t Average validation accuracy 0.7270\n",
      "\n",
      "Epoch [118][200]\t Batch [0][550]\t Training Loss 0.9817\t Accuracy 0.6800\n",
      "Epoch [118][200]\t Batch [50][550]\t Training Loss 0.9045\t Accuracy 0.7043\n",
      "Epoch [118][200]\t Batch [100][550]\t Training Loss 0.9102\t Accuracy 0.7010\n",
      "Epoch [118][200]\t Batch [150][550]\t Training Loss 0.9199\t Accuracy 0.6989\n",
      "Epoch [118][200]\t Batch [200][550]\t Training Loss 0.9217\t Accuracy 0.6996\n",
      "Epoch [118][200]\t Batch [250][550]\t Training Loss 0.9202\t Accuracy 0.7003\n",
      "Epoch [118][200]\t Batch [300][550]\t Training Loss 0.9184\t Accuracy 0.7003\n",
      "Epoch [118][200]\t Batch [350][550]\t Training Loss 0.9212\t Accuracy 0.7008\n",
      "Epoch [118][200]\t Batch [400][550]\t Training Loss 0.9201\t Accuracy 0.7001\n",
      "Epoch [118][200]\t Batch [450][550]\t Training Loss 0.9193\t Accuracy 0.7004\n",
      "Epoch [118][200]\t Batch [500][550]\t Training Loss 0.9176\t Accuracy 0.7004\n",
      "\n",
      "Epoch [118]\t Average training loss 0.9170\t Average training accuracy 0.7012\n",
      "Epoch [118]\t Average validation loss 0.8376\t Average validation accuracy 0.7356\n",
      "\n",
      "Epoch [119][200]\t Batch [0][550]\t Training Loss 0.9665\t Accuracy 0.6800\n",
      "Epoch [119][200]\t Batch [50][550]\t Training Loss 0.8879\t Accuracy 0.7104\n",
      "Epoch [119][200]\t Batch [100][550]\t Training Loss 0.8942\t Accuracy 0.7064\n",
      "Epoch [119][200]\t Batch [150][550]\t Training Loss 0.9043\t Accuracy 0.7035\n",
      "Epoch [119][200]\t Batch [200][550]\t Training Loss 0.9060\t Accuracy 0.7040\n",
      "Epoch [119][200]\t Batch [250][550]\t Training Loss 0.9045\t Accuracy 0.7047\n",
      "Epoch [119][200]\t Batch [300][550]\t Training Loss 0.9029\t Accuracy 0.7048\n",
      "Epoch [119][200]\t Batch [350][550]\t Training Loss 0.9057\t Accuracy 0.7052\n",
      "Epoch [119][200]\t Batch [400][550]\t Training Loss 0.9047\t Accuracy 0.7045\n",
      "Epoch [119][200]\t Batch [450][550]\t Training Loss 0.9039\t Accuracy 0.7050\n",
      "Epoch [119][200]\t Batch [500][550]\t Training Loss 0.9022\t Accuracy 0.7050\n",
      "\n",
      "Epoch [119]\t Average training loss 0.9017\t Average training accuracy 0.7059\n",
      "Epoch [119]\t Average validation loss 0.8216\t Average validation accuracy 0.7416\n",
      "\n",
      "Epoch [120][200]\t Batch [0][550]\t Training Loss 0.9525\t Accuracy 0.6800\n",
      "Epoch [120][200]\t Batch [50][550]\t Training Loss 0.8728\t Accuracy 0.7141\n",
      "Epoch [120][200]\t Batch [100][550]\t Training Loss 0.8795\t Accuracy 0.7110\n",
      "Epoch [120][200]\t Batch [150][550]\t Training Loss 0.8900\t Accuracy 0.7088\n",
      "Epoch [120][200]\t Batch [200][550]\t Training Loss 0.8916\t Accuracy 0.7091\n",
      "Epoch [120][200]\t Batch [250][550]\t Training Loss 0.8902\t Accuracy 0.7098\n",
      "Epoch [120][200]\t Batch [300][550]\t Training Loss 0.8887\t Accuracy 0.7098\n",
      "Epoch [120][200]\t Batch [350][550]\t Training Loss 0.8915\t Accuracy 0.7102\n",
      "Epoch [120][200]\t Batch [400][550]\t Training Loss 0.8905\t Accuracy 0.7096\n",
      "Epoch [120][200]\t Batch [450][550]\t Training Loss 0.8898\t Accuracy 0.7098\n",
      "Epoch [120][200]\t Batch [500][550]\t Training Loss 0.8882\t Accuracy 0.7098\n",
      "\n",
      "Epoch [120]\t Average training loss 0.8877\t Average training accuracy 0.7105\n",
      "Epoch [120]\t Average validation loss 0.8068\t Average validation accuracy 0.7484\n",
      "\n",
      "Epoch [121][200]\t Batch [0][550]\t Training Loss 0.9398\t Accuracy 0.6700\n",
      "Epoch [121][200]\t Batch [50][550]\t Training Loss 0.8589\t Accuracy 0.7212\n",
      "Epoch [121][200]\t Batch [100][550]\t Training Loss 0.8660\t Accuracy 0.7162\n",
      "Epoch [121][200]\t Batch [150][550]\t Training Loss 0.8768\t Accuracy 0.7137\n",
      "Epoch [121][200]\t Batch [200][550]\t Training Loss 0.8784\t Accuracy 0.7137\n",
      "Epoch [121][200]\t Batch [250][550]\t Training Loss 0.8770\t Accuracy 0.7144\n",
      "Epoch [121][200]\t Batch [300][550]\t Training Loss 0.8757\t Accuracy 0.7142\n",
      "Epoch [121][200]\t Batch [350][550]\t Training Loss 0.8784\t Accuracy 0.7143\n",
      "Epoch [121][200]\t Batch [400][550]\t Training Loss 0.8775\t Accuracy 0.7137\n",
      "Epoch [121][200]\t Batch [450][550]\t Training Loss 0.8768\t Accuracy 0.7141\n",
      "Epoch [121][200]\t Batch [500][550]\t Training Loss 0.8752\t Accuracy 0.7141\n",
      "\n",
      "Epoch [121]\t Average training loss 0.8748\t Average training accuracy 0.7149\n",
      "Epoch [121]\t Average validation loss 0.7931\t Average validation accuracy 0.7540\n",
      "\n",
      "Epoch [122][200]\t Batch [0][550]\t Training Loss 0.9279\t Accuracy 0.6700\n",
      "Epoch [122][200]\t Batch [50][550]\t Training Loss 0.8460\t Accuracy 0.7233\n",
      "Epoch [122][200]\t Batch [100][550]\t Training Loss 0.8535\t Accuracy 0.7199\n",
      "Epoch [122][200]\t Batch [150][550]\t Training Loss 0.8647\t Accuracy 0.7171\n",
      "Epoch [122][200]\t Batch [200][550]\t Training Loss 0.8662\t Accuracy 0.7165\n",
      "Epoch [122][200]\t Batch [250][550]\t Training Loss 0.8648\t Accuracy 0.7174\n",
      "Epoch [122][200]\t Batch [300][550]\t Training Loss 0.8636\t Accuracy 0.7171\n",
      "Epoch [122][200]\t Batch [350][550]\t Training Loss 0.8663\t Accuracy 0.7174\n",
      "Epoch [122][200]\t Batch [400][550]\t Training Loss 0.8655\t Accuracy 0.7166\n",
      "Epoch [122][200]\t Batch [450][550]\t Training Loss 0.8647\t Accuracy 0.7171\n",
      "Epoch [122][200]\t Batch [500][550]\t Training Loss 0.8632\t Accuracy 0.7171\n",
      "\n",
      "Epoch [122]\t Average training loss 0.8629\t Average training accuracy 0.7181\n",
      "Epoch [122]\t Average validation loss 0.7804\t Average validation accuracy 0.7590\n",
      "\n",
      "Epoch [123][200]\t Batch [0][550]\t Training Loss 0.9167\t Accuracy 0.6700\n",
      "Epoch [123][200]\t Batch [50][550]\t Training Loss 0.8339\t Accuracy 0.7261\n",
      "Epoch [123][200]\t Batch [100][550]\t Training Loss 0.8419\t Accuracy 0.7226\n",
      "Epoch [123][200]\t Batch [150][550]\t Training Loss 0.8534\t Accuracy 0.7205\n",
      "Epoch [123][200]\t Batch [200][550]\t Training Loss 0.8548\t Accuracy 0.7194\n",
      "Epoch [123][200]\t Batch [250][550]\t Training Loss 0.8535\t Accuracy 0.7206\n",
      "Epoch [123][200]\t Batch [300][550]\t Training Loss 0.8523\t Accuracy 0.7211\n",
      "Epoch [123][200]\t Batch [350][550]\t Training Loss 0.8550\t Accuracy 0.7212\n",
      "Epoch [123][200]\t Batch [400][550]\t Training Loss 0.8542\t Accuracy 0.7206\n",
      "Epoch [123][200]\t Batch [450][550]\t Training Loss 0.8534\t Accuracy 0.7211\n",
      "Epoch [123][200]\t Batch [500][550]\t Training Loss 0.8520\t Accuracy 0.7213\n",
      "\n",
      "Epoch [123]\t Average training loss 0.8517\t Average training accuracy 0.7222\n",
      "Epoch [123]\t Average validation loss 0.7684\t Average validation accuracy 0.7634\n",
      "\n",
      "Epoch [124][200]\t Batch [0][550]\t Training Loss 0.9061\t Accuracy 0.6800\n",
      "Epoch [124][200]\t Batch [50][550]\t Training Loss 0.8227\t Accuracy 0.7296\n",
      "Epoch [124][200]\t Batch [100][550]\t Training Loss 0.8310\t Accuracy 0.7259\n",
      "Epoch [124][200]\t Batch [150][550]\t Training Loss 0.8428\t Accuracy 0.7244\n",
      "Epoch [124][200]\t Batch [200][550]\t Training Loss 0.8442\t Accuracy 0.7233\n",
      "Epoch [124][200]\t Batch [250][550]\t Training Loss 0.8428\t Accuracy 0.7241\n",
      "Epoch [124][200]\t Batch [300][550]\t Training Loss 0.8418\t Accuracy 0.7245\n",
      "Epoch [124][200]\t Batch [350][550]\t Training Loss 0.8444\t Accuracy 0.7248\n",
      "Epoch [124][200]\t Batch [400][550]\t Training Loss 0.8436\t Accuracy 0.7242\n",
      "Epoch [124][200]\t Batch [450][550]\t Training Loss 0.8429\t Accuracy 0.7246\n",
      "Epoch [124][200]\t Batch [500][550]\t Training Loss 0.8415\t Accuracy 0.7247\n",
      "\n",
      "Epoch [124]\t Average training loss 0.8412\t Average training accuracy 0.7257\n",
      "Epoch [124]\t Average validation loss 0.7572\t Average validation accuracy 0.7674\n",
      "\n",
      "Epoch [125][200]\t Batch [0][550]\t Training Loss 0.8961\t Accuracy 0.6800\n",
      "Epoch [125][200]\t Batch [50][550]\t Training Loss 0.8121\t Accuracy 0.7320\n",
      "Epoch [125][200]\t Batch [100][550]\t Training Loss 0.8208\t Accuracy 0.7286\n",
      "Epoch [125][200]\t Batch [150][550]\t Training Loss 0.8329\t Accuracy 0.7271\n",
      "Epoch [125][200]\t Batch [200][550]\t Training Loss 0.8342\t Accuracy 0.7266\n",
      "Epoch [125][200]\t Batch [250][550]\t Training Loss 0.8328\t Accuracy 0.7272\n",
      "Epoch [125][200]\t Batch [300][550]\t Training Loss 0.8318\t Accuracy 0.7274\n",
      "Epoch [125][200]\t Batch [350][550]\t Training Loss 0.8344\t Accuracy 0.7277\n",
      "Epoch [125][200]\t Batch [400][550]\t Training Loss 0.8337\t Accuracy 0.7272\n",
      "Epoch [125][200]\t Batch [450][550]\t Training Loss 0.8329\t Accuracy 0.7277\n",
      "Epoch [125][200]\t Batch [500][550]\t Training Loss 0.8316\t Accuracy 0.7276\n",
      "\n",
      "Epoch [125]\t Average training loss 0.8314\t Average training accuracy 0.7286\n",
      "Epoch [125]\t Average validation loss 0.7466\t Average validation accuracy 0.7716\n",
      "\n",
      "Epoch [126][200]\t Batch [0][550]\t Training Loss 0.8866\t Accuracy 0.6800\n",
      "Epoch [126][200]\t Batch [50][550]\t Training Loss 0.8021\t Accuracy 0.7363\n",
      "Epoch [126][200]\t Batch [100][550]\t Training Loss 0.8112\t Accuracy 0.7323\n",
      "Epoch [126][200]\t Batch [150][550]\t Training Loss 0.8235\t Accuracy 0.7298\n",
      "Epoch [126][200]\t Batch [200][550]\t Training Loss 0.8247\t Accuracy 0.7299\n",
      "Epoch [126][200]\t Batch [250][550]\t Training Loss 0.8234\t Accuracy 0.7304\n",
      "Epoch [126][200]\t Batch [300][550]\t Training Loss 0.8224\t Accuracy 0.7307\n",
      "Epoch [126][200]\t Batch [350][550]\t Training Loss 0.8250\t Accuracy 0.7311\n",
      "Epoch [126][200]\t Batch [400][550]\t Training Loss 0.8243\t Accuracy 0.7304\n",
      "Epoch [126][200]\t Batch [450][550]\t Training Loss 0.8235\t Accuracy 0.7310\n",
      "Epoch [126][200]\t Batch [500][550]\t Training Loss 0.8223\t Accuracy 0.7310\n",
      "\n",
      "Epoch [126]\t Average training loss 0.8220\t Average training accuracy 0.7320\n",
      "Epoch [126]\t Average validation loss 0.7366\t Average validation accuracy 0.7764\n",
      "\n",
      "Epoch [127][200]\t Batch [0][550]\t Training Loss 0.8776\t Accuracy 0.7000\n",
      "Epoch [127][200]\t Batch [50][550]\t Training Loss 0.7927\t Accuracy 0.7388\n",
      "Epoch [127][200]\t Batch [100][550]\t Training Loss 0.8020\t Accuracy 0.7354\n",
      "Epoch [127][200]\t Batch [150][550]\t Training Loss 0.8146\t Accuracy 0.7333\n",
      "Epoch [127][200]\t Batch [200][550]\t Training Loss 0.8158\t Accuracy 0.7334\n",
      "Epoch [127][200]\t Batch [250][550]\t Training Loss 0.8144\t Accuracy 0.7338\n",
      "Epoch [127][200]\t Batch [300][550]\t Training Loss 0.8135\t Accuracy 0.7340\n",
      "Epoch [127][200]\t Batch [350][550]\t Training Loss 0.8160\t Accuracy 0.7342\n",
      "Epoch [127][200]\t Batch [400][550]\t Training Loss 0.8154\t Accuracy 0.7340\n",
      "Epoch [127][200]\t Batch [450][550]\t Training Loss 0.8146\t Accuracy 0.7346\n",
      "Epoch [127][200]\t Batch [500][550]\t Training Loss 0.8134\t Accuracy 0.7347\n",
      "\n",
      "Epoch [127]\t Average training loss 0.8132\t Average training accuracy 0.7357\n",
      "Epoch [127]\t Average validation loss 0.7271\t Average validation accuracy 0.7796\n",
      "\n",
      "Epoch [128][200]\t Batch [0][550]\t Training Loss 0.8691\t Accuracy 0.7100\n",
      "Epoch [128][200]\t Batch [50][550]\t Training Loss 0.7837\t Accuracy 0.7437\n",
      "Epoch [128][200]\t Batch [100][550]\t Training Loss 0.7934\t Accuracy 0.7389\n",
      "Epoch [128][200]\t Batch [150][550]\t Training Loss 0.8062\t Accuracy 0.7360\n",
      "Epoch [128][200]\t Batch [200][550]\t Training Loss 0.8072\t Accuracy 0.7361\n",
      "Epoch [128][200]\t Batch [250][550]\t Training Loss 0.8059\t Accuracy 0.7368\n",
      "Epoch [128][200]\t Batch [300][550]\t Training Loss 0.8051\t Accuracy 0.7371\n",
      "Epoch [128][200]\t Batch [350][550]\t Training Loss 0.8075\t Accuracy 0.7375\n",
      "Epoch [128][200]\t Batch [400][550]\t Training Loss 0.8070\t Accuracy 0.7373\n",
      "Epoch [128][200]\t Batch [450][550]\t Training Loss 0.8062\t Accuracy 0.7380\n",
      "Epoch [128][200]\t Batch [500][550]\t Training Loss 0.8050\t Accuracy 0.7381\n",
      "\n",
      "Epoch [128]\t Average training loss 0.8048\t Average training accuracy 0.7391\n",
      "Epoch [128]\t Average validation loss 0.7180\t Average validation accuracy 0.7832\n",
      "\n",
      "Epoch [129][200]\t Batch [0][550]\t Training Loss 0.8610\t Accuracy 0.7100\n",
      "Epoch [129][200]\t Batch [50][550]\t Training Loss 0.7752\t Accuracy 0.7461\n",
      "Epoch [129][200]\t Batch [100][550]\t Training Loss 0.7852\t Accuracy 0.7417\n",
      "Epoch [129][200]\t Batch [150][550]\t Training Loss 0.7982\t Accuracy 0.7390\n",
      "Epoch [129][200]\t Batch [200][550]\t Training Loss 0.7992\t Accuracy 0.7392\n",
      "Epoch [129][200]\t Batch [250][550]\t Training Loss 0.7978\t Accuracy 0.7400\n",
      "Epoch [129][200]\t Batch [300][550]\t Training Loss 0.7970\t Accuracy 0.7401\n",
      "Epoch [129][200]\t Batch [350][550]\t Training Loss 0.7995\t Accuracy 0.7408\n",
      "Epoch [129][200]\t Batch [400][550]\t Training Loss 0.7989\t Accuracy 0.7408\n",
      "Epoch [129][200]\t Batch [450][550]\t Training Loss 0.7981\t Accuracy 0.7416\n",
      "Epoch [129][200]\t Batch [500][550]\t Training Loss 0.7970\t Accuracy 0.7416\n",
      "\n",
      "Epoch [129]\t Average training loss 0.7968\t Average training accuracy 0.7428\n",
      "Epoch [129]\t Average validation loss 0.7094\t Average validation accuracy 0.7862\n",
      "\n",
      "Epoch [130][200]\t Batch [0][550]\t Training Loss 0.8533\t Accuracy 0.7100\n",
      "Epoch [130][200]\t Batch [50][550]\t Training Loss 0.7670\t Accuracy 0.7514\n",
      "Epoch [130][200]\t Batch [100][550]\t Training Loss 0.7774\t Accuracy 0.7464\n",
      "Epoch [130][200]\t Batch [150][550]\t Training Loss 0.7906\t Accuracy 0.7424\n",
      "Epoch [130][200]\t Batch [200][550]\t Training Loss 0.7914\t Accuracy 0.7428\n",
      "Epoch [130][200]\t Batch [250][550]\t Training Loss 0.7901\t Accuracy 0.7435\n",
      "Epoch [130][200]\t Batch [300][550]\t Training Loss 0.7893\t Accuracy 0.7436\n",
      "Epoch [130][200]\t Batch [350][550]\t Training Loss 0.7918\t Accuracy 0.7442\n",
      "Epoch [130][200]\t Batch [400][550]\t Training Loss 0.7912\t Accuracy 0.7442\n",
      "Epoch [130][200]\t Batch [450][550]\t Training Loss 0.7905\t Accuracy 0.7451\n",
      "Epoch [130][200]\t Batch [500][550]\t Training Loss 0.7893\t Accuracy 0.7451\n",
      "\n",
      "Epoch [130]\t Average training loss 0.7892\t Average training accuracy 0.7463\n",
      "Epoch [130]\t Average validation loss 0.7012\t Average validation accuracy 0.7892\n",
      "\n",
      "Epoch [131][200]\t Batch [0][550]\t Training Loss 0.8460\t Accuracy 0.7100\n",
      "Epoch [131][200]\t Batch [50][550]\t Training Loss 0.7593\t Accuracy 0.7543\n",
      "Epoch [131][200]\t Batch [100][550]\t Training Loss 0.7699\t Accuracy 0.7493\n",
      "Epoch [131][200]\t Batch [150][550]\t Training Loss 0.7833\t Accuracy 0.7455\n",
      "Epoch [131][200]\t Batch [200][550]\t Training Loss 0.7841\t Accuracy 0.7460\n",
      "Epoch [131][200]\t Batch [250][550]\t Training Loss 0.7827\t Accuracy 0.7467\n",
      "Epoch [131][200]\t Batch [300][550]\t Training Loss 0.7820\t Accuracy 0.7468\n",
      "Epoch [131][200]\t Batch [350][550]\t Training Loss 0.7844\t Accuracy 0.7473\n",
      "Epoch [131][200]\t Batch [400][550]\t Training Loss 0.7839\t Accuracy 0.7472\n",
      "Epoch [131][200]\t Batch [450][550]\t Training Loss 0.7831\t Accuracy 0.7480\n",
      "Epoch [131][200]\t Batch [500][550]\t Training Loss 0.7820\t Accuracy 0.7480\n",
      "\n",
      "Epoch [131]\t Average training loss 0.7819\t Average training accuracy 0.7492\n",
      "Epoch [131]\t Average validation loss 0.6933\t Average validation accuracy 0.7922\n",
      "\n",
      "Epoch [132][200]\t Batch [0][550]\t Training Loss 0.8390\t Accuracy 0.7200\n",
      "Epoch [132][200]\t Batch [50][550]\t Training Loss 0.7518\t Accuracy 0.7586\n",
      "Epoch [132][200]\t Batch [100][550]\t Training Loss 0.7628\t Accuracy 0.7536\n",
      "Epoch [132][200]\t Batch [150][550]\t Training Loss 0.7763\t Accuracy 0.7491\n",
      "Epoch [132][200]\t Batch [200][550]\t Training Loss 0.7770\t Accuracy 0.7496\n",
      "Epoch [132][200]\t Batch [250][550]\t Training Loss 0.7756\t Accuracy 0.7504\n",
      "Epoch [132][200]\t Batch [300][550]\t Training Loss 0.7750\t Accuracy 0.7505\n",
      "Epoch [132][200]\t Batch [350][550]\t Training Loss 0.7773\t Accuracy 0.7511\n",
      "Epoch [132][200]\t Batch [400][550]\t Training Loss 0.7769\t Accuracy 0.7507\n",
      "Epoch [132][200]\t Batch [450][550]\t Training Loss 0.7761\t Accuracy 0.7516\n",
      "Epoch [132][200]\t Batch [500][550]\t Training Loss 0.7750\t Accuracy 0.7515\n",
      "\n",
      "Epoch [132]\t Average training loss 0.7749\t Average training accuracy 0.7526\n",
      "Epoch [132]\t Average validation loss 0.6858\t Average validation accuracy 0.7958\n",
      "\n",
      "Epoch [133][200]\t Batch [0][550]\t Training Loss 0.8323\t Accuracy 0.7300\n",
      "Epoch [133][200]\t Batch [50][550]\t Training Loss 0.7447\t Accuracy 0.7627\n",
      "Epoch [133][200]\t Batch [100][550]\t Training Loss 0.7560\t Accuracy 0.7569\n",
      "Epoch [133][200]\t Batch [150][550]\t Training Loss 0.7696\t Accuracy 0.7527\n",
      "Epoch [133][200]\t Batch [200][550]\t Training Loss 0.7702\t Accuracy 0.7526\n",
      "Epoch [133][200]\t Batch [250][550]\t Training Loss 0.7688\t Accuracy 0.7534\n",
      "Epoch [133][200]\t Batch [300][550]\t Training Loss 0.7682\t Accuracy 0.7538\n",
      "Epoch [133][200]\t Batch [350][550]\t Training Loss 0.7706\t Accuracy 0.7544\n",
      "Epoch [133][200]\t Batch [400][550]\t Training Loss 0.7701\t Accuracy 0.7542\n",
      "Epoch [133][200]\t Batch [450][550]\t Training Loss 0.7694\t Accuracy 0.7550\n",
      "Epoch [133][200]\t Batch [500][550]\t Training Loss 0.7683\t Accuracy 0.7548\n",
      "\n",
      "Epoch [133]\t Average training loss 0.7682\t Average training accuracy 0.7559\n",
      "Epoch [133]\t Average validation loss 0.6785\t Average validation accuracy 0.7998\n",
      "\n",
      "Epoch [134][200]\t Batch [0][550]\t Training Loss 0.8259\t Accuracy 0.7300\n",
      "Epoch [134][200]\t Batch [50][550]\t Training Loss 0.7378\t Accuracy 0.7661\n",
      "Epoch [134][200]\t Batch [100][550]\t Training Loss 0.7494\t Accuracy 0.7599\n",
      "Epoch [134][200]\t Batch [150][550]\t Training Loss 0.7631\t Accuracy 0.7562\n",
      "Epoch [134][200]\t Batch [200][550]\t Training Loss 0.7637\t Accuracy 0.7559\n",
      "Epoch [134][200]\t Batch [250][550]\t Training Loss 0.7623\t Accuracy 0.7565\n",
      "Epoch [134][200]\t Batch [300][550]\t Training Loss 0.7618\t Accuracy 0.7567\n",
      "Epoch [134][200]\t Batch [350][550]\t Training Loss 0.7641\t Accuracy 0.7576\n",
      "Epoch [134][200]\t Batch [400][550]\t Training Loss 0.7637\t Accuracy 0.7573\n",
      "Epoch [134][200]\t Batch [450][550]\t Training Loss 0.7629\t Accuracy 0.7580\n",
      "Epoch [134][200]\t Batch [500][550]\t Training Loss 0.7619\t Accuracy 0.7577\n",
      "\n",
      "Epoch [134]\t Average training loss 0.7618\t Average training accuracy 0.7587\n",
      "Epoch [134]\t Average validation loss 0.6716\t Average validation accuracy 0.8026\n",
      "\n",
      "Epoch [135][200]\t Batch [0][550]\t Training Loss 0.8198\t Accuracy 0.7200\n",
      "Epoch [135][200]\t Batch [50][550]\t Training Loss 0.7313\t Accuracy 0.7690\n",
      "Epoch [135][200]\t Batch [100][550]\t Training Loss 0.7432\t Accuracy 0.7633\n",
      "Epoch [135][200]\t Batch [150][550]\t Training Loss 0.7570\t Accuracy 0.7597\n",
      "Epoch [135][200]\t Batch [200][550]\t Training Loss 0.7575\t Accuracy 0.7590\n",
      "Epoch [135][200]\t Batch [250][550]\t Training Loss 0.7561\t Accuracy 0.7594\n",
      "Epoch [135][200]\t Batch [300][550]\t Training Loss 0.7556\t Accuracy 0.7598\n",
      "Epoch [135][200]\t Batch [350][550]\t Training Loss 0.7579\t Accuracy 0.7604\n",
      "Epoch [135][200]\t Batch [400][550]\t Training Loss 0.7575\t Accuracy 0.7602\n",
      "Epoch [135][200]\t Batch [450][550]\t Training Loss 0.7567\t Accuracy 0.7609\n",
      "Epoch [135][200]\t Batch [500][550]\t Training Loss 0.7557\t Accuracy 0.7607\n",
      "\n",
      "Epoch [135]\t Average training loss 0.7557\t Average training accuracy 0.7617\n",
      "Epoch [135]\t Average validation loss 0.6650\t Average validation accuracy 0.8042\n",
      "\n",
      "Epoch [136][200]\t Batch [0][550]\t Training Loss 0.8139\t Accuracy 0.7200\n",
      "Epoch [136][200]\t Batch [50][550]\t Training Loss 0.7249\t Accuracy 0.7708\n",
      "Epoch [136][200]\t Batch [100][550]\t Training Loss 0.7371\t Accuracy 0.7660\n",
      "Epoch [136][200]\t Batch [150][550]\t Training Loss 0.7510\t Accuracy 0.7626\n",
      "Epoch [136][200]\t Batch [200][550]\t Training Loss 0.7515\t Accuracy 0.7621\n",
      "Epoch [136][200]\t Batch [250][550]\t Training Loss 0.7501\t Accuracy 0.7624\n",
      "Epoch [136][200]\t Batch [300][550]\t Training Loss 0.7496\t Accuracy 0.7632\n",
      "Epoch [136][200]\t Batch [350][550]\t Training Loss 0.7519\t Accuracy 0.7636\n",
      "Epoch [136][200]\t Batch [400][550]\t Training Loss 0.7515\t Accuracy 0.7633\n",
      "Epoch [136][200]\t Batch [450][550]\t Training Loss 0.7507\t Accuracy 0.7639\n",
      "Epoch [136][200]\t Batch [500][550]\t Training Loss 0.7498\t Accuracy 0.7636\n",
      "\n",
      "Epoch [136]\t Average training loss 0.7497\t Average training accuracy 0.7645\n",
      "Epoch [136]\t Average validation loss 0.6587\t Average validation accuracy 0.8054\n",
      "\n",
      "Epoch [137][200]\t Batch [0][550]\t Training Loss 0.8081\t Accuracy 0.7200\n",
      "Epoch [137][200]\t Batch [50][550]\t Training Loss 0.7189\t Accuracy 0.7735\n",
      "Epoch [137][200]\t Batch [100][550]\t Training Loss 0.7313\t Accuracy 0.7677\n",
      "Epoch [137][200]\t Batch [150][550]\t Training Loss 0.7453\t Accuracy 0.7648\n",
      "Epoch [137][200]\t Batch [200][550]\t Training Loss 0.7457\t Accuracy 0.7639\n",
      "Epoch [137][200]\t Batch [250][550]\t Training Loss 0.7443\t Accuracy 0.7643\n",
      "Epoch [137][200]\t Batch [300][550]\t Training Loss 0.7439\t Accuracy 0.7653\n",
      "Epoch [137][200]\t Batch [350][550]\t Training Loss 0.7462\t Accuracy 0.7658\n",
      "Epoch [137][200]\t Batch [400][550]\t Training Loss 0.7458\t Accuracy 0.7655\n",
      "Epoch [137][200]\t Batch [450][550]\t Training Loss 0.7450\t Accuracy 0.7660\n",
      "Epoch [137][200]\t Batch [500][550]\t Training Loss 0.7441\t Accuracy 0.7658\n",
      "\n",
      "Epoch [137]\t Average training loss 0.7440\t Average training accuracy 0.7667\n",
      "Epoch [137]\t Average validation loss 0.6526\t Average validation accuracy 0.8076\n",
      "\n",
      "Epoch [138][200]\t Batch [0][550]\t Training Loss 0.8025\t Accuracy 0.7200\n",
      "Epoch [138][200]\t Batch [50][550]\t Training Loss 0.7130\t Accuracy 0.7763\n",
      "Epoch [138][200]\t Batch [100][550]\t Training Loss 0.7258\t Accuracy 0.7701\n",
      "Epoch [138][200]\t Batch [150][550]\t Training Loss 0.7398\t Accuracy 0.7673\n",
      "Epoch [138][200]\t Batch [200][550]\t Training Loss 0.7401\t Accuracy 0.7664\n",
      "Epoch [138][200]\t Batch [250][550]\t Training Loss 0.7387\t Accuracy 0.7667\n",
      "Epoch [138][200]\t Batch [300][550]\t Training Loss 0.7383\t Accuracy 0.7678\n",
      "Epoch [138][200]\t Batch [350][550]\t Training Loss 0.7406\t Accuracy 0.7684\n",
      "Epoch [138][200]\t Batch [400][550]\t Training Loss 0.7402\t Accuracy 0.7682\n",
      "Epoch [138][200]\t Batch [450][550]\t Training Loss 0.7395\t Accuracy 0.7687\n",
      "Epoch [138][200]\t Batch [500][550]\t Training Loss 0.7386\t Accuracy 0.7685\n",
      "\n",
      "Epoch [138]\t Average training loss 0.7385\t Average training accuracy 0.7692\n",
      "Epoch [138]\t Average validation loss 0.6467\t Average validation accuracy 0.8102\n",
      "\n",
      "Epoch [139][200]\t Batch [0][550]\t Training Loss 0.7970\t Accuracy 0.7300\n",
      "Epoch [139][200]\t Batch [50][550]\t Training Loss 0.7073\t Accuracy 0.7773\n",
      "Epoch [139][200]\t Batch [100][550]\t Training Loss 0.7204\t Accuracy 0.7721\n",
      "Epoch [139][200]\t Batch [150][550]\t Training Loss 0.7344\t Accuracy 0.7687\n",
      "Epoch [139][200]\t Batch [200][550]\t Training Loss 0.7347\t Accuracy 0.7680\n",
      "Epoch [139][200]\t Batch [250][550]\t Training Loss 0.7333\t Accuracy 0.7685\n",
      "Epoch [139][200]\t Batch [300][550]\t Training Loss 0.7330\t Accuracy 0.7697\n",
      "Epoch [139][200]\t Batch [350][550]\t Training Loss 0.7353\t Accuracy 0.7702\n",
      "Epoch [139][200]\t Batch [400][550]\t Training Loss 0.7349\t Accuracy 0.7703\n",
      "Epoch [139][200]\t Batch [450][550]\t Training Loss 0.7341\t Accuracy 0.7708\n",
      "Epoch [139][200]\t Batch [500][550]\t Training Loss 0.7333\t Accuracy 0.7706\n",
      "\n",
      "Epoch [139]\t Average training loss 0.7332\t Average training accuracy 0.7712\n",
      "Epoch [139]\t Average validation loss 0.6410\t Average validation accuracy 0.8116\n",
      "\n",
      "Epoch [140][200]\t Batch [0][550]\t Training Loss 0.7917\t Accuracy 0.7400\n",
      "Epoch [140][200]\t Batch [50][550]\t Training Loss 0.7019\t Accuracy 0.7800\n",
      "Epoch [140][200]\t Batch [100][550]\t Training Loss 0.7152\t Accuracy 0.7747\n",
      "Epoch [140][200]\t Batch [150][550]\t Training Loss 0.7293\t Accuracy 0.7711\n",
      "Epoch [140][200]\t Batch [200][550]\t Training Loss 0.7295\t Accuracy 0.7702\n",
      "Epoch [140][200]\t Batch [250][550]\t Training Loss 0.7281\t Accuracy 0.7709\n",
      "Epoch [140][200]\t Batch [300][550]\t Training Loss 0.7278\t Accuracy 0.7721\n",
      "Epoch [140][200]\t Batch [350][550]\t Training Loss 0.7301\t Accuracy 0.7726\n",
      "Epoch [140][200]\t Batch [400][550]\t Training Loss 0.7297\t Accuracy 0.7725\n",
      "Epoch [140][200]\t Batch [450][550]\t Training Loss 0.7290\t Accuracy 0.7729\n",
      "Epoch [140][200]\t Batch [500][550]\t Training Loss 0.7281\t Accuracy 0.7726\n",
      "\n",
      "Epoch [140]\t Average training loss 0.7281\t Average training accuracy 0.7733\n",
      "Epoch [140]\t Average validation loss 0.6356\t Average validation accuracy 0.8136\n",
      "\n",
      "Epoch [141][200]\t Batch [0][550]\t Training Loss 0.7866\t Accuracy 0.7500\n",
      "Epoch [141][200]\t Batch [50][550]\t Training Loss 0.6966\t Accuracy 0.7829\n",
      "Epoch [141][200]\t Batch [100][550]\t Training Loss 0.7102\t Accuracy 0.7769\n",
      "Epoch [141][200]\t Batch [150][550]\t Training Loss 0.7243\t Accuracy 0.7733\n",
      "Epoch [141][200]\t Batch [200][550]\t Training Loss 0.7245\t Accuracy 0.7722\n",
      "Epoch [141][200]\t Batch [250][550]\t Training Loss 0.7231\t Accuracy 0.7731\n",
      "Epoch [141][200]\t Batch [300][550]\t Training Loss 0.7229\t Accuracy 0.7743\n",
      "Epoch [141][200]\t Batch [350][550]\t Training Loss 0.7251\t Accuracy 0.7746\n",
      "Epoch [141][200]\t Batch [400][550]\t Training Loss 0.7247\t Accuracy 0.7746\n",
      "Epoch [141][200]\t Batch [450][550]\t Training Loss 0.7240\t Accuracy 0.7750\n",
      "Epoch [141][200]\t Batch [500][550]\t Training Loss 0.7231\t Accuracy 0.7749\n",
      "\n",
      "Epoch [141]\t Average training loss 0.7232\t Average training accuracy 0.7754\n",
      "Epoch [141]\t Average validation loss 0.6303\t Average validation accuracy 0.8146\n",
      "\n",
      "Epoch [142][200]\t Batch [0][550]\t Training Loss 0.7815\t Accuracy 0.7500\n",
      "Epoch [142][200]\t Batch [50][550]\t Training Loss 0.6915\t Accuracy 0.7863\n",
      "Epoch [142][200]\t Batch [100][550]\t Training Loss 0.7054\t Accuracy 0.7793\n",
      "Epoch [142][200]\t Batch [150][550]\t Training Loss 0.7195\t Accuracy 0.7754\n",
      "Epoch [142][200]\t Batch [200][550]\t Training Loss 0.7196\t Accuracy 0.7747\n",
      "Epoch [142][200]\t Batch [250][550]\t Training Loss 0.7182\t Accuracy 0.7755\n",
      "Epoch [142][200]\t Batch [300][550]\t Training Loss 0.7180\t Accuracy 0.7766\n",
      "Epoch [142][200]\t Batch [350][550]\t Training Loss 0.7203\t Accuracy 0.7767\n",
      "Epoch [142][200]\t Batch [400][550]\t Training Loss 0.7199\t Accuracy 0.7767\n",
      "Epoch [142][200]\t Batch [450][550]\t Training Loss 0.7191\t Accuracy 0.7771\n",
      "Epoch [142][200]\t Batch [500][550]\t Training Loss 0.7183\t Accuracy 0.7769\n",
      "\n",
      "Epoch [142]\t Average training loss 0.7184\t Average training accuracy 0.7775\n",
      "Epoch [142]\t Average validation loss 0.6253\t Average validation accuracy 0.8172\n",
      "\n",
      "Epoch [143][200]\t Batch [0][550]\t Training Loss 0.7766\t Accuracy 0.7600\n",
      "Epoch [143][200]\t Batch [50][550]\t Training Loss 0.6865\t Accuracy 0.7884\n",
      "Epoch [143][200]\t Batch [100][550]\t Training Loss 0.7007\t Accuracy 0.7818\n",
      "Epoch [143][200]\t Batch [150][550]\t Training Loss 0.7149\t Accuracy 0.7779\n",
      "Epoch [143][200]\t Batch [200][550]\t Training Loss 0.7149\t Accuracy 0.7776\n",
      "Epoch [143][200]\t Batch [250][550]\t Training Loss 0.7135\t Accuracy 0.7779\n",
      "Epoch [143][200]\t Batch [300][550]\t Training Loss 0.7134\t Accuracy 0.7789\n",
      "Epoch [143][200]\t Batch [350][550]\t Training Loss 0.7156\t Accuracy 0.7788\n",
      "Epoch [143][200]\t Batch [400][550]\t Training Loss 0.7152\t Accuracy 0.7790\n",
      "Epoch [143][200]\t Batch [450][550]\t Training Loss 0.7145\t Accuracy 0.7793\n",
      "Epoch [143][200]\t Batch [500][550]\t Training Loss 0.7137\t Accuracy 0.7792\n",
      "\n",
      "Epoch [143]\t Average training loss 0.7137\t Average training accuracy 0.7797\n",
      "Epoch [143]\t Average validation loss 0.6204\t Average validation accuracy 0.8184\n",
      "\n",
      "Epoch [144][200]\t Batch [0][550]\t Training Loss 0.7718\t Accuracy 0.7600\n",
      "Epoch [144][200]\t Batch [50][550]\t Training Loss 0.6817\t Accuracy 0.7900\n",
      "Epoch [144][200]\t Batch [100][550]\t Training Loss 0.6962\t Accuracy 0.7839\n",
      "Epoch [144][200]\t Batch [150][550]\t Training Loss 0.7103\t Accuracy 0.7801\n",
      "Epoch [144][200]\t Batch [200][550]\t Training Loss 0.7104\t Accuracy 0.7799\n",
      "Epoch [144][200]\t Batch [250][550]\t Training Loss 0.7090\t Accuracy 0.7801\n",
      "Epoch [144][200]\t Batch [300][550]\t Training Loss 0.7088\t Accuracy 0.7811\n",
      "Epoch [144][200]\t Batch [350][550]\t Training Loss 0.7111\t Accuracy 0.7810\n",
      "Epoch [144][200]\t Batch [400][550]\t Training Loss 0.7107\t Accuracy 0.7811\n",
      "Epoch [144][200]\t Batch [450][550]\t Training Loss 0.7099\t Accuracy 0.7815\n",
      "Epoch [144][200]\t Batch [500][550]\t Training Loss 0.7091\t Accuracy 0.7813\n",
      "\n",
      "Epoch [144]\t Average training loss 0.7092\t Average training accuracy 0.7818\n",
      "Epoch [144]\t Average validation loss 0.6156\t Average validation accuracy 0.8202\n",
      "\n",
      "Epoch [145][200]\t Batch [0][550]\t Training Loss 0.7672\t Accuracy 0.7600\n",
      "Epoch [145][200]\t Batch [50][550]\t Training Loss 0.6771\t Accuracy 0.7918\n",
      "Epoch [145][200]\t Batch [100][550]\t Training Loss 0.6918\t Accuracy 0.7864\n",
      "Epoch [145][200]\t Batch [150][550]\t Training Loss 0.7060\t Accuracy 0.7828\n",
      "Epoch [145][200]\t Batch [200][550]\t Training Loss 0.7059\t Accuracy 0.7822\n",
      "Epoch [145][200]\t Batch [250][550]\t Training Loss 0.7045\t Accuracy 0.7822\n",
      "Epoch [145][200]\t Batch [300][550]\t Training Loss 0.7044\t Accuracy 0.7833\n",
      "Epoch [145][200]\t Batch [350][550]\t Training Loss 0.7067\t Accuracy 0.7831\n",
      "Epoch [145][200]\t Batch [400][550]\t Training Loss 0.7063\t Accuracy 0.7832\n",
      "Epoch [145][200]\t Batch [450][550]\t Training Loss 0.7055\t Accuracy 0.7835\n",
      "Epoch [145][200]\t Batch [500][550]\t Training Loss 0.7048\t Accuracy 0.7831\n",
      "\n",
      "Epoch [145]\t Average training loss 0.7048\t Average training accuracy 0.7837\n",
      "Epoch [145]\t Average validation loss 0.6109\t Average validation accuracy 0.8218\n",
      "\n",
      "Epoch [146][200]\t Batch [0][550]\t Training Loss 0.7626\t Accuracy 0.7700\n",
      "Epoch [146][200]\t Batch [50][550]\t Training Loss 0.6725\t Accuracy 0.7941\n",
      "Epoch [146][200]\t Batch [100][550]\t Training Loss 0.6875\t Accuracy 0.7882\n",
      "Epoch [146][200]\t Batch [150][550]\t Training Loss 0.7017\t Accuracy 0.7846\n",
      "Epoch [146][200]\t Batch [200][550]\t Training Loss 0.7016\t Accuracy 0.7837\n",
      "Epoch [146][200]\t Batch [250][550]\t Training Loss 0.7002\t Accuracy 0.7838\n",
      "Epoch [146][200]\t Batch [300][550]\t Training Loss 0.7001\t Accuracy 0.7847\n",
      "Epoch [146][200]\t Batch [350][550]\t Training Loss 0.7024\t Accuracy 0.7848\n",
      "Epoch [146][200]\t Batch [400][550]\t Training Loss 0.7020\t Accuracy 0.7848\n",
      "Epoch [146][200]\t Batch [450][550]\t Training Loss 0.7012\t Accuracy 0.7851\n",
      "Epoch [146][200]\t Batch [500][550]\t Training Loss 0.7005\t Accuracy 0.7848\n",
      "\n",
      "Epoch [146]\t Average training loss 0.7005\t Average training accuracy 0.7855\n",
      "Epoch [146]\t Average validation loss 0.6064\t Average validation accuracy 0.8234\n",
      "\n",
      "Epoch [147][200]\t Batch [0][550]\t Training Loss 0.7581\t Accuracy 0.7700\n",
      "Epoch [147][200]\t Batch [50][550]\t Training Loss 0.6681\t Accuracy 0.7971\n",
      "Epoch [147][200]\t Batch [100][550]\t Training Loss 0.6834\t Accuracy 0.7913\n",
      "Epoch [147][200]\t Batch [150][550]\t Training Loss 0.6975\t Accuracy 0.7872\n",
      "Epoch [147][200]\t Batch [200][550]\t Training Loss 0.6974\t Accuracy 0.7863\n",
      "Epoch [147][200]\t Batch [250][550]\t Training Loss 0.6960\t Accuracy 0.7860\n",
      "Epoch [147][200]\t Batch [300][550]\t Training Loss 0.6960\t Accuracy 0.7869\n",
      "Epoch [147][200]\t Batch [350][550]\t Training Loss 0.6982\t Accuracy 0.7868\n",
      "Epoch [147][200]\t Batch [400][550]\t Training Loss 0.6978\t Accuracy 0.7868\n",
      "Epoch [147][200]\t Batch [450][550]\t Training Loss 0.6970\t Accuracy 0.7870\n",
      "Epoch [147][200]\t Batch [500][550]\t Training Loss 0.6963\t Accuracy 0.7866\n",
      "\n",
      "Epoch [147]\t Average training loss 0.6964\t Average training accuracy 0.7873\n",
      "Epoch [147]\t Average validation loss 0.6021\t Average validation accuracy 0.8248\n",
      "\n",
      "Epoch [148][200]\t Batch [0][550]\t Training Loss 0.7537\t Accuracy 0.7700\n",
      "Epoch [148][200]\t Batch [50][550]\t Training Loss 0.6638\t Accuracy 0.7990\n",
      "Epoch [148][200]\t Batch [100][550]\t Training Loss 0.6793\t Accuracy 0.7935\n",
      "Epoch [148][200]\t Batch [150][550]\t Training Loss 0.6934\t Accuracy 0.7891\n",
      "Epoch [148][200]\t Batch [200][550]\t Training Loss 0.6932\t Accuracy 0.7883\n",
      "Epoch [148][200]\t Batch [250][550]\t Training Loss 0.6919\t Accuracy 0.7878\n",
      "Epoch [148][200]\t Batch [300][550]\t Training Loss 0.6919\t Accuracy 0.7887\n",
      "Epoch [148][200]\t Batch [350][550]\t Training Loss 0.6942\t Accuracy 0.7886\n",
      "Epoch [148][200]\t Batch [400][550]\t Training Loss 0.6937\t Accuracy 0.7885\n",
      "Epoch [148][200]\t Batch [450][550]\t Training Loss 0.6929\t Accuracy 0.7887\n",
      "Epoch [148][200]\t Batch [500][550]\t Training Loss 0.6922\t Accuracy 0.7884\n",
      "\n",
      "Epoch [148]\t Average training loss 0.6923\t Average training accuracy 0.7889\n",
      "Epoch [148]\t Average validation loss 0.5978\t Average validation accuracy 0.8252\n",
      "\n",
      "Epoch [149][200]\t Batch [0][550]\t Training Loss 0.7494\t Accuracy 0.7700\n",
      "Epoch [149][200]\t Batch [50][550]\t Training Loss 0.6596\t Accuracy 0.8014\n",
      "Epoch [149][200]\t Batch [100][550]\t Training Loss 0.6753\t Accuracy 0.7956\n",
      "Epoch [149][200]\t Batch [150][550]\t Training Loss 0.6894\t Accuracy 0.7908\n",
      "Epoch [149][200]\t Batch [200][550]\t Training Loss 0.6892\t Accuracy 0.7902\n",
      "Epoch [149][200]\t Batch [250][550]\t Training Loss 0.6878\t Accuracy 0.7894\n",
      "Epoch [149][200]\t Batch [300][550]\t Training Loss 0.6879\t Accuracy 0.7903\n",
      "Epoch [149][200]\t Batch [350][550]\t Training Loss 0.6902\t Accuracy 0.7902\n",
      "Epoch [149][200]\t Batch [400][550]\t Training Loss 0.6897\t Accuracy 0.7903\n",
      "Epoch [149][200]\t Batch [450][550]\t Training Loss 0.6889\t Accuracy 0.7905\n",
      "Epoch [149][200]\t Batch [500][550]\t Training Loss 0.6882\t Accuracy 0.7901\n",
      "\n",
      "Epoch [149]\t Average training loss 0.6883\t Average training accuracy 0.7907\n",
      "Epoch [149]\t Average validation loss 0.5936\t Average validation accuracy 0.8270\n",
      "\n",
      "Epoch [150][200]\t Batch [0][550]\t Training Loss 0.7451\t Accuracy 0.7700\n",
      "Epoch [150][200]\t Batch [50][550]\t Training Loss 0.6555\t Accuracy 0.8031\n",
      "Epoch [150][200]\t Batch [100][550]\t Training Loss 0.6714\t Accuracy 0.7974\n",
      "Epoch [150][200]\t Batch [150][550]\t Training Loss 0.6855\t Accuracy 0.7928\n",
      "Epoch [150][200]\t Batch [200][550]\t Training Loss 0.6852\t Accuracy 0.7920\n",
      "Epoch [150][200]\t Batch [250][550]\t Training Loss 0.6839\t Accuracy 0.7910\n",
      "Epoch [150][200]\t Batch [300][550]\t Training Loss 0.6839\t Accuracy 0.7920\n",
      "Epoch [150][200]\t Batch [350][550]\t Training Loss 0.6862\t Accuracy 0.7918\n",
      "Epoch [150][200]\t Batch [400][550]\t Training Loss 0.6857\t Accuracy 0.7920\n",
      "Epoch [150][200]\t Batch [450][550]\t Training Loss 0.6849\t Accuracy 0.7920\n",
      "Epoch [150][200]\t Batch [500][550]\t Training Loss 0.6843\t Accuracy 0.7916\n",
      "\n",
      "Epoch [150]\t Average training loss 0.6844\t Average training accuracy 0.7920\n",
      "Epoch [150]\t Average validation loss 0.5895\t Average validation accuracy 0.8284\n",
      "\n",
      "Epoch [151][200]\t Batch [0][550]\t Training Loss 0.7409\t Accuracy 0.7700\n",
      "Epoch [151][200]\t Batch [50][550]\t Training Loss 0.6514\t Accuracy 0.8049\n",
      "Epoch [151][200]\t Batch [100][550]\t Training Loss 0.6676\t Accuracy 0.7992\n",
      "Epoch [151][200]\t Batch [150][550]\t Training Loss 0.6817\t Accuracy 0.7948\n",
      "Epoch [151][200]\t Batch [200][550]\t Training Loss 0.6813\t Accuracy 0.7942\n",
      "Epoch [151][200]\t Batch [250][550]\t Training Loss 0.6800\t Accuracy 0.7931\n",
      "Epoch [151][200]\t Batch [300][550]\t Training Loss 0.6801\t Accuracy 0.7942\n",
      "Epoch [151][200]\t Batch [350][550]\t Training Loss 0.6824\t Accuracy 0.7938\n",
      "Epoch [151][200]\t Batch [400][550]\t Training Loss 0.6819\t Accuracy 0.7939\n",
      "Epoch [151][200]\t Batch [450][550]\t Training Loss 0.6811\t Accuracy 0.7937\n",
      "Epoch [151][200]\t Batch [500][550]\t Training Loss 0.6805\t Accuracy 0.7934\n",
      "\n",
      "Epoch [151]\t Average training loss 0.6806\t Average training accuracy 0.7939\n",
      "Epoch [151]\t Average validation loss 0.5855\t Average validation accuracy 0.8312\n",
      "\n",
      "Epoch [152][200]\t Batch [0][550]\t Training Loss 0.7367\t Accuracy 0.7700\n",
      "Epoch [152][200]\t Batch [50][550]\t Training Loss 0.6475\t Accuracy 0.8071\n",
      "Epoch [152][200]\t Batch [100][550]\t Training Loss 0.6638\t Accuracy 0.8009\n",
      "Epoch [152][200]\t Batch [150][550]\t Training Loss 0.6779\t Accuracy 0.7966\n",
      "Epoch [152][200]\t Batch [200][550]\t Training Loss 0.6775\t Accuracy 0.7960\n",
      "Epoch [152][200]\t Batch [250][550]\t Training Loss 0.6762\t Accuracy 0.7951\n",
      "Epoch [152][200]\t Batch [300][550]\t Training Loss 0.6763\t Accuracy 0.7963\n",
      "Epoch [152][200]\t Batch [350][550]\t Training Loss 0.6786\t Accuracy 0.7959\n",
      "Epoch [152][200]\t Batch [400][550]\t Training Loss 0.6781\t Accuracy 0.7958\n",
      "Epoch [152][200]\t Batch [450][550]\t Training Loss 0.6773\t Accuracy 0.7957\n",
      "Epoch [152][200]\t Batch [500][550]\t Training Loss 0.6767\t Accuracy 0.7953\n",
      "\n",
      "Epoch [152]\t Average training loss 0.6768\t Average training accuracy 0.7958\n",
      "Epoch [152]\t Average validation loss 0.5816\t Average validation accuracy 0.8334\n",
      "\n",
      "Epoch [153][200]\t Batch [0][550]\t Training Loss 0.7325\t Accuracy 0.7700\n",
      "Epoch [153][200]\t Batch [50][550]\t Training Loss 0.6436\t Accuracy 0.8096\n",
      "Epoch [153][200]\t Batch [100][550]\t Training Loss 0.6601\t Accuracy 0.8037\n",
      "Epoch [153][200]\t Batch [150][550]\t Training Loss 0.6742\t Accuracy 0.7987\n",
      "Epoch [153][200]\t Batch [200][550]\t Training Loss 0.6737\t Accuracy 0.7978\n",
      "Epoch [153][200]\t Batch [250][550]\t Training Loss 0.6724\t Accuracy 0.7966\n",
      "Epoch [153][200]\t Batch [300][550]\t Training Loss 0.6726\t Accuracy 0.7976\n",
      "Epoch [153][200]\t Batch [350][550]\t Training Loss 0.6749\t Accuracy 0.7972\n",
      "Epoch [153][200]\t Batch [400][550]\t Training Loss 0.6744\t Accuracy 0.7972\n",
      "Epoch [153][200]\t Batch [450][550]\t Training Loss 0.6735\t Accuracy 0.7972\n",
      "Epoch [153][200]\t Batch [500][550]\t Training Loss 0.6729\t Accuracy 0.7969\n",
      "\n",
      "Epoch [153]\t Average training loss 0.6731\t Average training accuracy 0.7972\n",
      "Epoch [153]\t Average validation loss 0.5777\t Average validation accuracy 0.8346\n",
      "\n",
      "Epoch [154][200]\t Batch [0][550]\t Training Loss 0.7284\t Accuracy 0.7700\n",
      "Epoch [154][200]\t Batch [50][550]\t Training Loss 0.6398\t Accuracy 0.8120\n",
      "Epoch [154][200]\t Batch [100][550]\t Training Loss 0.6565\t Accuracy 0.8053\n",
      "Epoch [154][200]\t Batch [150][550]\t Training Loss 0.6705\t Accuracy 0.8009\n",
      "Epoch [154][200]\t Batch [200][550]\t Training Loss 0.6700\t Accuracy 0.8000\n",
      "Epoch [154][200]\t Batch [250][550]\t Training Loss 0.6687\t Accuracy 0.7987\n",
      "Epoch [154][200]\t Batch [300][550]\t Training Loss 0.6689\t Accuracy 0.7995\n",
      "Epoch [154][200]\t Batch [350][550]\t Training Loss 0.6712\t Accuracy 0.7993\n",
      "Epoch [154][200]\t Batch [400][550]\t Training Loss 0.6707\t Accuracy 0.7992\n",
      "Epoch [154][200]\t Batch [450][550]\t Training Loss 0.6698\t Accuracy 0.7992\n",
      "Epoch [154][200]\t Batch [500][550]\t Training Loss 0.6693\t Accuracy 0.7987\n",
      "\n",
      "Epoch [154]\t Average training loss 0.6694\t Average training accuracy 0.7990\n",
      "Epoch [154]\t Average validation loss 0.5738\t Average validation accuracy 0.8352\n",
      "\n",
      "Epoch [155][200]\t Batch [0][550]\t Training Loss 0.7243\t Accuracy 0.7700\n",
      "Epoch [155][200]\t Batch [50][550]\t Training Loss 0.6360\t Accuracy 0.8127\n",
      "Epoch [155][200]\t Batch [100][550]\t Training Loss 0.6528\t Accuracy 0.8065\n",
      "Epoch [155][200]\t Batch [150][550]\t Training Loss 0.6669\t Accuracy 0.8024\n",
      "Epoch [155][200]\t Batch [200][550]\t Training Loss 0.6663\t Accuracy 0.8013\n",
      "Epoch [155][200]\t Batch [250][550]\t Training Loss 0.6650\t Accuracy 0.8000\n",
      "Epoch [155][200]\t Batch [300][550]\t Training Loss 0.6653\t Accuracy 0.8009\n",
      "Epoch [155][200]\t Batch [350][550]\t Training Loss 0.6676\t Accuracy 0.8007\n",
      "Epoch [155][200]\t Batch [400][550]\t Training Loss 0.6670\t Accuracy 0.8007\n",
      "Epoch [155][200]\t Batch [450][550]\t Training Loss 0.6662\t Accuracy 0.8007\n",
      "Epoch [155][200]\t Batch [500][550]\t Training Loss 0.6656\t Accuracy 0.8002\n",
      "\n",
      "Epoch [155]\t Average training loss 0.6658\t Average training accuracy 0.8005\n",
      "Epoch [155]\t Average validation loss 0.5700\t Average validation accuracy 0.8366\n",
      "\n",
      "Epoch [156][200]\t Batch [0][550]\t Training Loss 0.7202\t Accuracy 0.7700\n",
      "Epoch [156][200]\t Batch [50][550]\t Training Loss 0.6323\t Accuracy 0.8143\n",
      "Epoch [156][200]\t Batch [100][550]\t Training Loss 0.6493\t Accuracy 0.8081\n",
      "Epoch [156][200]\t Batch [150][550]\t Training Loss 0.6633\t Accuracy 0.8040\n",
      "Epoch [156][200]\t Batch [200][550]\t Training Loss 0.6627\t Accuracy 0.8027\n",
      "Epoch [156][200]\t Batch [250][550]\t Training Loss 0.6614\t Accuracy 0.8014\n",
      "Epoch [156][200]\t Batch [300][550]\t Training Loss 0.6616\t Accuracy 0.8024\n",
      "Epoch [156][200]\t Batch [350][550]\t Training Loss 0.6640\t Accuracy 0.8023\n",
      "Epoch [156][200]\t Batch [400][550]\t Training Loss 0.6634\t Accuracy 0.8024\n",
      "Epoch [156][200]\t Batch [450][550]\t Training Loss 0.6625\t Accuracy 0.8024\n",
      "Epoch [156][200]\t Batch [500][550]\t Training Loss 0.6620\t Accuracy 0.8020\n",
      "\n",
      "Epoch [156]\t Average training loss 0.6622\t Average training accuracy 0.8021\n",
      "Epoch [156]\t Average validation loss 0.5663\t Average validation accuracy 0.8376\n",
      "\n",
      "Epoch [157][200]\t Batch [0][550]\t Training Loss 0.7161\t Accuracy 0.7700\n",
      "Epoch [157][200]\t Batch [50][550]\t Training Loss 0.6286\t Accuracy 0.8155\n",
      "Epoch [157][200]\t Batch [100][550]\t Training Loss 0.6457\t Accuracy 0.8095\n",
      "Epoch [157][200]\t Batch [150][550]\t Training Loss 0.6597\t Accuracy 0.8053\n",
      "Epoch [157][200]\t Batch [200][550]\t Training Loss 0.6590\t Accuracy 0.8045\n",
      "Epoch [157][200]\t Batch [250][550]\t Training Loss 0.6578\t Accuracy 0.8031\n",
      "Epoch [157][200]\t Batch [300][550]\t Training Loss 0.6580\t Accuracy 0.8040\n",
      "Epoch [157][200]\t Batch [350][550]\t Training Loss 0.6604\t Accuracy 0.8036\n",
      "Epoch [157][200]\t Batch [400][550]\t Training Loss 0.6598\t Accuracy 0.8038\n",
      "Epoch [157][200]\t Batch [450][550]\t Training Loss 0.6589\t Accuracy 0.8040\n",
      "Epoch [157][200]\t Batch [500][550]\t Training Loss 0.6584\t Accuracy 0.8035\n",
      "\n",
      "Epoch [157]\t Average training loss 0.6586\t Average training accuracy 0.8036\n",
      "Epoch [157]\t Average validation loss 0.5625\t Average validation accuracy 0.8392\n",
      "\n",
      "Epoch [158][200]\t Batch [0][550]\t Training Loss 0.7121\t Accuracy 0.7700\n",
      "Epoch [158][200]\t Batch [50][550]\t Training Loss 0.6249\t Accuracy 0.8161\n",
      "Epoch [158][200]\t Batch [100][550]\t Training Loss 0.6421\t Accuracy 0.8104\n",
      "Epoch [158][200]\t Batch [150][550]\t Training Loss 0.6562\t Accuracy 0.8062\n",
      "Epoch [158][200]\t Batch [200][550]\t Training Loss 0.6554\t Accuracy 0.8058\n",
      "Epoch [158][200]\t Batch [250][550]\t Training Loss 0.6542\t Accuracy 0.8043\n",
      "Epoch [158][200]\t Batch [300][550]\t Training Loss 0.6545\t Accuracy 0.8051\n",
      "Epoch [158][200]\t Batch [350][550]\t Training Loss 0.6568\t Accuracy 0.8049\n",
      "Epoch [158][200]\t Batch [400][550]\t Training Loss 0.6562\t Accuracy 0.8052\n",
      "Epoch [158][200]\t Batch [450][550]\t Training Loss 0.6553\t Accuracy 0.8053\n",
      "Epoch [158][200]\t Batch [500][550]\t Training Loss 0.6548\t Accuracy 0.8050\n",
      "\n",
      "Epoch [158]\t Average training loss 0.6550\t Average training accuracy 0.8051\n",
      "Epoch [158]\t Average validation loss 0.5588\t Average validation accuracy 0.8406\n",
      "\n",
      "Epoch [159][200]\t Batch [0][550]\t Training Loss 0.7080\t Accuracy 0.7700\n",
      "Epoch [159][200]\t Batch [50][550]\t Training Loss 0.6213\t Accuracy 0.8175\n",
      "Epoch [159][200]\t Batch [100][550]\t Training Loss 0.6386\t Accuracy 0.8117\n",
      "Epoch [159][200]\t Batch [150][550]\t Training Loss 0.6526\t Accuracy 0.8077\n",
      "Epoch [159][200]\t Batch [200][550]\t Training Loss 0.6518\t Accuracy 0.8073\n",
      "Epoch [159][200]\t Batch [250][550]\t Training Loss 0.6506\t Accuracy 0.8057\n",
      "Epoch [159][200]\t Batch [300][550]\t Training Loss 0.6509\t Accuracy 0.8066\n",
      "Epoch [159][200]\t Batch [350][550]\t Training Loss 0.6532\t Accuracy 0.8064\n",
      "Epoch [159][200]\t Batch [400][550]\t Training Loss 0.6526\t Accuracy 0.8069\n",
      "Epoch [159][200]\t Batch [450][550]\t Training Loss 0.6517\t Accuracy 0.8069\n",
      "Epoch [159][200]\t Batch [500][550]\t Training Loss 0.6512\t Accuracy 0.8065\n",
      "\n",
      "Epoch [159]\t Average training loss 0.6514\t Average training accuracy 0.8065\n",
      "Epoch [159]\t Average validation loss 0.5551\t Average validation accuracy 0.8420\n",
      "\n",
      "Epoch [160][200]\t Batch [0][550]\t Training Loss 0.7039\t Accuracy 0.7800\n",
      "Epoch [160][200]\t Batch [50][550]\t Training Loss 0.6176\t Accuracy 0.8192\n",
      "Epoch [160][200]\t Batch [100][550]\t Training Loss 0.6351\t Accuracy 0.8131\n",
      "Epoch [160][200]\t Batch [150][550]\t Training Loss 0.6490\t Accuracy 0.8091\n",
      "Epoch [160][200]\t Batch [200][550]\t Training Loss 0.6482\t Accuracy 0.8086\n",
      "Epoch [160][200]\t Batch [250][550]\t Training Loss 0.6470\t Accuracy 0.8071\n",
      "Epoch [160][200]\t Batch [300][550]\t Training Loss 0.6473\t Accuracy 0.8077\n",
      "Epoch [160][200]\t Batch [350][550]\t Training Loss 0.6496\t Accuracy 0.8077\n",
      "Epoch [160][200]\t Batch [400][550]\t Training Loss 0.6490\t Accuracy 0.8081\n",
      "Epoch [160][200]\t Batch [450][550]\t Training Loss 0.6481\t Accuracy 0.8082\n",
      "Epoch [160][200]\t Batch [500][550]\t Training Loss 0.6476\t Accuracy 0.8078\n",
      "\n",
      "Epoch [160]\t Average training loss 0.6478\t Average training accuracy 0.8079\n",
      "Epoch [160]\t Average validation loss 0.5514\t Average validation accuracy 0.8430\n",
      "\n",
      "Epoch [161][200]\t Batch [0][550]\t Training Loss 0.6998\t Accuracy 0.7800\n",
      "Epoch [161][200]\t Batch [50][550]\t Training Loss 0.6140\t Accuracy 0.8218\n",
      "Epoch [161][200]\t Batch [100][550]\t Training Loss 0.6315\t Accuracy 0.8151\n",
      "Epoch [161][200]\t Batch [150][550]\t Training Loss 0.6455\t Accuracy 0.8114\n",
      "Epoch [161][200]\t Batch [200][550]\t Training Loss 0.6445\t Accuracy 0.8105\n",
      "Epoch [161][200]\t Batch [250][550]\t Training Loss 0.6433\t Accuracy 0.8088\n",
      "Epoch [161][200]\t Batch [300][550]\t Training Loss 0.6437\t Accuracy 0.8095\n",
      "Epoch [161][200]\t Batch [350][550]\t Training Loss 0.6460\t Accuracy 0.8094\n",
      "Epoch [161][200]\t Batch [400][550]\t Training Loss 0.6454\t Accuracy 0.8099\n",
      "Epoch [161][200]\t Batch [450][550]\t Training Loss 0.6445\t Accuracy 0.8099\n",
      "Epoch [161][200]\t Batch [500][550]\t Training Loss 0.6440\t Accuracy 0.8095\n",
      "\n",
      "Epoch [161]\t Average training loss 0.6442\t Average training accuracy 0.8094\n",
      "Epoch [161]\t Average validation loss 0.5477\t Average validation accuracy 0.8448\n",
      "\n",
      "Epoch [162][200]\t Batch [0][550]\t Training Loss 0.6956\t Accuracy 0.7800\n",
      "Epoch [162][200]\t Batch [50][550]\t Training Loss 0.6104\t Accuracy 0.8231\n",
      "Epoch [162][200]\t Batch [100][550]\t Training Loss 0.6280\t Accuracy 0.8167\n",
      "Epoch [162][200]\t Batch [150][550]\t Training Loss 0.6419\t Accuracy 0.8128\n",
      "Epoch [162][200]\t Batch [200][550]\t Training Loss 0.6409\t Accuracy 0.8116\n",
      "Epoch [162][200]\t Batch [250][550]\t Training Loss 0.6397\t Accuracy 0.8099\n",
      "Epoch [162][200]\t Batch [300][550]\t Training Loss 0.6401\t Accuracy 0.8105\n",
      "Epoch [162][200]\t Batch [350][550]\t Training Loss 0.6424\t Accuracy 0.8105\n",
      "Epoch [162][200]\t Batch [400][550]\t Training Loss 0.6418\t Accuracy 0.8109\n",
      "Epoch [162][200]\t Batch [450][550]\t Training Loss 0.6409\t Accuracy 0.8108\n",
      "Epoch [162][200]\t Batch [500][550]\t Training Loss 0.6404\t Accuracy 0.8105\n",
      "\n",
      "Epoch [162]\t Average training loss 0.6406\t Average training accuracy 0.8104\n",
      "Epoch [162]\t Average validation loss 0.5439\t Average validation accuracy 0.8468\n",
      "\n",
      "Epoch [163][200]\t Batch [0][550]\t Training Loss 0.6914\t Accuracy 0.7800\n",
      "Epoch [163][200]\t Batch [50][550]\t Training Loss 0.6067\t Accuracy 0.8241\n",
      "Epoch [163][200]\t Batch [100][550]\t Training Loss 0.6244\t Accuracy 0.8179\n",
      "Epoch [163][200]\t Batch [150][550]\t Training Loss 0.6383\t Accuracy 0.8142\n",
      "Epoch [163][200]\t Batch [200][550]\t Training Loss 0.6372\t Accuracy 0.8132\n",
      "Epoch [163][200]\t Batch [250][550]\t Training Loss 0.6361\t Accuracy 0.8114\n",
      "Epoch [163][200]\t Batch [300][550]\t Training Loss 0.6365\t Accuracy 0.8121\n",
      "Epoch [163][200]\t Batch [350][550]\t Training Loss 0.6388\t Accuracy 0.8121\n",
      "Epoch [163][200]\t Batch [400][550]\t Training Loss 0.6381\t Accuracy 0.8123\n",
      "Epoch [163][200]\t Batch [450][550]\t Training Loss 0.6372\t Accuracy 0.8121\n",
      "Epoch [163][200]\t Batch [500][550]\t Training Loss 0.6368\t Accuracy 0.8117\n",
      "\n",
      "Epoch [163]\t Average training loss 0.6370\t Average training accuracy 0.8117\n",
      "Epoch [163]\t Average validation loss 0.5402\t Average validation accuracy 0.8478\n",
      "\n",
      "Epoch [164][200]\t Batch [0][550]\t Training Loss 0.6871\t Accuracy 0.7800\n",
      "Epoch [164][200]\t Batch [50][550]\t Training Loss 0.6030\t Accuracy 0.8251\n",
      "Epoch [164][200]\t Batch [100][550]\t Training Loss 0.6208\t Accuracy 0.8192\n",
      "Epoch [164][200]\t Batch [150][550]\t Training Loss 0.6346\t Accuracy 0.8153\n",
      "Epoch [164][200]\t Batch [200][550]\t Training Loss 0.6335\t Accuracy 0.8143\n",
      "Epoch [164][200]\t Batch [250][550]\t Training Loss 0.6324\t Accuracy 0.8124\n",
      "Epoch [164][200]\t Batch [300][550]\t Training Loss 0.6328\t Accuracy 0.8134\n",
      "Epoch [164][200]\t Batch [350][550]\t Training Loss 0.6352\t Accuracy 0.8134\n",
      "Epoch [164][200]\t Batch [400][550]\t Training Loss 0.6344\t Accuracy 0.8136\n",
      "Epoch [164][200]\t Batch [450][550]\t Training Loss 0.6335\t Accuracy 0.8134\n",
      "Epoch [164][200]\t Batch [500][550]\t Training Loss 0.6331\t Accuracy 0.8130\n",
      "\n",
      "Epoch [164]\t Average training loss 0.6333\t Average training accuracy 0.8129\n",
      "Epoch [164]\t Average validation loss 0.5364\t Average validation accuracy 0.8480\n",
      "\n",
      "Epoch [165][200]\t Batch [0][550]\t Training Loss 0.6828\t Accuracy 0.7800\n",
      "Epoch [165][200]\t Batch [50][550]\t Training Loss 0.5993\t Accuracy 0.8251\n",
      "Epoch [165][200]\t Batch [100][550]\t Training Loss 0.6172\t Accuracy 0.8198\n",
      "Epoch [165][200]\t Batch [150][550]\t Training Loss 0.6310\t Accuracy 0.8164\n",
      "Epoch [165][200]\t Batch [200][550]\t Training Loss 0.6298\t Accuracy 0.8157\n",
      "Epoch [165][200]\t Batch [250][550]\t Training Loss 0.6287\t Accuracy 0.8139\n",
      "Epoch [165][200]\t Batch [300][550]\t Training Loss 0.6291\t Accuracy 0.8149\n",
      "Epoch [165][200]\t Batch [350][550]\t Training Loss 0.6315\t Accuracy 0.8149\n",
      "Epoch [165][200]\t Batch [400][550]\t Training Loss 0.6307\t Accuracy 0.8151\n",
      "Epoch [165][200]\t Batch [450][550]\t Training Loss 0.6298\t Accuracy 0.8149\n",
      "Epoch [165][200]\t Batch [500][550]\t Training Loss 0.6294\t Accuracy 0.8144\n",
      "\n",
      "Epoch [165]\t Average training loss 0.6296\t Average training accuracy 0.8145\n",
      "Epoch [165]\t Average validation loss 0.5326\t Average validation accuracy 0.8472\n",
      "\n",
      "Epoch [166][200]\t Batch [0][550]\t Training Loss 0.6784\t Accuracy 0.7800\n",
      "Epoch [166][200]\t Batch [50][550]\t Training Loss 0.5956\t Accuracy 0.8275\n",
      "Epoch [166][200]\t Batch [100][550]\t Training Loss 0.6135\t Accuracy 0.8217\n",
      "Epoch [166][200]\t Batch [150][550]\t Training Loss 0.6272\t Accuracy 0.8180\n",
      "Epoch [166][200]\t Batch [200][550]\t Training Loss 0.6260\t Accuracy 0.8175\n",
      "Epoch [166][200]\t Batch [250][550]\t Training Loss 0.6249\t Accuracy 0.8156\n",
      "Epoch [166][200]\t Batch [300][550]\t Training Loss 0.6254\t Accuracy 0.8166\n",
      "Epoch [166][200]\t Batch [350][550]\t Training Loss 0.6277\t Accuracy 0.8167\n",
      "Epoch [166][200]\t Batch [400][550]\t Training Loss 0.6270\t Accuracy 0.8166\n",
      "Epoch [166][200]\t Batch [450][550]\t Training Loss 0.6260\t Accuracy 0.8166\n",
      "Epoch [166][200]\t Batch [500][550]\t Training Loss 0.6257\t Accuracy 0.8162\n",
      "\n",
      "Epoch [166]\t Average training loss 0.6259\t Average training accuracy 0.8162\n",
      "Epoch [166]\t Average validation loss 0.5288\t Average validation accuracy 0.8486\n",
      "\n",
      "Epoch [167][200]\t Batch [0][550]\t Training Loss 0.6739\t Accuracy 0.7800\n",
      "Epoch [167][200]\t Batch [50][550]\t Training Loss 0.5918\t Accuracy 0.8284\n",
      "Epoch [167][200]\t Batch [100][550]\t Training Loss 0.6097\t Accuracy 0.8225\n",
      "Epoch [167][200]\t Batch [150][550]\t Training Loss 0.6235\t Accuracy 0.8189\n",
      "Epoch [167][200]\t Batch [200][550]\t Training Loss 0.6222\t Accuracy 0.8185\n",
      "Epoch [167][200]\t Batch [250][550]\t Training Loss 0.6211\t Accuracy 0.8167\n",
      "Epoch [167][200]\t Batch [300][550]\t Training Loss 0.6216\t Accuracy 0.8178\n",
      "Epoch [167][200]\t Batch [350][550]\t Training Loss 0.6239\t Accuracy 0.8180\n",
      "Epoch [167][200]\t Batch [400][550]\t Training Loss 0.6232\t Accuracy 0.8179\n",
      "Epoch [167][200]\t Batch [450][550]\t Training Loss 0.6222\t Accuracy 0.8179\n",
      "Epoch [167][200]\t Batch [500][550]\t Training Loss 0.6219\t Accuracy 0.8175\n",
      "\n",
      "Epoch [167]\t Average training loss 0.6221\t Average training accuracy 0.8175\n",
      "Epoch [167]\t Average validation loss 0.5249\t Average validation accuracy 0.8500\n",
      "\n",
      "Epoch [168][200]\t Batch [0][550]\t Training Loss 0.6693\t Accuracy 0.7800\n",
      "Epoch [168][200]\t Batch [50][550]\t Training Loss 0.5880\t Accuracy 0.8300\n",
      "Epoch [168][200]\t Batch [100][550]\t Training Loss 0.6060\t Accuracy 0.8240\n",
      "Epoch [168][200]\t Batch [150][550]\t Training Loss 0.6197\t Accuracy 0.8203\n",
      "Epoch [168][200]\t Batch [200][550]\t Training Loss 0.6183\t Accuracy 0.8196\n",
      "Epoch [168][200]\t Batch [250][550]\t Training Loss 0.6172\t Accuracy 0.8177\n",
      "Epoch [168][200]\t Batch [300][550]\t Training Loss 0.6178\t Accuracy 0.8189\n",
      "Epoch [168][200]\t Batch [350][550]\t Training Loss 0.6201\t Accuracy 0.8191\n",
      "Epoch [168][200]\t Batch [400][550]\t Training Loss 0.6193\t Accuracy 0.8192\n",
      "Epoch [168][200]\t Batch [450][550]\t Training Loss 0.6184\t Accuracy 0.8191\n",
      "Epoch [168][200]\t Batch [500][550]\t Training Loss 0.6180\t Accuracy 0.8188\n",
      "\n",
      "Epoch [168]\t Average training loss 0.6183\t Average training accuracy 0.8188\n",
      "Epoch [168]\t Average validation loss 0.5210\t Average validation accuracy 0.8510\n",
      "\n",
      "Epoch [169][200]\t Batch [0][550]\t Training Loss 0.6645\t Accuracy 0.7800\n",
      "Epoch [169][200]\t Batch [50][550]\t Training Loss 0.5842\t Accuracy 0.8320\n",
      "Epoch [169][200]\t Batch [100][550]\t Training Loss 0.6021\t Accuracy 0.8254\n",
      "Epoch [169][200]\t Batch [150][550]\t Training Loss 0.6158\t Accuracy 0.8213\n",
      "Epoch [169][200]\t Batch [200][550]\t Training Loss 0.6144\t Accuracy 0.8207\n",
      "Epoch [169][200]\t Batch [250][550]\t Training Loss 0.6133\t Accuracy 0.8190\n",
      "Epoch [169][200]\t Batch [300][550]\t Training Loss 0.6139\t Accuracy 0.8202\n",
      "Epoch [169][200]\t Batch [350][550]\t Training Loss 0.6162\t Accuracy 0.8203\n",
      "Epoch [169][200]\t Batch [400][550]\t Training Loss 0.6154\t Accuracy 0.8205\n",
      "Epoch [169][200]\t Batch [450][550]\t Training Loss 0.6144\t Accuracy 0.8205\n",
      "Epoch [169][200]\t Batch [500][550]\t Training Loss 0.6141\t Accuracy 0.8202\n",
      "\n",
      "Epoch [169]\t Average training loss 0.6144\t Average training accuracy 0.8203\n",
      "Epoch [169]\t Average validation loss 0.5170\t Average validation accuracy 0.8524\n",
      "\n",
      "Epoch [170][200]\t Batch [0][550]\t Training Loss 0.6597\t Accuracy 0.7800\n",
      "Epoch [170][200]\t Batch [50][550]\t Training Loss 0.5803\t Accuracy 0.8337\n",
      "Epoch [170][200]\t Batch [100][550]\t Training Loss 0.5983\t Accuracy 0.8268\n",
      "Epoch [170][200]\t Batch [150][550]\t Training Loss 0.6119\t Accuracy 0.8230\n",
      "Epoch [170][200]\t Batch [200][550]\t Training Loss 0.6104\t Accuracy 0.8225\n",
      "Epoch [170][200]\t Batch [250][550]\t Training Loss 0.6093\t Accuracy 0.8208\n",
      "Epoch [170][200]\t Batch [300][550]\t Training Loss 0.6099\t Accuracy 0.8217\n",
      "Epoch [170][200]\t Batch [350][550]\t Training Loss 0.6122\t Accuracy 0.8219\n",
      "Epoch [170][200]\t Batch [400][550]\t Training Loss 0.6114\t Accuracy 0.8221\n",
      "Epoch [170][200]\t Batch [450][550]\t Training Loss 0.6105\t Accuracy 0.8221\n",
      "Epoch [170][200]\t Batch [500][550]\t Training Loss 0.6102\t Accuracy 0.8219\n",
      "\n",
      "Epoch [170]\t Average training loss 0.6104\t Average training accuracy 0.8218\n",
      "Epoch [170]\t Average validation loss 0.5130\t Average validation accuracy 0.8538\n",
      "\n",
      "Epoch [171][200]\t Batch [0][550]\t Training Loss 0.6547\t Accuracy 0.7800\n",
      "Epoch [171][200]\t Batch [50][550]\t Training Loss 0.5763\t Accuracy 0.8355\n",
      "Epoch [171][200]\t Batch [100][550]\t Training Loss 0.5943\t Accuracy 0.8288\n",
      "Epoch [171][200]\t Batch [150][550]\t Training Loss 0.6079\t Accuracy 0.8248\n",
      "Epoch [171][200]\t Batch [200][550]\t Training Loss 0.6063\t Accuracy 0.8243\n",
      "Epoch [171][200]\t Batch [250][550]\t Training Loss 0.6053\t Accuracy 0.8228\n",
      "Epoch [171][200]\t Batch [300][550]\t Training Loss 0.6059\t Accuracy 0.8239\n",
      "Epoch [171][200]\t Batch [350][550]\t Training Loss 0.6082\t Accuracy 0.8238\n",
      "Epoch [171][200]\t Batch [400][550]\t Training Loss 0.6074\t Accuracy 0.8240\n",
      "Epoch [171][200]\t Batch [450][550]\t Training Loss 0.6064\t Accuracy 0.8240\n",
      "Epoch [171][200]\t Batch [500][550]\t Training Loss 0.6061\t Accuracy 0.8237\n",
      "\n",
      "Epoch [171]\t Average training loss 0.6064\t Average training accuracy 0.8237\n",
      "Epoch [171]\t Average validation loss 0.5089\t Average validation accuracy 0.8558\n",
      "\n",
      "Epoch [172][200]\t Batch [0][550]\t Training Loss 0.6495\t Accuracy 0.7900\n",
      "Epoch [172][200]\t Batch [50][550]\t Training Loss 0.5723\t Accuracy 0.8367\n",
      "Epoch [172][200]\t Batch [100][550]\t Training Loss 0.5903\t Accuracy 0.8306\n",
      "Epoch [172][200]\t Batch [150][550]\t Training Loss 0.6038\t Accuracy 0.8268\n",
      "Epoch [172][200]\t Batch [200][550]\t Training Loss 0.6022\t Accuracy 0.8261\n",
      "Epoch [172][200]\t Batch [250][550]\t Training Loss 0.6011\t Accuracy 0.8247\n",
      "Epoch [172][200]\t Batch [300][550]\t Training Loss 0.6018\t Accuracy 0.8256\n",
      "Epoch [172][200]\t Batch [350][550]\t Training Loss 0.6041\t Accuracy 0.8258\n",
      "Epoch [172][200]\t Batch [400][550]\t Training Loss 0.6033\t Accuracy 0.8258\n",
      "Epoch [172][200]\t Batch [450][550]\t Training Loss 0.6023\t Accuracy 0.8258\n",
      "Epoch [172][200]\t Batch [500][550]\t Training Loss 0.6020\t Accuracy 0.8255\n",
      "\n",
      "Epoch [172]\t Average training loss 0.6023\t Average training accuracy 0.8254\n",
      "Epoch [172]\t Average validation loss 0.5047\t Average validation accuracy 0.8568\n",
      "\n",
      "Epoch [173][200]\t Batch [0][550]\t Training Loss 0.6443\t Accuracy 0.7900\n",
      "Epoch [173][200]\t Batch [50][550]\t Training Loss 0.5682\t Accuracy 0.8384\n",
      "Epoch [173][200]\t Batch [100][550]\t Training Loss 0.5862\t Accuracy 0.8323\n",
      "Epoch [173][200]\t Batch [150][550]\t Training Loss 0.5997\t Accuracy 0.8286\n",
      "Epoch [173][200]\t Batch [200][550]\t Training Loss 0.5980\t Accuracy 0.8277\n",
      "Epoch [173][200]\t Batch [250][550]\t Training Loss 0.5970\t Accuracy 0.8263\n",
      "Epoch [173][200]\t Batch [300][550]\t Training Loss 0.5976\t Accuracy 0.8271\n",
      "Epoch [173][200]\t Batch [350][550]\t Training Loss 0.5999\t Accuracy 0.8272\n",
      "Epoch [173][200]\t Batch [400][550]\t Training Loss 0.5991\t Accuracy 0.8273\n",
      "Epoch [173][200]\t Batch [450][550]\t Training Loss 0.5982\t Accuracy 0.8273\n",
      "Epoch [173][200]\t Batch [500][550]\t Training Loss 0.5979\t Accuracy 0.8269\n",
      "\n",
      "Epoch [173]\t Average training loss 0.5981\t Average training accuracy 0.8269\n",
      "Epoch [173]\t Average validation loss 0.5005\t Average validation accuracy 0.8582\n",
      "\n",
      "Epoch [174][200]\t Batch [0][550]\t Training Loss 0.6389\t Accuracy 0.7900\n",
      "Epoch [174][200]\t Batch [50][550]\t Training Loss 0.5640\t Accuracy 0.8392\n",
      "Epoch [174][200]\t Batch [100][550]\t Training Loss 0.5821\t Accuracy 0.8340\n",
      "Epoch [174][200]\t Batch [150][550]\t Training Loss 0.5955\t Accuracy 0.8309\n",
      "Epoch [174][200]\t Batch [200][550]\t Training Loss 0.5937\t Accuracy 0.8301\n",
      "Epoch [174][200]\t Batch [250][550]\t Training Loss 0.5927\t Accuracy 0.8285\n",
      "Epoch [174][200]\t Batch [300][550]\t Training Loss 0.5934\t Accuracy 0.8291\n",
      "Epoch [174][200]\t Batch [350][550]\t Training Loss 0.5957\t Accuracy 0.8293\n",
      "Epoch [174][200]\t Batch [400][550]\t Training Loss 0.5949\t Accuracy 0.8293\n",
      "Epoch [174][200]\t Batch [450][550]\t Training Loss 0.5939\t Accuracy 0.8293\n",
      "Epoch [174][200]\t Batch [500][550]\t Training Loss 0.5937\t Accuracy 0.8289\n",
      "\n",
      "Epoch [174]\t Average training loss 0.5939\t Average training accuracy 0.8288\n",
      "Epoch [174]\t Average validation loss 0.4963\t Average validation accuracy 0.8600\n",
      "\n",
      "Epoch [175][200]\t Batch [0][550]\t Training Loss 0.6333\t Accuracy 0.7900\n",
      "Epoch [175][200]\t Batch [50][550]\t Training Loss 0.5598\t Accuracy 0.8410\n",
      "Epoch [175][200]\t Batch [100][550]\t Training Loss 0.5778\t Accuracy 0.8360\n",
      "Epoch [175][200]\t Batch [150][550]\t Training Loss 0.5912\t Accuracy 0.8327\n",
      "Epoch [175][200]\t Batch [200][550]\t Training Loss 0.5894\t Accuracy 0.8321\n",
      "Epoch [175][200]\t Batch [250][550]\t Training Loss 0.5884\t Accuracy 0.8306\n",
      "Epoch [175][200]\t Batch [300][550]\t Training Loss 0.5891\t Accuracy 0.8311\n",
      "Epoch [175][200]\t Batch [350][550]\t Training Loss 0.5914\t Accuracy 0.8313\n",
      "Epoch [175][200]\t Batch [400][550]\t Training Loss 0.5906\t Accuracy 0.8313\n",
      "Epoch [175][200]\t Batch [450][550]\t Training Loss 0.5896\t Accuracy 0.8312\n",
      "Epoch [175][200]\t Batch [500][550]\t Training Loss 0.5894\t Accuracy 0.8308\n",
      "\n",
      "Epoch [175]\t Average training loss 0.5896\t Average training accuracy 0.8307\n",
      "Epoch [175]\t Average validation loss 0.4920\t Average validation accuracy 0.8606\n",
      "\n",
      "Epoch [176][200]\t Batch [0][550]\t Training Loss 0.6275\t Accuracy 0.7900\n",
      "Epoch [176][200]\t Batch [50][550]\t Training Loss 0.5555\t Accuracy 0.8445\n",
      "Epoch [176][200]\t Batch [100][550]\t Training Loss 0.5735\t Accuracy 0.8383\n",
      "Epoch [176][200]\t Batch [150][550]\t Training Loss 0.5868\t Accuracy 0.8347\n",
      "Epoch [176][200]\t Batch [200][550]\t Training Loss 0.5850\t Accuracy 0.8343\n",
      "Epoch [176][200]\t Batch [250][550]\t Training Loss 0.5840\t Accuracy 0.8325\n",
      "Epoch [176][200]\t Batch [300][550]\t Training Loss 0.5847\t Accuracy 0.8330\n",
      "Epoch [176][200]\t Batch [350][550]\t Training Loss 0.5871\t Accuracy 0.8332\n",
      "Epoch [176][200]\t Batch [400][550]\t Training Loss 0.5862\t Accuracy 0.8332\n",
      "Epoch [176][200]\t Batch [450][550]\t Training Loss 0.5853\t Accuracy 0.8330\n",
      "Epoch [176][200]\t Batch [500][550]\t Training Loss 0.5850\t Accuracy 0.8324\n",
      "\n",
      "Epoch [176]\t Average training loss 0.5853\t Average training accuracy 0.8324\n",
      "Epoch [176]\t Average validation loss 0.4876\t Average validation accuracy 0.8630\n",
      "\n",
      "Epoch [177][200]\t Batch [0][550]\t Training Loss 0.6217\t Accuracy 0.7900\n",
      "Epoch [177][200]\t Batch [50][550]\t Training Loss 0.5512\t Accuracy 0.8455\n",
      "Epoch [177][200]\t Batch [100][550]\t Training Loss 0.5692\t Accuracy 0.8398\n",
      "Epoch [177][200]\t Batch [150][550]\t Training Loss 0.5824\t Accuracy 0.8362\n",
      "Epoch [177][200]\t Batch [200][550]\t Training Loss 0.5805\t Accuracy 0.8361\n",
      "Epoch [177][200]\t Batch [250][550]\t Training Loss 0.5795\t Accuracy 0.8343\n",
      "Epoch [177][200]\t Batch [300][550]\t Training Loss 0.5803\t Accuracy 0.8347\n",
      "Epoch [177][200]\t Batch [350][550]\t Training Loss 0.5826\t Accuracy 0.8349\n",
      "Epoch [177][200]\t Batch [400][550]\t Training Loss 0.5818\t Accuracy 0.8350\n",
      "Epoch [177][200]\t Batch [450][550]\t Training Loss 0.5808\t Accuracy 0.8348\n",
      "Epoch [177][200]\t Batch [500][550]\t Training Loss 0.5806\t Accuracy 0.8342\n",
      "\n",
      "Epoch [177]\t Average training loss 0.5809\t Average training accuracy 0.8341\n",
      "Epoch [177]\t Average validation loss 0.4832\t Average validation accuracy 0.8652\n",
      "\n",
      "Epoch [178][200]\t Batch [0][550]\t Training Loss 0.6156\t Accuracy 0.7900\n",
      "Epoch [178][200]\t Batch [50][550]\t Training Loss 0.5468\t Accuracy 0.8476\n",
      "Epoch [178][200]\t Batch [100][550]\t Training Loss 0.5648\t Accuracy 0.8418\n",
      "Epoch [178][200]\t Batch [150][550]\t Training Loss 0.5780\t Accuracy 0.8377\n",
      "Epoch [178][200]\t Batch [200][550]\t Training Loss 0.5760\t Accuracy 0.8377\n",
      "Epoch [178][200]\t Batch [250][550]\t Training Loss 0.5750\t Accuracy 0.8360\n",
      "Epoch [178][200]\t Batch [300][550]\t Training Loss 0.5758\t Accuracy 0.8363\n",
      "Epoch [178][200]\t Batch [350][550]\t Training Loss 0.5781\t Accuracy 0.8365\n",
      "Epoch [178][200]\t Batch [400][550]\t Training Loss 0.5773\t Accuracy 0.8366\n",
      "Epoch [178][200]\t Batch [450][550]\t Training Loss 0.5763\t Accuracy 0.8363\n",
      "Epoch [178][200]\t Batch [500][550]\t Training Loss 0.5761\t Accuracy 0.8358\n",
      "\n",
      "Epoch [178]\t Average training loss 0.5764\t Average training accuracy 0.8357\n",
      "Epoch [178]\t Average validation loss 0.4787\t Average validation accuracy 0.8672\n",
      "\n",
      "Epoch [179][200]\t Batch [0][550]\t Training Loss 0.6095\t Accuracy 0.7800\n",
      "Epoch [179][200]\t Batch [50][550]\t Training Loss 0.5424\t Accuracy 0.8502\n",
      "Epoch [179][200]\t Batch [100][550]\t Training Loss 0.5603\t Accuracy 0.8438\n",
      "Epoch [179][200]\t Batch [150][550]\t Training Loss 0.5735\t Accuracy 0.8398\n",
      "Epoch [179][200]\t Batch [200][550]\t Training Loss 0.5714\t Accuracy 0.8398\n",
      "Epoch [179][200]\t Batch [250][550]\t Training Loss 0.5705\t Accuracy 0.8381\n",
      "Epoch [179][200]\t Batch [300][550]\t Training Loss 0.5713\t Accuracy 0.8385\n",
      "Epoch [179][200]\t Batch [350][550]\t Training Loss 0.5736\t Accuracy 0.8385\n",
      "Epoch [179][200]\t Batch [400][550]\t Training Loss 0.5728\t Accuracy 0.8385\n",
      "Epoch [179][200]\t Batch [450][550]\t Training Loss 0.5718\t Accuracy 0.8383\n",
      "Epoch [179][200]\t Batch [500][550]\t Training Loss 0.5716\t Accuracy 0.8377\n",
      "\n",
      "Epoch [179]\t Average training loss 0.5719\t Average training accuracy 0.8376\n",
      "Epoch [179]\t Average validation loss 0.4742\t Average validation accuracy 0.8692\n",
      "\n",
      "Epoch [180][200]\t Batch [0][550]\t Training Loss 0.6032\t Accuracy 0.7900\n",
      "Epoch [180][200]\t Batch [50][550]\t Training Loss 0.5380\t Accuracy 0.8512\n",
      "Epoch [180][200]\t Batch [100][550]\t Training Loss 0.5558\t Accuracy 0.8450\n",
      "Epoch [180][200]\t Batch [150][550]\t Training Loss 0.5689\t Accuracy 0.8412\n",
      "Epoch [180][200]\t Batch [200][550]\t Training Loss 0.5668\t Accuracy 0.8412\n",
      "Epoch [180][200]\t Batch [250][550]\t Training Loss 0.5658\t Accuracy 0.8394\n",
      "Epoch [180][200]\t Batch [300][550]\t Training Loss 0.5667\t Accuracy 0.8397\n",
      "Epoch [180][200]\t Batch [350][550]\t Training Loss 0.5690\t Accuracy 0.8397\n",
      "Epoch [180][200]\t Batch [400][550]\t Training Loss 0.5682\t Accuracy 0.8396\n",
      "Epoch [180][200]\t Batch [450][550]\t Training Loss 0.5672\t Accuracy 0.8395\n",
      "Epoch [180][200]\t Batch [500][550]\t Training Loss 0.5671\t Accuracy 0.8389\n",
      "\n",
      "Epoch [180]\t Average training loss 0.5673\t Average training accuracy 0.8387\n",
      "Epoch [180]\t Average validation loss 0.4697\t Average validation accuracy 0.8712\n",
      "\n",
      "Epoch [181][200]\t Batch [0][550]\t Training Loss 0.5968\t Accuracy 0.7900\n",
      "Epoch [181][200]\t Batch [50][550]\t Training Loss 0.5335\t Accuracy 0.8541\n",
      "Epoch [181][200]\t Batch [100][550]\t Training Loss 0.5513\t Accuracy 0.8474\n",
      "Epoch [181][200]\t Batch [150][550]\t Training Loss 0.5643\t Accuracy 0.8430\n",
      "Epoch [181][200]\t Batch [200][550]\t Training Loss 0.5621\t Accuracy 0.8427\n",
      "Epoch [181][200]\t Batch [250][550]\t Training Loss 0.5612\t Accuracy 0.8410\n",
      "Epoch [181][200]\t Batch [300][550]\t Training Loss 0.5621\t Accuracy 0.8414\n",
      "Epoch [181][200]\t Batch [350][550]\t Training Loss 0.5644\t Accuracy 0.8413\n",
      "Epoch [181][200]\t Batch [400][550]\t Training Loss 0.5635\t Accuracy 0.8410\n",
      "Epoch [181][200]\t Batch [450][550]\t Training Loss 0.5626\t Accuracy 0.8409\n",
      "Epoch [181][200]\t Batch [500][550]\t Training Loss 0.5625\t Accuracy 0.8403\n",
      "\n",
      "Epoch [181]\t Average training loss 0.5627\t Average training accuracy 0.8402\n",
      "Epoch [181]\t Average validation loss 0.4651\t Average validation accuracy 0.8724\n",
      "\n",
      "Epoch [182][200]\t Batch [0][550]\t Training Loss 0.5903\t Accuracy 0.8000\n",
      "Epoch [182][200]\t Batch [50][550]\t Training Loss 0.5289\t Accuracy 0.8571\n",
      "Epoch [182][200]\t Batch [100][550]\t Training Loss 0.5467\t Accuracy 0.8496\n",
      "Epoch [182][200]\t Batch [150][550]\t Training Loss 0.5596\t Accuracy 0.8451\n",
      "Epoch [182][200]\t Batch [200][550]\t Training Loss 0.5574\t Accuracy 0.8446\n",
      "Epoch [182][200]\t Batch [250][550]\t Training Loss 0.5565\t Accuracy 0.8427\n",
      "Epoch [182][200]\t Batch [300][550]\t Training Loss 0.5574\t Accuracy 0.8430\n",
      "Epoch [182][200]\t Batch [350][550]\t Training Loss 0.5597\t Accuracy 0.8428\n",
      "Epoch [182][200]\t Batch [400][550]\t Training Loss 0.5589\t Accuracy 0.8425\n",
      "Epoch [182][200]\t Batch [450][550]\t Training Loss 0.5580\t Accuracy 0.8424\n",
      "Epoch [182][200]\t Batch [500][550]\t Training Loss 0.5578\t Accuracy 0.8418\n",
      "\n",
      "Epoch [182]\t Average training loss 0.5581\t Average training accuracy 0.8418\n",
      "Epoch [182]\t Average validation loss 0.4605\t Average validation accuracy 0.8738\n",
      "\n",
      "Epoch [183][200]\t Batch [0][550]\t Training Loss 0.5838\t Accuracy 0.8000\n",
      "Epoch [183][200]\t Batch [50][550]\t Training Loss 0.5244\t Accuracy 0.8582\n",
      "Epoch [183][200]\t Batch [100][550]\t Training Loss 0.5421\t Accuracy 0.8512\n",
      "Epoch [183][200]\t Batch [150][550]\t Training Loss 0.5549\t Accuracy 0.8466\n",
      "Epoch [183][200]\t Batch [200][550]\t Training Loss 0.5527\t Accuracy 0.8460\n",
      "Epoch [183][200]\t Batch [250][550]\t Training Loss 0.5517\t Accuracy 0.8443\n",
      "Epoch [183][200]\t Batch [300][550]\t Training Loss 0.5527\t Accuracy 0.8446\n",
      "Epoch [183][200]\t Batch [350][550]\t Training Loss 0.5550\t Accuracy 0.8446\n",
      "Epoch [183][200]\t Batch [400][550]\t Training Loss 0.5542\t Accuracy 0.8445\n",
      "Epoch [183][200]\t Batch [450][550]\t Training Loss 0.5533\t Accuracy 0.8445\n",
      "Epoch [183][200]\t Batch [500][550]\t Training Loss 0.5531\t Accuracy 0.8439\n",
      "\n",
      "Epoch [183]\t Average training loss 0.5534\t Average training accuracy 0.8438\n",
      "Epoch [183]\t Average validation loss 0.4559\t Average validation accuracy 0.8752\n",
      "\n",
      "Epoch [184][200]\t Batch [0][550]\t Training Loss 0.5772\t Accuracy 0.8000\n",
      "Epoch [184][200]\t Batch [50][550]\t Training Loss 0.5198\t Accuracy 0.8600\n",
      "Epoch [184][200]\t Batch [100][550]\t Training Loss 0.5374\t Accuracy 0.8531\n",
      "Epoch [184][200]\t Batch [150][550]\t Training Loss 0.5502\t Accuracy 0.8485\n",
      "Epoch [184][200]\t Batch [200][550]\t Training Loss 0.5479\t Accuracy 0.8477\n",
      "Epoch [184][200]\t Batch [250][550]\t Training Loss 0.5470\t Accuracy 0.8463\n",
      "Epoch [184][200]\t Batch [300][550]\t Training Loss 0.5479\t Accuracy 0.8464\n",
      "Epoch [184][200]\t Batch [350][550]\t Training Loss 0.5503\t Accuracy 0.8466\n",
      "Epoch [184][200]\t Batch [400][550]\t Training Loss 0.5495\t Accuracy 0.8466\n",
      "Epoch [184][200]\t Batch [450][550]\t Training Loss 0.5486\t Accuracy 0.8466\n",
      "Epoch [184][200]\t Batch [500][550]\t Training Loss 0.5485\t Accuracy 0.8459\n",
      "\n",
      "Epoch [184]\t Average training loss 0.5488\t Average training accuracy 0.8458\n",
      "Epoch [184]\t Average validation loss 0.4513\t Average validation accuracy 0.8770\n",
      "\n",
      "Epoch [185][200]\t Batch [0][550]\t Training Loss 0.5705\t Accuracy 0.8100\n",
      "Epoch [185][200]\t Batch [50][550]\t Training Loss 0.5152\t Accuracy 0.8608\n",
      "Epoch [185][200]\t Batch [100][550]\t Training Loss 0.5328\t Accuracy 0.8542\n",
      "Epoch [185][200]\t Batch [150][550]\t Training Loss 0.5455\t Accuracy 0.8500\n",
      "Epoch [185][200]\t Batch [200][550]\t Training Loss 0.5431\t Accuracy 0.8492\n",
      "Epoch [185][200]\t Batch [250][550]\t Training Loss 0.5422\t Accuracy 0.8478\n",
      "Epoch [185][200]\t Batch [300][550]\t Training Loss 0.5432\t Accuracy 0.8481\n",
      "Epoch [185][200]\t Batch [350][550]\t Training Loss 0.5455\t Accuracy 0.8483\n",
      "Epoch [185][200]\t Batch [400][550]\t Training Loss 0.5447\t Accuracy 0.8485\n",
      "Epoch [185][200]\t Batch [450][550]\t Training Loss 0.5438\t Accuracy 0.8485\n",
      "Epoch [185][200]\t Batch [500][550]\t Training Loss 0.5437\t Accuracy 0.8477\n",
      "\n",
      "Epoch [185]\t Average training loss 0.5441\t Average training accuracy 0.8475\n",
      "Epoch [185]\t Average validation loss 0.4467\t Average validation accuracy 0.8782\n",
      "\n",
      "Epoch [186][200]\t Batch [0][550]\t Training Loss 0.5638\t Accuracy 0.8200\n",
      "Epoch [186][200]\t Batch [50][550]\t Training Loss 0.5107\t Accuracy 0.8624\n",
      "Epoch [186][200]\t Batch [100][550]\t Training Loss 0.5281\t Accuracy 0.8563\n",
      "Epoch [186][200]\t Batch [150][550]\t Training Loss 0.5407\t Accuracy 0.8519\n",
      "Epoch [186][200]\t Batch [200][550]\t Training Loss 0.5383\t Accuracy 0.8510\n",
      "Epoch [186][200]\t Batch [250][550]\t Training Loss 0.5374\t Accuracy 0.8498\n",
      "Epoch [186][200]\t Batch [300][550]\t Training Loss 0.5384\t Accuracy 0.8501\n",
      "Epoch [186][200]\t Batch [350][550]\t Training Loss 0.5408\t Accuracy 0.8503\n",
      "Epoch [186][200]\t Batch [400][550]\t Training Loss 0.5400\t Accuracy 0.8503\n",
      "Epoch [186][200]\t Batch [450][550]\t Training Loss 0.5391\t Accuracy 0.8503\n",
      "Epoch [186][200]\t Batch [500][550]\t Training Loss 0.5390\t Accuracy 0.8495\n",
      "\n",
      "Epoch [186]\t Average training loss 0.5393\t Average training accuracy 0.8492\n",
      "Epoch [186]\t Average validation loss 0.4421\t Average validation accuracy 0.8784\n",
      "\n",
      "Epoch [187][200]\t Batch [0][550]\t Training Loss 0.5571\t Accuracy 0.8200\n",
      "Epoch [187][200]\t Batch [50][550]\t Training Loss 0.5061\t Accuracy 0.8639\n",
      "Epoch [187][200]\t Batch [100][550]\t Training Loss 0.5234\t Accuracy 0.8583\n",
      "Epoch [187][200]\t Batch [150][550]\t Training Loss 0.5360\t Accuracy 0.8536\n",
      "Epoch [187][200]\t Batch [200][550]\t Training Loss 0.5335\t Accuracy 0.8527\n",
      "Epoch [187][200]\t Batch [250][550]\t Training Loss 0.5326\t Accuracy 0.8516\n",
      "Epoch [187][200]\t Batch [300][550]\t Training Loss 0.5336\t Accuracy 0.8517\n",
      "Epoch [187][200]\t Batch [350][550]\t Training Loss 0.5360\t Accuracy 0.8518\n",
      "Epoch [187][200]\t Batch [400][550]\t Training Loss 0.5353\t Accuracy 0.8519\n",
      "Epoch [187][200]\t Batch [450][550]\t Training Loss 0.5344\t Accuracy 0.8520\n",
      "Epoch [187][200]\t Batch [500][550]\t Training Loss 0.5343\t Accuracy 0.8510\n",
      "\n",
      "Epoch [187]\t Average training loss 0.5347\t Average training accuracy 0.8507\n",
      "Epoch [187]\t Average validation loss 0.4376\t Average validation accuracy 0.8802\n",
      "\n",
      "Epoch [188][200]\t Batch [0][550]\t Training Loss 0.5504\t Accuracy 0.8200\n",
      "Epoch [188][200]\t Batch [50][550]\t Training Loss 0.5015\t Accuracy 0.8645\n",
      "Epoch [188][200]\t Batch [100][550]\t Training Loss 0.5188\t Accuracy 0.8594\n",
      "Epoch [188][200]\t Batch [150][550]\t Training Loss 0.5312\t Accuracy 0.8548\n",
      "Epoch [188][200]\t Batch [200][550]\t Training Loss 0.5288\t Accuracy 0.8542\n",
      "Epoch [188][200]\t Batch [250][550]\t Training Loss 0.5279\t Accuracy 0.8530\n",
      "Epoch [188][200]\t Batch [300][550]\t Training Loss 0.5289\t Accuracy 0.8530\n",
      "Epoch [188][200]\t Batch [350][550]\t Training Loss 0.5313\t Accuracy 0.8532\n",
      "Epoch [188][200]\t Batch [400][550]\t Training Loss 0.5306\t Accuracy 0.8535\n",
      "Epoch [188][200]\t Batch [450][550]\t Training Loss 0.5297\t Accuracy 0.8535\n",
      "Epoch [188][200]\t Batch [500][550]\t Training Loss 0.5296\t Accuracy 0.8527\n",
      "\n",
      "Epoch [188]\t Average training loss 0.5300\t Average training accuracy 0.8524\n",
      "Epoch [188]\t Average validation loss 0.4330\t Average validation accuracy 0.8810\n",
      "\n",
      "Epoch [189][200]\t Batch [0][550]\t Training Loss 0.5436\t Accuracy 0.8300\n",
      "Epoch [189][200]\t Batch [50][550]\t Training Loss 0.4970\t Accuracy 0.8676\n",
      "Epoch [189][200]\t Batch [100][550]\t Training Loss 0.5141\t Accuracy 0.8619\n",
      "Epoch [189][200]\t Batch [150][550]\t Training Loss 0.5265\t Accuracy 0.8572\n",
      "Epoch [189][200]\t Batch [200][550]\t Training Loss 0.5240\t Accuracy 0.8562\n",
      "Epoch [189][200]\t Batch [250][550]\t Training Loss 0.5231\t Accuracy 0.8549\n",
      "Epoch [189][200]\t Batch [300][550]\t Training Loss 0.5242\t Accuracy 0.8548\n",
      "Epoch [189][200]\t Batch [350][550]\t Training Loss 0.5266\t Accuracy 0.8550\n",
      "Epoch [189][200]\t Batch [400][550]\t Training Loss 0.5259\t Accuracy 0.8552\n",
      "Epoch [189][200]\t Batch [450][550]\t Training Loss 0.5250\t Accuracy 0.8553\n",
      "Epoch [189][200]\t Batch [500][550]\t Training Loss 0.5250\t Accuracy 0.8546\n",
      "\n",
      "Epoch [189]\t Average training loss 0.5253\t Average training accuracy 0.8541\n",
      "Epoch [189]\t Average validation loss 0.4285\t Average validation accuracy 0.8826\n",
      "\n",
      "Epoch [190][200]\t Batch [0][550]\t Training Loss 0.5370\t Accuracy 0.8300\n",
      "Epoch [190][200]\t Batch [50][550]\t Training Loss 0.4925\t Accuracy 0.8690\n",
      "Epoch [190][200]\t Batch [100][550]\t Training Loss 0.5095\t Accuracy 0.8631\n",
      "Epoch [190][200]\t Batch [150][550]\t Training Loss 0.5218\t Accuracy 0.8588\n",
      "Epoch [190][200]\t Batch [200][550]\t Training Loss 0.5193\t Accuracy 0.8578\n",
      "Epoch [190][200]\t Batch [250][550]\t Training Loss 0.5184\t Accuracy 0.8566\n",
      "Epoch [190][200]\t Batch [300][550]\t Training Loss 0.5195\t Accuracy 0.8565\n",
      "Epoch [190][200]\t Batch [350][550]\t Training Loss 0.5219\t Accuracy 0.8568\n",
      "Epoch [190][200]\t Batch [400][550]\t Training Loss 0.5212\t Accuracy 0.8569\n",
      "Epoch [190][200]\t Batch [450][550]\t Training Loss 0.5203\t Accuracy 0.8570\n",
      "Epoch [190][200]\t Batch [500][550]\t Training Loss 0.5203\t Accuracy 0.8561\n",
      "\n",
      "Epoch [190]\t Average training loss 0.5207\t Average training accuracy 0.8556\n",
      "Epoch [190]\t Average validation loss 0.4240\t Average validation accuracy 0.8846\n",
      "\n",
      "Epoch [191][200]\t Batch [0][550]\t Training Loss 0.5303\t Accuracy 0.8300\n",
      "Epoch [191][200]\t Batch [50][550]\t Training Loss 0.4880\t Accuracy 0.8706\n",
      "Epoch [191][200]\t Batch [100][550]\t Training Loss 0.5050\t Accuracy 0.8646\n",
      "Epoch [191][200]\t Batch [150][550]\t Training Loss 0.5172\t Accuracy 0.8599\n",
      "Epoch [191][200]\t Batch [200][550]\t Training Loss 0.5146\t Accuracy 0.8590\n",
      "Epoch [191][200]\t Batch [250][550]\t Training Loss 0.5137\t Accuracy 0.8578\n",
      "Epoch [191][200]\t Batch [300][550]\t Training Loss 0.5148\t Accuracy 0.8580\n",
      "Epoch [191][200]\t Batch [350][550]\t Training Loss 0.5172\t Accuracy 0.8582\n",
      "Epoch [191][200]\t Batch [400][550]\t Training Loss 0.5166\t Accuracy 0.8584\n",
      "Epoch [191][200]\t Batch [450][550]\t Training Loss 0.5157\t Accuracy 0.8584\n",
      "Epoch [191][200]\t Batch [500][550]\t Training Loss 0.5157\t Accuracy 0.8576\n",
      "\n",
      "Epoch [191]\t Average training loss 0.5161\t Average training accuracy 0.8571\n",
      "Epoch [191]\t Average validation loss 0.4196\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [192][200]\t Batch [0][550]\t Training Loss 0.5237\t Accuracy 0.8300\n",
      "Epoch [192][200]\t Batch [50][550]\t Training Loss 0.4836\t Accuracy 0.8720\n",
      "Epoch [192][200]\t Batch [100][550]\t Training Loss 0.5004\t Accuracy 0.8656\n",
      "Epoch [192][200]\t Batch [150][550]\t Training Loss 0.5126\t Accuracy 0.8612\n",
      "Epoch [192][200]\t Batch [200][550]\t Training Loss 0.5099\t Accuracy 0.8599\n",
      "Epoch [192][200]\t Batch [250][550]\t Training Loss 0.5090\t Accuracy 0.8586\n",
      "Epoch [192][200]\t Batch [300][550]\t Training Loss 0.5102\t Accuracy 0.8590\n",
      "Epoch [192][200]\t Batch [350][550]\t Training Loss 0.5126\t Accuracy 0.8593\n",
      "Epoch [192][200]\t Batch [400][550]\t Training Loss 0.5120\t Accuracy 0.8595\n",
      "Epoch [192][200]\t Batch [450][550]\t Training Loss 0.5112\t Accuracy 0.8596\n",
      "Epoch [192][200]\t Batch [500][550]\t Training Loss 0.5112\t Accuracy 0.8588\n",
      "\n",
      "Epoch [192]\t Average training loss 0.5116\t Average training accuracy 0.8585\n",
      "Epoch [192]\t Average validation loss 0.4153\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [193][200]\t Batch [0][550]\t Training Loss 0.5170\t Accuracy 0.8300\n",
      "Epoch [193][200]\t Batch [50][550]\t Training Loss 0.4793\t Accuracy 0.8727\n",
      "Epoch [193][200]\t Batch [100][550]\t Training Loss 0.4959\t Accuracy 0.8663\n",
      "Epoch [193][200]\t Batch [150][550]\t Training Loss 0.5080\t Accuracy 0.8619\n",
      "Epoch [193][200]\t Batch [200][550]\t Training Loss 0.5053\t Accuracy 0.8609\n",
      "Epoch [193][200]\t Batch [250][550]\t Training Loss 0.5044\t Accuracy 0.8595\n",
      "Epoch [193][200]\t Batch [300][550]\t Training Loss 0.5056\t Accuracy 0.8600\n",
      "Epoch [193][200]\t Batch [350][550]\t Training Loss 0.5081\t Accuracy 0.8605\n",
      "Epoch [193][200]\t Batch [400][550]\t Training Loss 0.5075\t Accuracy 0.8606\n",
      "Epoch [193][200]\t Batch [450][550]\t Training Loss 0.5067\t Accuracy 0.8606\n",
      "Epoch [193][200]\t Batch [500][550]\t Training Loss 0.5067\t Accuracy 0.8599\n",
      "\n",
      "Epoch [193]\t Average training loss 0.5071\t Average training accuracy 0.8596\n",
      "Epoch [193]\t Average validation loss 0.4111\t Average validation accuracy 0.8882\n",
      "\n",
      "Epoch [194][200]\t Batch [0][550]\t Training Loss 0.5103\t Accuracy 0.8400\n",
      "Epoch [194][200]\t Batch [50][550]\t Training Loss 0.4750\t Accuracy 0.8741\n",
      "Epoch [194][200]\t Batch [100][550]\t Training Loss 0.4915\t Accuracy 0.8678\n",
      "Epoch [194][200]\t Batch [150][550]\t Training Loss 0.5035\t Accuracy 0.8632\n",
      "Epoch [194][200]\t Batch [200][550]\t Training Loss 0.5008\t Accuracy 0.8623\n",
      "Epoch [194][200]\t Batch [250][550]\t Training Loss 0.4999\t Accuracy 0.8609\n",
      "Epoch [194][200]\t Batch [300][550]\t Training Loss 0.5011\t Accuracy 0.8615\n",
      "Epoch [194][200]\t Batch [350][550]\t Training Loss 0.5036\t Accuracy 0.8619\n",
      "Epoch [194][200]\t Batch [400][550]\t Training Loss 0.5030\t Accuracy 0.8620\n",
      "Epoch [194][200]\t Batch [450][550]\t Training Loss 0.5022\t Accuracy 0.8620\n",
      "Epoch [194][200]\t Batch [500][550]\t Training Loss 0.5023\t Accuracy 0.8612\n",
      "\n",
      "Epoch [194]\t Average training loss 0.5027\t Average training accuracy 0.8608\n",
      "Epoch [194]\t Average validation loss 0.4069\t Average validation accuracy 0.8908\n",
      "\n",
      "Epoch [195][200]\t Batch [0][550]\t Training Loss 0.5038\t Accuracy 0.8400\n",
      "Epoch [195][200]\t Batch [50][550]\t Training Loss 0.4708\t Accuracy 0.8747\n",
      "Epoch [195][200]\t Batch [100][550]\t Training Loss 0.4872\t Accuracy 0.8687\n",
      "Epoch [195][200]\t Batch [150][550]\t Training Loss 0.4991\t Accuracy 0.8642\n",
      "Epoch [195][200]\t Batch [200][550]\t Training Loss 0.4963\t Accuracy 0.8633\n",
      "Epoch [195][200]\t Batch [250][550]\t Training Loss 0.4955\t Accuracy 0.8621\n",
      "Epoch [195][200]\t Batch [300][550]\t Training Loss 0.4967\t Accuracy 0.8628\n",
      "Epoch [195][200]\t Batch [350][550]\t Training Loss 0.4991\t Accuracy 0.8632\n",
      "Epoch [195][200]\t Batch [400][550]\t Training Loss 0.4986\t Accuracy 0.8632\n",
      "Epoch [195][200]\t Batch [450][550]\t Training Loss 0.4978\t Accuracy 0.8632\n",
      "Epoch [195][200]\t Batch [500][550]\t Training Loss 0.4979\t Accuracy 0.8625\n",
      "\n",
      "Epoch [195]\t Average training loss 0.4983\t Average training accuracy 0.8622\n",
      "Epoch [195]\t Average validation loss 0.4028\t Average validation accuracy 0.8928\n",
      "\n",
      "Epoch [196][200]\t Batch [0][550]\t Training Loss 0.4973\t Accuracy 0.8600\n",
      "Epoch [196][200]\t Batch [50][550]\t Training Loss 0.4666\t Accuracy 0.8759\n",
      "Epoch [196][200]\t Batch [100][550]\t Training Loss 0.4829\t Accuracy 0.8700\n",
      "Epoch [196][200]\t Batch [150][550]\t Training Loss 0.4947\t Accuracy 0.8656\n",
      "Epoch [196][200]\t Batch [200][550]\t Training Loss 0.4920\t Accuracy 0.8646\n",
      "Epoch [196][200]\t Batch [250][550]\t Training Loss 0.4911\t Accuracy 0.8633\n",
      "Epoch [196][200]\t Batch [300][550]\t Training Loss 0.4924\t Accuracy 0.8640\n",
      "Epoch [196][200]\t Batch [350][550]\t Training Loss 0.4948\t Accuracy 0.8643\n",
      "Epoch [196][200]\t Batch [400][550]\t Training Loss 0.4943\t Accuracy 0.8643\n",
      "Epoch [196][200]\t Batch [450][550]\t Training Loss 0.4935\t Accuracy 0.8643\n",
      "Epoch [196][200]\t Batch [500][550]\t Training Loss 0.4937\t Accuracy 0.8637\n",
      "\n",
      "Epoch [196]\t Average training loss 0.4940\t Average training accuracy 0.8636\n",
      "Epoch [196]\t Average validation loss 0.3988\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [197][200]\t Batch [0][550]\t Training Loss 0.4910\t Accuracy 0.8600\n",
      "Epoch [197][200]\t Batch [50][550]\t Training Loss 0.4625\t Accuracy 0.8773\n",
      "Epoch [197][200]\t Batch [100][550]\t Training Loss 0.4787\t Accuracy 0.8717\n",
      "Epoch [197][200]\t Batch [150][550]\t Training Loss 0.4905\t Accuracy 0.8671\n",
      "Epoch [197][200]\t Batch [200][550]\t Training Loss 0.4877\t Accuracy 0.8662\n",
      "Epoch [197][200]\t Batch [250][550]\t Training Loss 0.4868\t Accuracy 0.8647\n",
      "Epoch [197][200]\t Batch [300][550]\t Training Loss 0.4881\t Accuracy 0.8653\n",
      "Epoch [197][200]\t Batch [350][550]\t Training Loss 0.4905\t Accuracy 0.8656\n",
      "Epoch [197][200]\t Batch [400][550]\t Training Loss 0.4901\t Accuracy 0.8655\n",
      "Epoch [197][200]\t Batch [450][550]\t Training Loss 0.4893\t Accuracy 0.8656\n",
      "Epoch [197][200]\t Batch [500][550]\t Training Loss 0.4895\t Accuracy 0.8650\n",
      "\n",
      "Epoch [197]\t Average training loss 0.4899\t Average training accuracy 0.8649\n",
      "Epoch [197]\t Average validation loss 0.3949\t Average validation accuracy 0.8952\n",
      "\n",
      "Epoch [198][200]\t Batch [0][550]\t Training Loss 0.4849\t Accuracy 0.8600\n",
      "Epoch [198][200]\t Batch [50][550]\t Training Loss 0.4586\t Accuracy 0.8784\n",
      "Epoch [198][200]\t Batch [100][550]\t Training Loss 0.4746\t Accuracy 0.8729\n",
      "Epoch [198][200]\t Batch [150][550]\t Training Loss 0.4863\t Accuracy 0.8681\n",
      "Epoch [198][200]\t Batch [200][550]\t Training Loss 0.4834\t Accuracy 0.8673\n",
      "Epoch [198][200]\t Batch [250][550]\t Training Loss 0.4826\t Accuracy 0.8659\n",
      "Epoch [198][200]\t Batch [300][550]\t Training Loss 0.4839\t Accuracy 0.8665\n",
      "Epoch [198][200]\t Batch [350][550]\t Training Loss 0.4864\t Accuracy 0.8666\n",
      "Epoch [198][200]\t Batch [400][550]\t Training Loss 0.4859\t Accuracy 0.8666\n",
      "Epoch [198][200]\t Batch [450][550]\t Training Loss 0.4852\t Accuracy 0.8666\n",
      "Epoch [198][200]\t Batch [500][550]\t Training Loss 0.4854\t Accuracy 0.8660\n",
      "\n",
      "Epoch [198]\t Average training loss 0.4858\t Average training accuracy 0.8659\n",
      "Epoch [198]\t Average validation loss 0.3911\t Average validation accuracy 0.8978\n",
      "\n",
      "Epoch [199][200]\t Batch [0][550]\t Training Loss 0.4789\t Accuracy 0.8600\n",
      "Epoch [199][200]\t Batch [50][550]\t Training Loss 0.4547\t Accuracy 0.8804\n",
      "Epoch [199][200]\t Batch [100][550]\t Training Loss 0.4705\t Accuracy 0.8747\n",
      "Epoch [199][200]\t Batch [150][550]\t Training Loss 0.4822\t Accuracy 0.8696\n",
      "Epoch [199][200]\t Batch [200][550]\t Training Loss 0.4793\t Accuracy 0.8688\n",
      "Epoch [199][200]\t Batch [250][550]\t Training Loss 0.4785\t Accuracy 0.8674\n",
      "Epoch [199][200]\t Batch [300][550]\t Training Loss 0.4798\t Accuracy 0.8679\n",
      "Epoch [199][200]\t Batch [350][550]\t Training Loss 0.4823\t Accuracy 0.8681\n",
      "Epoch [199][200]\t Batch [400][550]\t Training Loss 0.4819\t Accuracy 0.8680\n",
      "Epoch [199][200]\t Batch [450][550]\t Training Loss 0.4812\t Accuracy 0.8679\n",
      "Epoch [199][200]\t Batch [500][550]\t Training Loss 0.4814\t Accuracy 0.8674\n",
      "\n",
      "Epoch [199]\t Average training loss 0.4818\t Average training accuracy 0.8673\n",
      "Epoch [199]\t Average validation loss 0.3874\t Average validation accuracy 0.8986\n",
      "\n",
      "Epoch [0][200]\t Batch [0][550]\t Training Loss 0.4730\t Accuracy 0.8700\n",
      "Epoch [0][200]\t Batch [50][550]\t Training Loss 0.4509\t Accuracy 0.8820\n",
      "Epoch [0][200]\t Batch [100][550]\t Training Loss 0.4666\t Accuracy 0.8761\n",
      "Epoch [0][200]\t Batch [150][550]\t Training Loss 0.4782\t Accuracy 0.8709\n",
      "Epoch [0][200]\t Batch [200][550]\t Training Loss 0.4753\t Accuracy 0.8701\n",
      "Epoch [0][200]\t Batch [250][550]\t Training Loss 0.4744\t Accuracy 0.8686\n",
      "Epoch [0][200]\t Batch [300][550]\t Training Loss 0.4758\t Accuracy 0.8691\n",
      "Epoch [0][200]\t Batch [350][550]\t Training Loss 0.4783\t Accuracy 0.8692\n",
      "Epoch [0][200]\t Batch [400][550]\t Training Loss 0.4779\t Accuracy 0.8690\n",
      "Epoch [0][200]\t Batch [450][550]\t Training Loss 0.4772\t Accuracy 0.8689\n",
      "Epoch [0][200]\t Batch [500][550]\t Training Loss 0.4774\t Accuracy 0.8684\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4778\t Average training accuracy 0.8684\n",
      "Epoch [0]\t Average validation loss 0.3836\t Average validation accuracy 0.8998\n",
      "\n",
      "Epoch [1][200]\t Batch [0][550]\t Training Loss 0.4672\t Accuracy 0.8800\n",
      "Epoch [1][200]\t Batch [50][550]\t Training Loss 0.4471\t Accuracy 0.8831\n",
      "Epoch [1][200]\t Batch [100][550]\t Training Loss 0.4626\t Accuracy 0.8775\n",
      "Epoch [1][200]\t Batch [150][550]\t Training Loss 0.4742\t Accuracy 0.8723\n",
      "Epoch [1][200]\t Batch [200][550]\t Training Loss 0.4713\t Accuracy 0.8713\n",
      "Epoch [1][200]\t Batch [250][550]\t Training Loss 0.4704\t Accuracy 0.8698\n",
      "Epoch [1][200]\t Batch [300][550]\t Training Loss 0.4718\t Accuracy 0.8702\n",
      "Epoch [1][200]\t Batch [350][550]\t Training Loss 0.4743\t Accuracy 0.8704\n",
      "Epoch [1][200]\t Batch [400][550]\t Training Loss 0.4739\t Accuracy 0.8702\n",
      "Epoch [1][200]\t Batch [450][550]\t Training Loss 0.4732\t Accuracy 0.8701\n",
      "Epoch [1][200]\t Batch [500][550]\t Training Loss 0.4734\t Accuracy 0.8696\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4739\t Average training accuracy 0.8696\n",
      "Epoch [1]\t Average validation loss 0.3800\t Average validation accuracy 0.9004\n",
      "\n",
      "Epoch [2][200]\t Batch [0][550]\t Training Loss 0.4616\t Accuracy 0.8800\n",
      "Epoch [2][200]\t Batch [50][550]\t Training Loss 0.4433\t Accuracy 0.8841\n",
      "Epoch [2][200]\t Batch [100][550]\t Training Loss 0.4587\t Accuracy 0.8782\n",
      "Epoch [2][200]\t Batch [150][550]\t Training Loss 0.4703\t Accuracy 0.8730\n",
      "Epoch [2][200]\t Batch [200][550]\t Training Loss 0.4673\t Accuracy 0.8723\n",
      "Epoch [2][200]\t Batch [250][550]\t Training Loss 0.4665\t Accuracy 0.8710\n",
      "Epoch [2][200]\t Batch [300][550]\t Training Loss 0.4679\t Accuracy 0.8714\n",
      "Epoch [2][200]\t Batch [350][550]\t Training Loss 0.4704\t Accuracy 0.8715\n",
      "Epoch [2][200]\t Batch [400][550]\t Training Loss 0.4700\t Accuracy 0.8713\n",
      "Epoch [2][200]\t Batch [450][550]\t Training Loss 0.4694\t Accuracy 0.8712\n",
      "Epoch [2][200]\t Batch [500][550]\t Training Loss 0.4696\t Accuracy 0.8705\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4701\t Average training accuracy 0.8704\n",
      "Epoch [2]\t Average validation loss 0.3764\t Average validation accuracy 0.9012\n",
      "\n",
      "Epoch [3][200]\t Batch [0][550]\t Training Loss 0.4561\t Accuracy 0.8800\n",
      "Epoch [3][200]\t Batch [50][550]\t Training Loss 0.4397\t Accuracy 0.8847\n",
      "Epoch [3][200]\t Batch [100][550]\t Training Loss 0.4550\t Accuracy 0.8788\n",
      "Epoch [3][200]\t Batch [150][550]\t Training Loss 0.4665\t Accuracy 0.8738\n",
      "Epoch [3][200]\t Batch [200][550]\t Training Loss 0.4635\t Accuracy 0.8732\n",
      "Epoch [3][200]\t Batch [250][550]\t Training Loss 0.4627\t Accuracy 0.8720\n",
      "Epoch [3][200]\t Batch [300][550]\t Training Loss 0.4642\t Accuracy 0.8725\n",
      "Epoch [3][200]\t Batch [350][550]\t Training Loss 0.4666\t Accuracy 0.8727\n",
      "Epoch [3][200]\t Batch [400][550]\t Training Loss 0.4663\t Accuracy 0.8724\n",
      "Epoch [3][200]\t Batch [450][550]\t Training Loss 0.4657\t Accuracy 0.8724\n",
      "Epoch [3][200]\t Batch [500][550]\t Training Loss 0.4659\t Accuracy 0.8717\n",
      "\n",
      "Epoch [3]\t Average training loss 0.4664\t Average training accuracy 0.8716\n",
      "Epoch [3]\t Average validation loss 0.3730\t Average validation accuracy 0.9018\n",
      "\n",
      "Epoch [4][200]\t Batch [0][550]\t Training Loss 0.4508\t Accuracy 0.8900\n",
      "Epoch [4][200]\t Batch [50][550]\t Training Loss 0.4362\t Accuracy 0.8863\n",
      "Epoch [4][200]\t Batch [100][550]\t Training Loss 0.4513\t Accuracy 0.8803\n",
      "Epoch [4][200]\t Batch [150][550]\t Training Loss 0.4628\t Accuracy 0.8750\n",
      "Epoch [4][200]\t Batch [200][550]\t Training Loss 0.4598\t Accuracy 0.8744\n",
      "Epoch [4][200]\t Batch [250][550]\t Training Loss 0.4590\t Accuracy 0.8732\n",
      "Epoch [4][200]\t Batch [300][550]\t Training Loss 0.4605\t Accuracy 0.8737\n",
      "Epoch [4][200]\t Batch [350][550]\t Training Loss 0.4630\t Accuracy 0.8738\n",
      "Epoch [4][200]\t Batch [400][550]\t Training Loss 0.4627\t Accuracy 0.8734\n",
      "Epoch [4][200]\t Batch [450][550]\t Training Loss 0.4621\t Accuracy 0.8734\n",
      "Epoch [4][200]\t Batch [500][550]\t Training Loss 0.4623\t Accuracy 0.8727\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4628\t Average training accuracy 0.8727\n",
      "Epoch [4]\t Average validation loss 0.3697\t Average validation accuracy 0.9030\n",
      "\n",
      "Epoch [5][200]\t Batch [0][550]\t Training Loss 0.4457\t Accuracy 0.8900\n",
      "Epoch [5][200]\t Batch [50][550]\t Training Loss 0.4328\t Accuracy 0.8867\n",
      "Epoch [5][200]\t Batch [100][550]\t Training Loss 0.4478\t Accuracy 0.8809\n",
      "Epoch [5][200]\t Batch [150][550]\t Training Loss 0.4593\t Accuracy 0.8757\n",
      "Epoch [5][200]\t Batch [200][550]\t Training Loss 0.4562\t Accuracy 0.8753\n",
      "Epoch [5][200]\t Batch [250][550]\t Training Loss 0.4554\t Accuracy 0.8742\n",
      "Epoch [5][200]\t Batch [300][550]\t Training Loss 0.4570\t Accuracy 0.8745\n",
      "Epoch [5][200]\t Batch [350][550]\t Training Loss 0.4594\t Accuracy 0.8747\n",
      "Epoch [5][200]\t Batch [400][550]\t Training Loss 0.4591\t Accuracy 0.8744\n",
      "Epoch [5][200]\t Batch [450][550]\t Training Loss 0.4585\t Accuracy 0.8743\n",
      "Epoch [5][200]\t Batch [500][550]\t Training Loss 0.4588\t Accuracy 0.8736\n",
      "\n",
      "Epoch [5]\t Average training loss 0.4593\t Average training accuracy 0.8735\n",
      "Epoch [5]\t Average validation loss 0.3666\t Average validation accuracy 0.9038\n",
      "\n",
      "Epoch [6][200]\t Batch [0][550]\t Training Loss 0.4407\t Accuracy 0.8900\n",
      "Epoch [6][200]\t Batch [50][550]\t Training Loss 0.4295\t Accuracy 0.8873\n",
      "Epoch [6][200]\t Batch [100][550]\t Training Loss 0.4444\t Accuracy 0.8817\n",
      "Epoch [6][200]\t Batch [150][550]\t Training Loss 0.4558\t Accuracy 0.8766\n",
      "Epoch [6][200]\t Batch [200][550]\t Training Loss 0.4528\t Accuracy 0.8762\n",
      "Epoch [6][200]\t Batch [250][550]\t Training Loss 0.4519\t Accuracy 0.8752\n",
      "Epoch [6][200]\t Batch [300][550]\t Training Loss 0.4535\t Accuracy 0.8755\n",
      "Epoch [6][200]\t Batch [350][550]\t Training Loss 0.4560\t Accuracy 0.8756\n",
      "Epoch [6][200]\t Batch [400][550]\t Training Loss 0.4557\t Accuracy 0.8752\n",
      "Epoch [6][200]\t Batch [450][550]\t Training Loss 0.4551\t Accuracy 0.8752\n",
      "Epoch [6][200]\t Batch [500][550]\t Training Loss 0.4555\t Accuracy 0.8744\n",
      "\n",
      "Epoch [6]\t Average training loss 0.4559\t Average training accuracy 0.8744\n",
      "Epoch [6]\t Average validation loss 0.3635\t Average validation accuracy 0.9050\n",
      "\n",
      "Epoch [7][200]\t Batch [0][550]\t Training Loss 0.4359\t Accuracy 0.8900\n",
      "Epoch [7][200]\t Batch [50][550]\t Training Loss 0.4263\t Accuracy 0.8880\n",
      "Epoch [7][200]\t Batch [100][550]\t Training Loss 0.4410\t Accuracy 0.8826\n",
      "Epoch [7][200]\t Batch [150][550]\t Training Loss 0.4525\t Accuracy 0.8777\n",
      "Epoch [7][200]\t Batch [200][550]\t Training Loss 0.4494\t Accuracy 0.8771\n",
      "Epoch [7][200]\t Batch [250][550]\t Training Loss 0.4486\t Accuracy 0.8763\n",
      "Epoch [7][200]\t Batch [300][550]\t Training Loss 0.4502\t Accuracy 0.8767\n",
      "Epoch [7][200]\t Batch [350][550]\t Training Loss 0.4527\t Accuracy 0.8767\n",
      "Epoch [7][200]\t Batch [400][550]\t Training Loss 0.4524\t Accuracy 0.8762\n",
      "Epoch [7][200]\t Batch [450][550]\t Training Loss 0.4519\t Accuracy 0.8762\n",
      "Epoch [7][200]\t Batch [500][550]\t Training Loss 0.4522\t Accuracy 0.8755\n",
      "\n",
      "Epoch [7]\t Average training loss 0.4527\t Average training accuracy 0.8754\n",
      "Epoch [7]\t Average validation loss 0.3606\t Average validation accuracy 0.9048\n",
      "\n",
      "Epoch [8][200]\t Batch [0][550]\t Training Loss 0.4313\t Accuracy 0.9000\n",
      "Epoch [8][200]\t Batch [50][550]\t Training Loss 0.4232\t Accuracy 0.8890\n",
      "Epoch [8][200]\t Batch [100][550]\t Training Loss 0.4378\t Accuracy 0.8834\n",
      "Epoch [8][200]\t Batch [150][550]\t Training Loss 0.4493\t Accuracy 0.8786\n",
      "Epoch [8][200]\t Batch [200][550]\t Training Loss 0.4461\t Accuracy 0.8781\n",
      "Epoch [8][200]\t Batch [250][550]\t Training Loss 0.4453\t Accuracy 0.8772\n",
      "Epoch [8][200]\t Batch [300][550]\t Training Loss 0.4470\t Accuracy 0.8775\n",
      "Epoch [8][200]\t Batch [350][550]\t Training Loss 0.4495\t Accuracy 0.8775\n",
      "Epoch [8][200]\t Batch [400][550]\t Training Loss 0.4492\t Accuracy 0.8771\n",
      "Epoch [8][200]\t Batch [450][550]\t Training Loss 0.4487\t Accuracy 0.8771\n",
      "Epoch [8][200]\t Batch [500][550]\t Training Loss 0.4490\t Accuracy 0.8763\n",
      "\n",
      "Epoch [8]\t Average training loss 0.4495\t Average training accuracy 0.8761\n",
      "Epoch [8]\t Average validation loss 0.3577\t Average validation accuracy 0.9054\n",
      "\n",
      "Epoch [9][200]\t Batch [0][550]\t Training Loss 0.4268\t Accuracy 0.9100\n",
      "Epoch [9][200]\t Batch [50][550]\t Training Loss 0.4202\t Accuracy 0.8910\n",
      "Epoch [9][200]\t Batch [100][550]\t Training Loss 0.4347\t Accuracy 0.8849\n",
      "Epoch [9][200]\t Batch [150][550]\t Training Loss 0.4461\t Accuracy 0.8797\n",
      "Epoch [9][200]\t Batch [200][550]\t Training Loss 0.4430\t Accuracy 0.8792\n",
      "Epoch [9][200]\t Batch [250][550]\t Training Loss 0.4422\t Accuracy 0.8784\n",
      "Epoch [9][200]\t Batch [300][550]\t Training Loss 0.4439\t Accuracy 0.8788\n",
      "Epoch [9][200]\t Batch [350][550]\t Training Loss 0.4463\t Accuracy 0.8785\n",
      "Epoch [9][200]\t Batch [400][550]\t Training Loss 0.4461\t Accuracy 0.8781\n",
      "Epoch [9][200]\t Batch [450][550]\t Training Loss 0.4456\t Accuracy 0.8779\n",
      "Epoch [9][200]\t Batch [500][550]\t Training Loss 0.4460\t Accuracy 0.8771\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4464\t Average training accuracy 0.8769\n",
      "Epoch [9]\t Average validation loss 0.3550\t Average validation accuracy 0.9066\n",
      "\n",
      "Epoch [10][200]\t Batch [0][550]\t Training Loss 0.4225\t Accuracy 0.9100\n",
      "Epoch [10][200]\t Batch [50][550]\t Training Loss 0.4174\t Accuracy 0.8916\n",
      "Epoch [10][200]\t Batch [100][550]\t Training Loss 0.4317\t Accuracy 0.8856\n",
      "Epoch [10][200]\t Batch [150][550]\t Training Loss 0.4431\t Accuracy 0.8803\n",
      "Epoch [10][200]\t Batch [200][550]\t Training Loss 0.4400\t Accuracy 0.8798\n",
      "Epoch [10][200]\t Batch [250][550]\t Training Loss 0.4392\t Accuracy 0.8789\n",
      "Epoch [10][200]\t Batch [300][550]\t Training Loss 0.4408\t Accuracy 0.8793\n",
      "Epoch [10][200]\t Batch [350][550]\t Training Loss 0.4433\t Accuracy 0.8790\n",
      "Epoch [10][200]\t Batch [400][550]\t Training Loss 0.4431\t Accuracy 0.8785\n",
      "Epoch [10][200]\t Batch [450][550]\t Training Loss 0.4426\t Accuracy 0.8783\n",
      "Epoch [10][200]\t Batch [500][550]\t Training Loss 0.4430\t Accuracy 0.8776\n",
      "\n",
      "Epoch [10]\t Average training loss 0.4435\t Average training accuracy 0.8774\n",
      "Epoch [10]\t Average validation loss 0.3523\t Average validation accuracy 0.9072\n",
      "\n",
      "Epoch [11][200]\t Batch [0][550]\t Training Loss 0.4183\t Accuracy 0.9100\n",
      "Epoch [11][200]\t Batch [50][550]\t Training Loss 0.4146\t Accuracy 0.8916\n",
      "Epoch [11][200]\t Batch [100][550]\t Training Loss 0.4287\t Accuracy 0.8862\n",
      "Epoch [11][200]\t Batch [150][550]\t Training Loss 0.4402\t Accuracy 0.8808\n",
      "Epoch [11][200]\t Batch [200][550]\t Training Loss 0.4370\t Accuracy 0.8802\n",
      "Epoch [11][200]\t Batch [250][550]\t Training Loss 0.4362\t Accuracy 0.8794\n",
      "Epoch [11][200]\t Batch [300][550]\t Training Loss 0.4379\t Accuracy 0.8799\n",
      "Epoch [11][200]\t Batch [350][550]\t Training Loss 0.4404\t Accuracy 0.8795\n",
      "Epoch [11][200]\t Batch [400][550]\t Training Loss 0.4402\t Accuracy 0.8792\n",
      "Epoch [11][200]\t Batch [450][550]\t Training Loss 0.4397\t Accuracy 0.8791\n",
      "Epoch [11][200]\t Batch [500][550]\t Training Loss 0.4401\t Accuracy 0.8784\n",
      "\n",
      "Epoch [11]\t Average training loss 0.4406\t Average training accuracy 0.8782\n",
      "Epoch [11]\t Average validation loss 0.3498\t Average validation accuracy 0.9084\n",
      "\n",
      "Epoch [12][200]\t Batch [0][550]\t Training Loss 0.4143\t Accuracy 0.9100\n",
      "Epoch [12][200]\t Batch [50][550]\t Training Loss 0.4119\t Accuracy 0.8920\n",
      "Epoch [12][200]\t Batch [100][550]\t Training Loss 0.4259\t Accuracy 0.8864\n",
      "Epoch [12][200]\t Batch [150][550]\t Training Loss 0.4374\t Accuracy 0.8811\n",
      "Epoch [12][200]\t Batch [200][550]\t Training Loss 0.4342\t Accuracy 0.8807\n",
      "Epoch [12][200]\t Batch [250][550]\t Training Loss 0.4334\t Accuracy 0.8800\n",
      "Epoch [12][200]\t Batch [300][550]\t Training Loss 0.4351\t Accuracy 0.8805\n",
      "Epoch [12][200]\t Batch [350][550]\t Training Loss 0.4376\t Accuracy 0.8802\n",
      "Epoch [12][200]\t Batch [400][550]\t Training Loss 0.4374\t Accuracy 0.8799\n",
      "Epoch [12][200]\t Batch [450][550]\t Training Loss 0.4369\t Accuracy 0.8799\n",
      "Epoch [12][200]\t Batch [500][550]\t Training Loss 0.4374\t Accuracy 0.8791\n",
      "\n",
      "Epoch [12]\t Average training loss 0.4379\t Average training accuracy 0.8789\n",
      "Epoch [12]\t Average validation loss 0.3473\t Average validation accuracy 0.9090\n",
      "\n",
      "Epoch [13][200]\t Batch [0][550]\t Training Loss 0.4104\t Accuracy 0.9100\n",
      "Epoch [13][200]\t Batch [50][550]\t Training Loss 0.4093\t Accuracy 0.8920\n",
      "Epoch [13][200]\t Batch [100][550]\t Training Loss 0.4232\t Accuracy 0.8870\n",
      "Epoch [13][200]\t Batch [150][550]\t Training Loss 0.4346\t Accuracy 0.8816\n",
      "Epoch [13][200]\t Batch [200][550]\t Training Loss 0.4314\t Accuracy 0.8815\n",
      "Epoch [13][200]\t Batch [250][550]\t Training Loss 0.4306\t Accuracy 0.8807\n",
      "Epoch [13][200]\t Batch [300][550]\t Training Loss 0.4324\t Accuracy 0.8812\n",
      "Epoch [13][200]\t Batch [350][550]\t Training Loss 0.4349\t Accuracy 0.8807\n",
      "Epoch [13][200]\t Batch [400][550]\t Training Loss 0.4347\t Accuracy 0.8805\n",
      "Epoch [13][200]\t Batch [450][550]\t Training Loss 0.4342\t Accuracy 0.8805\n",
      "Epoch [13][200]\t Batch [500][550]\t Training Loss 0.4347\t Accuracy 0.8797\n",
      "\n",
      "Epoch [13]\t Average training loss 0.4352\t Average training accuracy 0.8795\n",
      "Epoch [13]\t Average validation loss 0.3450\t Average validation accuracy 0.9088\n",
      "\n",
      "Epoch [14][200]\t Batch [0][550]\t Training Loss 0.4066\t Accuracy 0.9100\n",
      "Epoch [14][200]\t Batch [50][550]\t Training Loss 0.4067\t Accuracy 0.8925\n",
      "Epoch [14][200]\t Batch [100][550]\t Training Loss 0.4205\t Accuracy 0.8877\n",
      "Epoch [14][200]\t Batch [150][550]\t Training Loss 0.4320\t Accuracy 0.8823\n",
      "Epoch [14][200]\t Batch [200][550]\t Training Loss 0.4287\t Accuracy 0.8822\n",
      "Epoch [14][200]\t Batch [250][550]\t Training Loss 0.4280\t Accuracy 0.8815\n",
      "Epoch [14][200]\t Batch [300][550]\t Training Loss 0.4298\t Accuracy 0.8818\n",
      "Epoch [14][200]\t Batch [350][550]\t Training Loss 0.4322\t Accuracy 0.8813\n",
      "Epoch [14][200]\t Batch [400][550]\t Training Loss 0.4321\t Accuracy 0.8810\n",
      "Epoch [14][200]\t Batch [450][550]\t Training Loss 0.4316\t Accuracy 0.8810\n",
      "Epoch [14][200]\t Batch [500][550]\t Training Loss 0.4321\t Accuracy 0.8802\n",
      "\n",
      "Epoch [14]\t Average training loss 0.4326\t Average training accuracy 0.8800\n",
      "Epoch [14]\t Average validation loss 0.3427\t Average validation accuracy 0.9094\n",
      "\n",
      "Epoch [15][200]\t Batch [0][550]\t Training Loss 0.4030\t Accuracy 0.9100\n",
      "Epoch [15][200]\t Batch [50][550]\t Training Loss 0.4043\t Accuracy 0.8933\n",
      "Epoch [15][200]\t Batch [100][550]\t Training Loss 0.4179\t Accuracy 0.8884\n",
      "Epoch [15][200]\t Batch [150][550]\t Training Loss 0.4294\t Accuracy 0.8828\n",
      "Epoch [15][200]\t Batch [200][550]\t Training Loss 0.4262\t Accuracy 0.8826\n",
      "Epoch [15][200]\t Batch [250][550]\t Training Loss 0.4254\t Accuracy 0.8819\n",
      "Epoch [15][200]\t Batch [300][550]\t Training Loss 0.4272\t Accuracy 0.8823\n",
      "Epoch [15][200]\t Batch [350][550]\t Training Loss 0.4297\t Accuracy 0.8818\n",
      "Epoch [15][200]\t Batch [400][550]\t Training Loss 0.4296\t Accuracy 0.8815\n",
      "Epoch [15][200]\t Batch [450][550]\t Training Loss 0.4291\t Accuracy 0.8816\n",
      "Epoch [15][200]\t Batch [500][550]\t Training Loss 0.4296\t Accuracy 0.8809\n",
      "\n",
      "Epoch [15]\t Average training loss 0.4301\t Average training accuracy 0.8806\n",
      "Epoch [15]\t Average validation loss 0.3405\t Average validation accuracy 0.9108\n",
      "\n",
      "Epoch [16][200]\t Batch [0][550]\t Training Loss 0.3995\t Accuracy 0.9100\n",
      "Epoch [16][200]\t Batch [50][550]\t Training Loss 0.4019\t Accuracy 0.8937\n",
      "Epoch [16][200]\t Batch [100][550]\t Training Loss 0.4154\t Accuracy 0.8887\n",
      "Epoch [16][200]\t Batch [150][550]\t Training Loss 0.4269\t Accuracy 0.8830\n",
      "Epoch [16][200]\t Batch [200][550]\t Training Loss 0.4237\t Accuracy 0.8830\n",
      "Epoch [16][200]\t Batch [250][550]\t Training Loss 0.4229\t Accuracy 0.8823\n",
      "Epoch [16][200]\t Batch [300][550]\t Training Loss 0.4248\t Accuracy 0.8827\n",
      "Epoch [16][200]\t Batch [350][550]\t Training Loss 0.4272\t Accuracy 0.8822\n",
      "Epoch [16][200]\t Batch [400][550]\t Training Loss 0.4271\t Accuracy 0.8820\n",
      "Epoch [16][200]\t Batch [450][550]\t Training Loss 0.4267\t Accuracy 0.8822\n",
      "Epoch [16][200]\t Batch [500][550]\t Training Loss 0.4272\t Accuracy 0.8815\n",
      "\n",
      "Epoch [16]\t Average training loss 0.4277\t Average training accuracy 0.8812\n",
      "Epoch [16]\t Average validation loss 0.3384\t Average validation accuracy 0.9108\n",
      "\n",
      "Epoch [17][200]\t Batch [0][550]\t Training Loss 0.3962\t Accuracy 0.9100\n",
      "Epoch [17][200]\t Batch [50][550]\t Training Loss 0.3996\t Accuracy 0.8945\n",
      "Epoch [17][200]\t Batch [100][550]\t Training Loss 0.4130\t Accuracy 0.8893\n",
      "Epoch [17][200]\t Batch [150][550]\t Training Loss 0.4246\t Accuracy 0.8836\n",
      "Epoch [17][200]\t Batch [200][550]\t Training Loss 0.4213\t Accuracy 0.8836\n",
      "Epoch [17][200]\t Batch [250][550]\t Training Loss 0.4205\t Accuracy 0.8830\n",
      "Epoch [17][200]\t Batch [300][550]\t Training Loss 0.4224\t Accuracy 0.8834\n",
      "Epoch [17][200]\t Batch [350][550]\t Training Loss 0.4248\t Accuracy 0.8828\n",
      "Epoch [17][200]\t Batch [400][550]\t Training Loss 0.4247\t Accuracy 0.8826\n",
      "Epoch [17][200]\t Batch [450][550]\t Training Loss 0.4243\t Accuracy 0.8827\n",
      "Epoch [17][200]\t Batch [500][550]\t Training Loss 0.4248\t Accuracy 0.8821\n",
      "\n",
      "Epoch [17]\t Average training loss 0.4254\t Average training accuracy 0.8818\n",
      "Epoch [17]\t Average validation loss 0.3364\t Average validation accuracy 0.9104\n",
      "\n",
      "Epoch [18][200]\t Batch [0][550]\t Training Loss 0.3930\t Accuracy 0.9100\n",
      "Epoch [18][200]\t Batch [50][550]\t Training Loss 0.3974\t Accuracy 0.8945\n",
      "Epoch [18][200]\t Batch [100][550]\t Training Loss 0.4107\t Accuracy 0.8897\n",
      "Epoch [18][200]\t Batch [150][550]\t Training Loss 0.4222\t Accuracy 0.8841\n",
      "Epoch [18][200]\t Batch [200][550]\t Training Loss 0.4190\t Accuracy 0.8841\n",
      "Epoch [18][200]\t Batch [250][550]\t Training Loss 0.4182\t Accuracy 0.8838\n",
      "Epoch [18][200]\t Batch [300][550]\t Training Loss 0.4201\t Accuracy 0.8841\n",
      "Epoch [18][200]\t Batch [350][550]\t Training Loss 0.4225\t Accuracy 0.8837\n",
      "Epoch [18][200]\t Batch [400][550]\t Training Loss 0.4224\t Accuracy 0.8834\n",
      "Epoch [18][200]\t Batch [450][550]\t Training Loss 0.4221\t Accuracy 0.8836\n",
      "Epoch [18][200]\t Batch [500][550]\t Training Loss 0.4226\t Accuracy 0.8829\n",
      "\n",
      "Epoch [18]\t Average training loss 0.4231\t Average training accuracy 0.8827\n",
      "Epoch [18]\t Average validation loss 0.3344\t Average validation accuracy 0.9112\n",
      "\n",
      "Epoch [19][200]\t Batch [0][550]\t Training Loss 0.3899\t Accuracy 0.9100\n",
      "Epoch [19][200]\t Batch [50][550]\t Training Loss 0.3953\t Accuracy 0.8955\n",
      "Epoch [19][200]\t Batch [100][550]\t Training Loss 0.4084\t Accuracy 0.8904\n",
      "Epoch [19][200]\t Batch [150][550]\t Training Loss 0.4200\t Accuracy 0.8850\n",
      "Epoch [19][200]\t Batch [200][550]\t Training Loss 0.4167\t Accuracy 0.8849\n",
      "Epoch [19][200]\t Batch [250][550]\t Training Loss 0.4160\t Accuracy 0.8847\n",
      "Epoch [19][200]\t Batch [300][550]\t Training Loss 0.4179\t Accuracy 0.8849\n",
      "Epoch [19][200]\t Batch [350][550]\t Training Loss 0.4203\t Accuracy 0.8845\n",
      "Epoch [19][200]\t Batch [400][550]\t Training Loss 0.4202\t Accuracy 0.8842\n",
      "Epoch [19][200]\t Batch [450][550]\t Training Loss 0.4198\t Accuracy 0.8843\n",
      "Epoch [19][200]\t Batch [500][550]\t Training Loss 0.4204\t Accuracy 0.8838\n",
      "\n",
      "Epoch [19]\t Average training loss 0.4209\t Average training accuracy 0.8835\n",
      "Epoch [19]\t Average validation loss 0.3326\t Average validation accuracy 0.9124\n",
      "\n",
      "Epoch [20][200]\t Batch [0][550]\t Training Loss 0.3869\t Accuracy 0.9100\n",
      "Epoch [20][200]\t Batch [50][550]\t Training Loss 0.3932\t Accuracy 0.8957\n",
      "Epoch [20][200]\t Batch [100][550]\t Training Loss 0.4062\t Accuracy 0.8908\n",
      "Epoch [20][200]\t Batch [150][550]\t Training Loss 0.4178\t Accuracy 0.8856\n",
      "Epoch [20][200]\t Batch [200][550]\t Training Loss 0.4145\t Accuracy 0.8856\n",
      "Epoch [20][200]\t Batch [250][550]\t Training Loss 0.4138\t Accuracy 0.8854\n",
      "Epoch [20][200]\t Batch [300][550]\t Training Loss 0.4157\t Accuracy 0.8855\n",
      "Epoch [20][200]\t Batch [350][550]\t Training Loss 0.4181\t Accuracy 0.8852\n",
      "Epoch [20][200]\t Batch [400][550]\t Training Loss 0.4181\t Accuracy 0.8847\n",
      "Epoch [20][200]\t Batch [450][550]\t Training Loss 0.4177\t Accuracy 0.8848\n",
      "Epoch [20][200]\t Batch [500][550]\t Training Loss 0.4182\t Accuracy 0.8842\n",
      "\n",
      "Epoch [20]\t Average training loss 0.4188\t Average training accuracy 0.8840\n",
      "Epoch [20]\t Average validation loss 0.3307\t Average validation accuracy 0.9128\n",
      "\n",
      "Epoch [21][200]\t Batch [0][550]\t Training Loss 0.3840\t Accuracy 0.9100\n",
      "Epoch [21][200]\t Batch [50][550]\t Training Loss 0.3912\t Accuracy 0.8965\n",
      "Epoch [21][200]\t Batch [100][550]\t Training Loss 0.4041\t Accuracy 0.8916\n",
      "Epoch [21][200]\t Batch [150][550]\t Training Loss 0.4157\t Accuracy 0.8862\n",
      "Epoch [21][200]\t Batch [200][550]\t Training Loss 0.4124\t Accuracy 0.8863\n",
      "Epoch [21][200]\t Batch [250][550]\t Training Loss 0.4117\t Accuracy 0.8861\n",
      "Epoch [21][200]\t Batch [300][550]\t Training Loss 0.4136\t Accuracy 0.8862\n",
      "Epoch [21][200]\t Batch [350][550]\t Training Loss 0.4160\t Accuracy 0.8858\n",
      "Epoch [21][200]\t Batch [400][550]\t Training Loss 0.4160\t Accuracy 0.8853\n",
      "Epoch [21][200]\t Batch [450][550]\t Training Loss 0.4156\t Accuracy 0.8854\n",
      "Epoch [21][200]\t Batch [500][550]\t Training Loss 0.4162\t Accuracy 0.8847\n",
      "\n",
      "Epoch [21]\t Average training loss 0.4167\t Average training accuracy 0.8844\n",
      "Epoch [21]\t Average validation loss 0.3290\t Average validation accuracy 0.9128\n",
      "\n",
      "Epoch [22][200]\t Batch [0][550]\t Training Loss 0.3812\t Accuracy 0.9100\n",
      "Epoch [22][200]\t Batch [50][550]\t Training Loss 0.3892\t Accuracy 0.8969\n",
      "Epoch [22][200]\t Batch [100][550]\t Training Loss 0.4021\t Accuracy 0.8923\n",
      "Epoch [22][200]\t Batch [150][550]\t Training Loss 0.4137\t Accuracy 0.8866\n",
      "Epoch [22][200]\t Batch [200][550]\t Training Loss 0.4104\t Accuracy 0.8869\n",
      "Epoch [22][200]\t Batch [250][550]\t Training Loss 0.4097\t Accuracy 0.8867\n",
      "Epoch [22][200]\t Batch [300][550]\t Training Loss 0.4116\t Accuracy 0.8868\n",
      "Epoch [22][200]\t Batch [350][550]\t Training Loss 0.4140\t Accuracy 0.8863\n",
      "Epoch [22][200]\t Batch [400][550]\t Training Loss 0.4140\t Accuracy 0.8858\n",
      "Epoch [22][200]\t Batch [450][550]\t Training Loss 0.4136\t Accuracy 0.8859\n",
      "Epoch [22][200]\t Batch [500][550]\t Training Loss 0.4142\t Accuracy 0.8852\n",
      "\n",
      "Epoch [22]\t Average training loss 0.4148\t Average training accuracy 0.8849\n",
      "Epoch [22]\t Average validation loss 0.3273\t Average validation accuracy 0.9128\n",
      "\n",
      "Epoch [23][200]\t Batch [0][550]\t Training Loss 0.3785\t Accuracy 0.9100\n",
      "Epoch [23][200]\t Batch [50][550]\t Training Loss 0.3873\t Accuracy 0.8969\n",
      "Epoch [23][200]\t Batch [100][550]\t Training Loss 0.4001\t Accuracy 0.8931\n",
      "Epoch [23][200]\t Batch [150][550]\t Training Loss 0.4117\t Accuracy 0.8873\n",
      "Epoch [23][200]\t Batch [200][550]\t Training Loss 0.4084\t Accuracy 0.8877\n",
      "Epoch [23][200]\t Batch [250][550]\t Training Loss 0.4077\t Accuracy 0.8876\n",
      "Epoch [23][200]\t Batch [300][550]\t Training Loss 0.4096\t Accuracy 0.8875\n",
      "Epoch [23][200]\t Batch [350][550]\t Training Loss 0.4120\t Accuracy 0.8870\n",
      "Epoch [23][200]\t Batch [400][550]\t Training Loss 0.4120\t Accuracy 0.8865\n",
      "Epoch [23][200]\t Batch [450][550]\t Training Loss 0.4117\t Accuracy 0.8865\n",
      "Epoch [23][200]\t Batch [500][550]\t Training Loss 0.4123\t Accuracy 0.8858\n",
      "\n",
      "Epoch [23]\t Average training loss 0.4128\t Average training accuracy 0.8855\n",
      "Epoch [23]\t Average validation loss 0.3256\t Average validation accuracy 0.9132\n",
      "\n",
      "Epoch [24][200]\t Batch [0][550]\t Training Loss 0.3758\t Accuracy 0.9200\n",
      "Epoch [24][200]\t Batch [50][550]\t Training Loss 0.3855\t Accuracy 0.8976\n",
      "Epoch [24][200]\t Batch [100][550]\t Training Loss 0.3981\t Accuracy 0.8939\n",
      "Epoch [24][200]\t Batch [150][550]\t Training Loss 0.4098\t Accuracy 0.8879\n",
      "Epoch [24][200]\t Batch [200][550]\t Training Loss 0.4065\t Accuracy 0.8881\n",
      "Epoch [24][200]\t Batch [250][550]\t Training Loss 0.4058\t Accuracy 0.8881\n",
      "Epoch [24][200]\t Batch [300][550]\t Training Loss 0.4077\t Accuracy 0.8881\n",
      "Epoch [24][200]\t Batch [350][550]\t Training Loss 0.4101\t Accuracy 0.8876\n",
      "Epoch [24][200]\t Batch [400][550]\t Training Loss 0.4101\t Accuracy 0.8870\n",
      "Epoch [24][200]\t Batch [450][550]\t Training Loss 0.4098\t Accuracy 0.8871\n",
      "Epoch [24][200]\t Batch [500][550]\t Training Loss 0.4104\t Accuracy 0.8863\n",
      "\n",
      "Epoch [24]\t Average training loss 0.4110\t Average training accuracy 0.8859\n",
      "Epoch [24]\t Average validation loss 0.3241\t Average validation accuracy 0.9134\n",
      "\n",
      "Epoch [25][200]\t Batch [0][550]\t Training Loss 0.3733\t Accuracy 0.9200\n",
      "Epoch [25][200]\t Batch [50][550]\t Training Loss 0.3837\t Accuracy 0.8978\n",
      "Epoch [25][200]\t Batch [100][550]\t Training Loss 0.3962\t Accuracy 0.8941\n",
      "Epoch [25][200]\t Batch [150][550]\t Training Loss 0.4080\t Accuracy 0.8879\n",
      "Epoch [25][200]\t Batch [200][550]\t Training Loss 0.4046\t Accuracy 0.8884\n",
      "Epoch [25][200]\t Batch [250][550]\t Training Loss 0.4039\t Accuracy 0.8884\n",
      "Epoch [25][200]\t Batch [300][550]\t Training Loss 0.4059\t Accuracy 0.8884\n",
      "Epoch [25][200]\t Batch [350][550]\t Training Loss 0.4082\t Accuracy 0.8879\n",
      "Epoch [25][200]\t Batch [400][550]\t Training Loss 0.4083\t Accuracy 0.8874\n",
      "Epoch [25][200]\t Batch [450][550]\t Training Loss 0.4080\t Accuracy 0.8875\n",
      "Epoch [25][200]\t Batch [500][550]\t Training Loss 0.4086\t Accuracy 0.8867\n",
      "\n",
      "Epoch [25]\t Average training loss 0.4091\t Average training accuracy 0.8864\n",
      "Epoch [25]\t Average validation loss 0.3225\t Average validation accuracy 0.9140\n",
      "\n",
      "Epoch [26][200]\t Batch [0][550]\t Training Loss 0.3708\t Accuracy 0.9200\n",
      "Epoch [26][200]\t Batch [50][550]\t Training Loss 0.3819\t Accuracy 0.8975\n",
      "Epoch [26][200]\t Batch [100][550]\t Training Loss 0.3944\t Accuracy 0.8940\n",
      "Epoch [26][200]\t Batch [150][550]\t Training Loss 0.4062\t Accuracy 0.8877\n",
      "Epoch [26][200]\t Batch [200][550]\t Training Loss 0.4028\t Accuracy 0.8885\n",
      "Epoch [26][200]\t Batch [250][550]\t Training Loss 0.4021\t Accuracy 0.8884\n",
      "Epoch [26][200]\t Batch [300][550]\t Training Loss 0.4041\t Accuracy 0.8885\n",
      "Epoch [26][200]\t Batch [350][550]\t Training Loss 0.4064\t Accuracy 0.8879\n",
      "Epoch [26][200]\t Batch [400][550]\t Training Loss 0.4065\t Accuracy 0.8875\n",
      "Epoch [26][200]\t Batch [450][550]\t Training Loss 0.4062\t Accuracy 0.8876\n",
      "Epoch [26][200]\t Batch [500][550]\t Training Loss 0.4068\t Accuracy 0.8869\n",
      "\n",
      "Epoch [26]\t Average training loss 0.4074\t Average training accuracy 0.8865\n",
      "Epoch [26]\t Average validation loss 0.3211\t Average validation accuracy 0.9142\n",
      "\n",
      "Epoch [27][200]\t Batch [0][550]\t Training Loss 0.3684\t Accuracy 0.9200\n",
      "Epoch [27][200]\t Batch [50][550]\t Training Loss 0.3802\t Accuracy 0.8978\n",
      "Epoch [27][200]\t Batch [100][550]\t Training Loss 0.3926\t Accuracy 0.8947\n",
      "Epoch [27][200]\t Batch [150][550]\t Training Loss 0.4044\t Accuracy 0.8881\n",
      "Epoch [27][200]\t Batch [200][550]\t Training Loss 0.4011\t Accuracy 0.8891\n",
      "Epoch [27][200]\t Batch [250][550]\t Training Loss 0.4004\t Accuracy 0.8891\n",
      "Epoch [27][200]\t Batch [300][550]\t Training Loss 0.4024\t Accuracy 0.8891\n",
      "Epoch [27][200]\t Batch [350][550]\t Training Loss 0.4047\t Accuracy 0.8886\n",
      "Epoch [27][200]\t Batch [400][550]\t Training Loss 0.4048\t Accuracy 0.8881\n",
      "Epoch [27][200]\t Batch [450][550]\t Training Loss 0.4045\t Accuracy 0.8882\n",
      "Epoch [27][200]\t Batch [500][550]\t Training Loss 0.4051\t Accuracy 0.8874\n",
      "\n",
      "Epoch [27]\t Average training loss 0.4057\t Average training accuracy 0.8870\n",
      "Epoch [27]\t Average validation loss 0.3196\t Average validation accuracy 0.9144\n",
      "\n",
      "Epoch [28][200]\t Batch [0][550]\t Training Loss 0.3661\t Accuracy 0.9200\n",
      "Epoch [28][200]\t Batch [50][550]\t Training Loss 0.3786\t Accuracy 0.8986\n",
      "Epoch [28][200]\t Batch [100][550]\t Training Loss 0.3909\t Accuracy 0.8954\n",
      "Epoch [28][200]\t Batch [150][550]\t Training Loss 0.4027\t Accuracy 0.8887\n",
      "Epoch [28][200]\t Batch [200][550]\t Training Loss 0.3994\t Accuracy 0.8897\n",
      "Epoch [28][200]\t Batch [250][550]\t Training Loss 0.3987\t Accuracy 0.8895\n",
      "Epoch [28][200]\t Batch [300][550]\t Training Loss 0.4007\t Accuracy 0.8894\n",
      "Epoch [28][200]\t Batch [350][550]\t Training Loss 0.4030\t Accuracy 0.8890\n",
      "Epoch [28][200]\t Batch [400][550]\t Training Loss 0.4031\t Accuracy 0.8884\n",
      "Epoch [28][200]\t Batch [450][550]\t Training Loss 0.4028\t Accuracy 0.8886\n",
      "Epoch [28][200]\t Batch [500][550]\t Training Loss 0.4034\t Accuracy 0.8878\n",
      "\n",
      "Epoch [28]\t Average training loss 0.4040\t Average training accuracy 0.8875\n",
      "Epoch [28]\t Average validation loss 0.3182\t Average validation accuracy 0.9140\n",
      "\n",
      "Epoch [29][200]\t Batch [0][550]\t Training Loss 0.3639\t Accuracy 0.9200\n",
      "Epoch [29][200]\t Batch [50][550]\t Training Loss 0.3770\t Accuracy 0.8992\n",
      "Epoch [29][200]\t Batch [100][550]\t Training Loss 0.3892\t Accuracy 0.8957\n",
      "Epoch [29][200]\t Batch [150][550]\t Training Loss 0.4011\t Accuracy 0.8888\n",
      "Epoch [29][200]\t Batch [200][550]\t Training Loss 0.3977\t Accuracy 0.8900\n",
      "Epoch [29][200]\t Batch [250][550]\t Training Loss 0.3971\t Accuracy 0.8899\n",
      "Epoch [29][200]\t Batch [300][550]\t Training Loss 0.3991\t Accuracy 0.8898\n",
      "Epoch [29][200]\t Batch [350][550]\t Training Loss 0.4014\t Accuracy 0.8894\n",
      "Epoch [29][200]\t Batch [400][550]\t Training Loss 0.4014\t Accuracy 0.8888\n",
      "Epoch [29][200]\t Batch [450][550]\t Training Loss 0.4012\t Accuracy 0.8890\n",
      "Epoch [29][200]\t Batch [500][550]\t Training Loss 0.4018\t Accuracy 0.8882\n",
      "\n",
      "Epoch [29]\t Average training loss 0.4024\t Average training accuracy 0.8879\n",
      "Epoch [29]\t Average validation loss 0.3169\t Average validation accuracy 0.9140\n",
      "\n",
      "Epoch [30][200]\t Batch [0][550]\t Training Loss 0.3617\t Accuracy 0.9200\n",
      "Epoch [30][200]\t Batch [50][550]\t Training Loss 0.3754\t Accuracy 0.8994\n",
      "Epoch [30][200]\t Batch [100][550]\t Training Loss 0.3876\t Accuracy 0.8960\n",
      "Epoch [30][200]\t Batch [150][550]\t Training Loss 0.3995\t Accuracy 0.8890\n",
      "Epoch [30][200]\t Batch [200][550]\t Training Loss 0.3961\t Accuracy 0.8902\n",
      "Epoch [30][200]\t Batch [250][550]\t Training Loss 0.3955\t Accuracy 0.8902\n",
      "Epoch [30][200]\t Batch [300][550]\t Training Loss 0.3975\t Accuracy 0.8902\n",
      "Epoch [30][200]\t Batch [350][550]\t Training Loss 0.3997\t Accuracy 0.8897\n",
      "Epoch [30][200]\t Batch [400][550]\t Training Loss 0.3998\t Accuracy 0.8891\n",
      "Epoch [30][200]\t Batch [450][550]\t Training Loss 0.3996\t Accuracy 0.8892\n",
      "Epoch [30][200]\t Batch [500][550]\t Training Loss 0.4002\t Accuracy 0.8884\n",
      "\n",
      "Epoch [30]\t Average training loss 0.4008\t Average training accuracy 0.8881\n",
      "Epoch [30]\t Average validation loss 0.3156\t Average validation accuracy 0.9148\n",
      "\n",
      "Epoch [31][200]\t Batch [0][550]\t Training Loss 0.3595\t Accuracy 0.9300\n",
      "Epoch [31][200]\t Batch [50][550]\t Training Loss 0.3739\t Accuracy 0.8994\n",
      "Epoch [31][200]\t Batch [100][550]\t Training Loss 0.3860\t Accuracy 0.8965\n",
      "Epoch [31][200]\t Batch [150][550]\t Training Loss 0.3979\t Accuracy 0.8895\n",
      "Epoch [31][200]\t Batch [200][550]\t Training Loss 0.3945\t Accuracy 0.8905\n",
      "Epoch [31][200]\t Batch [250][550]\t Training Loss 0.3939\t Accuracy 0.8906\n",
      "Epoch [31][200]\t Batch [300][550]\t Training Loss 0.3959\t Accuracy 0.8905\n",
      "Epoch [31][200]\t Batch [350][550]\t Training Loss 0.3982\t Accuracy 0.8900\n",
      "Epoch [31][200]\t Batch [400][550]\t Training Loss 0.3983\t Accuracy 0.8895\n",
      "Epoch [31][200]\t Batch [450][550]\t Training Loss 0.3981\t Accuracy 0.8896\n",
      "Epoch [31][200]\t Batch [500][550]\t Training Loss 0.3987\t Accuracy 0.8889\n",
      "\n",
      "Epoch [31]\t Average training loss 0.3993\t Average training accuracy 0.8885\n",
      "Epoch [31]\t Average validation loss 0.3143\t Average validation accuracy 0.9152\n",
      "\n",
      "Epoch [32][200]\t Batch [0][550]\t Training Loss 0.3575\t Accuracy 0.9300\n",
      "Epoch [32][200]\t Batch [50][550]\t Training Loss 0.3724\t Accuracy 0.9000\n",
      "Epoch [32][200]\t Batch [100][550]\t Training Loss 0.3844\t Accuracy 0.8967\n",
      "Epoch [32][200]\t Batch [150][550]\t Training Loss 0.3964\t Accuracy 0.8897\n",
      "Epoch [32][200]\t Batch [200][550]\t Training Loss 0.3930\t Accuracy 0.8910\n",
      "Epoch [32][200]\t Batch [250][550]\t Training Loss 0.3924\t Accuracy 0.8911\n",
      "Epoch [32][200]\t Batch [300][550]\t Training Loss 0.3944\t Accuracy 0.8910\n",
      "Epoch [32][200]\t Batch [350][550]\t Training Loss 0.3967\t Accuracy 0.8905\n",
      "Epoch [32][200]\t Batch [400][550]\t Training Loss 0.3968\t Accuracy 0.8899\n",
      "Epoch [32][200]\t Batch [450][550]\t Training Loss 0.3966\t Accuracy 0.8900\n",
      "Epoch [32][200]\t Batch [500][550]\t Training Loss 0.3972\t Accuracy 0.8893\n",
      "\n",
      "Epoch [32]\t Average training loss 0.3978\t Average training accuracy 0.8889\n",
      "Epoch [32]\t Average validation loss 0.3131\t Average validation accuracy 0.9156\n",
      "\n",
      "Epoch [33][200]\t Batch [0][550]\t Training Loss 0.3555\t Accuracy 0.9300\n",
      "Epoch [33][200]\t Batch [50][550]\t Training Loss 0.3710\t Accuracy 0.9004\n",
      "Epoch [33][200]\t Batch [100][550]\t Training Loss 0.3829\t Accuracy 0.8974\n",
      "Epoch [33][200]\t Batch [150][550]\t Training Loss 0.3950\t Accuracy 0.8902\n",
      "Epoch [33][200]\t Batch [200][550]\t Training Loss 0.3915\t Accuracy 0.8913\n",
      "Epoch [33][200]\t Batch [250][550]\t Training Loss 0.3909\t Accuracy 0.8914\n",
      "Epoch [33][200]\t Batch [300][550]\t Training Loss 0.3929\t Accuracy 0.8913\n",
      "Epoch [33][200]\t Batch [350][550]\t Training Loss 0.3952\t Accuracy 0.8907\n",
      "Epoch [33][200]\t Batch [400][550]\t Training Loss 0.3953\t Accuracy 0.8901\n",
      "Epoch [33][200]\t Batch [450][550]\t Training Loss 0.3951\t Accuracy 0.8902\n",
      "Epoch [33][200]\t Batch [500][550]\t Training Loss 0.3958\t Accuracy 0.8895\n",
      "\n",
      "Epoch [33]\t Average training loss 0.3964\t Average training accuracy 0.8892\n",
      "Epoch [33]\t Average validation loss 0.3119\t Average validation accuracy 0.9164\n",
      "\n",
      "Epoch [34][200]\t Batch [0][550]\t Training Loss 0.3535\t Accuracy 0.9300\n",
      "Epoch [34][200]\t Batch [50][550]\t Training Loss 0.3696\t Accuracy 0.9006\n",
      "Epoch [34][200]\t Batch [100][550]\t Training Loss 0.3814\t Accuracy 0.8975\n",
      "Epoch [34][200]\t Batch [150][550]\t Training Loss 0.3935\t Accuracy 0.8903\n",
      "Epoch [34][200]\t Batch [200][550]\t Training Loss 0.3901\t Accuracy 0.8915\n",
      "Epoch [34][200]\t Batch [250][550]\t Training Loss 0.3895\t Accuracy 0.8917\n",
      "Epoch [34][200]\t Batch [300][550]\t Training Loss 0.3915\t Accuracy 0.8917\n",
      "Epoch [34][200]\t Batch [350][550]\t Training Loss 0.3937\t Accuracy 0.8912\n",
      "Epoch [34][200]\t Batch [400][550]\t Training Loss 0.3939\t Accuracy 0.8906\n",
      "Epoch [34][200]\t Batch [450][550]\t Training Loss 0.3937\t Accuracy 0.8906\n",
      "Epoch [34][200]\t Batch [500][550]\t Training Loss 0.3943\t Accuracy 0.8899\n",
      "\n",
      "Epoch [34]\t Average training loss 0.3949\t Average training accuracy 0.8896\n",
      "Epoch [34]\t Average validation loss 0.3108\t Average validation accuracy 0.9166\n",
      "\n",
      "Epoch [35][200]\t Batch [0][550]\t Training Loss 0.3516\t Accuracy 0.9300\n",
      "Epoch [35][200]\t Batch [50][550]\t Training Loss 0.3682\t Accuracy 0.9014\n",
      "Epoch [35][200]\t Batch [100][550]\t Training Loss 0.3800\t Accuracy 0.8981\n",
      "Epoch [35][200]\t Batch [150][550]\t Training Loss 0.3921\t Accuracy 0.8911\n",
      "Epoch [35][200]\t Batch [200][550]\t Training Loss 0.3887\t Accuracy 0.8920\n",
      "Epoch [35][200]\t Batch [250][550]\t Training Loss 0.3881\t Accuracy 0.8922\n",
      "Epoch [35][200]\t Batch [300][550]\t Training Loss 0.3901\t Accuracy 0.8922\n",
      "Epoch [35][200]\t Batch [350][550]\t Training Loss 0.3923\t Accuracy 0.8917\n",
      "Epoch [35][200]\t Batch [400][550]\t Training Loss 0.3925\t Accuracy 0.8911\n",
      "Epoch [35][200]\t Batch [450][550]\t Training Loss 0.3923\t Accuracy 0.8911\n",
      "Epoch [35][200]\t Batch [500][550]\t Training Loss 0.3930\t Accuracy 0.8904\n",
      "\n",
      "Epoch [35]\t Average training loss 0.3936\t Average training accuracy 0.8901\n",
      "Epoch [35]\t Average validation loss 0.3096\t Average validation accuracy 0.9170\n",
      "\n",
      "Epoch [36][200]\t Batch [0][550]\t Training Loss 0.3497\t Accuracy 0.9300\n",
      "Epoch [36][200]\t Batch [50][550]\t Training Loss 0.3668\t Accuracy 0.9016\n",
      "Epoch [36][200]\t Batch [100][550]\t Training Loss 0.3786\t Accuracy 0.8984\n",
      "Epoch [36][200]\t Batch [150][550]\t Training Loss 0.3908\t Accuracy 0.8916\n",
      "Epoch [36][200]\t Batch [200][550]\t Training Loss 0.3873\t Accuracy 0.8924\n",
      "Epoch [36][200]\t Batch [250][550]\t Training Loss 0.3867\t Accuracy 0.8927\n",
      "Epoch [36][200]\t Batch [300][550]\t Training Loss 0.3888\t Accuracy 0.8927\n",
      "Epoch [36][200]\t Batch [350][550]\t Training Loss 0.3909\t Accuracy 0.8922\n",
      "Epoch [36][200]\t Batch [400][550]\t Training Loss 0.3911\t Accuracy 0.8915\n",
      "Epoch [36][200]\t Batch [450][550]\t Training Loss 0.3909\t Accuracy 0.8915\n",
      "Epoch [36][200]\t Batch [500][550]\t Training Loss 0.3916\t Accuracy 0.8908\n",
      "\n",
      "Epoch [36]\t Average training loss 0.3922\t Average training accuracy 0.8905\n",
      "Epoch [36]\t Average validation loss 0.3086\t Average validation accuracy 0.9172\n",
      "\n",
      "Epoch [37][200]\t Batch [0][550]\t Training Loss 0.3479\t Accuracy 0.9300\n",
      "Epoch [37][200]\t Batch [50][550]\t Training Loss 0.3655\t Accuracy 0.9022\n",
      "Epoch [37][200]\t Batch [100][550]\t Training Loss 0.3772\t Accuracy 0.8989\n",
      "Epoch [37][200]\t Batch [150][550]\t Training Loss 0.3894\t Accuracy 0.8921\n",
      "Epoch [37][200]\t Batch [200][550]\t Training Loss 0.3860\t Accuracy 0.8930\n",
      "Epoch [37][200]\t Batch [250][550]\t Training Loss 0.3854\t Accuracy 0.8933\n",
      "Epoch [37][200]\t Batch [300][550]\t Training Loss 0.3874\t Accuracy 0.8933\n",
      "Epoch [37][200]\t Batch [350][550]\t Training Loss 0.3896\t Accuracy 0.8927\n",
      "Epoch [37][200]\t Batch [400][550]\t Training Loss 0.3898\t Accuracy 0.8919\n",
      "Epoch [37][200]\t Batch [450][550]\t Training Loss 0.3896\t Accuracy 0.8919\n",
      "Epoch [37][200]\t Batch [500][550]\t Training Loss 0.3903\t Accuracy 0.8912\n",
      "\n",
      "Epoch [37]\t Average training loss 0.3909\t Average training accuracy 0.8909\n",
      "Epoch [37]\t Average validation loss 0.3075\t Average validation accuracy 0.9178\n",
      "\n",
      "Epoch [38][200]\t Batch [0][550]\t Training Loss 0.3461\t Accuracy 0.9400\n",
      "Epoch [38][200]\t Batch [50][550]\t Training Loss 0.3642\t Accuracy 0.9025\n",
      "Epoch [38][200]\t Batch [100][550]\t Training Loss 0.3759\t Accuracy 0.8994\n",
      "Epoch [38][200]\t Batch [150][550]\t Training Loss 0.3881\t Accuracy 0.8926\n",
      "Epoch [38][200]\t Batch [200][550]\t Training Loss 0.3847\t Accuracy 0.8935\n",
      "Epoch [38][200]\t Batch [250][550]\t Training Loss 0.3841\t Accuracy 0.8937\n",
      "Epoch [38][200]\t Batch [300][550]\t Training Loss 0.3861\t Accuracy 0.8936\n",
      "Epoch [38][200]\t Batch [350][550]\t Training Loss 0.3883\t Accuracy 0.8931\n",
      "Epoch [38][200]\t Batch [400][550]\t Training Loss 0.3885\t Accuracy 0.8924\n",
      "Epoch [38][200]\t Batch [450][550]\t Training Loss 0.3883\t Accuracy 0.8923\n",
      "Epoch [38][200]\t Batch [500][550]\t Training Loss 0.3890\t Accuracy 0.8915\n",
      "\n",
      "Epoch [38]\t Average training loss 0.3896\t Average training accuracy 0.8913\n",
      "Epoch [38]\t Average validation loss 0.3065\t Average validation accuracy 0.9184\n",
      "\n",
      "Epoch [39][200]\t Batch [0][550]\t Training Loss 0.3444\t Accuracy 0.9400\n",
      "Epoch [39][200]\t Batch [50][550]\t Training Loss 0.3630\t Accuracy 0.9027\n",
      "Epoch [39][200]\t Batch [100][550]\t Training Loss 0.3746\t Accuracy 0.8995\n",
      "Epoch [39][200]\t Batch [150][550]\t Training Loss 0.3869\t Accuracy 0.8930\n",
      "Epoch [39][200]\t Batch [200][550]\t Training Loss 0.3834\t Accuracy 0.8938\n",
      "Epoch [39][200]\t Batch [250][550]\t Training Loss 0.3829\t Accuracy 0.8940\n",
      "Epoch [39][200]\t Batch [300][550]\t Training Loss 0.3849\t Accuracy 0.8940\n",
      "Epoch [39][200]\t Batch [350][550]\t Training Loss 0.3870\t Accuracy 0.8934\n",
      "Epoch [39][200]\t Batch [400][550]\t Training Loss 0.3872\t Accuracy 0.8927\n",
      "Epoch [39][200]\t Batch [450][550]\t Training Loss 0.3871\t Accuracy 0.8926\n",
      "Epoch [39][200]\t Batch [500][550]\t Training Loss 0.3877\t Accuracy 0.8918\n",
      "\n",
      "Epoch [39]\t Average training loss 0.3884\t Average training accuracy 0.8915\n",
      "Epoch [39]\t Average validation loss 0.3055\t Average validation accuracy 0.9188\n",
      "\n",
      "Epoch [40][200]\t Batch [0][550]\t Training Loss 0.3428\t Accuracy 0.9400\n",
      "Epoch [40][200]\t Batch [50][550]\t Training Loss 0.3618\t Accuracy 0.9027\n",
      "Epoch [40][200]\t Batch [100][550]\t Training Loss 0.3733\t Accuracy 0.8996\n",
      "Epoch [40][200]\t Batch [150][550]\t Training Loss 0.3856\t Accuracy 0.8931\n",
      "Epoch [40][200]\t Batch [200][550]\t Training Loss 0.3822\t Accuracy 0.8939\n",
      "Epoch [40][200]\t Batch [250][550]\t Training Loss 0.3816\t Accuracy 0.8942\n",
      "Epoch [40][200]\t Batch [300][550]\t Training Loss 0.3836\t Accuracy 0.8942\n",
      "Epoch [40][200]\t Batch [350][550]\t Training Loss 0.3858\t Accuracy 0.8936\n",
      "Epoch [40][200]\t Batch [400][550]\t Training Loss 0.3860\t Accuracy 0.8930\n",
      "Epoch [40][200]\t Batch [450][550]\t Training Loss 0.3858\t Accuracy 0.8929\n",
      "Epoch [40][200]\t Batch [500][550]\t Training Loss 0.3865\t Accuracy 0.8921\n",
      "\n",
      "Epoch [40]\t Average training loss 0.3871\t Average training accuracy 0.8918\n",
      "Epoch [40]\t Average validation loss 0.3045\t Average validation accuracy 0.9196\n",
      "\n",
      "Epoch [41][200]\t Batch [0][550]\t Training Loss 0.3411\t Accuracy 0.9400\n",
      "Epoch [41][200]\t Batch [50][550]\t Training Loss 0.3606\t Accuracy 0.9031\n",
      "Epoch [41][200]\t Batch [100][550]\t Training Loss 0.3721\t Accuracy 0.9000\n",
      "Epoch [41][200]\t Batch [150][550]\t Training Loss 0.3844\t Accuracy 0.8934\n",
      "Epoch [41][200]\t Batch [200][550]\t Training Loss 0.3810\t Accuracy 0.8941\n",
      "Epoch [41][200]\t Batch [250][550]\t Training Loss 0.3804\t Accuracy 0.8944\n",
      "Epoch [41][200]\t Batch [300][550]\t Training Loss 0.3824\t Accuracy 0.8944\n",
      "Epoch [41][200]\t Batch [350][550]\t Training Loss 0.3846\t Accuracy 0.8938\n",
      "Epoch [41][200]\t Batch [400][550]\t Training Loss 0.3848\t Accuracy 0.8932\n",
      "Epoch [41][200]\t Batch [450][550]\t Training Loss 0.3846\t Accuracy 0.8932\n",
      "Epoch [41][200]\t Batch [500][550]\t Training Loss 0.3853\t Accuracy 0.8923\n",
      "\n",
      "Epoch [41]\t Average training loss 0.3859\t Average training accuracy 0.8920\n",
      "Epoch [41]\t Average validation loss 0.3035\t Average validation accuracy 0.9200\n",
      "\n",
      "Epoch [42][200]\t Batch [0][550]\t Training Loss 0.3395\t Accuracy 0.9400\n",
      "Epoch [42][200]\t Batch [50][550]\t Training Loss 0.3594\t Accuracy 0.9035\n",
      "Epoch [42][200]\t Batch [100][550]\t Training Loss 0.3708\t Accuracy 0.9006\n",
      "Epoch [42][200]\t Batch [150][550]\t Training Loss 0.3832\t Accuracy 0.8939\n",
      "Epoch [42][200]\t Batch [200][550]\t Training Loss 0.3798\t Accuracy 0.8946\n",
      "Epoch [42][200]\t Batch [250][550]\t Training Loss 0.3793\t Accuracy 0.8949\n",
      "Epoch [42][200]\t Batch [300][550]\t Training Loss 0.3813\t Accuracy 0.8948\n",
      "Epoch [42][200]\t Batch [350][550]\t Training Loss 0.3834\t Accuracy 0.8942\n",
      "Epoch [42][200]\t Batch [400][550]\t Training Loss 0.3836\t Accuracy 0.8935\n",
      "Epoch [42][200]\t Batch [450][550]\t Training Loss 0.3835\t Accuracy 0.8935\n",
      "Epoch [42][200]\t Batch [500][550]\t Training Loss 0.3842\t Accuracy 0.8926\n",
      "\n",
      "Epoch [42]\t Average training loss 0.3848\t Average training accuracy 0.8923\n",
      "Epoch [42]\t Average validation loss 0.3026\t Average validation accuracy 0.9198\n",
      "\n",
      "Epoch [43][200]\t Batch [0][550]\t Training Loss 0.3380\t Accuracy 0.9400\n",
      "Epoch [43][200]\t Batch [50][550]\t Training Loss 0.3582\t Accuracy 0.9035\n",
      "Epoch [43][200]\t Batch [100][550]\t Training Loss 0.3696\t Accuracy 0.9008\n",
      "Epoch [43][200]\t Batch [150][550]\t Training Loss 0.3821\t Accuracy 0.8941\n",
      "Epoch [43][200]\t Batch [200][550]\t Training Loss 0.3787\t Accuracy 0.8949\n",
      "Epoch [43][200]\t Batch [250][550]\t Training Loss 0.3781\t Accuracy 0.8951\n",
      "Epoch [43][200]\t Batch [300][550]\t Training Loss 0.3801\t Accuracy 0.8950\n",
      "Epoch [43][200]\t Batch [350][550]\t Training Loss 0.3822\t Accuracy 0.8944\n",
      "Epoch [43][200]\t Batch [400][550]\t Training Loss 0.3824\t Accuracy 0.8938\n",
      "Epoch [43][200]\t Batch [450][550]\t Training Loss 0.3823\t Accuracy 0.8938\n",
      "Epoch [43][200]\t Batch [500][550]\t Training Loss 0.3830\t Accuracy 0.8929\n",
      "\n",
      "Epoch [43]\t Average training loss 0.3836\t Average training accuracy 0.8927\n",
      "Epoch [43]\t Average validation loss 0.3017\t Average validation accuracy 0.9202\n",
      "\n",
      "Epoch [44][200]\t Batch [0][550]\t Training Loss 0.3365\t Accuracy 0.9400\n",
      "Epoch [44][200]\t Batch [50][550]\t Training Loss 0.3571\t Accuracy 0.9035\n",
      "Epoch [44][200]\t Batch [100][550]\t Training Loss 0.3685\t Accuracy 0.9010\n",
      "Epoch [44][200]\t Batch [150][550]\t Training Loss 0.3809\t Accuracy 0.8943\n",
      "Epoch [44][200]\t Batch [200][550]\t Training Loss 0.3775\t Accuracy 0.8951\n",
      "Epoch [44][200]\t Batch [250][550]\t Training Loss 0.3770\t Accuracy 0.8954\n",
      "Epoch [44][200]\t Batch [300][550]\t Training Loss 0.3790\t Accuracy 0.8954\n",
      "Epoch [44][200]\t Batch [350][550]\t Training Loss 0.3811\t Accuracy 0.8947\n",
      "Epoch [44][200]\t Batch [400][550]\t Training Loss 0.3813\t Accuracy 0.8941\n",
      "Epoch [44][200]\t Batch [450][550]\t Training Loss 0.3812\t Accuracy 0.8941\n",
      "Epoch [44][200]\t Batch [500][550]\t Training Loss 0.3819\t Accuracy 0.8932\n",
      "\n",
      "Epoch [44]\t Average training loss 0.3825\t Average training accuracy 0.8929\n",
      "Epoch [44]\t Average validation loss 0.3008\t Average validation accuracy 0.9200\n",
      "\n",
      "Epoch [45][200]\t Batch [0][550]\t Training Loss 0.3350\t Accuracy 0.9400\n",
      "Epoch [45][200]\t Batch [50][550]\t Training Loss 0.3560\t Accuracy 0.9037\n",
      "Epoch [45][200]\t Batch [100][550]\t Training Loss 0.3673\t Accuracy 0.9011\n",
      "Epoch [45][200]\t Batch [150][550]\t Training Loss 0.3798\t Accuracy 0.8944\n",
      "Epoch [45][200]\t Batch [200][550]\t Training Loss 0.3764\t Accuracy 0.8951\n",
      "Epoch [45][200]\t Batch [250][550]\t Training Loss 0.3759\t Accuracy 0.8955\n",
      "Epoch [45][200]\t Batch [300][550]\t Training Loss 0.3779\t Accuracy 0.8955\n",
      "Epoch [45][200]\t Batch [350][550]\t Training Loss 0.3799\t Accuracy 0.8949\n",
      "Epoch [45][200]\t Batch [400][550]\t Training Loss 0.3802\t Accuracy 0.8943\n",
      "Epoch [45][200]\t Batch [450][550]\t Training Loss 0.3801\t Accuracy 0.8943\n",
      "Epoch [45][200]\t Batch [500][550]\t Training Loss 0.3808\t Accuracy 0.8934\n",
      "\n",
      "Epoch [45]\t Average training loss 0.3814\t Average training accuracy 0.8931\n",
      "Epoch [45]\t Average validation loss 0.2999\t Average validation accuracy 0.9202\n",
      "\n",
      "Epoch [46][200]\t Batch [0][550]\t Training Loss 0.3336\t Accuracy 0.9400\n",
      "Epoch [46][200]\t Batch [50][550]\t Training Loss 0.3549\t Accuracy 0.9039\n",
      "Epoch [46][200]\t Batch [100][550]\t Training Loss 0.3662\t Accuracy 0.9016\n",
      "Epoch [46][200]\t Batch [150][550]\t Training Loss 0.3788\t Accuracy 0.8948\n",
      "Epoch [46][200]\t Batch [200][550]\t Training Loss 0.3753\t Accuracy 0.8955\n",
      "Epoch [46][200]\t Batch [250][550]\t Training Loss 0.3748\t Accuracy 0.8961\n",
      "Epoch [46][200]\t Batch [300][550]\t Training Loss 0.3768\t Accuracy 0.8960\n",
      "Epoch [46][200]\t Batch [350][550]\t Training Loss 0.3788\t Accuracy 0.8954\n",
      "Epoch [46][200]\t Batch [400][550]\t Training Loss 0.3791\t Accuracy 0.8949\n",
      "Epoch [46][200]\t Batch [450][550]\t Training Loss 0.3790\t Accuracy 0.8948\n",
      "Epoch [46][200]\t Batch [500][550]\t Training Loss 0.3797\t Accuracy 0.8939\n",
      "\n",
      "Epoch [46]\t Average training loss 0.3803\t Average training accuracy 0.8936\n",
      "Epoch [46]\t Average validation loss 0.2991\t Average validation accuracy 0.9210\n",
      "\n",
      "Epoch [47][200]\t Batch [0][550]\t Training Loss 0.3323\t Accuracy 0.9400\n",
      "Epoch [47][200]\t Batch [50][550]\t Training Loss 0.3538\t Accuracy 0.9041\n",
      "Epoch [47][200]\t Batch [100][550]\t Training Loss 0.3651\t Accuracy 0.9020\n",
      "Epoch [47][200]\t Batch [150][550]\t Training Loss 0.3777\t Accuracy 0.8950\n",
      "Epoch [47][200]\t Batch [200][550]\t Training Loss 0.3743\t Accuracy 0.8959\n",
      "Epoch [47][200]\t Batch [250][550]\t Training Loss 0.3738\t Accuracy 0.8965\n",
      "Epoch [47][200]\t Batch [300][550]\t Training Loss 0.3758\t Accuracy 0.8965\n",
      "Epoch [47][200]\t Batch [350][550]\t Training Loss 0.3778\t Accuracy 0.8958\n",
      "Epoch [47][200]\t Batch [400][550]\t Training Loss 0.3781\t Accuracy 0.8953\n",
      "Epoch [47][200]\t Batch [450][550]\t Training Loss 0.3779\t Accuracy 0.8952\n",
      "Epoch [47][200]\t Batch [500][550]\t Training Loss 0.3787\t Accuracy 0.8943\n",
      "\n",
      "Epoch [47]\t Average training loss 0.3793\t Average training accuracy 0.8940\n",
      "Epoch [47]\t Average validation loss 0.2983\t Average validation accuracy 0.9212\n",
      "\n",
      "Epoch [48][200]\t Batch [0][550]\t Training Loss 0.3309\t Accuracy 0.9400\n",
      "Epoch [48][200]\t Batch [50][550]\t Training Loss 0.3528\t Accuracy 0.9045\n",
      "Epoch [48][200]\t Batch [100][550]\t Training Loss 0.3640\t Accuracy 0.9024\n",
      "Epoch [48][200]\t Batch [150][550]\t Training Loss 0.3767\t Accuracy 0.8954\n",
      "Epoch [48][200]\t Batch [200][550]\t Training Loss 0.3732\t Accuracy 0.8961\n",
      "Epoch [48][200]\t Batch [250][550]\t Training Loss 0.3727\t Accuracy 0.8967\n",
      "Epoch [48][200]\t Batch [300][550]\t Training Loss 0.3747\t Accuracy 0.8966\n",
      "Epoch [48][200]\t Batch [350][550]\t Training Loss 0.3767\t Accuracy 0.8959\n",
      "Epoch [48][200]\t Batch [400][550]\t Training Loss 0.3770\t Accuracy 0.8954\n",
      "Epoch [48][200]\t Batch [450][550]\t Training Loss 0.3769\t Accuracy 0.8953\n",
      "Epoch [48][200]\t Batch [500][550]\t Training Loss 0.3776\t Accuracy 0.8944\n",
      "\n",
      "Epoch [48]\t Average training loss 0.3782\t Average training accuracy 0.8941\n",
      "Epoch [48]\t Average validation loss 0.2975\t Average validation accuracy 0.9216\n",
      "\n",
      "Epoch [49][200]\t Batch [0][550]\t Training Loss 0.3297\t Accuracy 0.9400\n",
      "Epoch [49][200]\t Batch [50][550]\t Training Loss 0.3518\t Accuracy 0.9041\n",
      "Epoch [49][200]\t Batch [100][550]\t Training Loss 0.3630\t Accuracy 0.9023\n",
      "Epoch [49][200]\t Batch [150][550]\t Training Loss 0.3756\t Accuracy 0.8954\n",
      "Epoch [49][200]\t Batch [200][550]\t Training Loss 0.3722\t Accuracy 0.8962\n",
      "Epoch [49][200]\t Batch [250][550]\t Training Loss 0.3717\t Accuracy 0.8967\n",
      "Epoch [49][200]\t Batch [300][550]\t Training Loss 0.3737\t Accuracy 0.8968\n",
      "Epoch [49][200]\t Batch [350][550]\t Training Loss 0.3757\t Accuracy 0.8960\n",
      "Epoch [49][200]\t Batch [400][550]\t Training Loss 0.3760\t Accuracy 0.8956\n",
      "Epoch [49][200]\t Batch [450][550]\t Training Loss 0.3759\t Accuracy 0.8954\n",
      "Epoch [49][200]\t Batch [500][550]\t Training Loss 0.3766\t Accuracy 0.8945\n",
      "\n",
      "Epoch [49]\t Average training loss 0.3772\t Average training accuracy 0.8943\n",
      "Epoch [49]\t Average validation loss 0.2967\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [50][200]\t Batch [0][550]\t Training Loss 0.3284\t Accuracy 0.9400\n",
      "Epoch [50][200]\t Batch [50][550]\t Training Loss 0.3508\t Accuracy 0.9043\n",
      "Epoch [50][200]\t Batch [100][550]\t Training Loss 0.3619\t Accuracy 0.9027\n",
      "Epoch [50][200]\t Batch [150][550]\t Training Loss 0.3746\t Accuracy 0.8955\n",
      "Epoch [50][200]\t Batch [200][550]\t Training Loss 0.3712\t Accuracy 0.8963\n",
      "Epoch [50][200]\t Batch [250][550]\t Training Loss 0.3707\t Accuracy 0.8969\n",
      "Epoch [50][200]\t Batch [300][550]\t Training Loss 0.3727\t Accuracy 0.8970\n",
      "Epoch [50][200]\t Batch [350][550]\t Training Loss 0.3747\t Accuracy 0.8963\n",
      "Epoch [50][200]\t Batch [400][550]\t Training Loss 0.3750\t Accuracy 0.8959\n",
      "Epoch [50][200]\t Batch [450][550]\t Training Loss 0.3749\t Accuracy 0.8957\n",
      "Epoch [50][200]\t Batch [500][550]\t Training Loss 0.3756\t Accuracy 0.8947\n",
      "\n",
      "Epoch [50]\t Average training loss 0.3762\t Average training accuracy 0.8946\n",
      "Epoch [50]\t Average validation loss 0.2959\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [51][200]\t Batch [0][550]\t Training Loss 0.3272\t Accuracy 0.9400\n",
      "Epoch [51][200]\t Batch [50][550]\t Training Loss 0.3498\t Accuracy 0.9043\n",
      "Epoch [51][200]\t Batch [100][550]\t Training Loss 0.3609\t Accuracy 0.9027\n",
      "Epoch [51][200]\t Batch [150][550]\t Training Loss 0.3736\t Accuracy 0.8960\n",
      "Epoch [51][200]\t Batch [200][550]\t Training Loss 0.3702\t Accuracy 0.8969\n",
      "Epoch [51][200]\t Batch [250][550]\t Training Loss 0.3698\t Accuracy 0.8975\n",
      "Epoch [51][200]\t Batch [300][550]\t Training Loss 0.3717\t Accuracy 0.8976\n",
      "Epoch [51][200]\t Batch [350][550]\t Training Loss 0.3737\t Accuracy 0.8968\n",
      "Epoch [51][200]\t Batch [400][550]\t Training Loss 0.3740\t Accuracy 0.8964\n",
      "Epoch [51][200]\t Batch [450][550]\t Training Loss 0.3739\t Accuracy 0.8961\n",
      "Epoch [51][200]\t Batch [500][550]\t Training Loss 0.3746\t Accuracy 0.8951\n",
      "\n",
      "Epoch [51]\t Average training loss 0.3753\t Average training accuracy 0.8950\n",
      "Epoch [51]\t Average validation loss 0.2951\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [52][200]\t Batch [0][550]\t Training Loss 0.3260\t Accuracy 0.9400\n",
      "Epoch [52][200]\t Batch [50][550]\t Training Loss 0.3488\t Accuracy 0.9049\n",
      "Epoch [52][200]\t Batch [100][550]\t Training Loss 0.3599\t Accuracy 0.9033\n",
      "Epoch [52][200]\t Batch [150][550]\t Training Loss 0.3727\t Accuracy 0.8965\n",
      "Epoch [52][200]\t Batch [200][550]\t Training Loss 0.3693\t Accuracy 0.8974\n",
      "Epoch [52][200]\t Batch [250][550]\t Training Loss 0.3688\t Accuracy 0.8979\n",
      "Epoch [52][200]\t Batch [300][550]\t Training Loss 0.3708\t Accuracy 0.8979\n",
      "Epoch [52][200]\t Batch [350][550]\t Training Loss 0.3727\t Accuracy 0.8972\n",
      "Epoch [52][200]\t Batch [400][550]\t Training Loss 0.3730\t Accuracy 0.8968\n",
      "Epoch [52][200]\t Batch [450][550]\t Training Loss 0.3729\t Accuracy 0.8965\n",
      "Epoch [52][200]\t Batch [500][550]\t Training Loss 0.3737\t Accuracy 0.8955\n",
      "\n",
      "Epoch [52]\t Average training loss 0.3743\t Average training accuracy 0.8954\n",
      "Epoch [52]\t Average validation loss 0.2944\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [53][200]\t Batch [0][550]\t Training Loss 0.3248\t Accuracy 0.9400\n",
      "Epoch [53][200]\t Batch [50][550]\t Training Loss 0.3478\t Accuracy 0.9051\n",
      "Epoch [53][200]\t Batch [100][550]\t Training Loss 0.3589\t Accuracy 0.9037\n",
      "Epoch [53][200]\t Batch [150][550]\t Training Loss 0.3717\t Accuracy 0.8968\n",
      "Epoch [53][200]\t Batch [200][550]\t Training Loss 0.3683\t Accuracy 0.8976\n",
      "Epoch [53][200]\t Batch [250][550]\t Training Loss 0.3679\t Accuracy 0.8981\n",
      "Epoch [53][200]\t Batch [300][550]\t Training Loss 0.3698\t Accuracy 0.8981\n",
      "Epoch [53][200]\t Batch [350][550]\t Training Loss 0.3717\t Accuracy 0.8974\n",
      "Epoch [53][200]\t Batch [400][550]\t Training Loss 0.3721\t Accuracy 0.8969\n",
      "Epoch [53][200]\t Batch [450][550]\t Training Loss 0.3720\t Accuracy 0.8967\n",
      "Epoch [53][200]\t Batch [500][550]\t Training Loss 0.3727\t Accuracy 0.8957\n",
      "\n",
      "Epoch [53]\t Average training loss 0.3734\t Average training accuracy 0.8956\n",
      "Epoch [53]\t Average validation loss 0.2936\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [54][200]\t Batch [0][550]\t Training Loss 0.3236\t Accuracy 0.9400\n",
      "Epoch [54][200]\t Batch [50][550]\t Training Loss 0.3469\t Accuracy 0.9051\n",
      "Epoch [54][200]\t Batch [100][550]\t Training Loss 0.3579\t Accuracy 0.9040\n",
      "Epoch [54][200]\t Batch [150][550]\t Training Loss 0.3708\t Accuracy 0.8970\n",
      "Epoch [54][200]\t Batch [200][550]\t Training Loss 0.3674\t Accuracy 0.8978\n",
      "Epoch [54][200]\t Batch [250][550]\t Training Loss 0.3669\t Accuracy 0.8982\n",
      "Epoch [54][200]\t Batch [300][550]\t Training Loss 0.3689\t Accuracy 0.8982\n",
      "Epoch [54][200]\t Batch [350][550]\t Training Loss 0.3708\t Accuracy 0.8976\n",
      "Epoch [54][200]\t Batch [400][550]\t Training Loss 0.3711\t Accuracy 0.8971\n",
      "Epoch [54][200]\t Batch [450][550]\t Training Loss 0.3711\t Accuracy 0.8969\n",
      "Epoch [54][200]\t Batch [500][550]\t Training Loss 0.3718\t Accuracy 0.8959\n",
      "\n",
      "Epoch [54]\t Average training loss 0.3724\t Average training accuracy 0.8958\n",
      "Epoch [54]\t Average validation loss 0.2929\t Average validation accuracy 0.9212\n",
      "\n",
      "Epoch [55][200]\t Batch [0][550]\t Training Loss 0.3225\t Accuracy 0.9400\n",
      "Epoch [55][200]\t Batch [50][550]\t Training Loss 0.3460\t Accuracy 0.9055\n",
      "Epoch [55][200]\t Batch [100][550]\t Training Loss 0.3570\t Accuracy 0.9043\n",
      "Epoch [55][200]\t Batch [150][550]\t Training Loss 0.3699\t Accuracy 0.8972\n",
      "Epoch [55][200]\t Batch [200][550]\t Training Loss 0.3665\t Accuracy 0.8981\n",
      "Epoch [55][200]\t Batch [250][550]\t Training Loss 0.3660\t Accuracy 0.8986\n",
      "Epoch [55][200]\t Batch [300][550]\t Training Loss 0.3680\t Accuracy 0.8986\n",
      "Epoch [55][200]\t Batch [350][550]\t Training Loss 0.3699\t Accuracy 0.8979\n",
      "Epoch [55][200]\t Batch [400][550]\t Training Loss 0.3702\t Accuracy 0.8974\n",
      "Epoch [55][200]\t Batch [450][550]\t Training Loss 0.3702\t Accuracy 0.8971\n",
      "Epoch [55][200]\t Batch [500][550]\t Training Loss 0.3709\t Accuracy 0.8962\n",
      "\n",
      "Epoch [55]\t Average training loss 0.3715\t Average training accuracy 0.8960\n",
      "Epoch [55]\t Average validation loss 0.2922\t Average validation accuracy 0.9216\n",
      "\n",
      "Epoch [56][200]\t Batch [0][550]\t Training Loss 0.3214\t Accuracy 0.9400\n",
      "Epoch [56][200]\t Batch [50][550]\t Training Loss 0.3451\t Accuracy 0.9059\n",
      "Epoch [56][200]\t Batch [100][550]\t Training Loss 0.3561\t Accuracy 0.9046\n",
      "Epoch [56][200]\t Batch [150][550]\t Training Loss 0.3690\t Accuracy 0.8974\n",
      "Epoch [56][200]\t Batch [200][550]\t Training Loss 0.3656\t Accuracy 0.8984\n",
      "Epoch [56][200]\t Batch [250][550]\t Training Loss 0.3651\t Accuracy 0.8988\n",
      "Epoch [56][200]\t Batch [300][550]\t Training Loss 0.3671\t Accuracy 0.8988\n",
      "Epoch [56][200]\t Batch [350][550]\t Training Loss 0.3690\t Accuracy 0.8982\n",
      "Epoch [56][200]\t Batch [400][550]\t Training Loss 0.3693\t Accuracy 0.8976\n",
      "Epoch [56][200]\t Batch [450][550]\t Training Loss 0.3693\t Accuracy 0.8973\n",
      "Epoch [56][200]\t Batch [500][550]\t Training Loss 0.3700\t Accuracy 0.8964\n",
      "\n",
      "Epoch [56]\t Average training loss 0.3706\t Average training accuracy 0.8963\n",
      "Epoch [56]\t Average validation loss 0.2915\t Average validation accuracy 0.9218\n",
      "\n",
      "Epoch [57][200]\t Batch [0][550]\t Training Loss 0.3203\t Accuracy 0.9400\n",
      "Epoch [57][200]\t Batch [50][550]\t Training Loss 0.3442\t Accuracy 0.9065\n",
      "Epoch [57][200]\t Batch [100][550]\t Training Loss 0.3551\t Accuracy 0.9048\n",
      "Epoch [57][200]\t Batch [150][550]\t Training Loss 0.3681\t Accuracy 0.8975\n",
      "Epoch [57][200]\t Batch [200][550]\t Training Loss 0.3647\t Accuracy 0.8986\n",
      "Epoch [57][200]\t Batch [250][550]\t Training Loss 0.3643\t Accuracy 0.8990\n",
      "Epoch [57][200]\t Batch [300][550]\t Training Loss 0.3662\t Accuracy 0.8991\n",
      "Epoch [57][200]\t Batch [350][550]\t Training Loss 0.3681\t Accuracy 0.8984\n",
      "Epoch [57][200]\t Batch [400][550]\t Training Loss 0.3684\t Accuracy 0.8977\n",
      "Epoch [57][200]\t Batch [450][550]\t Training Loss 0.3684\t Accuracy 0.8975\n",
      "Epoch [57][200]\t Batch [500][550]\t Training Loss 0.3691\t Accuracy 0.8966\n",
      "\n",
      "Epoch [57]\t Average training loss 0.3697\t Average training accuracy 0.8965\n",
      "Epoch [57]\t Average validation loss 0.2908\t Average validation accuracy 0.9222\n",
      "\n",
      "Epoch [58][200]\t Batch [0][550]\t Training Loss 0.3193\t Accuracy 0.9400\n",
      "Epoch [58][200]\t Batch [50][550]\t Training Loss 0.3433\t Accuracy 0.9065\n",
      "Epoch [58][200]\t Batch [100][550]\t Training Loss 0.3542\t Accuracy 0.9047\n",
      "Epoch [58][200]\t Batch [150][550]\t Training Loss 0.3672\t Accuracy 0.8977\n",
      "Epoch [58][200]\t Batch [200][550]\t Training Loss 0.3638\t Accuracy 0.8987\n",
      "Epoch [58][200]\t Batch [250][550]\t Training Loss 0.3634\t Accuracy 0.8991\n",
      "Epoch [58][200]\t Batch [300][550]\t Training Loss 0.3653\t Accuracy 0.8992\n",
      "Epoch [58][200]\t Batch [350][550]\t Training Loss 0.3672\t Accuracy 0.8985\n",
      "Epoch [58][200]\t Batch [400][550]\t Training Loss 0.3676\t Accuracy 0.8979\n",
      "Epoch [58][200]\t Batch [450][550]\t Training Loss 0.3675\t Accuracy 0.8976\n",
      "Epoch [58][200]\t Batch [500][550]\t Training Loss 0.3683\t Accuracy 0.8967\n",
      "\n",
      "Epoch [58]\t Average training loss 0.3689\t Average training accuracy 0.8966\n",
      "Epoch [58]\t Average validation loss 0.2902\t Average validation accuracy 0.9222\n",
      "\n",
      "Epoch [59][200]\t Batch [0][550]\t Training Loss 0.3183\t Accuracy 0.9400\n",
      "Epoch [59][200]\t Batch [50][550]\t Training Loss 0.3424\t Accuracy 0.9075\n",
      "Epoch [59][200]\t Batch [100][550]\t Training Loss 0.3533\t Accuracy 0.9053\n",
      "Epoch [59][200]\t Batch [150][550]\t Training Loss 0.3664\t Accuracy 0.8983\n",
      "Epoch [59][200]\t Batch [200][550]\t Training Loss 0.3630\t Accuracy 0.8992\n",
      "Epoch [59][200]\t Batch [250][550]\t Training Loss 0.3626\t Accuracy 0.8996\n",
      "Epoch [59][200]\t Batch [300][550]\t Training Loss 0.3645\t Accuracy 0.8995\n",
      "Epoch [59][200]\t Batch [350][550]\t Training Loss 0.3663\t Accuracy 0.8988\n",
      "Epoch [59][200]\t Batch [400][550]\t Training Loss 0.3667\t Accuracy 0.8982\n",
      "Epoch [59][200]\t Batch [450][550]\t Training Loss 0.3666\t Accuracy 0.8978\n",
      "Epoch [59][200]\t Batch [500][550]\t Training Loss 0.3674\t Accuracy 0.8970\n",
      "\n",
      "Epoch [59]\t Average training loss 0.3680\t Average training accuracy 0.8969\n",
      "Epoch [59]\t Average validation loss 0.2895\t Average validation accuracy 0.9218\n",
      "\n",
      "Epoch [60][200]\t Batch [0][550]\t Training Loss 0.3173\t Accuracy 0.9400\n",
      "Epoch [60][200]\t Batch [50][550]\t Training Loss 0.3416\t Accuracy 0.9075\n",
      "Epoch [60][200]\t Batch [100][550]\t Training Loss 0.3525\t Accuracy 0.9051\n",
      "Epoch [60][200]\t Batch [150][550]\t Training Loss 0.3655\t Accuracy 0.8987\n",
      "Epoch [60][200]\t Batch [200][550]\t Training Loss 0.3621\t Accuracy 0.8995\n",
      "Epoch [60][200]\t Batch [250][550]\t Training Loss 0.3617\t Accuracy 0.8998\n",
      "Epoch [60][200]\t Batch [300][550]\t Training Loss 0.3636\t Accuracy 0.8997\n",
      "Epoch [60][200]\t Batch [350][550]\t Training Loss 0.3655\t Accuracy 0.8989\n",
      "Epoch [60][200]\t Batch [400][550]\t Training Loss 0.3658\t Accuracy 0.8983\n",
      "Epoch [60][200]\t Batch [450][550]\t Training Loss 0.3658\t Accuracy 0.8980\n",
      "Epoch [60][200]\t Batch [500][550]\t Training Loss 0.3666\t Accuracy 0.8971\n",
      "\n",
      "Epoch [60]\t Average training loss 0.3672\t Average training accuracy 0.8970\n",
      "Epoch [60]\t Average validation loss 0.2889\t Average validation accuracy 0.9218\n",
      "\n",
      "Epoch [61][200]\t Batch [0][550]\t Training Loss 0.3163\t Accuracy 0.9400\n",
      "Epoch [61][200]\t Batch [50][550]\t Training Loss 0.3407\t Accuracy 0.9076\n",
      "Epoch [61][200]\t Batch [100][550]\t Training Loss 0.3516\t Accuracy 0.9053\n",
      "Epoch [61][200]\t Batch [150][550]\t Training Loss 0.3647\t Accuracy 0.8991\n",
      "Epoch [61][200]\t Batch [200][550]\t Training Loss 0.3613\t Accuracy 0.8998\n",
      "Epoch [61][200]\t Batch [250][550]\t Training Loss 0.3609\t Accuracy 0.9000\n",
      "Epoch [61][200]\t Batch [300][550]\t Training Loss 0.3628\t Accuracy 0.8999\n",
      "Epoch [61][200]\t Batch [350][550]\t Training Loss 0.3646\t Accuracy 0.8992\n",
      "Epoch [61][200]\t Batch [400][550]\t Training Loss 0.3650\t Accuracy 0.8985\n",
      "Epoch [61][200]\t Batch [450][550]\t Training Loss 0.3650\t Accuracy 0.8982\n",
      "Epoch [61][200]\t Batch [500][550]\t Training Loss 0.3657\t Accuracy 0.8974\n",
      "\n",
      "Epoch [61]\t Average training loss 0.3664\t Average training accuracy 0.8972\n",
      "Epoch [61]\t Average validation loss 0.2883\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [62][200]\t Batch [0][550]\t Training Loss 0.3153\t Accuracy 0.9400\n",
      "Epoch [62][200]\t Batch [50][550]\t Training Loss 0.3399\t Accuracy 0.9075\n",
      "Epoch [62][200]\t Batch [100][550]\t Training Loss 0.3508\t Accuracy 0.9054\n",
      "Epoch [62][200]\t Batch [150][550]\t Training Loss 0.3639\t Accuracy 0.8993\n",
      "Epoch [62][200]\t Batch [200][550]\t Training Loss 0.3605\t Accuracy 0.9000\n",
      "Epoch [62][200]\t Batch [250][550]\t Training Loss 0.3601\t Accuracy 0.9002\n",
      "Epoch [62][200]\t Batch [300][550]\t Training Loss 0.3620\t Accuracy 0.9002\n",
      "Epoch [62][200]\t Batch [350][550]\t Training Loss 0.3638\t Accuracy 0.8995\n",
      "Epoch [62][200]\t Batch [400][550]\t Training Loss 0.3642\t Accuracy 0.8988\n",
      "Epoch [62][200]\t Batch [450][550]\t Training Loss 0.3642\t Accuracy 0.8984\n",
      "Epoch [62][200]\t Batch [500][550]\t Training Loss 0.3649\t Accuracy 0.8976\n",
      "\n",
      "Epoch [62]\t Average training loss 0.3655\t Average training accuracy 0.8974\n",
      "Epoch [62]\t Average validation loss 0.2876\t Average validation accuracy 0.9210\n",
      "\n",
      "Epoch [63][200]\t Batch [0][550]\t Training Loss 0.3143\t Accuracy 0.9400\n",
      "Epoch [63][200]\t Batch [50][550]\t Training Loss 0.3391\t Accuracy 0.9080\n",
      "Epoch [63][200]\t Batch [100][550]\t Training Loss 0.3499\t Accuracy 0.9059\n",
      "Epoch [63][200]\t Batch [150][550]\t Training Loss 0.3630\t Accuracy 0.8996\n",
      "Epoch [63][200]\t Batch [200][550]\t Training Loss 0.3597\t Accuracy 0.9004\n",
      "Epoch [63][200]\t Batch [250][550]\t Training Loss 0.3593\t Accuracy 0.9006\n",
      "Epoch [63][200]\t Batch [300][550]\t Training Loss 0.3612\t Accuracy 0.9005\n",
      "Epoch [63][200]\t Batch [350][550]\t Training Loss 0.3630\t Accuracy 0.8997\n",
      "Epoch [63][200]\t Batch [400][550]\t Training Loss 0.3634\t Accuracy 0.8989\n",
      "Epoch [63][200]\t Batch [450][550]\t Training Loss 0.3633\t Accuracy 0.8986\n",
      "Epoch [63][200]\t Batch [500][550]\t Training Loss 0.3641\t Accuracy 0.8978\n",
      "\n",
      "Epoch [63]\t Average training loss 0.3647\t Average training accuracy 0.8976\n",
      "Epoch [63]\t Average validation loss 0.2870\t Average validation accuracy 0.9212\n",
      "\n",
      "Epoch [64][200]\t Batch [0][550]\t Training Loss 0.3134\t Accuracy 0.9400\n",
      "Epoch [64][200]\t Batch [50][550]\t Training Loss 0.3383\t Accuracy 0.9080\n",
      "Epoch [64][200]\t Batch [100][550]\t Training Loss 0.3491\t Accuracy 0.9059\n",
      "Epoch [64][200]\t Batch [150][550]\t Training Loss 0.3623\t Accuracy 0.8997\n",
      "Epoch [64][200]\t Batch [200][550]\t Training Loss 0.3589\t Accuracy 0.9004\n",
      "Epoch [64][200]\t Batch [250][550]\t Training Loss 0.3585\t Accuracy 0.9006\n",
      "Epoch [64][200]\t Batch [300][550]\t Training Loss 0.3604\t Accuracy 0.9006\n",
      "Epoch [64][200]\t Batch [350][550]\t Training Loss 0.3622\t Accuracy 0.8997\n",
      "Epoch [64][200]\t Batch [400][550]\t Training Loss 0.3626\t Accuracy 0.8990\n",
      "Epoch [64][200]\t Batch [450][550]\t Training Loss 0.3626\t Accuracy 0.8986\n",
      "Epoch [64][200]\t Batch [500][550]\t Training Loss 0.3633\t Accuracy 0.8979\n",
      "\n",
      "Epoch [64]\t Average training loss 0.3639\t Average training accuracy 0.8977\n",
      "Epoch [64]\t Average validation loss 0.2864\t Average validation accuracy 0.9216\n",
      "\n",
      "Epoch [65][200]\t Batch [0][550]\t Training Loss 0.3125\t Accuracy 0.9400\n",
      "Epoch [65][200]\t Batch [50][550]\t Training Loss 0.3375\t Accuracy 0.9080\n",
      "Epoch [65][200]\t Batch [100][550]\t Training Loss 0.3483\t Accuracy 0.9062\n",
      "Epoch [65][200]\t Batch [150][550]\t Training Loss 0.3615\t Accuracy 0.9001\n",
      "Epoch [65][200]\t Batch [200][550]\t Training Loss 0.3581\t Accuracy 0.9007\n",
      "Epoch [65][200]\t Batch [250][550]\t Training Loss 0.3577\t Accuracy 0.9010\n",
      "Epoch [65][200]\t Batch [300][550]\t Training Loss 0.3596\t Accuracy 0.9009\n",
      "Epoch [65][200]\t Batch [350][550]\t Training Loss 0.3614\t Accuracy 0.9000\n",
      "Epoch [65][200]\t Batch [400][550]\t Training Loss 0.3618\t Accuracy 0.8992\n",
      "Epoch [65][200]\t Batch [450][550]\t Training Loss 0.3618\t Accuracy 0.8989\n",
      "Epoch [65][200]\t Batch [500][550]\t Training Loss 0.3625\t Accuracy 0.8981\n",
      "\n",
      "Epoch [65]\t Average training loss 0.3632\t Average training accuracy 0.8979\n",
      "Epoch [65]\t Average validation loss 0.2858\t Average validation accuracy 0.9222\n",
      "\n",
      "Epoch [66][200]\t Batch [0][550]\t Training Loss 0.3116\t Accuracy 0.9400\n",
      "Epoch [66][200]\t Batch [50][550]\t Training Loss 0.3367\t Accuracy 0.9084\n",
      "Epoch [66][200]\t Batch [100][550]\t Training Loss 0.3475\t Accuracy 0.9065\n",
      "Epoch [66][200]\t Batch [150][550]\t Training Loss 0.3607\t Accuracy 0.9005\n",
      "Epoch [66][200]\t Batch [200][550]\t Training Loss 0.3573\t Accuracy 0.9011\n",
      "Epoch [66][200]\t Batch [250][550]\t Training Loss 0.3570\t Accuracy 0.9013\n",
      "Epoch [66][200]\t Batch [300][550]\t Training Loss 0.3588\t Accuracy 0.9012\n",
      "Epoch [66][200]\t Batch [350][550]\t Training Loss 0.3606\t Accuracy 0.9002\n",
      "Epoch [66][200]\t Batch [400][550]\t Training Loss 0.3610\t Accuracy 0.8995\n",
      "Epoch [66][200]\t Batch [450][550]\t Training Loss 0.3610\t Accuracy 0.8992\n",
      "Epoch [66][200]\t Batch [500][550]\t Training Loss 0.3618\t Accuracy 0.8985\n",
      "\n",
      "Epoch [66]\t Average training loss 0.3624\t Average training accuracy 0.8983\n",
      "Epoch [66]\t Average validation loss 0.2852\t Average validation accuracy 0.9224\n",
      "\n",
      "Epoch [67][200]\t Batch [0][550]\t Training Loss 0.3107\t Accuracy 0.9400\n",
      "Epoch [67][200]\t Batch [50][550]\t Training Loss 0.3359\t Accuracy 0.9084\n",
      "Epoch [67][200]\t Batch [100][550]\t Training Loss 0.3467\t Accuracy 0.9064\n",
      "Epoch [67][200]\t Batch [150][550]\t Training Loss 0.3599\t Accuracy 0.9003\n",
      "Epoch [67][200]\t Batch [200][550]\t Training Loss 0.3566\t Accuracy 0.9010\n",
      "Epoch [67][200]\t Batch [250][550]\t Training Loss 0.3562\t Accuracy 0.9013\n",
      "Epoch [67][200]\t Batch [300][550]\t Training Loss 0.3581\t Accuracy 0.9012\n",
      "Epoch [67][200]\t Batch [350][550]\t Training Loss 0.3598\t Accuracy 0.9003\n",
      "Epoch [67][200]\t Batch [400][550]\t Training Loss 0.3603\t Accuracy 0.8995\n",
      "Epoch [67][200]\t Batch [450][550]\t Training Loss 0.3602\t Accuracy 0.8993\n",
      "Epoch [67][200]\t Batch [500][550]\t Training Loss 0.3610\t Accuracy 0.8986\n",
      "\n",
      "Epoch [67]\t Average training loss 0.3616\t Average training accuracy 0.8984\n",
      "Epoch [67]\t Average validation loss 0.2847\t Average validation accuracy 0.9228\n",
      "\n",
      "Epoch [68][200]\t Batch [0][550]\t Training Loss 0.3098\t Accuracy 0.9400\n",
      "Epoch [68][200]\t Batch [50][550]\t Training Loss 0.3351\t Accuracy 0.9092\n",
      "Epoch [68][200]\t Batch [100][550]\t Training Loss 0.3459\t Accuracy 0.9068\n",
      "Epoch [68][200]\t Batch [150][550]\t Training Loss 0.3592\t Accuracy 0.9008\n",
      "Epoch [68][200]\t Batch [200][550]\t Training Loss 0.3558\t Accuracy 0.9015\n",
      "Epoch [68][200]\t Batch [250][550]\t Training Loss 0.3555\t Accuracy 0.9018\n",
      "Epoch [68][200]\t Batch [300][550]\t Training Loss 0.3573\t Accuracy 0.9016\n",
      "Epoch [68][200]\t Batch [350][550]\t Training Loss 0.3590\t Accuracy 0.9006\n",
      "Epoch [68][200]\t Batch [400][550]\t Training Loss 0.3595\t Accuracy 0.8999\n",
      "Epoch [68][200]\t Batch [450][550]\t Training Loss 0.3595\t Accuracy 0.8996\n",
      "Epoch [68][200]\t Batch [500][550]\t Training Loss 0.3603\t Accuracy 0.8989\n",
      "\n",
      "Epoch [68]\t Average training loss 0.3609\t Average training accuracy 0.8987\n",
      "Epoch [68]\t Average validation loss 0.2841\t Average validation accuracy 0.9232\n",
      "\n",
      "Epoch [69][200]\t Batch [0][550]\t Training Loss 0.3090\t Accuracy 0.9400\n",
      "Epoch [69][200]\t Batch [50][550]\t Training Loss 0.3344\t Accuracy 0.9094\n",
      "Epoch [69][200]\t Batch [100][550]\t Training Loss 0.3452\t Accuracy 0.9069\n",
      "Epoch [69][200]\t Batch [150][550]\t Training Loss 0.3584\t Accuracy 0.9009\n",
      "Epoch [69][200]\t Batch [200][550]\t Training Loss 0.3551\t Accuracy 0.9018\n",
      "Epoch [69][200]\t Batch [250][550]\t Training Loss 0.3548\t Accuracy 0.9020\n",
      "Epoch [69][200]\t Batch [300][550]\t Training Loss 0.3566\t Accuracy 0.9019\n",
      "Epoch [69][200]\t Batch [350][550]\t Training Loss 0.3583\t Accuracy 0.9008\n",
      "Epoch [69][200]\t Batch [400][550]\t Training Loss 0.3588\t Accuracy 0.9001\n",
      "Epoch [69][200]\t Batch [450][550]\t Training Loss 0.3587\t Accuracy 0.8998\n",
      "Epoch [69][200]\t Batch [500][550]\t Training Loss 0.3595\t Accuracy 0.8991\n",
      "\n",
      "Epoch [69]\t Average training loss 0.3601\t Average training accuracy 0.8989\n",
      "Epoch [69]\t Average validation loss 0.2835\t Average validation accuracy 0.9234\n",
      "\n",
      "Epoch [70][200]\t Batch [0][550]\t Training Loss 0.3081\t Accuracy 0.9400\n",
      "Epoch [70][200]\t Batch [50][550]\t Training Loss 0.3336\t Accuracy 0.9094\n",
      "Epoch [70][200]\t Batch [100][550]\t Training Loss 0.3444\t Accuracy 0.9073\n",
      "Epoch [70][200]\t Batch [150][550]\t Training Loss 0.3577\t Accuracy 0.9011\n",
      "Epoch [70][200]\t Batch [200][550]\t Training Loss 0.3544\t Accuracy 0.9022\n",
      "Epoch [70][200]\t Batch [250][550]\t Training Loss 0.3540\t Accuracy 0.9024\n",
      "Epoch [70][200]\t Batch [300][550]\t Training Loss 0.3558\t Accuracy 0.9021\n",
      "Epoch [70][200]\t Batch [350][550]\t Training Loss 0.3575\t Accuracy 0.9010\n",
      "Epoch [70][200]\t Batch [400][550]\t Training Loss 0.3580\t Accuracy 0.9003\n",
      "Epoch [70][200]\t Batch [450][550]\t Training Loss 0.3580\t Accuracy 0.9001\n",
      "Epoch [70][200]\t Batch [500][550]\t Training Loss 0.3588\t Accuracy 0.8994\n",
      "\n",
      "Epoch [70]\t Average training loss 0.3594\t Average training accuracy 0.8991\n",
      "Epoch [70]\t Average validation loss 0.2830\t Average validation accuracy 0.9234\n",
      "\n",
      "Epoch [71][200]\t Batch [0][550]\t Training Loss 0.3073\t Accuracy 0.9400\n",
      "Epoch [71][200]\t Batch [50][550]\t Training Loss 0.3329\t Accuracy 0.9096\n",
      "Epoch [71][200]\t Batch [100][550]\t Training Loss 0.3436\t Accuracy 0.9073\n",
      "Epoch [71][200]\t Batch [150][550]\t Training Loss 0.3570\t Accuracy 0.9012\n",
      "Epoch [71][200]\t Batch [200][550]\t Training Loss 0.3537\t Accuracy 0.9023\n",
      "Epoch [71][200]\t Batch [250][550]\t Training Loss 0.3533\t Accuracy 0.9025\n",
      "Epoch [71][200]\t Batch [300][550]\t Training Loss 0.3551\t Accuracy 0.9022\n",
      "Epoch [71][200]\t Batch [350][550]\t Training Loss 0.3568\t Accuracy 0.9011\n",
      "Epoch [71][200]\t Batch [400][550]\t Training Loss 0.3573\t Accuracy 0.9004\n",
      "Epoch [71][200]\t Batch [450][550]\t Training Loss 0.3573\t Accuracy 0.9002\n",
      "Epoch [71][200]\t Batch [500][550]\t Training Loss 0.3581\t Accuracy 0.8995\n",
      "\n",
      "Epoch [71]\t Average training loss 0.3587\t Average training accuracy 0.8993\n",
      "Epoch [71]\t Average validation loss 0.2824\t Average validation accuracy 0.9240\n",
      "\n",
      "Epoch [72][200]\t Batch [0][550]\t Training Loss 0.3065\t Accuracy 0.9400\n",
      "Epoch [72][200]\t Batch [50][550]\t Training Loss 0.3322\t Accuracy 0.9098\n",
      "Epoch [72][200]\t Batch [100][550]\t Training Loss 0.3429\t Accuracy 0.9074\n",
      "Epoch [72][200]\t Batch [150][550]\t Training Loss 0.3563\t Accuracy 0.9013\n",
      "Epoch [72][200]\t Batch [200][550]\t Training Loss 0.3529\t Accuracy 0.9024\n",
      "Epoch [72][200]\t Batch [250][550]\t Training Loss 0.3526\t Accuracy 0.9026\n",
      "Epoch [72][200]\t Batch [300][550]\t Training Loss 0.3544\t Accuracy 0.9023\n",
      "Epoch [72][200]\t Batch [350][550]\t Training Loss 0.3561\t Accuracy 0.9012\n",
      "Epoch [72][200]\t Batch [400][550]\t Training Loss 0.3566\t Accuracy 0.9005\n",
      "Epoch [72][200]\t Batch [450][550]\t Training Loss 0.3566\t Accuracy 0.9003\n",
      "Epoch [72][200]\t Batch [500][550]\t Training Loss 0.3573\t Accuracy 0.8996\n",
      "\n",
      "Epoch [72]\t Average training loss 0.3580\t Average training accuracy 0.8994\n",
      "Epoch [72]\t Average validation loss 0.2819\t Average validation accuracy 0.9240\n",
      "\n",
      "Epoch [73][200]\t Batch [0][550]\t Training Loss 0.3057\t Accuracy 0.9400\n",
      "Epoch [73][200]\t Batch [50][550]\t Training Loss 0.3314\t Accuracy 0.9094\n",
      "Epoch [73][200]\t Batch [100][550]\t Training Loss 0.3422\t Accuracy 0.9072\n",
      "Epoch [73][200]\t Batch [150][550]\t Training Loss 0.3556\t Accuracy 0.9011\n",
      "Epoch [73][200]\t Batch [200][550]\t Training Loss 0.3522\t Accuracy 0.9023\n",
      "Epoch [73][200]\t Batch [250][550]\t Training Loss 0.3519\t Accuracy 0.9025\n",
      "Epoch [73][200]\t Batch [300][550]\t Training Loss 0.3537\t Accuracy 0.9024\n",
      "Epoch [73][200]\t Batch [350][550]\t Training Loss 0.3554\t Accuracy 0.9012\n",
      "Epoch [73][200]\t Batch [400][550]\t Training Loss 0.3559\t Accuracy 0.9005\n",
      "Epoch [73][200]\t Batch [450][550]\t Training Loss 0.3559\t Accuracy 0.9003\n",
      "Epoch [73][200]\t Batch [500][550]\t Training Loss 0.3566\t Accuracy 0.8997\n",
      "\n",
      "Epoch [73]\t Average training loss 0.3573\t Average training accuracy 0.8995\n",
      "Epoch [73]\t Average validation loss 0.2814\t Average validation accuracy 0.9244\n",
      "\n",
      "Epoch [74][200]\t Batch [0][550]\t Training Loss 0.3050\t Accuracy 0.9400\n",
      "Epoch [74][200]\t Batch [50][550]\t Training Loss 0.3307\t Accuracy 0.9096\n",
      "Epoch [74][200]\t Batch [100][550]\t Training Loss 0.3415\t Accuracy 0.9074\n",
      "Epoch [74][200]\t Batch [150][550]\t Training Loss 0.3549\t Accuracy 0.9015\n",
      "Epoch [74][200]\t Batch [200][550]\t Training Loss 0.3515\t Accuracy 0.9025\n",
      "Epoch [74][200]\t Batch [250][550]\t Training Loss 0.3512\t Accuracy 0.9028\n",
      "Epoch [74][200]\t Batch [300][550]\t Training Loss 0.3530\t Accuracy 0.9026\n",
      "Epoch [74][200]\t Batch [350][550]\t Training Loss 0.3547\t Accuracy 0.9015\n",
      "Epoch [74][200]\t Batch [400][550]\t Training Loss 0.3551\t Accuracy 0.9007\n",
      "Epoch [74][200]\t Batch [450][550]\t Training Loss 0.3552\t Accuracy 0.9005\n",
      "Epoch [74][200]\t Batch [500][550]\t Training Loss 0.3559\t Accuracy 0.8999\n",
      "\n",
      "Epoch [74]\t Average training loss 0.3566\t Average training accuracy 0.8997\n",
      "Epoch [74]\t Average validation loss 0.2809\t Average validation accuracy 0.9244\n",
      "\n",
      "Epoch [75][200]\t Batch [0][550]\t Training Loss 0.3042\t Accuracy 0.9400\n",
      "Epoch [75][200]\t Batch [50][550]\t Training Loss 0.3300\t Accuracy 0.9100\n",
      "Epoch [75][200]\t Batch [100][550]\t Training Loss 0.3407\t Accuracy 0.9077\n",
      "Epoch [75][200]\t Batch [150][550]\t Training Loss 0.3542\t Accuracy 0.9018\n",
      "Epoch [75][200]\t Batch [200][550]\t Training Loss 0.3509\t Accuracy 0.9028\n",
      "Epoch [75][200]\t Batch [250][550]\t Training Loss 0.3506\t Accuracy 0.9030\n",
      "Epoch [75][200]\t Batch [300][550]\t Training Loss 0.3523\t Accuracy 0.9029\n",
      "Epoch [75][200]\t Batch [350][550]\t Training Loss 0.3540\t Accuracy 0.9018\n",
      "Epoch [75][200]\t Batch [400][550]\t Training Loss 0.3545\t Accuracy 0.9010\n",
      "Epoch [75][200]\t Batch [450][550]\t Training Loss 0.3545\t Accuracy 0.9007\n",
      "Epoch [75][200]\t Batch [500][550]\t Training Loss 0.3553\t Accuracy 0.9002\n",
      "\n",
      "Epoch [75]\t Average training loss 0.3559\t Average training accuracy 0.8999\n",
      "Epoch [75]\t Average validation loss 0.2803\t Average validation accuracy 0.9246\n",
      "\n",
      "Epoch [76][200]\t Batch [0][550]\t Training Loss 0.3035\t Accuracy 0.9400\n",
      "Epoch [76][200]\t Batch [50][550]\t Training Loss 0.3293\t Accuracy 0.9100\n",
      "Epoch [76][200]\t Batch [100][550]\t Training Loss 0.3400\t Accuracy 0.9078\n",
      "Epoch [76][200]\t Batch [150][550]\t Training Loss 0.3535\t Accuracy 0.9020\n",
      "Epoch [76][200]\t Batch [200][550]\t Training Loss 0.3502\t Accuracy 0.9030\n",
      "Epoch [76][200]\t Batch [250][550]\t Training Loss 0.3499\t Accuracy 0.9033\n",
      "Epoch [76][200]\t Batch [300][550]\t Training Loss 0.3516\t Accuracy 0.9031\n",
      "Epoch [76][200]\t Batch [350][550]\t Training Loss 0.3533\t Accuracy 0.9019\n",
      "Epoch [76][200]\t Batch [400][550]\t Training Loss 0.3538\t Accuracy 0.9011\n",
      "Epoch [76][200]\t Batch [450][550]\t Training Loss 0.3538\t Accuracy 0.9009\n",
      "Epoch [76][200]\t Batch [500][550]\t Training Loss 0.3546\t Accuracy 0.9004\n",
      "\n",
      "Epoch [76]\t Average training loss 0.3552\t Average training accuracy 0.9001\n",
      "Epoch [76]\t Average validation loss 0.2798\t Average validation accuracy 0.9248\n",
      "\n",
      "Epoch [77][200]\t Batch [0][550]\t Training Loss 0.3027\t Accuracy 0.9400\n",
      "Epoch [77][200]\t Batch [50][550]\t Training Loss 0.3286\t Accuracy 0.9102\n",
      "Epoch [77][200]\t Batch [100][550]\t Training Loss 0.3393\t Accuracy 0.9082\n",
      "Epoch [77][200]\t Batch [150][550]\t Training Loss 0.3528\t Accuracy 0.9022\n",
      "Epoch [77][200]\t Batch [200][550]\t Training Loss 0.3495\t Accuracy 0.9033\n",
      "Epoch [77][200]\t Batch [250][550]\t Training Loss 0.3492\t Accuracy 0.9035\n",
      "Epoch [77][200]\t Batch [300][550]\t Training Loss 0.3509\t Accuracy 0.9033\n",
      "Epoch [77][200]\t Batch [350][550]\t Training Loss 0.3526\t Accuracy 0.9022\n",
      "Epoch [77][200]\t Batch [400][550]\t Training Loss 0.3531\t Accuracy 0.9014\n",
      "Epoch [77][200]\t Batch [450][550]\t Training Loss 0.3531\t Accuracy 0.9011\n",
      "Epoch [77][200]\t Batch [500][550]\t Training Loss 0.3539\t Accuracy 0.9006\n",
      "\n",
      "Epoch [77]\t Average training loss 0.3545\t Average training accuracy 0.9004\n",
      "Epoch [77]\t Average validation loss 0.2793\t Average validation accuracy 0.9248\n",
      "\n",
      "Epoch [78][200]\t Batch [0][550]\t Training Loss 0.3020\t Accuracy 0.9400\n",
      "Epoch [78][200]\t Batch [50][550]\t Training Loss 0.3280\t Accuracy 0.9104\n",
      "Epoch [78][200]\t Batch [100][550]\t Training Loss 0.3387\t Accuracy 0.9086\n",
      "Epoch [78][200]\t Batch [150][550]\t Training Loss 0.3521\t Accuracy 0.9026\n",
      "Epoch [78][200]\t Batch [200][550]\t Training Loss 0.3488\t Accuracy 0.9038\n",
      "Epoch [78][200]\t Batch [250][550]\t Training Loss 0.3486\t Accuracy 0.9039\n",
      "Epoch [78][200]\t Batch [300][550]\t Training Loss 0.3503\t Accuracy 0.9037\n",
      "Epoch [78][200]\t Batch [350][550]\t Training Loss 0.3519\t Accuracy 0.9025\n",
      "Epoch [78][200]\t Batch [400][550]\t Training Loss 0.3524\t Accuracy 0.9016\n",
      "Epoch [78][200]\t Batch [450][550]\t Training Loss 0.3524\t Accuracy 0.9014\n",
      "Epoch [78][200]\t Batch [500][550]\t Training Loss 0.3532\t Accuracy 0.9009\n",
      "\n",
      "Epoch [78]\t Average training loss 0.3538\t Average training accuracy 0.9007\n",
      "Epoch [78]\t Average validation loss 0.2788\t Average validation accuracy 0.9250\n",
      "\n",
      "Epoch [79][200]\t Batch [0][550]\t Training Loss 0.3013\t Accuracy 0.9400\n",
      "Epoch [79][200]\t Batch [50][550]\t Training Loss 0.3273\t Accuracy 0.9114\n",
      "Epoch [79][200]\t Batch [100][550]\t Training Loss 0.3380\t Accuracy 0.9091\n",
      "Epoch [79][200]\t Batch [150][550]\t Training Loss 0.3515\t Accuracy 0.9030\n",
      "Epoch [79][200]\t Batch [200][550]\t Training Loss 0.3482\t Accuracy 0.9041\n",
      "Epoch [79][200]\t Batch [250][550]\t Training Loss 0.3479\t Accuracy 0.9042\n",
      "Epoch [79][200]\t Batch [300][550]\t Training Loss 0.3496\t Accuracy 0.9040\n",
      "Epoch [79][200]\t Batch [350][550]\t Training Loss 0.3512\t Accuracy 0.9028\n",
      "Epoch [79][200]\t Batch [400][550]\t Training Loss 0.3517\t Accuracy 0.9019\n",
      "Epoch [79][200]\t Batch [450][550]\t Training Loss 0.3518\t Accuracy 0.9016\n",
      "Epoch [79][200]\t Batch [500][550]\t Training Loss 0.3526\t Accuracy 0.9011\n",
      "\n",
      "Epoch [79]\t Average training loss 0.3532\t Average training accuracy 0.9009\n",
      "Epoch [79]\t Average validation loss 0.2783\t Average validation accuracy 0.9254\n",
      "\n",
      "Epoch [80][200]\t Batch [0][550]\t Training Loss 0.3006\t Accuracy 0.9400\n",
      "Epoch [80][200]\t Batch [50][550]\t Training Loss 0.3266\t Accuracy 0.9118\n",
      "Epoch [80][200]\t Batch [100][550]\t Training Loss 0.3373\t Accuracy 0.9092\n",
      "Epoch [80][200]\t Batch [150][550]\t Training Loss 0.3508\t Accuracy 0.9031\n",
      "Epoch [80][200]\t Batch [200][550]\t Training Loss 0.3475\t Accuracy 0.9043\n",
      "Epoch [80][200]\t Batch [250][550]\t Training Loss 0.3473\t Accuracy 0.9044\n",
      "Epoch [80][200]\t Batch [300][550]\t Training Loss 0.3490\t Accuracy 0.9042\n",
      "Epoch [80][200]\t Batch [350][550]\t Training Loss 0.3506\t Accuracy 0.9030\n",
      "Epoch [80][200]\t Batch [400][550]\t Training Loss 0.3511\t Accuracy 0.9021\n",
      "Epoch [80][200]\t Batch [450][550]\t Training Loss 0.3511\t Accuracy 0.9018\n",
      "Epoch [80][200]\t Batch [500][550]\t Training Loss 0.3519\t Accuracy 0.9013\n",
      "\n",
      "Epoch [80]\t Average training loss 0.3525\t Average training accuracy 0.9011\n",
      "Epoch [80]\t Average validation loss 0.2778\t Average validation accuracy 0.9256\n",
      "\n",
      "Epoch [81][200]\t Batch [0][550]\t Training Loss 0.3000\t Accuracy 0.9400\n",
      "Epoch [81][200]\t Batch [50][550]\t Training Loss 0.3260\t Accuracy 0.9118\n",
      "Epoch [81][200]\t Batch [100][550]\t Training Loss 0.3366\t Accuracy 0.9094\n",
      "Epoch [81][200]\t Batch [150][550]\t Training Loss 0.3502\t Accuracy 0.9034\n",
      "Epoch [81][200]\t Batch [200][550]\t Training Loss 0.3469\t Accuracy 0.9045\n",
      "Epoch [81][200]\t Batch [250][550]\t Training Loss 0.3466\t Accuracy 0.9047\n",
      "Epoch [81][200]\t Batch [300][550]\t Training Loss 0.3483\t Accuracy 0.9045\n",
      "Epoch [81][200]\t Batch [350][550]\t Training Loss 0.3499\t Accuracy 0.9032\n",
      "Epoch [81][200]\t Batch [400][550]\t Training Loss 0.3504\t Accuracy 0.9023\n",
      "Epoch [81][200]\t Batch [450][550]\t Training Loss 0.3505\t Accuracy 0.9021\n",
      "Epoch [81][200]\t Batch [500][550]\t Training Loss 0.3513\t Accuracy 0.9015\n",
      "\n",
      "Epoch [81]\t Average training loss 0.3519\t Average training accuracy 0.9013\n",
      "Epoch [81]\t Average validation loss 0.2774\t Average validation accuracy 0.9256\n",
      "\n",
      "Epoch [82][200]\t Batch [0][550]\t Training Loss 0.2993\t Accuracy 0.9400\n",
      "Epoch [82][200]\t Batch [50][550]\t Training Loss 0.3253\t Accuracy 0.9120\n",
      "Epoch [82][200]\t Batch [100][550]\t Training Loss 0.3360\t Accuracy 0.9095\n",
      "Epoch [82][200]\t Batch [150][550]\t Training Loss 0.3495\t Accuracy 0.9035\n",
      "Epoch [82][200]\t Batch [200][550]\t Training Loss 0.3462\t Accuracy 0.9046\n",
      "Epoch [82][200]\t Batch [250][550]\t Training Loss 0.3460\t Accuracy 0.9048\n",
      "Epoch [82][200]\t Batch [300][550]\t Training Loss 0.3477\t Accuracy 0.9047\n",
      "Epoch [82][200]\t Batch [350][550]\t Training Loss 0.3492\t Accuracy 0.9034\n",
      "Epoch [82][200]\t Batch [400][550]\t Training Loss 0.3498\t Accuracy 0.9024\n",
      "Epoch [82][200]\t Batch [450][550]\t Training Loss 0.3498\t Accuracy 0.9022\n",
      "Epoch [82][200]\t Batch [500][550]\t Training Loss 0.3506\t Accuracy 0.9017\n",
      "\n",
      "Epoch [82]\t Average training loss 0.3512\t Average training accuracy 0.9015\n",
      "Epoch [82]\t Average validation loss 0.2769\t Average validation accuracy 0.9256\n",
      "\n",
      "Epoch [83][200]\t Batch [0][550]\t Training Loss 0.2986\t Accuracy 0.9400\n",
      "Epoch [83][200]\t Batch [50][550]\t Training Loss 0.3247\t Accuracy 0.9120\n",
      "Epoch [83][200]\t Batch [100][550]\t Training Loss 0.3353\t Accuracy 0.9097\n",
      "Epoch [83][200]\t Batch [150][550]\t Training Loss 0.3489\t Accuracy 0.9037\n",
      "Epoch [83][200]\t Batch [200][550]\t Training Loss 0.3456\t Accuracy 0.9047\n",
      "Epoch [83][200]\t Batch [250][550]\t Training Loss 0.3454\t Accuracy 0.9049\n",
      "Epoch [83][200]\t Batch [300][550]\t Training Loss 0.3470\t Accuracy 0.9048\n",
      "Epoch [83][200]\t Batch [350][550]\t Training Loss 0.3486\t Accuracy 0.9035\n",
      "Epoch [83][200]\t Batch [400][550]\t Training Loss 0.3491\t Accuracy 0.9025\n",
      "Epoch [83][200]\t Batch [450][550]\t Training Loss 0.3492\t Accuracy 0.9024\n",
      "Epoch [83][200]\t Batch [500][550]\t Training Loss 0.3500\t Accuracy 0.9018\n",
      "\n",
      "Epoch [83]\t Average training loss 0.3506\t Average training accuracy 0.9016\n",
      "Epoch [83]\t Average validation loss 0.2764\t Average validation accuracy 0.9258\n",
      "\n",
      "Epoch [84][200]\t Batch [0][550]\t Training Loss 0.2980\t Accuracy 0.9400\n",
      "Epoch [84][200]\t Batch [50][550]\t Training Loss 0.3240\t Accuracy 0.9122\n",
      "Epoch [84][200]\t Batch [100][550]\t Training Loss 0.3347\t Accuracy 0.9100\n",
      "Epoch [84][200]\t Batch [150][550]\t Training Loss 0.3483\t Accuracy 0.9041\n",
      "Epoch [84][200]\t Batch [200][550]\t Training Loss 0.3450\t Accuracy 0.9050\n",
      "Epoch [84][200]\t Batch [250][550]\t Training Loss 0.3448\t Accuracy 0.9052\n",
      "Epoch [84][200]\t Batch [300][550]\t Training Loss 0.3464\t Accuracy 0.9051\n",
      "Epoch [84][200]\t Batch [350][550]\t Training Loss 0.3480\t Accuracy 0.9037\n",
      "Epoch [84][200]\t Batch [400][550]\t Training Loss 0.3485\t Accuracy 0.9027\n",
      "Epoch [84][200]\t Batch [450][550]\t Training Loss 0.3486\t Accuracy 0.9025\n",
      "Epoch [84][200]\t Batch [500][550]\t Training Loss 0.3494\t Accuracy 0.9020\n",
      "\n",
      "Epoch [84]\t Average training loss 0.3500\t Average training accuracy 0.9018\n",
      "Epoch [84]\t Average validation loss 0.2759\t Average validation accuracy 0.9260\n",
      "\n",
      "Epoch [85][200]\t Batch [0][550]\t Training Loss 0.2973\t Accuracy 0.9400\n",
      "Epoch [85][200]\t Batch [50][550]\t Training Loss 0.3234\t Accuracy 0.9124\n",
      "Epoch [85][200]\t Batch [100][550]\t Training Loss 0.3340\t Accuracy 0.9102\n",
      "Epoch [85][200]\t Batch [150][550]\t Training Loss 0.3476\t Accuracy 0.9042\n",
      "Epoch [85][200]\t Batch [200][550]\t Training Loss 0.3444\t Accuracy 0.9052\n",
      "Epoch [85][200]\t Batch [250][550]\t Training Loss 0.3441\t Accuracy 0.9054\n",
      "Epoch [85][200]\t Batch [300][550]\t Training Loss 0.3458\t Accuracy 0.9052\n",
      "Epoch [85][200]\t Batch [350][550]\t Training Loss 0.3473\t Accuracy 0.9038\n",
      "Epoch [85][200]\t Batch [400][550]\t Training Loss 0.3479\t Accuracy 0.9029\n",
      "Epoch [85][200]\t Batch [450][550]\t Training Loss 0.3479\t Accuracy 0.9027\n",
      "Epoch [85][200]\t Batch [500][550]\t Training Loss 0.3487\t Accuracy 0.9022\n",
      "\n",
      "Epoch [85]\t Average training loss 0.3493\t Average training accuracy 0.9020\n",
      "Epoch [85]\t Average validation loss 0.2755\t Average validation accuracy 0.9262\n",
      "\n",
      "Epoch [86][200]\t Batch [0][550]\t Training Loss 0.2967\t Accuracy 0.9400\n",
      "Epoch [86][200]\t Batch [50][550]\t Training Loss 0.3228\t Accuracy 0.9124\n",
      "Epoch [86][200]\t Batch [100][550]\t Training Loss 0.3334\t Accuracy 0.9103\n",
      "Epoch [86][200]\t Batch [150][550]\t Training Loss 0.3470\t Accuracy 0.9043\n",
      "Epoch [86][200]\t Batch [200][550]\t Training Loss 0.3437\t Accuracy 0.9054\n",
      "Epoch [86][200]\t Batch [250][550]\t Training Loss 0.3435\t Accuracy 0.9055\n",
      "Epoch [86][200]\t Batch [300][550]\t Training Loss 0.3452\t Accuracy 0.9054\n",
      "Epoch [86][200]\t Batch [350][550]\t Training Loss 0.3467\t Accuracy 0.9040\n",
      "Epoch [86][200]\t Batch [400][550]\t Training Loss 0.3473\t Accuracy 0.9030\n",
      "Epoch [86][200]\t Batch [450][550]\t Training Loss 0.3473\t Accuracy 0.9028\n",
      "Epoch [86][200]\t Batch [500][550]\t Training Loss 0.3481\t Accuracy 0.9023\n",
      "\n",
      "Epoch [86]\t Average training loss 0.3487\t Average training accuracy 0.9021\n",
      "Epoch [86]\t Average validation loss 0.2750\t Average validation accuracy 0.9262\n",
      "\n",
      "Epoch [87][200]\t Batch [0][550]\t Training Loss 0.2961\t Accuracy 0.9400\n",
      "Epoch [87][200]\t Batch [50][550]\t Training Loss 0.3221\t Accuracy 0.9124\n",
      "Epoch [87][200]\t Batch [100][550]\t Training Loss 0.3328\t Accuracy 0.9103\n",
      "Epoch [87][200]\t Batch [150][550]\t Training Loss 0.3464\t Accuracy 0.9044\n",
      "Epoch [87][200]\t Batch [200][550]\t Training Loss 0.3431\t Accuracy 0.9055\n",
      "Epoch [87][200]\t Batch [250][550]\t Training Loss 0.3429\t Accuracy 0.9057\n",
      "Epoch [87][200]\t Batch [300][550]\t Training Loss 0.3445\t Accuracy 0.9056\n",
      "Epoch [87][200]\t Batch [350][550]\t Training Loss 0.3461\t Accuracy 0.9042\n",
      "Epoch [87][200]\t Batch [400][550]\t Training Loss 0.3466\t Accuracy 0.9032\n",
      "Epoch [87][200]\t Batch [450][550]\t Training Loss 0.3467\t Accuracy 0.9031\n",
      "Epoch [87][200]\t Batch [500][550]\t Training Loss 0.3475\t Accuracy 0.9025\n",
      "\n",
      "Epoch [87]\t Average training loss 0.3481\t Average training accuracy 0.9023\n",
      "Epoch [87]\t Average validation loss 0.2746\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [88][200]\t Batch [0][550]\t Training Loss 0.2955\t Accuracy 0.9400\n",
      "Epoch [88][200]\t Batch [50][550]\t Training Loss 0.3215\t Accuracy 0.9125\n",
      "Epoch [88][200]\t Batch [100][550]\t Training Loss 0.3321\t Accuracy 0.9104\n",
      "Epoch [88][200]\t Batch [150][550]\t Training Loss 0.3458\t Accuracy 0.9044\n",
      "Epoch [88][200]\t Batch [200][550]\t Training Loss 0.3425\t Accuracy 0.9056\n",
      "Epoch [88][200]\t Batch [250][550]\t Training Loss 0.3423\t Accuracy 0.9059\n",
      "Epoch [88][200]\t Batch [300][550]\t Training Loss 0.3439\t Accuracy 0.9057\n",
      "Epoch [88][200]\t Batch [350][550]\t Training Loss 0.3455\t Accuracy 0.9043\n",
      "Epoch [88][200]\t Batch [400][550]\t Training Loss 0.3460\t Accuracy 0.9033\n",
      "Epoch [88][200]\t Batch [450][550]\t Training Loss 0.3461\t Accuracy 0.9032\n",
      "Epoch [88][200]\t Batch [500][550]\t Training Loss 0.3469\t Accuracy 0.9026\n",
      "\n",
      "Epoch [88]\t Average training loss 0.3475\t Average training accuracy 0.9024\n",
      "Epoch [88]\t Average validation loss 0.2741\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [89][200]\t Batch [0][550]\t Training Loss 0.2949\t Accuracy 0.9400\n",
      "Epoch [89][200]\t Batch [50][550]\t Training Loss 0.3209\t Accuracy 0.9124\n",
      "Epoch [89][200]\t Batch [100][550]\t Training Loss 0.3315\t Accuracy 0.9104\n",
      "Epoch [89][200]\t Batch [150][550]\t Training Loss 0.3452\t Accuracy 0.9044\n",
      "Epoch [89][200]\t Batch [200][550]\t Training Loss 0.3419\t Accuracy 0.9056\n",
      "Epoch [89][200]\t Batch [250][550]\t Training Loss 0.3417\t Accuracy 0.9059\n",
      "Epoch [89][200]\t Batch [300][550]\t Training Loss 0.3433\t Accuracy 0.9058\n",
      "Epoch [89][200]\t Batch [350][550]\t Training Loss 0.3448\t Accuracy 0.9044\n",
      "Epoch [89][200]\t Batch [400][550]\t Training Loss 0.3454\t Accuracy 0.9034\n",
      "Epoch [89][200]\t Batch [450][550]\t Training Loss 0.3455\t Accuracy 0.9033\n",
      "Epoch [89][200]\t Batch [500][550]\t Training Loss 0.3463\t Accuracy 0.9027\n",
      "\n",
      "Epoch [89]\t Average training loss 0.3469\t Average training accuracy 0.9025\n",
      "Epoch [89]\t Average validation loss 0.2736\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [90][200]\t Batch [0][550]\t Training Loss 0.2943\t Accuracy 0.9400\n",
      "Epoch [90][200]\t Batch [50][550]\t Training Loss 0.3203\t Accuracy 0.9125\n",
      "Epoch [90][200]\t Batch [100][550]\t Training Loss 0.3309\t Accuracy 0.9105\n",
      "Epoch [90][200]\t Batch [150][550]\t Training Loss 0.3446\t Accuracy 0.9046\n",
      "Epoch [90][200]\t Batch [200][550]\t Training Loss 0.3413\t Accuracy 0.9059\n",
      "Epoch [90][200]\t Batch [250][550]\t Training Loss 0.3412\t Accuracy 0.9062\n",
      "Epoch [90][200]\t Batch [300][550]\t Training Loss 0.3427\t Accuracy 0.9061\n",
      "Epoch [90][200]\t Batch [350][550]\t Training Loss 0.3442\t Accuracy 0.9046\n",
      "Epoch [90][200]\t Batch [400][550]\t Training Loss 0.3448\t Accuracy 0.9037\n",
      "Epoch [90][200]\t Batch [450][550]\t Training Loss 0.3449\t Accuracy 0.9036\n",
      "Epoch [90][200]\t Batch [500][550]\t Training Loss 0.3457\t Accuracy 0.9030\n",
      "\n",
      "Epoch [90]\t Average training loss 0.3463\t Average training accuracy 0.9028\n",
      "Epoch [90]\t Average validation loss 0.2732\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [91][200]\t Batch [0][550]\t Training Loss 0.2937\t Accuracy 0.9400\n",
      "Epoch [91][200]\t Batch [50][550]\t Training Loss 0.3197\t Accuracy 0.9127\n",
      "Epoch [91][200]\t Batch [100][550]\t Training Loss 0.3303\t Accuracy 0.9106\n",
      "Epoch [91][200]\t Batch [150][550]\t Training Loss 0.3440\t Accuracy 0.9047\n",
      "Epoch [91][200]\t Batch [200][550]\t Training Loss 0.3407\t Accuracy 0.9061\n",
      "Epoch [91][200]\t Batch [250][550]\t Training Loss 0.3406\t Accuracy 0.9065\n",
      "Epoch [91][200]\t Batch [300][550]\t Training Loss 0.3421\t Accuracy 0.9063\n",
      "Epoch [91][200]\t Batch [350][550]\t Training Loss 0.3436\t Accuracy 0.9049\n",
      "Epoch [91][200]\t Batch [400][550]\t Training Loss 0.3442\t Accuracy 0.9039\n",
      "Epoch [91][200]\t Batch [450][550]\t Training Loss 0.3443\t Accuracy 0.9038\n",
      "Epoch [91][200]\t Batch [500][550]\t Training Loss 0.3451\t Accuracy 0.9032\n",
      "\n",
      "Epoch [91]\t Average training loss 0.3457\t Average training accuracy 0.9031\n",
      "Epoch [91]\t Average validation loss 0.2728\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [92][200]\t Batch [0][550]\t Training Loss 0.2931\t Accuracy 0.9400\n",
      "Epoch [92][200]\t Batch [50][550]\t Training Loss 0.3191\t Accuracy 0.9127\n",
      "Epoch [92][200]\t Batch [100][550]\t Training Loss 0.3297\t Accuracy 0.9109\n",
      "Epoch [92][200]\t Batch [150][550]\t Training Loss 0.3434\t Accuracy 0.9049\n",
      "Epoch [92][200]\t Batch [200][550]\t Training Loss 0.3402\t Accuracy 0.9063\n",
      "Epoch [92][200]\t Batch [250][550]\t Training Loss 0.3400\t Accuracy 0.9067\n",
      "Epoch [92][200]\t Batch [300][550]\t Training Loss 0.3415\t Accuracy 0.9066\n",
      "Epoch [92][200]\t Batch [350][550]\t Training Loss 0.3430\t Accuracy 0.9051\n",
      "Epoch [92][200]\t Batch [400][550]\t Training Loss 0.3436\t Accuracy 0.9041\n",
      "Epoch [92][200]\t Batch [450][550]\t Training Loss 0.3437\t Accuracy 0.9040\n",
      "Epoch [92][200]\t Batch [500][550]\t Training Loss 0.3445\t Accuracy 0.9034\n",
      "\n",
      "Epoch [92]\t Average training loss 0.3451\t Average training accuracy 0.9033\n",
      "Epoch [92]\t Average validation loss 0.2723\t Average validation accuracy 0.9262\n",
      "\n",
      "Epoch [93][200]\t Batch [0][550]\t Training Loss 0.2926\t Accuracy 0.9400\n",
      "Epoch [93][200]\t Batch [50][550]\t Training Loss 0.3185\t Accuracy 0.9131\n",
      "Epoch [93][200]\t Batch [100][550]\t Training Loss 0.3291\t Accuracy 0.9111\n",
      "Epoch [93][200]\t Batch [150][550]\t Training Loss 0.3428\t Accuracy 0.9052\n",
      "Epoch [93][200]\t Batch [200][550]\t Training Loss 0.3396\t Accuracy 0.9064\n",
      "Epoch [93][200]\t Batch [250][550]\t Training Loss 0.3394\t Accuracy 0.9069\n",
      "Epoch [93][200]\t Batch [300][550]\t Training Loss 0.3410\t Accuracy 0.9069\n",
      "Epoch [93][200]\t Batch [350][550]\t Training Loss 0.3424\t Accuracy 0.9054\n",
      "Epoch [93][200]\t Batch [400][550]\t Training Loss 0.3430\t Accuracy 0.9044\n",
      "Epoch [93][200]\t Batch [450][550]\t Training Loss 0.3431\t Accuracy 0.9043\n",
      "Epoch [93][200]\t Batch [500][550]\t Training Loss 0.3439\t Accuracy 0.9038\n",
      "\n",
      "Epoch [93]\t Average training loss 0.3445\t Average training accuracy 0.9036\n",
      "Epoch [93]\t Average validation loss 0.2719\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [94][200]\t Batch [0][550]\t Training Loss 0.2920\t Accuracy 0.9400\n",
      "Epoch [94][200]\t Batch [50][550]\t Training Loss 0.3179\t Accuracy 0.9133\n",
      "Epoch [94][200]\t Batch [100][550]\t Training Loss 0.3285\t Accuracy 0.9113\n",
      "Epoch [94][200]\t Batch [150][550]\t Training Loss 0.3422\t Accuracy 0.9054\n",
      "Epoch [94][200]\t Batch [200][550]\t Training Loss 0.3390\t Accuracy 0.9066\n",
      "Epoch [94][200]\t Batch [250][550]\t Training Loss 0.3389\t Accuracy 0.9071\n",
      "Epoch [94][200]\t Batch [300][550]\t Training Loss 0.3404\t Accuracy 0.9071\n",
      "Epoch [94][200]\t Batch [350][550]\t Training Loss 0.3419\t Accuracy 0.9056\n",
      "Epoch [94][200]\t Batch [400][550]\t Training Loss 0.3424\t Accuracy 0.9046\n",
      "Epoch [94][200]\t Batch [450][550]\t Training Loss 0.3425\t Accuracy 0.9045\n",
      "Epoch [94][200]\t Batch [500][550]\t Training Loss 0.3433\t Accuracy 0.9040\n",
      "\n",
      "Epoch [94]\t Average training loss 0.3439\t Average training accuracy 0.9038\n",
      "Epoch [94]\t Average validation loss 0.2714\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [95][200]\t Batch [0][550]\t Training Loss 0.2915\t Accuracy 0.9400\n",
      "Epoch [95][200]\t Batch [50][550]\t Training Loss 0.3173\t Accuracy 0.9137\n",
      "Epoch [95][200]\t Batch [100][550]\t Training Loss 0.3279\t Accuracy 0.9115\n",
      "Epoch [95][200]\t Batch [150][550]\t Training Loss 0.3417\t Accuracy 0.9056\n",
      "Epoch [95][200]\t Batch [200][550]\t Training Loss 0.3384\t Accuracy 0.9067\n",
      "Epoch [95][200]\t Batch [250][550]\t Training Loss 0.3383\t Accuracy 0.9073\n",
      "Epoch [95][200]\t Batch [300][550]\t Training Loss 0.3398\t Accuracy 0.9071\n",
      "Epoch [95][200]\t Batch [350][550]\t Training Loss 0.3413\t Accuracy 0.9057\n",
      "Epoch [95][200]\t Batch [400][550]\t Training Loss 0.3419\t Accuracy 0.9047\n",
      "Epoch [95][200]\t Batch [450][550]\t Training Loss 0.3419\t Accuracy 0.9045\n",
      "Epoch [95][200]\t Batch [500][550]\t Training Loss 0.3428\t Accuracy 0.9041\n",
      "\n",
      "Epoch [95]\t Average training loss 0.3434\t Average training accuracy 0.9040\n",
      "Epoch [95]\t Average validation loss 0.2710\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [96][200]\t Batch [0][550]\t Training Loss 0.2910\t Accuracy 0.9400\n",
      "Epoch [96][200]\t Batch [50][550]\t Training Loss 0.3167\t Accuracy 0.9139\n",
      "Epoch [96][200]\t Batch [100][550]\t Training Loss 0.3274\t Accuracy 0.9117\n",
      "Epoch [96][200]\t Batch [150][550]\t Training Loss 0.3411\t Accuracy 0.9058\n",
      "Epoch [96][200]\t Batch [200][550]\t Training Loss 0.3379\t Accuracy 0.9069\n",
      "Epoch [96][200]\t Batch [250][550]\t Training Loss 0.3377\t Accuracy 0.9074\n",
      "Epoch [96][200]\t Batch [300][550]\t Training Loss 0.3392\t Accuracy 0.9072\n",
      "Epoch [96][200]\t Batch [350][550]\t Training Loss 0.3407\t Accuracy 0.9058\n",
      "Epoch [96][200]\t Batch [400][550]\t Training Loss 0.3413\t Accuracy 0.9047\n",
      "Epoch [96][200]\t Batch [450][550]\t Training Loss 0.3414\t Accuracy 0.9047\n",
      "Epoch [96][200]\t Batch [500][550]\t Training Loss 0.3422\t Accuracy 0.9042\n",
      "\n",
      "Epoch [96]\t Average training loss 0.3428\t Average training accuracy 0.9040\n",
      "Epoch [96]\t Average validation loss 0.2706\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [97][200]\t Batch [0][550]\t Training Loss 0.2904\t Accuracy 0.9400\n",
      "Epoch [97][200]\t Batch [50][550]\t Training Loss 0.3162\t Accuracy 0.9137\n",
      "Epoch [97][200]\t Batch [100][550]\t Training Loss 0.3268\t Accuracy 0.9118\n",
      "Epoch [97][200]\t Batch [150][550]\t Training Loss 0.3405\t Accuracy 0.9060\n",
      "Epoch [97][200]\t Batch [200][550]\t Training Loss 0.3373\t Accuracy 0.9071\n",
      "Epoch [97][200]\t Batch [250][550]\t Training Loss 0.3372\t Accuracy 0.9075\n",
      "Epoch [97][200]\t Batch [300][550]\t Training Loss 0.3387\t Accuracy 0.9074\n",
      "Epoch [97][200]\t Batch [350][550]\t Training Loss 0.3401\t Accuracy 0.9059\n",
      "Epoch [97][200]\t Batch [400][550]\t Training Loss 0.3407\t Accuracy 0.9049\n",
      "Epoch [97][200]\t Batch [450][550]\t Training Loss 0.3408\t Accuracy 0.9048\n",
      "Epoch [97][200]\t Batch [500][550]\t Training Loss 0.3416\t Accuracy 0.9043\n",
      "\n",
      "Epoch [97]\t Average training loss 0.3422\t Average training accuracy 0.9042\n",
      "Epoch [97]\t Average validation loss 0.2702\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [98][200]\t Batch [0][550]\t Training Loss 0.2899\t Accuracy 0.9400\n",
      "Epoch [98][200]\t Batch [50][550]\t Training Loss 0.3156\t Accuracy 0.9141\n",
      "Epoch [98][200]\t Batch [100][550]\t Training Loss 0.3262\t Accuracy 0.9120\n",
      "Epoch [98][200]\t Batch [150][550]\t Training Loss 0.3400\t Accuracy 0.9062\n",
      "Epoch [98][200]\t Batch [200][550]\t Training Loss 0.3367\t Accuracy 0.9071\n",
      "Epoch [98][200]\t Batch [250][550]\t Training Loss 0.3366\t Accuracy 0.9075\n",
      "Epoch [98][200]\t Batch [300][550]\t Training Loss 0.3381\t Accuracy 0.9074\n",
      "Epoch [98][200]\t Batch [350][550]\t Training Loss 0.3395\t Accuracy 0.9060\n",
      "Epoch [98][200]\t Batch [400][550]\t Training Loss 0.3401\t Accuracy 0.9049\n",
      "Epoch [98][200]\t Batch [450][550]\t Training Loss 0.3402\t Accuracy 0.9048\n",
      "Epoch [98][200]\t Batch [500][550]\t Training Loss 0.3410\t Accuracy 0.9044\n",
      "\n",
      "Epoch [98]\t Average training loss 0.3416\t Average training accuracy 0.9043\n",
      "Epoch [98]\t Average validation loss 0.2698\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [99][200]\t Batch [0][550]\t Training Loss 0.2894\t Accuracy 0.9400\n",
      "Epoch [99][200]\t Batch [50][550]\t Training Loss 0.3150\t Accuracy 0.9145\n",
      "Epoch [99][200]\t Batch [100][550]\t Training Loss 0.3256\t Accuracy 0.9123\n",
      "Epoch [99][200]\t Batch [150][550]\t Training Loss 0.3394\t Accuracy 0.9066\n",
      "Epoch [99][200]\t Batch [200][550]\t Training Loss 0.3362\t Accuracy 0.9075\n",
      "Epoch [99][200]\t Batch [250][550]\t Training Loss 0.3361\t Accuracy 0.9078\n",
      "Epoch [99][200]\t Batch [300][550]\t Training Loss 0.3375\t Accuracy 0.9077\n",
      "Epoch [99][200]\t Batch [350][550]\t Training Loss 0.3390\t Accuracy 0.9063\n",
      "Epoch [99][200]\t Batch [400][550]\t Training Loss 0.3396\t Accuracy 0.9052\n",
      "Epoch [99][200]\t Batch [450][550]\t Training Loss 0.3397\t Accuracy 0.9051\n",
      "Epoch [99][200]\t Batch [500][550]\t Training Loss 0.3405\t Accuracy 0.9047\n",
      "\n",
      "Epoch [99]\t Average training loss 0.3411\t Average training accuracy 0.9045\n",
      "Epoch [99]\t Average validation loss 0.2693\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [100][200]\t Batch [0][550]\t Training Loss 0.2889\t Accuracy 0.9400\n",
      "Epoch [100][200]\t Batch [50][550]\t Training Loss 0.3144\t Accuracy 0.9143\n",
      "Epoch [100][200]\t Batch [100][550]\t Training Loss 0.3251\t Accuracy 0.9123\n",
      "Epoch [100][200]\t Batch [150][550]\t Training Loss 0.3388\t Accuracy 0.9066\n",
      "Epoch [100][200]\t Batch [200][550]\t Training Loss 0.3356\t Accuracy 0.9076\n",
      "Epoch [100][200]\t Batch [250][550]\t Training Loss 0.3355\t Accuracy 0.9079\n",
      "Epoch [100][200]\t Batch [300][550]\t Training Loss 0.3370\t Accuracy 0.9078\n",
      "Epoch [100][200]\t Batch [350][550]\t Training Loss 0.3384\t Accuracy 0.9063\n",
      "Epoch [100][200]\t Batch [400][550]\t Training Loss 0.3390\t Accuracy 0.9053\n",
      "Epoch [100][200]\t Batch [450][550]\t Training Loss 0.3391\t Accuracy 0.9052\n",
      "Epoch [100][200]\t Batch [500][550]\t Training Loss 0.3399\t Accuracy 0.9048\n",
      "\n",
      "Epoch [100]\t Average training loss 0.3405\t Average training accuracy 0.9046\n",
      "Epoch [100]\t Average validation loss 0.2689\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [101][200]\t Batch [0][550]\t Training Loss 0.2884\t Accuracy 0.9400\n",
      "Epoch [101][200]\t Batch [50][550]\t Training Loss 0.3139\t Accuracy 0.9143\n",
      "Epoch [101][200]\t Batch [100][550]\t Training Loss 0.3245\t Accuracy 0.9124\n",
      "Epoch [101][200]\t Batch [150][550]\t Training Loss 0.3383\t Accuracy 0.9066\n",
      "Epoch [101][200]\t Batch [200][550]\t Training Loss 0.3351\t Accuracy 0.9078\n",
      "Epoch [101][200]\t Batch [250][550]\t Training Loss 0.3350\t Accuracy 0.9080\n",
      "Epoch [101][200]\t Batch [300][550]\t Training Loss 0.3364\t Accuracy 0.9079\n",
      "Epoch [101][200]\t Batch [350][550]\t Training Loss 0.3378\t Accuracy 0.9065\n",
      "Epoch [101][200]\t Batch [400][550]\t Training Loss 0.3385\t Accuracy 0.9054\n",
      "Epoch [101][200]\t Batch [450][550]\t Training Loss 0.3386\t Accuracy 0.9053\n",
      "Epoch [101][200]\t Batch [500][550]\t Training Loss 0.3394\t Accuracy 0.9050\n",
      "\n",
      "Epoch [101]\t Average training loss 0.3400\t Average training accuracy 0.9048\n",
      "Epoch [101]\t Average validation loss 0.2685\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [102][200]\t Batch [0][550]\t Training Loss 0.2879\t Accuracy 0.9400\n",
      "Epoch [102][200]\t Batch [50][550]\t Training Loss 0.3133\t Accuracy 0.9145\n",
      "Epoch [102][200]\t Batch [100][550]\t Training Loss 0.3239\t Accuracy 0.9125\n",
      "Epoch [102][200]\t Batch [150][550]\t Training Loss 0.3377\t Accuracy 0.9069\n",
      "Epoch [102][200]\t Batch [200][550]\t Training Loss 0.3345\t Accuracy 0.9080\n",
      "Epoch [102][200]\t Batch [250][550]\t Training Loss 0.3345\t Accuracy 0.9082\n",
      "Epoch [102][200]\t Batch [300][550]\t Training Loss 0.3359\t Accuracy 0.9081\n",
      "Epoch [102][200]\t Batch [350][550]\t Training Loss 0.3373\t Accuracy 0.9066\n",
      "Epoch [102][200]\t Batch [400][550]\t Training Loss 0.3379\t Accuracy 0.9056\n",
      "Epoch [102][200]\t Batch [450][550]\t Training Loss 0.3380\t Accuracy 0.9054\n",
      "Epoch [102][200]\t Batch [500][550]\t Training Loss 0.3388\t Accuracy 0.9050\n",
      "\n",
      "Epoch [102]\t Average training loss 0.3394\t Average training accuracy 0.9049\n",
      "Epoch [102]\t Average validation loss 0.2681\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [103][200]\t Batch [0][550]\t Training Loss 0.2875\t Accuracy 0.9400\n",
      "Epoch [103][200]\t Batch [50][550]\t Training Loss 0.3128\t Accuracy 0.9147\n",
      "Epoch [103][200]\t Batch [100][550]\t Training Loss 0.3234\t Accuracy 0.9127\n",
      "Epoch [103][200]\t Batch [150][550]\t Training Loss 0.3372\t Accuracy 0.9070\n",
      "Epoch [103][200]\t Batch [200][550]\t Training Loss 0.3340\t Accuracy 0.9081\n",
      "Epoch [103][200]\t Batch [250][550]\t Training Loss 0.3339\t Accuracy 0.9084\n",
      "Epoch [103][200]\t Batch [300][550]\t Training Loss 0.3353\t Accuracy 0.9083\n",
      "Epoch [103][200]\t Batch [350][550]\t Training Loss 0.3367\t Accuracy 0.9068\n",
      "Epoch [103][200]\t Batch [400][550]\t Training Loss 0.3373\t Accuracy 0.9057\n",
      "Epoch [103][200]\t Batch [450][550]\t Training Loss 0.3374\t Accuracy 0.9055\n",
      "Epoch [103][200]\t Batch [500][550]\t Training Loss 0.3383\t Accuracy 0.9052\n",
      "\n",
      "Epoch [103]\t Average training loss 0.3389\t Average training accuracy 0.9051\n",
      "Epoch [103]\t Average validation loss 0.2677\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [104][200]\t Batch [0][550]\t Training Loss 0.2870\t Accuracy 0.9400\n",
      "Epoch [104][200]\t Batch [50][550]\t Training Loss 0.3122\t Accuracy 0.9147\n",
      "Epoch [104][200]\t Batch [100][550]\t Training Loss 0.3228\t Accuracy 0.9128\n",
      "Epoch [104][200]\t Batch [150][550]\t Training Loss 0.3366\t Accuracy 0.9072\n",
      "Epoch [104][200]\t Batch [200][550]\t Training Loss 0.3334\t Accuracy 0.9082\n",
      "Epoch [104][200]\t Batch [250][550]\t Training Loss 0.3334\t Accuracy 0.9084\n",
      "Epoch [104][200]\t Batch [300][550]\t Training Loss 0.3348\t Accuracy 0.9083\n",
      "Epoch [104][200]\t Batch [350][550]\t Training Loss 0.3362\t Accuracy 0.9069\n",
      "Epoch [104][200]\t Batch [400][550]\t Training Loss 0.3368\t Accuracy 0.9058\n",
      "Epoch [104][200]\t Batch [450][550]\t Training Loss 0.3369\t Accuracy 0.9056\n",
      "Epoch [104][200]\t Batch [500][550]\t Training Loss 0.3377\t Accuracy 0.9052\n",
      "\n",
      "Epoch [104]\t Average training loss 0.3383\t Average training accuracy 0.9051\n",
      "Epoch [104]\t Average validation loss 0.2673\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [105][200]\t Batch [0][550]\t Training Loss 0.2865\t Accuracy 0.9400\n",
      "Epoch [105][200]\t Batch [50][550]\t Training Loss 0.3117\t Accuracy 0.9143\n",
      "Epoch [105][200]\t Batch [100][550]\t Training Loss 0.3223\t Accuracy 0.9127\n",
      "Epoch [105][200]\t Batch [150][550]\t Training Loss 0.3361\t Accuracy 0.9073\n",
      "Epoch [105][200]\t Batch [200][550]\t Training Loss 0.3329\t Accuracy 0.9083\n",
      "Epoch [105][200]\t Batch [250][550]\t Training Loss 0.3329\t Accuracy 0.9086\n",
      "Epoch [105][200]\t Batch [300][550]\t Training Loss 0.3342\t Accuracy 0.9085\n",
      "Epoch [105][200]\t Batch [350][550]\t Training Loss 0.3356\t Accuracy 0.9071\n",
      "Epoch [105][200]\t Batch [400][550]\t Training Loss 0.3362\t Accuracy 0.9060\n",
      "Epoch [105][200]\t Batch [450][550]\t Training Loss 0.3364\t Accuracy 0.9058\n",
      "Epoch [105][200]\t Batch [500][550]\t Training Loss 0.3372\t Accuracy 0.9055\n",
      "\n",
      "Epoch [105]\t Average training loss 0.3378\t Average training accuracy 0.9053\n",
      "Epoch [105]\t Average validation loss 0.2669\t Average validation accuracy 0.9272\n",
      "\n",
      "Epoch [106][200]\t Batch [0][550]\t Training Loss 0.2861\t Accuracy 0.9400\n",
      "Epoch [106][200]\t Batch [50][550]\t Training Loss 0.3111\t Accuracy 0.9143\n",
      "Epoch [106][200]\t Batch [100][550]\t Training Loss 0.3217\t Accuracy 0.9127\n",
      "Epoch [106][200]\t Batch [150][550]\t Training Loss 0.3355\t Accuracy 0.9073\n",
      "Epoch [106][200]\t Batch [200][550]\t Training Loss 0.3324\t Accuracy 0.9083\n",
      "Epoch [106][200]\t Batch [250][550]\t Training Loss 0.3323\t Accuracy 0.9086\n",
      "Epoch [106][200]\t Batch [300][550]\t Training Loss 0.3337\t Accuracy 0.9087\n",
      "Epoch [106][200]\t Batch [350][550]\t Training Loss 0.3351\t Accuracy 0.9073\n",
      "Epoch [106][200]\t Batch [400][550]\t Training Loss 0.3357\t Accuracy 0.9062\n",
      "Epoch [106][200]\t Batch [450][550]\t Training Loss 0.3358\t Accuracy 0.9060\n",
      "Epoch [106][200]\t Batch [500][550]\t Training Loss 0.3366\t Accuracy 0.9056\n",
      "\n",
      "Epoch [106]\t Average training loss 0.3372\t Average training accuracy 0.9055\n",
      "Epoch [106]\t Average validation loss 0.2665\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [107][200]\t Batch [0][550]\t Training Loss 0.2856\t Accuracy 0.9400\n",
      "Epoch [107][200]\t Batch [50][550]\t Training Loss 0.3106\t Accuracy 0.9147\n",
      "Epoch [107][200]\t Batch [100][550]\t Training Loss 0.3212\t Accuracy 0.9129\n",
      "Epoch [107][200]\t Batch [150][550]\t Training Loss 0.3350\t Accuracy 0.9076\n",
      "Epoch [107][200]\t Batch [200][550]\t Training Loss 0.3318\t Accuracy 0.9086\n",
      "Epoch [107][200]\t Batch [250][550]\t Training Loss 0.3318\t Accuracy 0.9088\n",
      "Epoch [107][200]\t Batch [300][550]\t Training Loss 0.3331\t Accuracy 0.9089\n",
      "Epoch [107][200]\t Batch [350][550]\t Training Loss 0.3345\t Accuracy 0.9074\n",
      "Epoch [107][200]\t Batch [400][550]\t Training Loss 0.3352\t Accuracy 0.9064\n",
      "Epoch [107][200]\t Batch [450][550]\t Training Loss 0.3353\t Accuracy 0.9061\n",
      "Epoch [107][200]\t Batch [500][550]\t Training Loss 0.3361\t Accuracy 0.9057\n",
      "\n",
      "Epoch [107]\t Average training loss 0.3367\t Average training accuracy 0.9056\n",
      "Epoch [107]\t Average validation loss 0.2661\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [108][200]\t Batch [0][550]\t Training Loss 0.2852\t Accuracy 0.9400\n",
      "Epoch [108][200]\t Batch [50][550]\t Training Loss 0.3100\t Accuracy 0.9149\n",
      "Epoch [108][200]\t Batch [100][550]\t Training Loss 0.3207\t Accuracy 0.9130\n",
      "Epoch [108][200]\t Batch [150][550]\t Training Loss 0.3345\t Accuracy 0.9077\n",
      "Epoch [108][200]\t Batch [200][550]\t Training Loss 0.3313\t Accuracy 0.9087\n",
      "Epoch [108][200]\t Batch [250][550]\t Training Loss 0.3313\t Accuracy 0.9090\n",
      "Epoch [108][200]\t Batch [300][550]\t Training Loss 0.3326\t Accuracy 0.9091\n",
      "Epoch [108][200]\t Batch [350][550]\t Training Loss 0.3340\t Accuracy 0.9076\n",
      "Epoch [108][200]\t Batch [400][550]\t Training Loss 0.3346\t Accuracy 0.9065\n",
      "Epoch [108][200]\t Batch [450][550]\t Training Loss 0.3347\t Accuracy 0.9063\n",
      "Epoch [108][200]\t Batch [500][550]\t Training Loss 0.3356\t Accuracy 0.9059\n",
      "\n",
      "Epoch [108]\t Average training loss 0.3362\t Average training accuracy 0.9057\n",
      "Epoch [108]\t Average validation loss 0.2657\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [109][200]\t Batch [0][550]\t Training Loss 0.2847\t Accuracy 0.9400\n",
      "Epoch [109][200]\t Batch [50][550]\t Training Loss 0.3095\t Accuracy 0.9151\n",
      "Epoch [109][200]\t Batch [100][550]\t Training Loss 0.3201\t Accuracy 0.9132\n",
      "Epoch [109][200]\t Batch [150][550]\t Training Loss 0.3339\t Accuracy 0.9079\n",
      "Epoch [109][200]\t Batch [200][550]\t Training Loss 0.3308\t Accuracy 0.9089\n",
      "Epoch [109][200]\t Batch [250][550]\t Training Loss 0.3308\t Accuracy 0.9092\n",
      "Epoch [109][200]\t Batch [300][550]\t Training Loss 0.3321\t Accuracy 0.9093\n",
      "Epoch [109][200]\t Batch [350][550]\t Training Loss 0.3335\t Accuracy 0.9078\n",
      "Epoch [109][200]\t Batch [400][550]\t Training Loss 0.3341\t Accuracy 0.9067\n",
      "Epoch [109][200]\t Batch [450][550]\t Training Loss 0.3342\t Accuracy 0.9065\n",
      "Epoch [109][200]\t Batch [500][550]\t Training Loss 0.3350\t Accuracy 0.9061\n",
      "\n",
      "Epoch [109]\t Average training loss 0.3356\t Average training accuracy 0.9059\n",
      "Epoch [109]\t Average validation loss 0.2653\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [110][200]\t Batch [0][550]\t Training Loss 0.2843\t Accuracy 0.9400\n",
      "Epoch [110][200]\t Batch [50][550]\t Training Loss 0.3090\t Accuracy 0.9157\n",
      "Epoch [110][200]\t Batch [100][550]\t Training Loss 0.3196\t Accuracy 0.9135\n",
      "Epoch [110][200]\t Batch [150][550]\t Training Loss 0.3334\t Accuracy 0.9081\n",
      "Epoch [110][200]\t Batch [200][550]\t Training Loss 0.3303\t Accuracy 0.9091\n",
      "Epoch [110][200]\t Batch [250][550]\t Training Loss 0.3302\t Accuracy 0.9094\n",
      "Epoch [110][200]\t Batch [300][550]\t Training Loss 0.3315\t Accuracy 0.9095\n",
      "Epoch [110][200]\t Batch [350][550]\t Training Loss 0.3329\t Accuracy 0.9080\n",
      "Epoch [110][200]\t Batch [400][550]\t Training Loss 0.3336\t Accuracy 0.9069\n",
      "Epoch [110][200]\t Batch [450][550]\t Training Loss 0.3337\t Accuracy 0.9067\n",
      "Epoch [110][200]\t Batch [500][550]\t Training Loss 0.3345\t Accuracy 0.9063\n",
      "\n",
      "Epoch [110]\t Average training loss 0.3351\t Average training accuracy 0.9061\n",
      "Epoch [110]\t Average validation loss 0.2650\t Average validation accuracy 0.9274\n",
      "\n",
      "Epoch [111][200]\t Batch [0][550]\t Training Loss 0.2839\t Accuracy 0.9400\n",
      "Epoch [111][200]\t Batch [50][550]\t Training Loss 0.3084\t Accuracy 0.9157\n",
      "Epoch [111][200]\t Batch [100][550]\t Training Loss 0.3191\t Accuracy 0.9136\n",
      "Epoch [111][200]\t Batch [150][550]\t Training Loss 0.3329\t Accuracy 0.9083\n",
      "Epoch [111][200]\t Batch [200][550]\t Training Loss 0.3297\t Accuracy 0.9092\n",
      "Epoch [111][200]\t Batch [250][550]\t Training Loss 0.3297\t Accuracy 0.9095\n",
      "Epoch [111][200]\t Batch [300][550]\t Training Loss 0.3310\t Accuracy 0.9096\n",
      "Epoch [111][200]\t Batch [350][550]\t Training Loss 0.3324\t Accuracy 0.9081\n",
      "Epoch [111][200]\t Batch [400][550]\t Training Loss 0.3330\t Accuracy 0.9070\n",
      "Epoch [111][200]\t Batch [450][550]\t Training Loss 0.3332\t Accuracy 0.9069\n",
      "Epoch [111][200]\t Batch [500][550]\t Training Loss 0.3340\t Accuracy 0.9064\n",
      "\n",
      "Epoch [111]\t Average training loss 0.3346\t Average training accuracy 0.9062\n",
      "Epoch [111]\t Average validation loss 0.2646\t Average validation accuracy 0.9274\n",
      "\n",
      "Epoch [112][200]\t Batch [0][550]\t Training Loss 0.2834\t Accuracy 0.9400\n",
      "Epoch [112][200]\t Batch [50][550]\t Training Loss 0.3079\t Accuracy 0.9157\n",
      "Epoch [112][200]\t Batch [100][550]\t Training Loss 0.3185\t Accuracy 0.9136\n",
      "Epoch [112][200]\t Batch [150][550]\t Training Loss 0.3324\t Accuracy 0.9083\n",
      "Epoch [112][200]\t Batch [200][550]\t Training Loss 0.3292\t Accuracy 0.9091\n",
      "Epoch [112][200]\t Batch [250][550]\t Training Loss 0.3292\t Accuracy 0.9095\n",
      "Epoch [112][200]\t Batch [300][550]\t Training Loss 0.3305\t Accuracy 0.9097\n",
      "Epoch [112][200]\t Batch [350][550]\t Training Loss 0.3319\t Accuracy 0.9082\n",
      "Epoch [112][200]\t Batch [400][550]\t Training Loss 0.3325\t Accuracy 0.9071\n",
      "Epoch [112][200]\t Batch [450][550]\t Training Loss 0.3326\t Accuracy 0.9070\n",
      "Epoch [112][200]\t Batch [500][550]\t Training Loss 0.3335\t Accuracy 0.9065\n",
      "\n",
      "Epoch [112]\t Average training loss 0.3341\t Average training accuracy 0.9063\n",
      "Epoch [112]\t Average validation loss 0.2642\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [113][200]\t Batch [0][550]\t Training Loss 0.2830\t Accuracy 0.9400\n",
      "Epoch [113][200]\t Batch [50][550]\t Training Loss 0.3074\t Accuracy 0.9157\n",
      "Epoch [113][200]\t Batch [100][550]\t Training Loss 0.3180\t Accuracy 0.9137\n",
      "Epoch [113][200]\t Batch [150][550]\t Training Loss 0.3318\t Accuracy 0.9084\n",
      "Epoch [113][200]\t Batch [200][550]\t Training Loss 0.3287\t Accuracy 0.9092\n",
      "Epoch [113][200]\t Batch [250][550]\t Training Loss 0.3287\t Accuracy 0.9096\n",
      "Epoch [113][200]\t Batch [300][550]\t Training Loss 0.3300\t Accuracy 0.9097\n",
      "Epoch [113][200]\t Batch [350][550]\t Training Loss 0.3314\t Accuracy 0.9082\n",
      "Epoch [113][200]\t Batch [400][550]\t Training Loss 0.3320\t Accuracy 0.9072\n",
      "Epoch [113][200]\t Batch [450][550]\t Training Loss 0.3321\t Accuracy 0.9070\n",
      "Epoch [113][200]\t Batch [500][550]\t Training Loss 0.3330\t Accuracy 0.9066\n",
      "\n",
      "Epoch [113]\t Average training loss 0.3335\t Average training accuracy 0.9063\n",
      "Epoch [113]\t Average validation loss 0.2638\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [114][200]\t Batch [0][550]\t Training Loss 0.2826\t Accuracy 0.9400\n",
      "Epoch [114][200]\t Batch [50][550]\t Training Loss 0.3069\t Accuracy 0.9159\n",
      "Epoch [114][200]\t Batch [100][550]\t Training Loss 0.3175\t Accuracy 0.9138\n",
      "Epoch [114][200]\t Batch [150][550]\t Training Loss 0.3313\t Accuracy 0.9085\n",
      "Epoch [114][200]\t Batch [200][550]\t Training Loss 0.3282\t Accuracy 0.9093\n",
      "Epoch [114][200]\t Batch [250][550]\t Training Loss 0.3282\t Accuracy 0.9096\n",
      "Epoch [114][200]\t Batch [300][550]\t Training Loss 0.3295\t Accuracy 0.9097\n",
      "Epoch [114][200]\t Batch [350][550]\t Training Loss 0.3308\t Accuracy 0.9082\n",
      "Epoch [114][200]\t Batch [400][550]\t Training Loss 0.3315\t Accuracy 0.9072\n",
      "Epoch [114][200]\t Batch [450][550]\t Training Loss 0.3316\t Accuracy 0.9071\n",
      "Epoch [114][200]\t Batch [500][550]\t Training Loss 0.3324\t Accuracy 0.9066\n",
      "\n",
      "Epoch [114]\t Average training loss 0.3330\t Average training accuracy 0.9064\n",
      "Epoch [114]\t Average validation loss 0.2634\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [115][200]\t Batch [0][550]\t Training Loss 0.2822\t Accuracy 0.9400\n",
      "Epoch [115][200]\t Batch [50][550]\t Training Loss 0.3063\t Accuracy 0.9159\n",
      "Epoch [115][200]\t Batch [100][550]\t Training Loss 0.3170\t Accuracy 0.9138\n",
      "Epoch [115][200]\t Batch [150][550]\t Training Loss 0.3308\t Accuracy 0.9086\n",
      "Epoch [115][200]\t Batch [200][550]\t Training Loss 0.3277\t Accuracy 0.9093\n",
      "Epoch [115][200]\t Batch [250][550]\t Training Loss 0.3277\t Accuracy 0.9096\n",
      "Epoch [115][200]\t Batch [300][550]\t Training Loss 0.3289\t Accuracy 0.9098\n",
      "Epoch [115][200]\t Batch [350][550]\t Training Loss 0.3303\t Accuracy 0.9082\n",
      "Epoch [115][200]\t Batch [400][550]\t Training Loss 0.3309\t Accuracy 0.9072\n",
      "Epoch [115][200]\t Batch [450][550]\t Training Loss 0.3311\t Accuracy 0.9071\n",
      "Epoch [115][200]\t Batch [500][550]\t Training Loss 0.3319\t Accuracy 0.9067\n",
      "\n",
      "Epoch [115]\t Average training loss 0.3325\t Average training accuracy 0.9065\n",
      "Epoch [115]\t Average validation loss 0.2630\t Average validation accuracy 0.9278\n",
      "\n",
      "Epoch [116][200]\t Batch [0][550]\t Training Loss 0.2818\t Accuracy 0.9400\n",
      "Epoch [116][200]\t Batch [50][550]\t Training Loss 0.3058\t Accuracy 0.9163\n",
      "Epoch [116][200]\t Batch [100][550]\t Training Loss 0.3165\t Accuracy 0.9142\n",
      "Epoch [116][200]\t Batch [150][550]\t Training Loss 0.3303\t Accuracy 0.9089\n",
      "Epoch [116][200]\t Batch [200][550]\t Training Loss 0.3272\t Accuracy 0.9096\n",
      "Epoch [116][200]\t Batch [250][550]\t Training Loss 0.3272\t Accuracy 0.9098\n",
      "Epoch [116][200]\t Batch [300][550]\t Training Loss 0.3284\t Accuracy 0.9100\n",
      "Epoch [116][200]\t Batch [350][550]\t Training Loss 0.3298\t Accuracy 0.9084\n",
      "Epoch [116][200]\t Batch [400][550]\t Training Loss 0.3304\t Accuracy 0.9074\n",
      "Epoch [116][200]\t Batch [450][550]\t Training Loss 0.3306\t Accuracy 0.9072\n",
      "Epoch [116][200]\t Batch [500][550]\t Training Loss 0.3314\t Accuracy 0.9068\n",
      "\n",
      "Epoch [116]\t Average training loss 0.3320\t Average training accuracy 0.9066\n",
      "Epoch [116]\t Average validation loss 0.2627\t Average validation accuracy 0.9282\n",
      "\n",
      "Epoch [117][200]\t Batch [0][550]\t Training Loss 0.2813\t Accuracy 0.9400\n",
      "Epoch [117][200]\t Batch [50][550]\t Training Loss 0.3053\t Accuracy 0.9165\n",
      "Epoch [117][200]\t Batch [100][550]\t Training Loss 0.3159\t Accuracy 0.9145\n",
      "Epoch [117][200]\t Batch [150][550]\t Training Loss 0.3298\t Accuracy 0.9093\n",
      "Epoch [117][200]\t Batch [200][550]\t Training Loss 0.3267\t Accuracy 0.9099\n",
      "Epoch [117][200]\t Batch [250][550]\t Training Loss 0.3267\t Accuracy 0.9101\n",
      "Epoch [117][200]\t Batch [300][550]\t Training Loss 0.3279\t Accuracy 0.9103\n",
      "Epoch [117][200]\t Batch [350][550]\t Training Loss 0.3293\t Accuracy 0.9087\n",
      "Epoch [117][200]\t Batch [400][550]\t Training Loss 0.3299\t Accuracy 0.9077\n",
      "Epoch [117][200]\t Batch [450][550]\t Training Loss 0.3301\t Accuracy 0.9075\n",
      "Epoch [117][200]\t Batch [500][550]\t Training Loss 0.3309\t Accuracy 0.9070\n",
      "\n",
      "Epoch [117]\t Average training loss 0.3315\t Average training accuracy 0.9068\n",
      "Epoch [117]\t Average validation loss 0.2623\t Average validation accuracy 0.9282\n",
      "\n",
      "Epoch [118][200]\t Batch [0][550]\t Training Loss 0.2809\t Accuracy 0.9400\n",
      "Epoch [118][200]\t Batch [50][550]\t Training Loss 0.3048\t Accuracy 0.9167\n",
      "Epoch [118][200]\t Batch [100][550]\t Training Loss 0.3154\t Accuracy 0.9147\n",
      "Epoch [118][200]\t Batch [150][550]\t Training Loss 0.3293\t Accuracy 0.9095\n",
      "Epoch [118][200]\t Batch [200][550]\t Training Loss 0.3262\t Accuracy 0.9102\n",
      "Epoch [118][200]\t Batch [250][550]\t Training Loss 0.3262\t Accuracy 0.9104\n",
      "Epoch [118][200]\t Batch [300][550]\t Training Loss 0.3274\t Accuracy 0.9106\n",
      "Epoch [118][200]\t Batch [350][550]\t Training Loss 0.3288\t Accuracy 0.9091\n",
      "Epoch [118][200]\t Batch [400][550]\t Training Loss 0.3294\t Accuracy 0.9080\n",
      "Epoch [118][200]\t Batch [450][550]\t Training Loss 0.3296\t Accuracy 0.9078\n",
      "Epoch [118][200]\t Batch [500][550]\t Training Loss 0.3304\t Accuracy 0.9074\n",
      "\n",
      "Epoch [118]\t Average training loss 0.3310\t Average training accuracy 0.9071\n",
      "Epoch [118]\t Average validation loss 0.2619\t Average validation accuracy 0.9288\n",
      "\n",
      "Epoch [119][200]\t Batch [0][550]\t Training Loss 0.2805\t Accuracy 0.9400\n",
      "Epoch [119][200]\t Batch [50][550]\t Training Loss 0.3043\t Accuracy 0.9169\n",
      "Epoch [119][200]\t Batch [100][550]\t Training Loss 0.3149\t Accuracy 0.9149\n",
      "Epoch [119][200]\t Batch [150][550]\t Training Loss 0.3288\t Accuracy 0.9096\n",
      "Epoch [119][200]\t Batch [200][550]\t Training Loss 0.3257\t Accuracy 0.9104\n",
      "Epoch [119][200]\t Batch [250][550]\t Training Loss 0.3257\t Accuracy 0.9105\n",
      "Epoch [119][200]\t Batch [300][550]\t Training Loss 0.3269\t Accuracy 0.9107\n",
      "Epoch [119][200]\t Batch [350][550]\t Training Loss 0.3283\t Accuracy 0.9092\n",
      "Epoch [119][200]\t Batch [400][550]\t Training Loss 0.3289\t Accuracy 0.9082\n",
      "Epoch [119][200]\t Batch [450][550]\t Training Loss 0.3291\t Accuracy 0.9079\n",
      "Epoch [119][200]\t Batch [500][550]\t Training Loss 0.3299\t Accuracy 0.9075\n",
      "\n",
      "Epoch [119]\t Average training loss 0.3305\t Average training accuracy 0.9072\n",
      "Epoch [119]\t Average validation loss 0.2615\t Average validation accuracy 0.9286\n",
      "\n",
      "Epoch [120][200]\t Batch [0][550]\t Training Loss 0.2801\t Accuracy 0.9400\n",
      "Epoch [120][200]\t Batch [50][550]\t Training Loss 0.3038\t Accuracy 0.9169\n",
      "Epoch [120][200]\t Batch [100][550]\t Training Loss 0.3144\t Accuracy 0.9150\n",
      "Epoch [120][200]\t Batch [150][550]\t Training Loss 0.3283\t Accuracy 0.9099\n",
      "Epoch [120][200]\t Batch [200][550]\t Training Loss 0.3252\t Accuracy 0.9106\n",
      "Epoch [120][200]\t Batch [250][550]\t Training Loss 0.3252\t Accuracy 0.9107\n",
      "Epoch [120][200]\t Batch [300][550]\t Training Loss 0.3264\t Accuracy 0.9109\n",
      "Epoch [120][200]\t Batch [350][550]\t Training Loss 0.3278\t Accuracy 0.9094\n",
      "Epoch [120][200]\t Batch [400][550]\t Training Loss 0.3284\t Accuracy 0.9083\n",
      "Epoch [120][200]\t Batch [450][550]\t Training Loss 0.3285\t Accuracy 0.9081\n",
      "Epoch [120][200]\t Batch [500][550]\t Training Loss 0.3294\t Accuracy 0.9076\n",
      "\n",
      "Epoch [120]\t Average training loss 0.3300\t Average training accuracy 0.9073\n",
      "Epoch [120]\t Average validation loss 0.2611\t Average validation accuracy 0.9288\n",
      "\n",
      "Epoch [121][200]\t Batch [0][550]\t Training Loss 0.2796\t Accuracy 0.9500\n",
      "Epoch [121][200]\t Batch [50][550]\t Training Loss 0.3033\t Accuracy 0.9173\n",
      "Epoch [121][200]\t Batch [100][550]\t Training Loss 0.3139\t Accuracy 0.9151\n",
      "Epoch [121][200]\t Batch [150][550]\t Training Loss 0.3278\t Accuracy 0.9103\n",
      "Epoch [121][200]\t Batch [200][550]\t Training Loss 0.3247\t Accuracy 0.9110\n",
      "Epoch [121][200]\t Batch [250][550]\t Training Loss 0.3247\t Accuracy 0.9110\n",
      "Epoch [121][200]\t Batch [300][550]\t Training Loss 0.3259\t Accuracy 0.9111\n",
      "Epoch [121][200]\t Batch [350][550]\t Training Loss 0.3273\t Accuracy 0.9096\n",
      "Epoch [121][200]\t Batch [400][550]\t Training Loss 0.3279\t Accuracy 0.9086\n",
      "Epoch [121][200]\t Batch [450][550]\t Training Loss 0.3280\t Accuracy 0.9083\n",
      "Epoch [121][200]\t Batch [500][550]\t Training Loss 0.3289\t Accuracy 0.9079\n",
      "\n",
      "Epoch [121]\t Average training loss 0.3295\t Average training accuracy 0.9075\n",
      "Epoch [121]\t Average validation loss 0.2608\t Average validation accuracy 0.9288\n",
      "\n",
      "Epoch [122][200]\t Batch [0][550]\t Training Loss 0.2790\t Accuracy 0.9500\n",
      "Epoch [122][200]\t Batch [50][550]\t Training Loss 0.3028\t Accuracy 0.9169\n",
      "Epoch [122][200]\t Batch [100][550]\t Training Loss 0.3134\t Accuracy 0.9151\n",
      "Epoch [122][200]\t Batch [150][550]\t Training Loss 0.3273\t Accuracy 0.9103\n",
      "Epoch [122][200]\t Batch [200][550]\t Training Loss 0.3242\t Accuracy 0.9111\n",
      "Epoch [122][200]\t Batch [250][550]\t Training Loss 0.3242\t Accuracy 0.9111\n",
      "Epoch [122][200]\t Batch [300][550]\t Training Loss 0.3254\t Accuracy 0.9112\n",
      "Epoch [122][200]\t Batch [350][550]\t Training Loss 0.3268\t Accuracy 0.9097\n",
      "Epoch [122][200]\t Batch [400][550]\t Training Loss 0.3274\t Accuracy 0.9086\n",
      "Epoch [122][200]\t Batch [450][550]\t Training Loss 0.3276\t Accuracy 0.9084\n",
      "Epoch [122][200]\t Batch [500][550]\t Training Loss 0.3284\t Accuracy 0.9079\n",
      "\n",
      "Epoch [122]\t Average training loss 0.3290\t Average training accuracy 0.9076\n",
      "Epoch [122]\t Average validation loss 0.2604\t Average validation accuracy 0.9290\n",
      "\n",
      "Epoch [123][200]\t Batch [0][550]\t Training Loss 0.2784\t Accuracy 0.9500\n",
      "Epoch [123][200]\t Batch [50][550]\t Training Loss 0.3022\t Accuracy 0.9176\n",
      "Epoch [123][200]\t Batch [100][550]\t Training Loss 0.3129\t Accuracy 0.9155\n",
      "Epoch [123][200]\t Batch [150][550]\t Training Loss 0.3268\t Accuracy 0.9107\n",
      "Epoch [123][200]\t Batch [200][550]\t Training Loss 0.3237\t Accuracy 0.9113\n",
      "Epoch [123][200]\t Batch [250][550]\t Training Loss 0.3238\t Accuracy 0.9114\n",
      "Epoch [123][200]\t Batch [300][550]\t Training Loss 0.3249\t Accuracy 0.9115\n",
      "Epoch [123][200]\t Batch [350][550]\t Training Loss 0.3263\t Accuracy 0.9101\n",
      "Epoch [123][200]\t Batch [400][550]\t Training Loss 0.3269\t Accuracy 0.9089\n",
      "Epoch [123][200]\t Batch [450][550]\t Training Loss 0.3271\t Accuracy 0.9087\n",
      "Epoch [123][200]\t Batch [500][550]\t Training Loss 0.3279\t Accuracy 0.9082\n",
      "\n",
      "Epoch [123]\t Average training loss 0.3285\t Average training accuracy 0.9079\n",
      "Epoch [123]\t Average validation loss 0.2600\t Average validation accuracy 0.9292\n",
      "\n",
      "Epoch [124][200]\t Batch [0][550]\t Training Loss 0.2778\t Accuracy 0.9500\n",
      "Epoch [124][200]\t Batch [50][550]\t Training Loss 0.3017\t Accuracy 0.9178\n",
      "Epoch [124][200]\t Batch [100][550]\t Training Loss 0.3124\t Accuracy 0.9156\n",
      "Epoch [124][200]\t Batch [150][550]\t Training Loss 0.3263\t Accuracy 0.9108\n",
      "Epoch [124][200]\t Batch [200][550]\t Training Loss 0.3232\t Accuracy 0.9114\n",
      "Epoch [124][200]\t Batch [250][550]\t Training Loss 0.3233\t Accuracy 0.9114\n",
      "Epoch [124][200]\t Batch [300][550]\t Training Loss 0.3244\t Accuracy 0.9116\n",
      "Epoch [124][200]\t Batch [350][550]\t Training Loss 0.3258\t Accuracy 0.9101\n",
      "Epoch [124][200]\t Batch [400][550]\t Training Loss 0.3264\t Accuracy 0.9090\n",
      "Epoch [124][200]\t Batch [450][550]\t Training Loss 0.3266\t Accuracy 0.9088\n",
      "Epoch [124][200]\t Batch [500][550]\t Training Loss 0.3274\t Accuracy 0.9083\n",
      "\n",
      "Epoch [124]\t Average training loss 0.3280\t Average training accuracy 0.9079\n",
      "Epoch [124]\t Average validation loss 0.2597\t Average validation accuracy 0.9292\n",
      "\n",
      "Epoch [125][200]\t Batch [0][550]\t Training Loss 0.2772\t Accuracy 0.9500\n",
      "Epoch [125][200]\t Batch [50][550]\t Training Loss 0.3012\t Accuracy 0.9178\n",
      "Epoch [125][200]\t Batch [100][550]\t Training Loss 0.3119\t Accuracy 0.9156\n",
      "Epoch [125][200]\t Batch [150][550]\t Training Loss 0.3258\t Accuracy 0.9108\n",
      "Epoch [125][200]\t Batch [200][550]\t Training Loss 0.3227\t Accuracy 0.9115\n",
      "Epoch [125][200]\t Batch [250][550]\t Training Loss 0.3228\t Accuracy 0.9116\n",
      "Epoch [125][200]\t Batch [300][550]\t Training Loss 0.3239\t Accuracy 0.9117\n",
      "Epoch [125][200]\t Batch [350][550]\t Training Loss 0.3253\t Accuracy 0.9103\n",
      "Epoch [125][200]\t Batch [400][550]\t Training Loss 0.3259\t Accuracy 0.9091\n",
      "Epoch [125][200]\t Batch [450][550]\t Training Loss 0.3261\t Accuracy 0.9089\n",
      "Epoch [125][200]\t Batch [500][550]\t Training Loss 0.3269\t Accuracy 0.9084\n",
      "\n",
      "Epoch [125]\t Average training loss 0.3275\t Average training accuracy 0.9081\n",
      "Epoch [125]\t Average validation loss 0.2593\t Average validation accuracy 0.9290\n",
      "\n",
      "Epoch [126][200]\t Batch [0][550]\t Training Loss 0.2767\t Accuracy 0.9500\n",
      "Epoch [126][200]\t Batch [50][550]\t Training Loss 0.3007\t Accuracy 0.9178\n",
      "Epoch [126][200]\t Batch [100][550]\t Training Loss 0.3114\t Accuracy 0.9158\n",
      "Epoch [126][200]\t Batch [150][550]\t Training Loss 0.3253\t Accuracy 0.9110\n",
      "Epoch [126][200]\t Batch [200][550]\t Training Loss 0.3222\t Accuracy 0.9117\n",
      "Epoch [126][200]\t Batch [250][550]\t Training Loss 0.3223\t Accuracy 0.9118\n",
      "Epoch [126][200]\t Batch [300][550]\t Training Loss 0.3234\t Accuracy 0.9119\n",
      "Epoch [126][200]\t Batch [350][550]\t Training Loss 0.3248\t Accuracy 0.9104\n",
      "Epoch [126][200]\t Batch [400][550]\t Training Loss 0.3254\t Accuracy 0.9093\n",
      "Epoch [126][200]\t Batch [450][550]\t Training Loss 0.3256\t Accuracy 0.9091\n",
      "Epoch [126][200]\t Batch [500][550]\t Training Loss 0.3264\t Accuracy 0.9086\n",
      "\n",
      "Epoch [126]\t Average training loss 0.3270\t Average training accuracy 0.9082\n",
      "Epoch [126]\t Average validation loss 0.2589\t Average validation accuracy 0.9294\n",
      "\n",
      "Epoch [127][200]\t Batch [0][550]\t Training Loss 0.2761\t Accuracy 0.9500\n",
      "Epoch [127][200]\t Batch [50][550]\t Training Loss 0.3003\t Accuracy 0.9178\n",
      "Epoch [127][200]\t Batch [100][550]\t Training Loss 0.3110\t Accuracy 0.9157\n",
      "Epoch [127][200]\t Batch [150][550]\t Training Loss 0.3248\t Accuracy 0.9109\n",
      "Epoch [127][200]\t Batch [200][550]\t Training Loss 0.3217\t Accuracy 0.9118\n",
      "Epoch [127][200]\t Batch [250][550]\t Training Loss 0.3218\t Accuracy 0.9118\n",
      "Epoch [127][200]\t Batch [300][550]\t Training Loss 0.3229\t Accuracy 0.9120\n",
      "Epoch [127][200]\t Batch [350][550]\t Training Loss 0.3243\t Accuracy 0.9106\n",
      "Epoch [127][200]\t Batch [400][550]\t Training Loss 0.3249\t Accuracy 0.9095\n",
      "Epoch [127][200]\t Batch [450][550]\t Training Loss 0.3251\t Accuracy 0.9093\n",
      "Epoch [127][200]\t Batch [500][550]\t Training Loss 0.3260\t Accuracy 0.9088\n",
      "\n",
      "Epoch [127]\t Average training loss 0.3265\t Average training accuracy 0.9084\n",
      "Epoch [127]\t Average validation loss 0.2586\t Average validation accuracy 0.9296\n",
      "\n",
      "Epoch [128][200]\t Batch [0][550]\t Training Loss 0.2755\t Accuracy 0.9500\n",
      "Epoch [128][200]\t Batch [50][550]\t Training Loss 0.2998\t Accuracy 0.9178\n",
      "Epoch [128][200]\t Batch [100][550]\t Training Loss 0.3105\t Accuracy 0.9159\n",
      "Epoch [128][200]\t Batch [150][550]\t Training Loss 0.3243\t Accuracy 0.9110\n",
      "Epoch [128][200]\t Batch [200][550]\t Training Loss 0.3212\t Accuracy 0.9118\n",
      "Epoch [128][200]\t Batch [250][550]\t Training Loss 0.3214\t Accuracy 0.9118\n",
      "Epoch [128][200]\t Batch [300][550]\t Training Loss 0.3224\t Accuracy 0.9121\n",
      "Epoch [128][200]\t Batch [350][550]\t Training Loss 0.3238\t Accuracy 0.9107\n",
      "Epoch [128][200]\t Batch [400][550]\t Training Loss 0.3244\t Accuracy 0.9095\n",
      "Epoch [128][200]\t Batch [450][550]\t Training Loss 0.3246\t Accuracy 0.9094\n",
      "Epoch [128][200]\t Batch [500][550]\t Training Loss 0.3255\t Accuracy 0.9088\n",
      "\n",
      "Epoch [128]\t Average training loss 0.3260\t Average training accuracy 0.9085\n",
      "Epoch [128]\t Average validation loss 0.2582\t Average validation accuracy 0.9302\n",
      "\n",
      "Epoch [129][200]\t Batch [0][550]\t Training Loss 0.2750\t Accuracy 0.9500\n",
      "Epoch [129][200]\t Batch [50][550]\t Training Loss 0.2993\t Accuracy 0.9178\n",
      "Epoch [129][200]\t Batch [100][550]\t Training Loss 0.3100\t Accuracy 0.9160\n",
      "Epoch [129][200]\t Batch [150][550]\t Training Loss 0.3239\t Accuracy 0.9112\n",
      "Epoch [129][200]\t Batch [200][550]\t Training Loss 0.3208\t Accuracy 0.9120\n",
      "Epoch [129][200]\t Batch [250][550]\t Training Loss 0.3209\t Accuracy 0.9120\n",
      "Epoch [129][200]\t Batch [300][550]\t Training Loss 0.3219\t Accuracy 0.9123\n",
      "Epoch [129][200]\t Batch [350][550]\t Training Loss 0.3233\t Accuracy 0.9109\n",
      "Epoch [129][200]\t Batch [400][550]\t Training Loss 0.3239\t Accuracy 0.9097\n",
      "Epoch [129][200]\t Batch [450][550]\t Training Loss 0.3241\t Accuracy 0.9096\n",
      "Epoch [129][200]\t Batch [500][550]\t Training Loss 0.3250\t Accuracy 0.9090\n",
      "\n",
      "Epoch [129]\t Average training loss 0.3256\t Average training accuracy 0.9087\n",
      "Epoch [129]\t Average validation loss 0.2578\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [130][200]\t Batch [0][550]\t Training Loss 0.2744\t Accuracy 0.9500\n",
      "Epoch [130][200]\t Batch [50][550]\t Training Loss 0.2988\t Accuracy 0.9176\n",
      "Epoch [130][200]\t Batch [100][550]\t Training Loss 0.3095\t Accuracy 0.9158\n",
      "Epoch [130][200]\t Batch [150][550]\t Training Loss 0.3234\t Accuracy 0.9111\n",
      "Epoch [130][200]\t Batch [200][550]\t Training Loss 0.3203\t Accuracy 0.9119\n",
      "Epoch [130][200]\t Batch [250][550]\t Training Loss 0.3204\t Accuracy 0.9119\n",
      "Epoch [130][200]\t Batch [300][550]\t Training Loss 0.3215\t Accuracy 0.9122\n",
      "Epoch [130][200]\t Batch [350][550]\t Training Loss 0.3228\t Accuracy 0.9108\n",
      "Epoch [130][200]\t Batch [400][550]\t Training Loss 0.3235\t Accuracy 0.9097\n",
      "Epoch [130][200]\t Batch [450][550]\t Training Loss 0.3236\t Accuracy 0.9095\n",
      "Epoch [130][200]\t Batch [500][550]\t Training Loss 0.3245\t Accuracy 0.9090\n",
      "\n",
      "Epoch [130]\t Average training loss 0.3251\t Average training accuracy 0.9088\n",
      "Epoch [130]\t Average validation loss 0.2575\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [131][200]\t Batch [0][550]\t Training Loss 0.2739\t Accuracy 0.9500\n",
      "Epoch [131][200]\t Batch [50][550]\t Training Loss 0.2983\t Accuracy 0.9178\n",
      "Epoch [131][200]\t Batch [100][550]\t Training Loss 0.3090\t Accuracy 0.9161\n",
      "Epoch [131][200]\t Batch [150][550]\t Training Loss 0.3229\t Accuracy 0.9113\n",
      "Epoch [131][200]\t Batch [200][550]\t Training Loss 0.3198\t Accuracy 0.9121\n",
      "Epoch [131][200]\t Batch [250][550]\t Training Loss 0.3199\t Accuracy 0.9122\n",
      "Epoch [131][200]\t Batch [300][550]\t Training Loss 0.3210\t Accuracy 0.9124\n",
      "Epoch [131][200]\t Batch [350][550]\t Training Loss 0.3223\t Accuracy 0.9110\n",
      "Epoch [131][200]\t Batch [400][550]\t Training Loss 0.3230\t Accuracy 0.9099\n",
      "Epoch [131][200]\t Batch [450][550]\t Training Loss 0.3232\t Accuracy 0.9097\n",
      "Epoch [131][200]\t Batch [500][550]\t Training Loss 0.3240\t Accuracy 0.9092\n",
      "\n",
      "Epoch [131]\t Average training loss 0.3246\t Average training accuracy 0.9090\n",
      "Epoch [131]\t Average validation loss 0.2571\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [132][200]\t Batch [0][550]\t Training Loss 0.2733\t Accuracy 0.9500\n",
      "Epoch [132][200]\t Batch [50][550]\t Training Loss 0.2978\t Accuracy 0.9182\n",
      "Epoch [132][200]\t Batch [100][550]\t Training Loss 0.3085\t Accuracy 0.9165\n",
      "Epoch [132][200]\t Batch [150][550]\t Training Loss 0.3224\t Accuracy 0.9115\n",
      "Epoch [132][200]\t Batch [200][550]\t Training Loss 0.3193\t Accuracy 0.9123\n",
      "Epoch [132][200]\t Batch [250][550]\t Training Loss 0.3195\t Accuracy 0.9123\n",
      "Epoch [132][200]\t Batch [300][550]\t Training Loss 0.3205\t Accuracy 0.9126\n",
      "Epoch [132][200]\t Batch [350][550]\t Training Loss 0.3219\t Accuracy 0.9112\n",
      "Epoch [132][200]\t Batch [400][550]\t Training Loss 0.3225\t Accuracy 0.9101\n",
      "Epoch [132][200]\t Batch [450][550]\t Training Loss 0.3227\t Accuracy 0.9099\n",
      "Epoch [132][200]\t Batch [500][550]\t Training Loss 0.3236\t Accuracy 0.9095\n",
      "\n",
      "Epoch [132]\t Average training loss 0.3241\t Average training accuracy 0.9092\n",
      "Epoch [132]\t Average validation loss 0.2568\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [133][200]\t Batch [0][550]\t Training Loss 0.2728\t Accuracy 0.9500\n",
      "Epoch [133][200]\t Batch [50][550]\t Training Loss 0.2973\t Accuracy 0.9182\n",
      "Epoch [133][200]\t Batch [100][550]\t Training Loss 0.3081\t Accuracy 0.9166\n",
      "Epoch [133][200]\t Batch [150][550]\t Training Loss 0.3219\t Accuracy 0.9115\n",
      "Epoch [133][200]\t Batch [200][550]\t Training Loss 0.3188\t Accuracy 0.9123\n",
      "Epoch [133][200]\t Batch [250][550]\t Training Loss 0.3190\t Accuracy 0.9124\n",
      "Epoch [133][200]\t Batch [300][550]\t Training Loss 0.3200\t Accuracy 0.9126\n",
      "Epoch [133][200]\t Batch [350][550]\t Training Loss 0.3214\t Accuracy 0.9112\n",
      "Epoch [133][200]\t Batch [400][550]\t Training Loss 0.3220\t Accuracy 0.9101\n",
      "Epoch [133][200]\t Batch [450][550]\t Training Loss 0.3222\t Accuracy 0.9100\n",
      "Epoch [133][200]\t Batch [500][550]\t Training Loss 0.3231\t Accuracy 0.9095\n",
      "\n",
      "Epoch [133]\t Average training loss 0.3236\t Average training accuracy 0.9092\n",
      "Epoch [133]\t Average validation loss 0.2564\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [134][200]\t Batch [0][550]\t Training Loss 0.2723\t Accuracy 0.9500\n",
      "Epoch [134][200]\t Batch [50][550]\t Training Loss 0.2968\t Accuracy 0.9184\n",
      "Epoch [134][200]\t Batch [100][550]\t Training Loss 0.3076\t Accuracy 0.9170\n",
      "Epoch [134][200]\t Batch [150][550]\t Training Loss 0.3215\t Accuracy 0.9119\n",
      "Epoch [134][200]\t Batch [200][550]\t Training Loss 0.3184\t Accuracy 0.9126\n",
      "Epoch [134][200]\t Batch [250][550]\t Training Loss 0.3185\t Accuracy 0.9126\n",
      "Epoch [134][200]\t Batch [300][550]\t Training Loss 0.3195\t Accuracy 0.9128\n",
      "Epoch [134][200]\t Batch [350][550]\t Training Loss 0.3209\t Accuracy 0.9115\n",
      "Epoch [134][200]\t Batch [400][550]\t Training Loss 0.3215\t Accuracy 0.9104\n",
      "Epoch [134][200]\t Batch [450][550]\t Training Loss 0.3217\t Accuracy 0.9103\n",
      "Epoch [134][200]\t Batch [500][550]\t Training Loss 0.3226\t Accuracy 0.9099\n",
      "\n",
      "Epoch [134]\t Average training loss 0.3232\t Average training accuracy 0.9095\n",
      "Epoch [134]\t Average validation loss 0.2561\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [135][200]\t Batch [0][550]\t Training Loss 0.2717\t Accuracy 0.9500\n",
      "Epoch [135][200]\t Batch [50][550]\t Training Loss 0.2964\t Accuracy 0.9184\n",
      "Epoch [135][200]\t Batch [100][550]\t Training Loss 0.3071\t Accuracy 0.9170\n",
      "Epoch [135][200]\t Batch [150][550]\t Training Loss 0.3210\t Accuracy 0.9119\n",
      "Epoch [135][200]\t Batch [200][550]\t Training Loss 0.3179\t Accuracy 0.9126\n",
      "Epoch [135][200]\t Batch [250][550]\t Training Loss 0.3181\t Accuracy 0.9127\n",
      "Epoch [135][200]\t Batch [300][550]\t Training Loss 0.3191\t Accuracy 0.9129\n",
      "Epoch [135][200]\t Batch [350][550]\t Training Loss 0.3204\t Accuracy 0.9115\n",
      "Epoch [135][200]\t Batch [400][550]\t Training Loss 0.3211\t Accuracy 0.9105\n",
      "Epoch [135][200]\t Batch [450][550]\t Training Loss 0.3213\t Accuracy 0.9104\n",
      "Epoch [135][200]\t Batch [500][550]\t Training Loss 0.3221\t Accuracy 0.9100\n",
      "\n",
      "Epoch [135]\t Average training loss 0.3227\t Average training accuracy 0.9097\n",
      "Epoch [135]\t Average validation loss 0.2557\t Average validation accuracy 0.9302\n",
      "\n",
      "Epoch [136][200]\t Batch [0][550]\t Training Loss 0.2712\t Accuracy 0.9500\n",
      "Epoch [136][200]\t Batch [50][550]\t Training Loss 0.2959\t Accuracy 0.9186\n",
      "Epoch [136][200]\t Batch [100][550]\t Training Loss 0.3066\t Accuracy 0.9172\n",
      "Epoch [136][200]\t Batch [150][550]\t Training Loss 0.3205\t Accuracy 0.9121\n",
      "Epoch [136][200]\t Batch [200][550]\t Training Loss 0.3174\t Accuracy 0.9127\n",
      "Epoch [136][200]\t Batch [250][550]\t Training Loss 0.3176\t Accuracy 0.9128\n",
      "Epoch [136][200]\t Batch [300][550]\t Training Loss 0.3186\t Accuracy 0.9131\n",
      "Epoch [136][200]\t Batch [350][550]\t Training Loss 0.3200\t Accuracy 0.9117\n",
      "Epoch [136][200]\t Batch [400][550]\t Training Loss 0.3206\t Accuracy 0.9107\n",
      "Epoch [136][200]\t Batch [450][550]\t Training Loss 0.3208\t Accuracy 0.9105\n",
      "Epoch [136][200]\t Batch [500][550]\t Training Loss 0.3217\t Accuracy 0.9101\n",
      "\n",
      "Epoch [136]\t Average training loss 0.3222\t Average training accuracy 0.9098\n",
      "Epoch [136]\t Average validation loss 0.2554\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [137][200]\t Batch [0][550]\t Training Loss 0.2707\t Accuracy 0.9500\n",
      "Epoch [137][200]\t Batch [50][550]\t Training Loss 0.2954\t Accuracy 0.9188\n",
      "Epoch [137][200]\t Batch [100][550]\t Training Loss 0.3062\t Accuracy 0.9173\n",
      "Epoch [137][200]\t Batch [150][550]\t Training Loss 0.3201\t Accuracy 0.9122\n",
      "Epoch [137][200]\t Batch [200][550]\t Training Loss 0.3170\t Accuracy 0.9128\n",
      "Epoch [137][200]\t Batch [250][550]\t Training Loss 0.3172\t Accuracy 0.9129\n",
      "Epoch [137][200]\t Batch [300][550]\t Training Loss 0.3181\t Accuracy 0.9132\n",
      "Epoch [137][200]\t Batch [350][550]\t Training Loss 0.3195\t Accuracy 0.9118\n",
      "Epoch [137][200]\t Batch [400][550]\t Training Loss 0.3201\t Accuracy 0.9108\n",
      "Epoch [137][200]\t Batch [450][550]\t Training Loss 0.3203\t Accuracy 0.9107\n",
      "Epoch [137][200]\t Batch [500][550]\t Training Loss 0.3212\t Accuracy 0.9102\n",
      "\n",
      "Epoch [137]\t Average training loss 0.3218\t Average training accuracy 0.9099\n",
      "Epoch [137]\t Average validation loss 0.2550\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [138][200]\t Batch [0][550]\t Training Loss 0.2702\t Accuracy 0.9500\n",
      "Epoch [138][200]\t Batch [50][550]\t Training Loss 0.2949\t Accuracy 0.9188\n",
      "Epoch [138][200]\t Batch [100][550]\t Training Loss 0.3057\t Accuracy 0.9174\n",
      "Epoch [138][200]\t Batch [150][550]\t Training Loss 0.3196\t Accuracy 0.9122\n",
      "Epoch [138][200]\t Batch [200][550]\t Training Loss 0.3165\t Accuracy 0.9128\n",
      "Epoch [138][200]\t Batch [250][550]\t Training Loss 0.3167\t Accuracy 0.9129\n",
      "Epoch [138][200]\t Batch [300][550]\t Training Loss 0.3176\t Accuracy 0.9132\n",
      "Epoch [138][200]\t Batch [350][550]\t Training Loss 0.3190\t Accuracy 0.9118\n",
      "Epoch [138][200]\t Batch [400][550]\t Training Loss 0.3197\t Accuracy 0.9109\n",
      "Epoch [138][200]\t Batch [450][550]\t Training Loss 0.3199\t Accuracy 0.9108\n",
      "Epoch [138][200]\t Batch [500][550]\t Training Loss 0.3208\t Accuracy 0.9103\n",
      "\n",
      "Epoch [138]\t Average training loss 0.3213\t Average training accuracy 0.9100\n",
      "Epoch [138]\t Average validation loss 0.2547\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [139][200]\t Batch [0][550]\t Training Loss 0.2696\t Accuracy 0.9500\n",
      "Epoch [139][200]\t Batch [50][550]\t Training Loss 0.2945\t Accuracy 0.9190\n",
      "Epoch [139][200]\t Batch [100][550]\t Training Loss 0.3053\t Accuracy 0.9175\n",
      "Epoch [139][200]\t Batch [150][550]\t Training Loss 0.3191\t Accuracy 0.9123\n",
      "Epoch [139][200]\t Batch [200][550]\t Training Loss 0.3160\t Accuracy 0.9129\n",
      "Epoch [139][200]\t Batch [250][550]\t Training Loss 0.3162\t Accuracy 0.9130\n",
      "Epoch [139][200]\t Batch [300][550]\t Training Loss 0.3172\t Accuracy 0.9133\n",
      "Epoch [139][200]\t Batch [350][550]\t Training Loss 0.3185\t Accuracy 0.9119\n",
      "Epoch [139][200]\t Batch [400][550]\t Training Loss 0.3192\t Accuracy 0.9110\n",
      "Epoch [139][200]\t Batch [450][550]\t Training Loss 0.3194\t Accuracy 0.9109\n",
      "Epoch [139][200]\t Batch [500][550]\t Training Loss 0.3203\t Accuracy 0.9104\n",
      "\n",
      "Epoch [139]\t Average training loss 0.3208\t Average training accuracy 0.9102\n",
      "Epoch [139]\t Average validation loss 0.2543\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [140][200]\t Batch [0][550]\t Training Loss 0.2691\t Accuracy 0.9500\n",
      "Epoch [140][200]\t Batch [50][550]\t Training Loss 0.2940\t Accuracy 0.9190\n",
      "Epoch [140][200]\t Batch [100][550]\t Training Loss 0.3048\t Accuracy 0.9176\n",
      "Epoch [140][200]\t Batch [150][550]\t Training Loss 0.3187\t Accuracy 0.9123\n",
      "Epoch [140][200]\t Batch [200][550]\t Training Loss 0.3156\t Accuracy 0.9129\n",
      "Epoch [140][200]\t Batch [250][550]\t Training Loss 0.3158\t Accuracy 0.9130\n",
      "Epoch [140][200]\t Batch [300][550]\t Training Loss 0.3167\t Accuracy 0.9134\n",
      "Epoch [140][200]\t Batch [350][550]\t Training Loss 0.3181\t Accuracy 0.9120\n",
      "Epoch [140][200]\t Batch [400][550]\t Training Loss 0.3187\t Accuracy 0.9111\n",
      "Epoch [140][200]\t Batch [450][550]\t Training Loss 0.3189\t Accuracy 0.9110\n",
      "Epoch [140][200]\t Batch [500][550]\t Training Loss 0.3198\t Accuracy 0.9106\n",
      "\n",
      "Epoch [140]\t Average training loss 0.3204\t Average training accuracy 0.9103\n",
      "Epoch [140]\t Average validation loss 0.2540\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [141][200]\t Batch [0][550]\t Training Loss 0.2686\t Accuracy 0.9500\n",
      "Epoch [141][200]\t Batch [50][550]\t Training Loss 0.2935\t Accuracy 0.9192\n",
      "Epoch [141][200]\t Batch [100][550]\t Training Loss 0.3043\t Accuracy 0.9177\n",
      "Epoch [141][200]\t Batch [150][550]\t Training Loss 0.3182\t Accuracy 0.9125\n",
      "Epoch [141][200]\t Batch [200][550]\t Training Loss 0.3151\t Accuracy 0.9131\n",
      "Epoch [141][200]\t Batch [250][550]\t Training Loss 0.3153\t Accuracy 0.9132\n",
      "Epoch [141][200]\t Batch [300][550]\t Training Loss 0.3162\t Accuracy 0.9135\n",
      "Epoch [141][200]\t Batch [350][550]\t Training Loss 0.3176\t Accuracy 0.9121\n",
      "Epoch [141][200]\t Batch [400][550]\t Training Loss 0.3183\t Accuracy 0.9113\n",
      "Epoch [141][200]\t Batch [450][550]\t Training Loss 0.3185\t Accuracy 0.9111\n",
      "Epoch [141][200]\t Batch [500][550]\t Training Loss 0.3194\t Accuracy 0.9107\n",
      "\n",
      "Epoch [141]\t Average training loss 0.3199\t Average training accuracy 0.9105\n",
      "Epoch [141]\t Average validation loss 0.2536\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [142][200]\t Batch [0][550]\t Training Loss 0.2681\t Accuracy 0.9500\n",
      "Epoch [142][200]\t Batch [50][550]\t Training Loss 0.2930\t Accuracy 0.9192\n",
      "Epoch [142][200]\t Batch [100][550]\t Training Loss 0.3039\t Accuracy 0.9179\n",
      "Epoch [142][200]\t Batch [150][550]\t Training Loss 0.3177\t Accuracy 0.9126\n",
      "Epoch [142][200]\t Batch [200][550]\t Training Loss 0.3147\t Accuracy 0.9133\n",
      "Epoch [142][200]\t Batch [250][550]\t Training Loss 0.3149\t Accuracy 0.9133\n",
      "Epoch [142][200]\t Batch [300][550]\t Training Loss 0.3158\t Accuracy 0.9137\n",
      "Epoch [142][200]\t Batch [350][550]\t Training Loss 0.3172\t Accuracy 0.9123\n",
      "Epoch [142][200]\t Batch [400][550]\t Training Loss 0.3178\t Accuracy 0.9114\n",
      "Epoch [142][200]\t Batch [450][550]\t Training Loss 0.3180\t Accuracy 0.9112\n",
      "Epoch [142][200]\t Batch [500][550]\t Training Loss 0.3189\t Accuracy 0.9109\n",
      "\n",
      "Epoch [142]\t Average training loss 0.3195\t Average training accuracy 0.9106\n",
      "Epoch [142]\t Average validation loss 0.2533\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [143][200]\t Batch [0][550]\t Training Loss 0.2676\t Accuracy 0.9500\n",
      "Epoch [143][200]\t Batch [50][550]\t Training Loss 0.2926\t Accuracy 0.9192\n",
      "Epoch [143][200]\t Batch [100][550]\t Training Loss 0.3034\t Accuracy 0.9181\n",
      "Epoch [143][200]\t Batch [150][550]\t Training Loss 0.3173\t Accuracy 0.9127\n",
      "Epoch [143][200]\t Batch [200][550]\t Training Loss 0.3142\t Accuracy 0.9133\n",
      "Epoch [143][200]\t Batch [250][550]\t Training Loss 0.3144\t Accuracy 0.9135\n",
      "Epoch [143][200]\t Batch [300][550]\t Training Loss 0.3153\t Accuracy 0.9138\n",
      "Epoch [143][200]\t Batch [350][550]\t Training Loss 0.3167\t Accuracy 0.9124\n",
      "Epoch [143][200]\t Batch [400][550]\t Training Loss 0.3173\t Accuracy 0.9116\n",
      "Epoch [143][200]\t Batch [450][550]\t Training Loss 0.3176\t Accuracy 0.9114\n",
      "Epoch [143][200]\t Batch [500][550]\t Training Loss 0.3185\t Accuracy 0.9111\n",
      "\n",
      "Epoch [143]\t Average training loss 0.3190\t Average training accuracy 0.9108\n",
      "Epoch [143]\t Average validation loss 0.2530\t Average validation accuracy 0.9308\n",
      "\n",
      "Epoch [144][200]\t Batch [0][550]\t Training Loss 0.2671\t Accuracy 0.9500\n",
      "Epoch [144][200]\t Batch [50][550]\t Training Loss 0.2921\t Accuracy 0.9190\n",
      "Epoch [144][200]\t Batch [100][550]\t Training Loss 0.3030\t Accuracy 0.9181\n",
      "Epoch [144][200]\t Batch [150][550]\t Training Loss 0.3168\t Accuracy 0.9128\n",
      "Epoch [144][200]\t Batch [200][550]\t Training Loss 0.3138\t Accuracy 0.9134\n",
      "Epoch [144][200]\t Batch [250][550]\t Training Loss 0.3140\t Accuracy 0.9135\n",
      "Epoch [144][200]\t Batch [300][550]\t Training Loss 0.3149\t Accuracy 0.9138\n",
      "Epoch [144][200]\t Batch [350][550]\t Training Loss 0.3162\t Accuracy 0.9125\n",
      "Epoch [144][200]\t Batch [400][550]\t Training Loss 0.3169\t Accuracy 0.9116\n",
      "Epoch [144][200]\t Batch [450][550]\t Training Loss 0.3171\t Accuracy 0.9115\n",
      "Epoch [144][200]\t Batch [500][550]\t Training Loss 0.3180\t Accuracy 0.9111\n",
      "\n",
      "Epoch [144]\t Average training loss 0.3186\t Average training accuracy 0.9109\n",
      "Epoch [144]\t Average validation loss 0.2526\t Average validation accuracy 0.9310\n",
      "\n",
      "Epoch [145][200]\t Batch [0][550]\t Training Loss 0.2666\t Accuracy 0.9500\n",
      "Epoch [145][200]\t Batch [50][550]\t Training Loss 0.2917\t Accuracy 0.9192\n",
      "Epoch [145][200]\t Batch [100][550]\t Training Loss 0.3025\t Accuracy 0.9182\n",
      "Epoch [145][200]\t Batch [150][550]\t Training Loss 0.3164\t Accuracy 0.9129\n",
      "Epoch [145][200]\t Batch [200][550]\t Training Loss 0.3133\t Accuracy 0.9135\n",
      "Epoch [145][200]\t Batch [250][550]\t Training Loss 0.3135\t Accuracy 0.9135\n",
      "Epoch [145][200]\t Batch [300][550]\t Training Loss 0.3144\t Accuracy 0.9139\n",
      "Epoch [145][200]\t Batch [350][550]\t Training Loss 0.3158\t Accuracy 0.9126\n",
      "Epoch [145][200]\t Batch [400][550]\t Training Loss 0.3164\t Accuracy 0.9118\n",
      "Epoch [145][200]\t Batch [450][550]\t Training Loss 0.3167\t Accuracy 0.9117\n",
      "Epoch [145][200]\t Batch [500][550]\t Training Loss 0.3176\t Accuracy 0.9113\n",
      "\n",
      "Epoch [145]\t Average training loss 0.3181\t Average training accuracy 0.9110\n",
      "Epoch [145]\t Average validation loss 0.2523\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [146][200]\t Batch [0][550]\t Training Loss 0.2661\t Accuracy 0.9500\n",
      "Epoch [146][200]\t Batch [50][550]\t Training Loss 0.2912\t Accuracy 0.9198\n",
      "Epoch [146][200]\t Batch [100][550]\t Training Loss 0.3021\t Accuracy 0.9186\n",
      "Epoch [146][200]\t Batch [150][550]\t Training Loss 0.3159\t Accuracy 0.9132\n",
      "Epoch [146][200]\t Batch [200][550]\t Training Loss 0.3128\t Accuracy 0.9137\n",
      "Epoch [146][200]\t Batch [250][550]\t Training Loss 0.3131\t Accuracy 0.9137\n",
      "Epoch [146][200]\t Batch [300][550]\t Training Loss 0.3140\t Accuracy 0.9141\n",
      "Epoch [146][200]\t Batch [350][550]\t Training Loss 0.3153\t Accuracy 0.9128\n",
      "Epoch [146][200]\t Batch [400][550]\t Training Loss 0.3160\t Accuracy 0.9121\n",
      "Epoch [146][200]\t Batch [450][550]\t Training Loss 0.3162\t Accuracy 0.9119\n",
      "Epoch [146][200]\t Batch [500][550]\t Training Loss 0.3171\t Accuracy 0.9115\n",
      "\n",
      "Epoch [146]\t Average training loss 0.3177\t Average training accuracy 0.9112\n",
      "Epoch [146]\t Average validation loss 0.2519\t Average validation accuracy 0.9316\n",
      "\n",
      "Epoch [147][200]\t Batch [0][550]\t Training Loss 0.2656\t Accuracy 0.9500\n",
      "Epoch [147][200]\t Batch [50][550]\t Training Loss 0.2907\t Accuracy 0.9198\n",
      "Epoch [147][200]\t Batch [100][550]\t Training Loss 0.3016\t Accuracy 0.9186\n",
      "Epoch [147][200]\t Batch [150][550]\t Training Loss 0.3155\t Accuracy 0.9132\n",
      "Epoch [147][200]\t Batch [200][550]\t Training Loss 0.3124\t Accuracy 0.9139\n",
      "Epoch [147][200]\t Batch [250][550]\t Training Loss 0.3127\t Accuracy 0.9138\n",
      "Epoch [147][200]\t Batch [300][550]\t Training Loss 0.3135\t Accuracy 0.9142\n",
      "Epoch [147][200]\t Batch [350][550]\t Training Loss 0.3149\t Accuracy 0.9129\n",
      "Epoch [147][200]\t Batch [400][550]\t Training Loss 0.3155\t Accuracy 0.9121\n",
      "Epoch [147][200]\t Batch [450][550]\t Training Loss 0.3158\t Accuracy 0.9119\n",
      "Epoch [147][200]\t Batch [500][550]\t Training Loss 0.3167\t Accuracy 0.9115\n",
      "\n",
      "Epoch [147]\t Average training loss 0.3172\t Average training accuracy 0.9112\n",
      "Epoch [147]\t Average validation loss 0.2516\t Average validation accuracy 0.9316\n",
      "\n",
      "Epoch [148][200]\t Batch [0][550]\t Training Loss 0.2651\t Accuracy 0.9500\n",
      "Epoch [148][200]\t Batch [50][550]\t Training Loss 0.2903\t Accuracy 0.9200\n",
      "Epoch [148][200]\t Batch [100][550]\t Training Loss 0.3012\t Accuracy 0.9187\n",
      "Epoch [148][200]\t Batch [150][550]\t Training Loss 0.3150\t Accuracy 0.9135\n",
      "Epoch [148][200]\t Batch [200][550]\t Training Loss 0.3120\t Accuracy 0.9141\n",
      "Epoch [148][200]\t Batch [250][550]\t Training Loss 0.3122\t Accuracy 0.9140\n",
      "Epoch [148][200]\t Batch [300][550]\t Training Loss 0.3130\t Accuracy 0.9143\n",
      "Epoch [148][200]\t Batch [350][550]\t Training Loss 0.3144\t Accuracy 0.9130\n",
      "Epoch [148][200]\t Batch [400][550]\t Training Loss 0.3151\t Accuracy 0.9123\n",
      "Epoch [148][200]\t Batch [450][550]\t Training Loss 0.3153\t Accuracy 0.9120\n",
      "Epoch [148][200]\t Batch [500][550]\t Training Loss 0.3162\t Accuracy 0.9116\n",
      "\n",
      "Epoch [148]\t Average training loss 0.3168\t Average training accuracy 0.9113\n",
      "Epoch [148]\t Average validation loss 0.2513\t Average validation accuracy 0.9316\n",
      "\n",
      "Epoch [149][200]\t Batch [0][550]\t Training Loss 0.2646\t Accuracy 0.9500\n",
      "Epoch [149][200]\t Batch [50][550]\t Training Loss 0.2898\t Accuracy 0.9202\n",
      "Epoch [149][200]\t Batch [100][550]\t Training Loss 0.3007\t Accuracy 0.9189\n",
      "Epoch [149][200]\t Batch [150][550]\t Training Loss 0.3146\t Accuracy 0.9138\n",
      "Epoch [149][200]\t Batch [200][550]\t Training Loss 0.3115\t Accuracy 0.9142\n",
      "Epoch [149][200]\t Batch [250][550]\t Training Loss 0.3118\t Accuracy 0.9141\n",
      "Epoch [149][200]\t Batch [300][550]\t Training Loss 0.3126\t Accuracy 0.9144\n",
      "Epoch [149][200]\t Batch [350][550]\t Training Loss 0.3140\t Accuracy 0.9131\n",
      "Epoch [149][200]\t Batch [400][550]\t Training Loss 0.3146\t Accuracy 0.9124\n",
      "Epoch [149][200]\t Batch [450][550]\t Training Loss 0.3149\t Accuracy 0.9121\n",
      "Epoch [149][200]\t Batch [500][550]\t Training Loss 0.3158\t Accuracy 0.9118\n",
      "\n",
      "Epoch [149]\t Average training loss 0.3163\t Average training accuracy 0.9115\n",
      "Epoch [149]\t Average validation loss 0.2510\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [150][200]\t Batch [0][550]\t Training Loss 0.2642\t Accuracy 0.9500\n",
      "Epoch [150][200]\t Batch [50][550]\t Training Loss 0.2894\t Accuracy 0.9206\n",
      "Epoch [150][200]\t Batch [100][550]\t Training Loss 0.3003\t Accuracy 0.9191\n",
      "Epoch [150][200]\t Batch [150][550]\t Training Loss 0.3141\t Accuracy 0.9139\n",
      "Epoch [150][200]\t Batch [200][550]\t Training Loss 0.3111\t Accuracy 0.9143\n",
      "Epoch [150][200]\t Batch [250][550]\t Training Loss 0.3113\t Accuracy 0.9142\n",
      "Epoch [150][200]\t Batch [300][550]\t Training Loss 0.3122\t Accuracy 0.9146\n",
      "Epoch [150][200]\t Batch [350][550]\t Training Loss 0.3135\t Accuracy 0.9132\n",
      "Epoch [150][200]\t Batch [400][550]\t Training Loss 0.3142\t Accuracy 0.9125\n",
      "Epoch [150][200]\t Batch [450][550]\t Training Loss 0.3145\t Accuracy 0.9122\n",
      "Epoch [150][200]\t Batch [500][550]\t Training Loss 0.3154\t Accuracy 0.9119\n",
      "\n",
      "Epoch [150]\t Average training loss 0.3159\t Average training accuracy 0.9116\n",
      "Epoch [150]\t Average validation loss 0.2506\t Average validation accuracy 0.9320\n",
      "\n",
      "Epoch [151][200]\t Batch [0][550]\t Training Loss 0.2637\t Accuracy 0.9500\n",
      "Epoch [151][200]\t Batch [50][550]\t Training Loss 0.2889\t Accuracy 0.9208\n",
      "Epoch [151][200]\t Batch [100][550]\t Training Loss 0.2999\t Accuracy 0.9193\n",
      "Epoch [151][200]\t Batch [150][550]\t Training Loss 0.3137\t Accuracy 0.9141\n",
      "Epoch [151][200]\t Batch [200][550]\t Training Loss 0.3106\t Accuracy 0.9145\n",
      "Epoch [151][200]\t Batch [250][550]\t Training Loss 0.3109\t Accuracy 0.9145\n",
      "Epoch [151][200]\t Batch [300][550]\t Training Loss 0.3117\t Accuracy 0.9148\n",
      "Epoch [151][200]\t Batch [350][550]\t Training Loss 0.3131\t Accuracy 0.9135\n",
      "Epoch [151][200]\t Batch [400][550]\t Training Loss 0.3137\t Accuracy 0.9127\n",
      "Epoch [151][200]\t Batch [450][550]\t Training Loss 0.3140\t Accuracy 0.9124\n",
      "Epoch [151][200]\t Batch [500][550]\t Training Loss 0.3149\t Accuracy 0.9120\n",
      "\n",
      "Epoch [151]\t Average training loss 0.3155\t Average training accuracy 0.9117\n",
      "Epoch [151]\t Average validation loss 0.2503\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [152][200]\t Batch [0][550]\t Training Loss 0.2632\t Accuracy 0.9500\n",
      "Epoch [152][200]\t Batch [50][550]\t Training Loss 0.2885\t Accuracy 0.9212\n",
      "Epoch [152][200]\t Batch [100][550]\t Training Loss 0.2994\t Accuracy 0.9195\n",
      "Epoch [152][200]\t Batch [150][550]\t Training Loss 0.3132\t Accuracy 0.9144\n",
      "Epoch [152][200]\t Batch [200][550]\t Training Loss 0.3102\t Accuracy 0.9148\n",
      "Epoch [152][200]\t Batch [250][550]\t Training Loss 0.3105\t Accuracy 0.9146\n",
      "Epoch [152][200]\t Batch [300][550]\t Training Loss 0.3113\t Accuracy 0.9150\n",
      "Epoch [152][200]\t Batch [350][550]\t Training Loss 0.3127\t Accuracy 0.9136\n",
      "Epoch [152][200]\t Batch [400][550]\t Training Loss 0.3133\t Accuracy 0.9129\n",
      "Epoch [152][200]\t Batch [450][550]\t Training Loss 0.3136\t Accuracy 0.9125\n",
      "Epoch [152][200]\t Batch [500][550]\t Training Loss 0.3145\t Accuracy 0.9122\n",
      "\n",
      "Epoch [152]\t Average training loss 0.3150\t Average training accuracy 0.9119\n",
      "Epoch [152]\t Average validation loss 0.2500\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [153][200]\t Batch [0][550]\t Training Loss 0.2627\t Accuracy 0.9500\n",
      "Epoch [153][200]\t Batch [50][550]\t Training Loss 0.2880\t Accuracy 0.9214\n",
      "Epoch [153][200]\t Batch [100][550]\t Training Loss 0.2990\t Accuracy 0.9197\n",
      "Epoch [153][200]\t Batch [150][550]\t Training Loss 0.3128\t Accuracy 0.9145\n",
      "Epoch [153][200]\t Batch [200][550]\t Training Loss 0.3098\t Accuracy 0.9149\n",
      "Epoch [153][200]\t Batch [250][550]\t Training Loss 0.3101\t Accuracy 0.9147\n",
      "Epoch [153][200]\t Batch [300][550]\t Training Loss 0.3108\t Accuracy 0.9150\n",
      "Epoch [153][200]\t Batch [350][550]\t Training Loss 0.3122\t Accuracy 0.9137\n",
      "Epoch [153][200]\t Batch [400][550]\t Training Loss 0.3129\t Accuracy 0.9130\n",
      "Epoch [153][200]\t Batch [450][550]\t Training Loss 0.3131\t Accuracy 0.9127\n",
      "Epoch [153][200]\t Batch [500][550]\t Training Loss 0.3141\t Accuracy 0.9123\n",
      "\n",
      "Epoch [153]\t Average training loss 0.3146\t Average training accuracy 0.9120\n",
      "Epoch [153]\t Average validation loss 0.2497\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [154][200]\t Batch [0][550]\t Training Loss 0.2622\t Accuracy 0.9500\n",
      "Epoch [154][200]\t Batch [50][550]\t Training Loss 0.2876\t Accuracy 0.9216\n",
      "Epoch [154][200]\t Batch [100][550]\t Training Loss 0.2986\t Accuracy 0.9199\n",
      "Epoch [154][200]\t Batch [150][550]\t Training Loss 0.3124\t Accuracy 0.9147\n",
      "Epoch [154][200]\t Batch [200][550]\t Training Loss 0.3093\t Accuracy 0.9149\n",
      "Epoch [154][200]\t Batch [250][550]\t Training Loss 0.3096\t Accuracy 0.9148\n",
      "Epoch [154][200]\t Batch [300][550]\t Training Loss 0.3104\t Accuracy 0.9151\n",
      "Epoch [154][200]\t Batch [350][550]\t Training Loss 0.3118\t Accuracy 0.9138\n",
      "Epoch [154][200]\t Batch [400][550]\t Training Loss 0.3124\t Accuracy 0.9130\n",
      "Epoch [154][200]\t Batch [450][550]\t Training Loss 0.3127\t Accuracy 0.9128\n",
      "Epoch [154][200]\t Batch [500][550]\t Training Loss 0.3136\t Accuracy 0.9124\n",
      "\n",
      "Epoch [154]\t Average training loss 0.3142\t Average training accuracy 0.9121\n",
      "Epoch [154]\t Average validation loss 0.2493\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [155][200]\t Batch [0][550]\t Training Loss 0.2617\t Accuracy 0.9500\n",
      "Epoch [155][200]\t Batch [50][550]\t Training Loss 0.2872\t Accuracy 0.9216\n",
      "Epoch [155][200]\t Batch [100][550]\t Training Loss 0.2981\t Accuracy 0.9199\n",
      "Epoch [155][200]\t Batch [150][550]\t Training Loss 0.3119\t Accuracy 0.9148\n",
      "Epoch [155][200]\t Batch [200][550]\t Training Loss 0.3089\t Accuracy 0.9150\n",
      "Epoch [155][200]\t Batch [250][550]\t Training Loss 0.3092\t Accuracy 0.9149\n",
      "Epoch [155][200]\t Batch [300][550]\t Training Loss 0.3100\t Accuracy 0.9152\n",
      "Epoch [155][200]\t Batch [350][550]\t Training Loss 0.3114\t Accuracy 0.9138\n",
      "Epoch [155][200]\t Batch [400][550]\t Training Loss 0.3120\t Accuracy 0.9131\n",
      "Epoch [155][200]\t Batch [450][550]\t Training Loss 0.3123\t Accuracy 0.9129\n",
      "Epoch [155][200]\t Batch [500][550]\t Training Loss 0.3132\t Accuracy 0.9125\n",
      "\n",
      "Epoch [155]\t Average training loss 0.3137\t Average training accuracy 0.9122\n",
      "Epoch [155]\t Average validation loss 0.2490\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [156][200]\t Batch [0][550]\t Training Loss 0.2612\t Accuracy 0.9500\n",
      "Epoch [156][200]\t Batch [50][550]\t Training Loss 0.2867\t Accuracy 0.9216\n",
      "Epoch [156][200]\t Batch [100][550]\t Training Loss 0.2977\t Accuracy 0.9200\n",
      "Epoch [156][200]\t Batch [150][550]\t Training Loss 0.3115\t Accuracy 0.9149\n",
      "Epoch [156][200]\t Batch [200][550]\t Training Loss 0.3085\t Accuracy 0.9151\n",
      "Epoch [156][200]\t Batch [250][550]\t Training Loss 0.3088\t Accuracy 0.9149\n",
      "Epoch [156][200]\t Batch [300][550]\t Training Loss 0.3095\t Accuracy 0.9152\n",
      "Epoch [156][200]\t Batch [350][550]\t Training Loss 0.3109\t Accuracy 0.9139\n",
      "Epoch [156][200]\t Batch [400][550]\t Training Loss 0.3116\t Accuracy 0.9132\n",
      "Epoch [156][200]\t Batch [450][550]\t Training Loss 0.3119\t Accuracy 0.9130\n",
      "Epoch [156][200]\t Batch [500][550]\t Training Loss 0.3128\t Accuracy 0.9126\n",
      "\n",
      "Epoch [156]\t Average training loss 0.3133\t Average training accuracy 0.9123\n",
      "Epoch [156]\t Average validation loss 0.2487\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [157][200]\t Batch [0][550]\t Training Loss 0.2608\t Accuracy 0.9500\n",
      "Epoch [157][200]\t Batch [50][550]\t Training Loss 0.2863\t Accuracy 0.9220\n",
      "Epoch [157][200]\t Batch [100][550]\t Training Loss 0.2973\t Accuracy 0.9202\n",
      "Epoch [157][200]\t Batch [150][550]\t Training Loss 0.3111\t Accuracy 0.9151\n",
      "Epoch [157][200]\t Batch [200][550]\t Training Loss 0.3080\t Accuracy 0.9152\n",
      "Epoch [157][200]\t Batch [250][550]\t Training Loss 0.3084\t Accuracy 0.9150\n",
      "Epoch [157][200]\t Batch [300][550]\t Training Loss 0.3091\t Accuracy 0.9153\n",
      "Epoch [157][200]\t Batch [350][550]\t Training Loss 0.3105\t Accuracy 0.9140\n",
      "Epoch [157][200]\t Batch [400][550]\t Training Loss 0.3111\t Accuracy 0.9133\n",
      "Epoch [157][200]\t Batch [450][550]\t Training Loss 0.3114\t Accuracy 0.9132\n",
      "Epoch [157][200]\t Batch [500][550]\t Training Loss 0.3123\t Accuracy 0.9127\n",
      "\n",
      "Epoch [157]\t Average training loss 0.3129\t Average training accuracy 0.9125\n",
      "Epoch [157]\t Average validation loss 0.2484\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [158][200]\t Batch [0][550]\t Training Loss 0.2603\t Accuracy 0.9500\n",
      "Epoch [158][200]\t Batch [50][550]\t Training Loss 0.2858\t Accuracy 0.9220\n",
      "Epoch [158][200]\t Batch [100][550]\t Training Loss 0.2969\t Accuracy 0.9201\n",
      "Epoch [158][200]\t Batch [150][550]\t Training Loss 0.3106\t Accuracy 0.9151\n",
      "Epoch [158][200]\t Batch [200][550]\t Training Loss 0.3076\t Accuracy 0.9152\n",
      "Epoch [158][200]\t Batch [250][550]\t Training Loss 0.3079\t Accuracy 0.9150\n",
      "Epoch [158][200]\t Batch [300][550]\t Training Loss 0.3087\t Accuracy 0.9154\n",
      "Epoch [158][200]\t Batch [350][550]\t Training Loss 0.3101\t Accuracy 0.9141\n",
      "Epoch [158][200]\t Batch [400][550]\t Training Loss 0.3107\t Accuracy 0.9134\n",
      "Epoch [158][200]\t Batch [450][550]\t Training Loss 0.3110\t Accuracy 0.9133\n",
      "Epoch [158][200]\t Batch [500][550]\t Training Loss 0.3119\t Accuracy 0.9128\n",
      "\n",
      "Epoch [158]\t Average training loss 0.3125\t Average training accuracy 0.9126\n",
      "Epoch [158]\t Average validation loss 0.2481\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [159][200]\t Batch [0][550]\t Training Loss 0.2598\t Accuracy 0.9500\n",
      "Epoch [159][200]\t Batch [50][550]\t Training Loss 0.2854\t Accuracy 0.9220\n",
      "Epoch [159][200]\t Batch [100][550]\t Training Loss 0.2964\t Accuracy 0.9200\n",
      "Epoch [159][200]\t Batch [150][550]\t Training Loss 0.3102\t Accuracy 0.9150\n",
      "Epoch [159][200]\t Batch [200][550]\t Training Loss 0.3072\t Accuracy 0.9152\n",
      "Epoch [159][200]\t Batch [250][550]\t Training Loss 0.3075\t Accuracy 0.9150\n",
      "Epoch [159][200]\t Batch [300][550]\t Training Loss 0.3082\t Accuracy 0.9154\n",
      "Epoch [159][200]\t Batch [350][550]\t Training Loss 0.3096\t Accuracy 0.9141\n",
      "Epoch [159][200]\t Batch [400][550]\t Training Loss 0.3103\t Accuracy 0.9135\n",
      "Epoch [159][200]\t Batch [450][550]\t Training Loss 0.3106\t Accuracy 0.9133\n",
      "Epoch [159][200]\t Batch [500][550]\t Training Loss 0.3115\t Accuracy 0.9128\n",
      "\n",
      "Epoch [159]\t Average training loss 0.3120\t Average training accuracy 0.9126\n",
      "Epoch [159]\t Average validation loss 0.2477\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [160][200]\t Batch [0][550]\t Training Loss 0.2593\t Accuracy 0.9500\n",
      "Epoch [160][200]\t Batch [50][550]\t Training Loss 0.2850\t Accuracy 0.9218\n",
      "Epoch [160][200]\t Batch [100][550]\t Training Loss 0.2960\t Accuracy 0.9200\n",
      "Epoch [160][200]\t Batch [150][550]\t Training Loss 0.3098\t Accuracy 0.9152\n",
      "Epoch [160][200]\t Batch [200][550]\t Training Loss 0.3068\t Accuracy 0.9153\n",
      "Epoch [160][200]\t Batch [250][550]\t Training Loss 0.3071\t Accuracy 0.9151\n",
      "Epoch [160][200]\t Batch [300][550]\t Training Loss 0.3078\t Accuracy 0.9156\n",
      "Epoch [160][200]\t Batch [350][550]\t Training Loss 0.3092\t Accuracy 0.9143\n",
      "Epoch [160][200]\t Batch [400][550]\t Training Loss 0.3099\t Accuracy 0.9137\n",
      "Epoch [160][200]\t Batch [450][550]\t Training Loss 0.3102\t Accuracy 0.9135\n",
      "Epoch [160][200]\t Batch [500][550]\t Training Loss 0.3111\t Accuracy 0.9130\n",
      "\n",
      "Epoch [160]\t Average training loss 0.3116\t Average training accuracy 0.9127\n",
      "Epoch [160]\t Average validation loss 0.2474\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [161][200]\t Batch [0][550]\t Training Loss 0.2589\t Accuracy 0.9500\n",
      "Epoch [161][200]\t Batch [50][550]\t Training Loss 0.2846\t Accuracy 0.9218\n",
      "Epoch [161][200]\t Batch [100][550]\t Training Loss 0.2956\t Accuracy 0.9201\n",
      "Epoch [161][200]\t Batch [150][550]\t Training Loss 0.3094\t Accuracy 0.9154\n",
      "Epoch [161][200]\t Batch [200][550]\t Training Loss 0.3063\t Accuracy 0.9154\n",
      "Epoch [161][200]\t Batch [250][550]\t Training Loss 0.3067\t Accuracy 0.9152\n",
      "Epoch [161][200]\t Batch [300][550]\t Training Loss 0.3074\t Accuracy 0.9157\n",
      "Epoch [161][200]\t Batch [350][550]\t Training Loss 0.3088\t Accuracy 0.9143\n",
      "Epoch [161][200]\t Batch [400][550]\t Training Loss 0.3094\t Accuracy 0.9137\n",
      "Epoch [161][200]\t Batch [450][550]\t Training Loss 0.3097\t Accuracy 0.9135\n",
      "Epoch [161][200]\t Batch [500][550]\t Training Loss 0.3107\t Accuracy 0.9130\n",
      "\n",
      "Epoch [161]\t Average training loss 0.3112\t Average training accuracy 0.9128\n",
      "Epoch [161]\t Average validation loss 0.2471\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [162][200]\t Batch [0][550]\t Training Loss 0.2584\t Accuracy 0.9500\n",
      "Epoch [162][200]\t Batch [50][550]\t Training Loss 0.2841\t Accuracy 0.9218\n",
      "Epoch [162][200]\t Batch [100][550]\t Training Loss 0.2952\t Accuracy 0.9202\n",
      "Epoch [162][200]\t Batch [150][550]\t Training Loss 0.3089\t Accuracy 0.9156\n",
      "Epoch [162][200]\t Batch [200][550]\t Training Loss 0.3059\t Accuracy 0.9156\n",
      "Epoch [162][200]\t Batch [250][550]\t Training Loss 0.3063\t Accuracy 0.9154\n",
      "Epoch [162][200]\t Batch [300][550]\t Training Loss 0.3070\t Accuracy 0.9158\n",
      "Epoch [162][200]\t Batch [350][550]\t Training Loss 0.3084\t Accuracy 0.9145\n",
      "Epoch [162][200]\t Batch [400][550]\t Training Loss 0.3090\t Accuracy 0.9139\n",
      "Epoch [162][200]\t Batch [450][550]\t Training Loss 0.3093\t Accuracy 0.9137\n",
      "Epoch [162][200]\t Batch [500][550]\t Training Loss 0.3103\t Accuracy 0.9132\n",
      "\n",
      "Epoch [162]\t Average training loss 0.3108\t Average training accuracy 0.9129\n",
      "Epoch [162]\t Average validation loss 0.2468\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [163][200]\t Batch [0][550]\t Training Loss 0.2579\t Accuracy 0.9500\n",
      "Epoch [163][200]\t Batch [50][550]\t Training Loss 0.2837\t Accuracy 0.9222\n",
      "Epoch [163][200]\t Batch [100][550]\t Training Loss 0.2948\t Accuracy 0.9205\n",
      "Epoch [163][200]\t Batch [150][550]\t Training Loss 0.3085\t Accuracy 0.9158\n",
      "Epoch [163][200]\t Batch [200][550]\t Training Loss 0.3055\t Accuracy 0.9157\n",
      "Epoch [163][200]\t Batch [250][550]\t Training Loss 0.3059\t Accuracy 0.9155\n",
      "Epoch [163][200]\t Batch [300][550]\t Training Loss 0.3065\t Accuracy 0.9159\n",
      "Epoch [163][200]\t Batch [350][550]\t Training Loss 0.3080\t Accuracy 0.9146\n",
      "Epoch [163][200]\t Batch [400][550]\t Training Loss 0.3086\t Accuracy 0.9140\n",
      "Epoch [163][200]\t Batch [450][550]\t Training Loss 0.3089\t Accuracy 0.9138\n",
      "Epoch [163][200]\t Batch [500][550]\t Training Loss 0.3098\t Accuracy 0.9132\n",
      "\n",
      "Epoch [163]\t Average training loss 0.3104\t Average training accuracy 0.9130\n",
      "Epoch [163]\t Average validation loss 0.2465\t Average validation accuracy 0.9328\n",
      "\n",
      "Epoch [164][200]\t Batch [0][550]\t Training Loss 0.2575\t Accuracy 0.9500\n",
      "Epoch [164][200]\t Batch [50][550]\t Training Loss 0.2833\t Accuracy 0.9224\n",
      "Epoch [164][200]\t Batch [100][550]\t Training Loss 0.2944\t Accuracy 0.9206\n",
      "Epoch [164][200]\t Batch [150][550]\t Training Loss 0.3081\t Accuracy 0.9160\n",
      "Epoch [164][200]\t Batch [200][550]\t Training Loss 0.3051\t Accuracy 0.9158\n",
      "Epoch [164][200]\t Batch [250][550]\t Training Loss 0.3055\t Accuracy 0.9156\n",
      "Epoch [164][200]\t Batch [300][550]\t Training Loss 0.3061\t Accuracy 0.9160\n",
      "Epoch [164][200]\t Batch [350][550]\t Training Loss 0.3075\t Accuracy 0.9147\n",
      "Epoch [164][200]\t Batch [400][550]\t Training Loss 0.3082\t Accuracy 0.9141\n",
      "Epoch [164][200]\t Batch [450][550]\t Training Loss 0.3085\t Accuracy 0.9139\n",
      "Epoch [164][200]\t Batch [500][550]\t Training Loss 0.3094\t Accuracy 0.9134\n",
      "\n",
      "Epoch [164]\t Average training loss 0.3100\t Average training accuracy 0.9131\n",
      "Epoch [164]\t Average validation loss 0.2462\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [165][200]\t Batch [0][550]\t Training Loss 0.2570\t Accuracy 0.9500\n",
      "Epoch [165][200]\t Batch [50][550]\t Training Loss 0.2829\t Accuracy 0.9224\n",
      "Epoch [165][200]\t Batch [100][550]\t Training Loss 0.2939\t Accuracy 0.9206\n",
      "Epoch [165][200]\t Batch [150][550]\t Training Loss 0.3077\t Accuracy 0.9160\n",
      "Epoch [165][200]\t Batch [200][550]\t Training Loss 0.3047\t Accuracy 0.9159\n",
      "Epoch [165][200]\t Batch [250][550]\t Training Loss 0.3051\t Accuracy 0.9157\n",
      "Epoch [165][200]\t Batch [300][550]\t Training Loss 0.3057\t Accuracy 0.9161\n",
      "Epoch [165][200]\t Batch [350][550]\t Training Loss 0.3071\t Accuracy 0.9147\n",
      "Epoch [165][200]\t Batch [400][550]\t Training Loss 0.3078\t Accuracy 0.9142\n",
      "Epoch [165][200]\t Batch [450][550]\t Training Loss 0.3081\t Accuracy 0.9139\n",
      "Epoch [165][200]\t Batch [500][550]\t Training Loss 0.3090\t Accuracy 0.9134\n",
      "\n",
      "Epoch [165]\t Average training loss 0.3096\t Average training accuracy 0.9132\n",
      "Epoch [165]\t Average validation loss 0.2459\t Average validation accuracy 0.9332\n",
      "\n",
      "Epoch [166][200]\t Batch [0][550]\t Training Loss 0.2566\t Accuracy 0.9500\n",
      "Epoch [166][200]\t Batch [50][550]\t Training Loss 0.2824\t Accuracy 0.9225\n",
      "Epoch [166][200]\t Batch [100][550]\t Training Loss 0.2935\t Accuracy 0.9207\n",
      "Epoch [166][200]\t Batch [150][550]\t Training Loss 0.3073\t Accuracy 0.9160\n",
      "Epoch [166][200]\t Batch [200][550]\t Training Loss 0.3043\t Accuracy 0.9159\n",
      "Epoch [166][200]\t Batch [250][550]\t Training Loss 0.3047\t Accuracy 0.9157\n",
      "Epoch [166][200]\t Batch [300][550]\t Training Loss 0.3053\t Accuracy 0.9161\n",
      "Epoch [166][200]\t Batch [350][550]\t Training Loss 0.3067\t Accuracy 0.9148\n",
      "Epoch [166][200]\t Batch [400][550]\t Training Loss 0.3074\t Accuracy 0.9143\n",
      "Epoch [166][200]\t Batch [450][550]\t Training Loss 0.3077\t Accuracy 0.9141\n",
      "Epoch [166][200]\t Batch [500][550]\t Training Loss 0.3086\t Accuracy 0.9135\n",
      "\n",
      "Epoch [166]\t Average training loss 0.3092\t Average training accuracy 0.9133\n",
      "Epoch [166]\t Average validation loss 0.2456\t Average validation accuracy 0.9336\n",
      "\n",
      "Epoch [167][200]\t Batch [0][550]\t Training Loss 0.2561\t Accuracy 0.9500\n",
      "Epoch [167][200]\t Batch [50][550]\t Training Loss 0.2820\t Accuracy 0.9225\n",
      "Epoch [167][200]\t Batch [100][550]\t Training Loss 0.2931\t Accuracy 0.9207\n",
      "Epoch [167][200]\t Batch [150][550]\t Training Loss 0.3068\t Accuracy 0.9161\n",
      "Epoch [167][200]\t Batch [200][550]\t Training Loss 0.3039\t Accuracy 0.9159\n",
      "Epoch [167][200]\t Batch [250][550]\t Training Loss 0.3043\t Accuracy 0.9158\n",
      "Epoch [167][200]\t Batch [300][550]\t Training Loss 0.3049\t Accuracy 0.9162\n",
      "Epoch [167][200]\t Batch [350][550]\t Training Loss 0.3063\t Accuracy 0.9149\n",
      "Epoch [167][200]\t Batch [400][550]\t Training Loss 0.3069\t Accuracy 0.9143\n",
      "Epoch [167][200]\t Batch [450][550]\t Training Loss 0.3073\t Accuracy 0.9141\n",
      "Epoch [167][200]\t Batch [500][550]\t Training Loss 0.3082\t Accuracy 0.9135\n",
      "\n",
      "Epoch [167]\t Average training loss 0.3088\t Average training accuracy 0.9133\n",
      "Epoch [167]\t Average validation loss 0.2453\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [168][200]\t Batch [0][550]\t Training Loss 0.2557\t Accuracy 0.9500\n",
      "Epoch [168][200]\t Batch [50][550]\t Training Loss 0.2816\t Accuracy 0.9225\n",
      "Epoch [168][200]\t Batch [100][550]\t Training Loss 0.2927\t Accuracy 0.9207\n",
      "Epoch [168][200]\t Batch [150][550]\t Training Loss 0.3064\t Accuracy 0.9162\n",
      "Epoch [168][200]\t Batch [200][550]\t Training Loss 0.3034\t Accuracy 0.9160\n",
      "Epoch [168][200]\t Batch [250][550]\t Training Loss 0.3039\t Accuracy 0.9159\n",
      "Epoch [168][200]\t Batch [300][550]\t Training Loss 0.3045\t Accuracy 0.9162\n",
      "Epoch [168][200]\t Batch [350][550]\t Training Loss 0.3059\t Accuracy 0.9150\n",
      "Epoch [168][200]\t Batch [400][550]\t Training Loss 0.3065\t Accuracy 0.9144\n",
      "Epoch [168][200]\t Batch [450][550]\t Training Loss 0.3069\t Accuracy 0.9142\n",
      "Epoch [168][200]\t Batch [500][550]\t Training Loss 0.3078\t Accuracy 0.9136\n",
      "\n",
      "Epoch [168]\t Average training loss 0.3084\t Average training accuracy 0.9134\n",
      "Epoch [168]\t Average validation loss 0.2450\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [169][200]\t Batch [0][550]\t Training Loss 0.2552\t Accuracy 0.9500\n",
      "Epoch [169][200]\t Batch [50][550]\t Training Loss 0.2812\t Accuracy 0.9225\n",
      "Epoch [169][200]\t Batch [100][550]\t Training Loss 0.2923\t Accuracy 0.9206\n",
      "Epoch [169][200]\t Batch [150][550]\t Training Loss 0.3060\t Accuracy 0.9162\n",
      "Epoch [169][200]\t Batch [200][550]\t Training Loss 0.3030\t Accuracy 0.9160\n",
      "Epoch [169][200]\t Batch [250][550]\t Training Loss 0.3035\t Accuracy 0.9159\n",
      "Epoch [169][200]\t Batch [300][550]\t Training Loss 0.3041\t Accuracy 0.9163\n",
      "Epoch [169][200]\t Batch [350][550]\t Training Loss 0.3055\t Accuracy 0.9150\n",
      "Epoch [169][200]\t Batch [400][550]\t Training Loss 0.3061\t Accuracy 0.9144\n",
      "Epoch [169][200]\t Batch [450][550]\t Training Loss 0.3065\t Accuracy 0.9142\n",
      "Epoch [169][200]\t Batch [500][550]\t Training Loss 0.3074\t Accuracy 0.9137\n",
      "\n",
      "Epoch [169]\t Average training loss 0.3080\t Average training accuracy 0.9135\n",
      "Epoch [169]\t Average validation loss 0.2447\t Average validation accuracy 0.9338\n",
      "\n",
      "Epoch [170][200]\t Batch [0][550]\t Training Loss 0.2548\t Accuracy 0.9500\n",
      "Epoch [170][200]\t Batch [50][550]\t Training Loss 0.2808\t Accuracy 0.9224\n",
      "Epoch [170][200]\t Batch [100][550]\t Training Loss 0.2919\t Accuracy 0.9207\n",
      "Epoch [170][200]\t Batch [150][550]\t Training Loss 0.3056\t Accuracy 0.9162\n",
      "Epoch [170][200]\t Batch [200][550]\t Training Loss 0.3026\t Accuracy 0.9161\n",
      "Epoch [170][200]\t Batch [250][550]\t Training Loss 0.3031\t Accuracy 0.9160\n",
      "Epoch [170][200]\t Batch [300][550]\t Training Loss 0.3037\t Accuracy 0.9164\n",
      "Epoch [170][200]\t Batch [350][550]\t Training Loss 0.3051\t Accuracy 0.9152\n",
      "Epoch [170][200]\t Batch [400][550]\t Training Loss 0.3057\t Accuracy 0.9146\n",
      "Epoch [170][200]\t Batch [450][550]\t Training Loss 0.3061\t Accuracy 0.9144\n",
      "Epoch [170][200]\t Batch [500][550]\t Training Loss 0.3070\t Accuracy 0.9139\n",
      "\n",
      "Epoch [170]\t Average training loss 0.3076\t Average training accuracy 0.9137\n",
      "Epoch [170]\t Average validation loss 0.2444\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [171][200]\t Batch [0][550]\t Training Loss 0.2543\t Accuracy 0.9500\n",
      "Epoch [171][200]\t Batch [50][550]\t Training Loss 0.2804\t Accuracy 0.9225\n",
      "Epoch [171][200]\t Batch [100][550]\t Training Loss 0.2915\t Accuracy 0.9207\n",
      "Epoch [171][200]\t Batch [150][550]\t Training Loss 0.3052\t Accuracy 0.9162\n",
      "Epoch [171][200]\t Batch [200][550]\t Training Loss 0.3022\t Accuracy 0.9162\n",
      "Epoch [171][200]\t Batch [250][550]\t Training Loss 0.3027\t Accuracy 0.9160\n",
      "Epoch [171][200]\t Batch [300][550]\t Training Loss 0.3032\t Accuracy 0.9164\n",
      "Epoch [171][200]\t Batch [350][550]\t Training Loss 0.3047\t Accuracy 0.9152\n",
      "Epoch [171][200]\t Batch [400][550]\t Training Loss 0.3053\t Accuracy 0.9147\n",
      "Epoch [171][200]\t Batch [450][550]\t Training Loss 0.3057\t Accuracy 0.9145\n",
      "Epoch [171][200]\t Batch [500][550]\t Training Loss 0.3066\t Accuracy 0.9140\n",
      "\n",
      "Epoch [171]\t Average training loss 0.3072\t Average training accuracy 0.9138\n",
      "Epoch [171]\t Average validation loss 0.2441\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [172][200]\t Batch [0][550]\t Training Loss 0.2539\t Accuracy 0.9500\n",
      "Epoch [172][200]\t Batch [50][550]\t Training Loss 0.2800\t Accuracy 0.9224\n",
      "Epoch [172][200]\t Batch [100][550]\t Training Loss 0.2911\t Accuracy 0.9206\n",
      "Epoch [172][200]\t Batch [150][550]\t Training Loss 0.3048\t Accuracy 0.9160\n",
      "Epoch [172][200]\t Batch [200][550]\t Training Loss 0.3018\t Accuracy 0.9161\n",
      "Epoch [172][200]\t Batch [250][550]\t Training Loss 0.3023\t Accuracy 0.9159\n",
      "Epoch [172][200]\t Batch [300][550]\t Training Loss 0.3028\t Accuracy 0.9164\n",
      "Epoch [172][200]\t Batch [350][550]\t Training Loss 0.3043\t Accuracy 0.9152\n",
      "Epoch [172][200]\t Batch [400][550]\t Training Loss 0.3049\t Accuracy 0.9147\n",
      "Epoch [172][200]\t Batch [450][550]\t Training Loss 0.3053\t Accuracy 0.9144\n",
      "Epoch [172][200]\t Batch [500][550]\t Training Loss 0.3063\t Accuracy 0.9139\n",
      "\n",
      "Epoch [172]\t Average training loss 0.3068\t Average training accuracy 0.9138\n",
      "Epoch [172]\t Average validation loss 0.2438\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [173][200]\t Batch [0][550]\t Training Loss 0.2534\t Accuracy 0.9500\n",
      "Epoch [173][200]\t Batch [50][550]\t Training Loss 0.2796\t Accuracy 0.9224\n",
      "Epoch [173][200]\t Batch [100][550]\t Training Loss 0.2907\t Accuracy 0.9206\n",
      "Epoch [173][200]\t Batch [150][550]\t Training Loss 0.3044\t Accuracy 0.9160\n",
      "Epoch [173][200]\t Batch [200][550]\t Training Loss 0.3014\t Accuracy 0.9161\n",
      "Epoch [173][200]\t Batch [250][550]\t Training Loss 0.3019\t Accuracy 0.9159\n",
      "Epoch [173][200]\t Batch [300][550]\t Training Loss 0.3024\t Accuracy 0.9164\n",
      "Epoch [173][200]\t Batch [350][550]\t Training Loss 0.3039\t Accuracy 0.9152\n",
      "Epoch [173][200]\t Batch [400][550]\t Training Loss 0.3045\t Accuracy 0.9146\n",
      "Epoch [173][200]\t Batch [450][550]\t Training Loss 0.3049\t Accuracy 0.9144\n",
      "Epoch [173][200]\t Batch [500][550]\t Training Loss 0.3059\t Accuracy 0.9139\n",
      "\n",
      "Epoch [173]\t Average training loss 0.3064\t Average training accuracy 0.9138\n",
      "Epoch [173]\t Average validation loss 0.2435\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [174][200]\t Batch [0][550]\t Training Loss 0.2530\t Accuracy 0.9500\n",
      "Epoch [174][200]\t Batch [50][550]\t Training Loss 0.2792\t Accuracy 0.9224\n",
      "Epoch [174][200]\t Batch [100][550]\t Training Loss 0.2904\t Accuracy 0.9207\n",
      "Epoch [174][200]\t Batch [150][550]\t Training Loss 0.3040\t Accuracy 0.9161\n",
      "Epoch [174][200]\t Batch [200][550]\t Training Loss 0.3010\t Accuracy 0.9161\n",
      "Epoch [174][200]\t Batch [250][550]\t Training Loss 0.3015\t Accuracy 0.9159\n",
      "Epoch [174][200]\t Batch [300][550]\t Training Loss 0.3020\t Accuracy 0.9165\n",
      "Epoch [174][200]\t Batch [350][550]\t Training Loss 0.3035\t Accuracy 0.9152\n",
      "Epoch [174][200]\t Batch [400][550]\t Training Loss 0.3041\t Accuracy 0.9147\n",
      "Epoch [174][200]\t Batch [450][550]\t Training Loss 0.3045\t Accuracy 0.9144\n",
      "Epoch [174][200]\t Batch [500][550]\t Training Loss 0.3055\t Accuracy 0.9139\n",
      "\n",
      "Epoch [174]\t Average training loss 0.3060\t Average training accuracy 0.9138\n",
      "Epoch [174]\t Average validation loss 0.2432\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [175][200]\t Batch [0][550]\t Training Loss 0.2525\t Accuracy 0.9500\n",
      "Epoch [175][200]\t Batch [50][550]\t Training Loss 0.2788\t Accuracy 0.9225\n",
      "Epoch [175][200]\t Batch [100][550]\t Training Loss 0.2900\t Accuracy 0.9208\n",
      "Epoch [175][200]\t Batch [150][550]\t Training Loss 0.3036\t Accuracy 0.9162\n",
      "Epoch [175][200]\t Batch [200][550]\t Training Loss 0.3006\t Accuracy 0.9162\n",
      "Epoch [175][200]\t Batch [250][550]\t Training Loss 0.3011\t Accuracy 0.9161\n",
      "Epoch [175][200]\t Batch [300][550]\t Training Loss 0.3017\t Accuracy 0.9166\n",
      "Epoch [175][200]\t Batch [350][550]\t Training Loss 0.3031\t Accuracy 0.9153\n",
      "Epoch [175][200]\t Batch [400][550]\t Training Loss 0.3037\t Accuracy 0.9148\n",
      "Epoch [175][200]\t Batch [450][550]\t Training Loss 0.3041\t Accuracy 0.9146\n",
      "Epoch [175][200]\t Batch [500][550]\t Training Loss 0.3051\t Accuracy 0.9141\n",
      "\n",
      "Epoch [175]\t Average training loss 0.3056\t Average training accuracy 0.9140\n",
      "Epoch [175]\t Average validation loss 0.2429\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [176][200]\t Batch [0][550]\t Training Loss 0.2521\t Accuracy 0.9500\n",
      "Epoch [176][200]\t Batch [50][550]\t Training Loss 0.2784\t Accuracy 0.9227\n",
      "Epoch [176][200]\t Batch [100][550]\t Training Loss 0.2896\t Accuracy 0.9208\n",
      "Epoch [176][200]\t Batch [150][550]\t Training Loss 0.3032\t Accuracy 0.9163\n",
      "Epoch [176][200]\t Batch [200][550]\t Training Loss 0.3002\t Accuracy 0.9162\n",
      "Epoch [176][200]\t Batch [250][550]\t Training Loss 0.3007\t Accuracy 0.9161\n",
      "Epoch [176][200]\t Batch [300][550]\t Training Loss 0.3013\t Accuracy 0.9165\n",
      "Epoch [176][200]\t Batch [350][550]\t Training Loss 0.3027\t Accuracy 0.9152\n",
      "Epoch [176][200]\t Batch [400][550]\t Training Loss 0.3034\t Accuracy 0.9147\n",
      "Epoch [176][200]\t Batch [450][550]\t Training Loss 0.3037\t Accuracy 0.9145\n",
      "Epoch [176][200]\t Batch [500][550]\t Training Loss 0.3047\t Accuracy 0.9141\n",
      "\n",
      "Epoch [176]\t Average training loss 0.3052\t Average training accuracy 0.9139\n",
      "Epoch [176]\t Average validation loss 0.2426\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [177][200]\t Batch [0][550]\t Training Loss 0.2517\t Accuracy 0.9500\n",
      "Epoch [177][200]\t Batch [50][550]\t Training Loss 0.2780\t Accuracy 0.9227\n",
      "Epoch [177][200]\t Batch [100][550]\t Training Loss 0.2892\t Accuracy 0.9209\n",
      "Epoch [177][200]\t Batch [150][550]\t Training Loss 0.3028\t Accuracy 0.9163\n",
      "Epoch [177][200]\t Batch [200][550]\t Training Loss 0.2998\t Accuracy 0.9162\n",
      "Epoch [177][200]\t Batch [250][550]\t Training Loss 0.3003\t Accuracy 0.9161\n",
      "Epoch [177][200]\t Batch [300][550]\t Training Loss 0.3009\t Accuracy 0.9165\n",
      "Epoch [177][200]\t Batch [350][550]\t Training Loss 0.3023\t Accuracy 0.9152\n",
      "Epoch [177][200]\t Batch [400][550]\t Training Loss 0.3030\t Accuracy 0.9148\n",
      "Epoch [177][200]\t Batch [450][550]\t Training Loss 0.3033\t Accuracy 0.9145\n",
      "Epoch [177][200]\t Batch [500][550]\t Training Loss 0.3043\t Accuracy 0.9141\n",
      "\n",
      "Epoch [177]\t Average training loss 0.3048\t Average training accuracy 0.9140\n",
      "Epoch [177]\t Average validation loss 0.2423\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [178][200]\t Batch [0][550]\t Training Loss 0.2512\t Accuracy 0.9500\n",
      "Epoch [178][200]\t Batch [50][550]\t Training Loss 0.2776\t Accuracy 0.9225\n",
      "Epoch [178][200]\t Batch [100][550]\t Training Loss 0.2888\t Accuracy 0.9207\n",
      "Epoch [178][200]\t Batch [150][550]\t Training Loss 0.3024\t Accuracy 0.9162\n",
      "Epoch [178][200]\t Batch [200][550]\t Training Loss 0.2995\t Accuracy 0.9163\n",
      "Epoch [178][200]\t Batch [250][550]\t Training Loss 0.3000\t Accuracy 0.9162\n",
      "Epoch [178][200]\t Batch [300][550]\t Training Loss 0.3005\t Accuracy 0.9166\n",
      "Epoch [178][200]\t Batch [350][550]\t Training Loss 0.3019\t Accuracy 0.9153\n",
      "Epoch [178][200]\t Batch [400][550]\t Training Loss 0.3026\t Accuracy 0.9149\n",
      "Epoch [178][200]\t Batch [450][550]\t Training Loss 0.3030\t Accuracy 0.9146\n",
      "Epoch [178][200]\t Batch [500][550]\t Training Loss 0.3039\t Accuracy 0.9141\n",
      "\n",
      "Epoch [178]\t Average training loss 0.3045\t Average training accuracy 0.9140\n",
      "Epoch [178]\t Average validation loss 0.2421\t Average validation accuracy 0.9344\n",
      "\n",
      "Epoch [179][200]\t Batch [0][550]\t Training Loss 0.2508\t Accuracy 0.9500\n",
      "Epoch [179][200]\t Batch [50][550]\t Training Loss 0.2772\t Accuracy 0.9225\n",
      "Epoch [179][200]\t Batch [100][550]\t Training Loss 0.2884\t Accuracy 0.9208\n",
      "Epoch [179][200]\t Batch [150][550]\t Training Loss 0.3020\t Accuracy 0.9163\n",
      "Epoch [179][200]\t Batch [200][550]\t Training Loss 0.2991\t Accuracy 0.9164\n",
      "Epoch [179][200]\t Batch [250][550]\t Training Loss 0.2996\t Accuracy 0.9163\n",
      "Epoch [179][200]\t Batch [300][550]\t Training Loss 0.3001\t Accuracy 0.9167\n",
      "Epoch [179][200]\t Batch [350][550]\t Training Loss 0.3016\t Accuracy 0.9155\n",
      "Epoch [179][200]\t Batch [400][550]\t Training Loss 0.3022\t Accuracy 0.9150\n",
      "Epoch [179][200]\t Batch [450][550]\t Training Loss 0.3026\t Accuracy 0.9147\n",
      "Epoch [179][200]\t Batch [500][550]\t Training Loss 0.3036\t Accuracy 0.9143\n",
      "\n",
      "Epoch [179]\t Average training loss 0.3041\t Average training accuracy 0.9141\n",
      "Epoch [179]\t Average validation loss 0.2418\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [180][200]\t Batch [0][550]\t Training Loss 0.2504\t Accuracy 0.9500\n",
      "Epoch [180][200]\t Batch [50][550]\t Training Loss 0.2768\t Accuracy 0.9227\n",
      "Epoch [180][200]\t Batch [100][550]\t Training Loss 0.2880\t Accuracy 0.9210\n",
      "Epoch [180][200]\t Batch [150][550]\t Training Loss 0.3017\t Accuracy 0.9164\n",
      "Epoch [180][200]\t Batch [200][550]\t Training Loss 0.2987\t Accuracy 0.9165\n",
      "Epoch [180][200]\t Batch [250][550]\t Training Loss 0.2992\t Accuracy 0.9164\n",
      "Epoch [180][200]\t Batch [300][550]\t Training Loss 0.2997\t Accuracy 0.9168\n",
      "Epoch [180][200]\t Batch [350][550]\t Training Loss 0.3012\t Accuracy 0.9156\n",
      "Epoch [180][200]\t Batch [400][550]\t Training Loss 0.3018\t Accuracy 0.9151\n",
      "Epoch [180][200]\t Batch [450][550]\t Training Loss 0.3022\t Accuracy 0.9149\n",
      "Epoch [180][200]\t Batch [500][550]\t Training Loss 0.3032\t Accuracy 0.9144\n",
      "\n",
      "Epoch [180]\t Average training loss 0.3037\t Average training accuracy 0.9143\n",
      "Epoch [180]\t Average validation loss 0.2415\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [181][200]\t Batch [0][550]\t Training Loss 0.2499\t Accuracy 0.9500\n",
      "Epoch [181][200]\t Batch [50][550]\t Training Loss 0.2764\t Accuracy 0.9231\n",
      "Epoch [181][200]\t Batch [100][550]\t Training Loss 0.2877\t Accuracy 0.9212\n",
      "Epoch [181][200]\t Batch [150][550]\t Training Loss 0.3013\t Accuracy 0.9166\n",
      "Epoch [181][200]\t Batch [200][550]\t Training Loss 0.2983\t Accuracy 0.9167\n",
      "Epoch [181][200]\t Batch [250][550]\t Training Loss 0.2988\t Accuracy 0.9165\n",
      "Epoch [181][200]\t Batch [300][550]\t Training Loss 0.2993\t Accuracy 0.9169\n",
      "Epoch [181][200]\t Batch [350][550]\t Training Loss 0.3008\t Accuracy 0.9157\n",
      "Epoch [181][200]\t Batch [400][550]\t Training Loss 0.3014\t Accuracy 0.9152\n",
      "Epoch [181][200]\t Batch [450][550]\t Training Loss 0.3018\t Accuracy 0.9149\n",
      "Epoch [181][200]\t Batch [500][550]\t Training Loss 0.3028\t Accuracy 0.9144\n",
      "\n",
      "Epoch [181]\t Average training loss 0.3033\t Average training accuracy 0.9143\n",
      "Epoch [181]\t Average validation loss 0.2412\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [182][200]\t Batch [0][550]\t Training Loss 0.2495\t Accuracy 0.9500\n",
      "Epoch [182][200]\t Batch [50][550]\t Training Loss 0.2760\t Accuracy 0.9231\n",
      "Epoch [182][200]\t Batch [100][550]\t Training Loss 0.2873\t Accuracy 0.9211\n",
      "Epoch [182][200]\t Batch [150][550]\t Training Loss 0.3009\t Accuracy 0.9166\n",
      "Epoch [182][200]\t Batch [200][550]\t Training Loss 0.2979\t Accuracy 0.9167\n",
      "Epoch [182][200]\t Batch [250][550]\t Training Loss 0.2984\t Accuracy 0.9165\n",
      "Epoch [182][200]\t Batch [300][550]\t Training Loss 0.2989\t Accuracy 0.9169\n",
      "Epoch [182][200]\t Batch [350][550]\t Training Loss 0.3004\t Accuracy 0.9157\n",
      "Epoch [182][200]\t Batch [400][550]\t Training Loss 0.3010\t Accuracy 0.9152\n",
      "Epoch [182][200]\t Batch [450][550]\t Training Loss 0.3014\t Accuracy 0.9149\n",
      "Epoch [182][200]\t Batch [500][550]\t Training Loss 0.3024\t Accuracy 0.9145\n",
      "\n",
      "Epoch [182]\t Average training loss 0.3030\t Average training accuracy 0.9143\n",
      "Epoch [182]\t Average validation loss 0.2409\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [183][200]\t Batch [0][550]\t Training Loss 0.2491\t Accuracy 0.9500\n",
      "Epoch [183][200]\t Batch [50][550]\t Training Loss 0.2756\t Accuracy 0.9231\n",
      "Epoch [183][200]\t Batch [100][550]\t Training Loss 0.2869\t Accuracy 0.9211\n",
      "Epoch [183][200]\t Batch [150][550]\t Training Loss 0.3005\t Accuracy 0.9166\n",
      "Epoch [183][200]\t Batch [200][550]\t Training Loss 0.2975\t Accuracy 0.9167\n",
      "Epoch [183][200]\t Batch [250][550]\t Training Loss 0.2981\t Accuracy 0.9165\n",
      "Epoch [183][200]\t Batch [300][550]\t Training Loss 0.2985\t Accuracy 0.9169\n",
      "Epoch [183][200]\t Batch [350][550]\t Training Loss 0.3000\t Accuracy 0.9158\n",
      "Epoch [183][200]\t Batch [400][550]\t Training Loss 0.3007\t Accuracy 0.9153\n",
      "Epoch [183][200]\t Batch [450][550]\t Training Loss 0.3011\t Accuracy 0.9151\n",
      "Epoch [183][200]\t Batch [500][550]\t Training Loss 0.3021\t Accuracy 0.9146\n",
      "\n",
      "Epoch [183]\t Average training loss 0.3026\t Average training accuracy 0.9145\n",
      "Epoch [183]\t Average validation loss 0.2407\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [184][200]\t Batch [0][550]\t Training Loss 0.2487\t Accuracy 0.9500\n",
      "Epoch [184][200]\t Batch [50][550]\t Training Loss 0.2753\t Accuracy 0.9231\n",
      "Epoch [184][200]\t Batch [100][550]\t Training Loss 0.2865\t Accuracy 0.9211\n",
      "Epoch [184][200]\t Batch [150][550]\t Training Loss 0.3001\t Accuracy 0.9166\n",
      "Epoch [184][200]\t Batch [200][550]\t Training Loss 0.2972\t Accuracy 0.9167\n",
      "Epoch [184][200]\t Batch [250][550]\t Training Loss 0.2977\t Accuracy 0.9166\n",
      "Epoch [184][200]\t Batch [300][550]\t Training Loss 0.2982\t Accuracy 0.9170\n",
      "Epoch [184][200]\t Batch [350][550]\t Training Loss 0.2997\t Accuracy 0.9159\n",
      "Epoch [184][200]\t Batch [400][550]\t Training Loss 0.3003\t Accuracy 0.9154\n",
      "Epoch [184][200]\t Batch [450][550]\t Training Loss 0.3007\t Accuracy 0.9152\n",
      "Epoch [184][200]\t Batch [500][550]\t Training Loss 0.3017\t Accuracy 0.9147\n",
      "\n",
      "Epoch [184]\t Average training loss 0.3022\t Average training accuracy 0.9145\n",
      "Epoch [184]\t Average validation loss 0.2404\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [185][200]\t Batch [0][550]\t Training Loss 0.2482\t Accuracy 0.9500\n",
      "Epoch [185][200]\t Batch [50][550]\t Training Loss 0.2749\t Accuracy 0.9233\n",
      "Epoch [185][200]\t Batch [100][550]\t Training Loss 0.2862\t Accuracy 0.9213\n",
      "Epoch [185][200]\t Batch [150][550]\t Training Loss 0.2997\t Accuracy 0.9167\n",
      "Epoch [185][200]\t Batch [200][550]\t Training Loss 0.2968\t Accuracy 0.9168\n",
      "Epoch [185][200]\t Batch [250][550]\t Training Loss 0.2973\t Accuracy 0.9167\n",
      "Epoch [185][200]\t Batch [300][550]\t Training Loss 0.2978\t Accuracy 0.9171\n",
      "Epoch [185][200]\t Batch [350][550]\t Training Loss 0.2993\t Accuracy 0.9160\n",
      "Epoch [185][200]\t Batch [400][550]\t Training Loss 0.2999\t Accuracy 0.9156\n",
      "Epoch [185][200]\t Batch [450][550]\t Training Loss 0.3003\t Accuracy 0.9153\n",
      "Epoch [185][200]\t Batch [500][550]\t Training Loss 0.3013\t Accuracy 0.9149\n",
      "\n",
      "Epoch [185]\t Average training loss 0.3019\t Average training accuracy 0.9147\n",
      "Epoch [185]\t Average validation loss 0.2401\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [186][200]\t Batch [0][550]\t Training Loss 0.2478\t Accuracy 0.9500\n",
      "Epoch [186][200]\t Batch [50][550]\t Training Loss 0.2745\t Accuracy 0.9237\n",
      "Epoch [186][200]\t Batch [100][550]\t Training Loss 0.2858\t Accuracy 0.9218\n",
      "Epoch [186][200]\t Batch [150][550]\t Training Loss 0.2994\t Accuracy 0.9171\n",
      "Epoch [186][200]\t Batch [200][550]\t Training Loss 0.2964\t Accuracy 0.9171\n",
      "Epoch [186][200]\t Batch [250][550]\t Training Loss 0.2970\t Accuracy 0.9169\n",
      "Epoch [186][200]\t Batch [300][550]\t Training Loss 0.2974\t Accuracy 0.9174\n",
      "Epoch [186][200]\t Batch [350][550]\t Training Loss 0.2989\t Accuracy 0.9162\n",
      "Epoch [186][200]\t Batch [400][550]\t Training Loss 0.2995\t Accuracy 0.9158\n",
      "Epoch [186][200]\t Batch [450][550]\t Training Loss 0.3000\t Accuracy 0.9155\n",
      "Epoch [186][200]\t Batch [500][550]\t Training Loss 0.3010\t Accuracy 0.9150\n",
      "\n",
      "Epoch [186]\t Average training loss 0.3015\t Average training accuracy 0.9149\n",
      "Epoch [186]\t Average validation loss 0.2398\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [187][200]\t Batch [0][550]\t Training Loss 0.2474\t Accuracy 0.9500\n",
      "Epoch [187][200]\t Batch [50][550]\t Training Loss 0.2741\t Accuracy 0.9237\n",
      "Epoch [187][200]\t Batch [100][550]\t Training Loss 0.2854\t Accuracy 0.9218\n",
      "Epoch [187][200]\t Batch [150][550]\t Training Loss 0.2990\t Accuracy 0.9171\n",
      "Epoch [187][200]\t Batch [200][550]\t Training Loss 0.2960\t Accuracy 0.9171\n",
      "Epoch [187][200]\t Batch [250][550]\t Training Loss 0.2966\t Accuracy 0.9169\n",
      "Epoch [187][200]\t Batch [300][550]\t Training Loss 0.2970\t Accuracy 0.9174\n",
      "Epoch [187][200]\t Batch [350][550]\t Training Loss 0.2985\t Accuracy 0.9163\n",
      "Epoch [187][200]\t Batch [400][550]\t Training Loss 0.2992\t Accuracy 0.9158\n",
      "Epoch [187][200]\t Batch [450][550]\t Training Loss 0.2996\t Accuracy 0.9155\n",
      "Epoch [187][200]\t Batch [500][550]\t Training Loss 0.3006\t Accuracy 0.9151\n",
      "\n",
      "Epoch [187]\t Average training loss 0.3011\t Average training accuracy 0.9149\n",
      "Epoch [187]\t Average validation loss 0.2395\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [188][200]\t Batch [0][550]\t Training Loss 0.2470\t Accuracy 0.9500\n",
      "Epoch [188][200]\t Batch [50][550]\t Training Loss 0.2737\t Accuracy 0.9237\n",
      "Epoch [188][200]\t Batch [100][550]\t Training Loss 0.2851\t Accuracy 0.9218\n",
      "Epoch [188][200]\t Batch [150][550]\t Training Loss 0.2986\t Accuracy 0.9171\n",
      "Epoch [188][200]\t Batch [200][550]\t Training Loss 0.2957\t Accuracy 0.9171\n",
      "Epoch [188][200]\t Batch [250][550]\t Training Loss 0.2962\t Accuracy 0.9168\n",
      "Epoch [188][200]\t Batch [300][550]\t Training Loss 0.2967\t Accuracy 0.9173\n",
      "Epoch [188][200]\t Batch [350][550]\t Training Loss 0.2982\t Accuracy 0.9162\n",
      "Epoch [188][200]\t Batch [400][550]\t Training Loss 0.2988\t Accuracy 0.9158\n",
      "Epoch [188][200]\t Batch [450][550]\t Training Loss 0.2992\t Accuracy 0.9155\n",
      "Epoch [188][200]\t Batch [500][550]\t Training Loss 0.3002\t Accuracy 0.9151\n",
      "\n",
      "Epoch [188]\t Average training loss 0.3008\t Average training accuracy 0.9150\n",
      "Epoch [188]\t Average validation loss 0.2393\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [189][200]\t Batch [0][550]\t Training Loss 0.2466\t Accuracy 0.9500\n",
      "Epoch [189][200]\t Batch [50][550]\t Training Loss 0.2734\t Accuracy 0.9239\n",
      "Epoch [189][200]\t Batch [100][550]\t Training Loss 0.2847\t Accuracy 0.9219\n",
      "Epoch [189][200]\t Batch [150][550]\t Training Loss 0.2982\t Accuracy 0.9172\n",
      "Epoch [189][200]\t Batch [200][550]\t Training Loss 0.2953\t Accuracy 0.9172\n",
      "Epoch [189][200]\t Batch [250][550]\t Training Loss 0.2959\t Accuracy 0.9169\n",
      "Epoch [189][200]\t Batch [300][550]\t Training Loss 0.2963\t Accuracy 0.9174\n",
      "Epoch [189][200]\t Batch [350][550]\t Training Loss 0.2978\t Accuracy 0.9163\n",
      "Epoch [189][200]\t Batch [400][550]\t Training Loss 0.2984\t Accuracy 0.9158\n",
      "Epoch [189][200]\t Batch [450][550]\t Training Loss 0.2989\t Accuracy 0.9156\n",
      "Epoch [189][200]\t Batch [500][550]\t Training Loss 0.2999\t Accuracy 0.9152\n",
      "\n",
      "Epoch [189]\t Average training loss 0.3004\t Average training accuracy 0.9151\n",
      "Epoch [189]\t Average validation loss 0.2390\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [190][200]\t Batch [0][550]\t Training Loss 0.2462\t Accuracy 0.9500\n",
      "Epoch [190][200]\t Batch [50][550]\t Training Loss 0.2730\t Accuracy 0.9241\n",
      "Epoch [190][200]\t Batch [100][550]\t Training Loss 0.2844\t Accuracy 0.9220\n",
      "Epoch [190][200]\t Batch [150][550]\t Training Loss 0.2979\t Accuracy 0.9173\n",
      "Epoch [190][200]\t Batch [200][550]\t Training Loss 0.2949\t Accuracy 0.9172\n",
      "Epoch [190][200]\t Batch [250][550]\t Training Loss 0.2955\t Accuracy 0.9169\n",
      "Epoch [190][200]\t Batch [300][550]\t Training Loss 0.2959\t Accuracy 0.9174\n",
      "Epoch [190][200]\t Batch [350][550]\t Training Loss 0.2975\t Accuracy 0.9164\n",
      "Epoch [190][200]\t Batch [400][550]\t Training Loss 0.2981\t Accuracy 0.9159\n",
      "Epoch [190][200]\t Batch [450][550]\t Training Loss 0.2985\t Accuracy 0.9156\n",
      "Epoch [190][200]\t Batch [500][550]\t Training Loss 0.2995\t Accuracy 0.9153\n",
      "\n",
      "Epoch [190]\t Average training loss 0.3001\t Average training accuracy 0.9151\n",
      "Epoch [190]\t Average validation loss 0.2387\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [191][200]\t Batch [0][550]\t Training Loss 0.2458\t Accuracy 0.9500\n",
      "Epoch [191][200]\t Batch [50][550]\t Training Loss 0.2726\t Accuracy 0.9243\n",
      "Epoch [191][200]\t Batch [100][550]\t Training Loss 0.2840\t Accuracy 0.9221\n",
      "Epoch [191][200]\t Batch [150][550]\t Training Loss 0.2975\t Accuracy 0.9174\n",
      "Epoch [191][200]\t Batch [200][550]\t Training Loss 0.2946\t Accuracy 0.9173\n",
      "Epoch [191][200]\t Batch [250][550]\t Training Loss 0.2952\t Accuracy 0.9171\n",
      "Epoch [191][200]\t Batch [300][550]\t Training Loss 0.2956\t Accuracy 0.9175\n",
      "Epoch [191][200]\t Batch [350][550]\t Training Loss 0.2971\t Accuracy 0.9165\n",
      "Epoch [191][200]\t Batch [400][550]\t Training Loss 0.2977\t Accuracy 0.9159\n",
      "Epoch [191][200]\t Batch [450][550]\t Training Loss 0.2982\t Accuracy 0.9157\n",
      "Epoch [191][200]\t Batch [500][550]\t Training Loss 0.2992\t Accuracy 0.9154\n",
      "\n",
      "Epoch [191]\t Average training loss 0.2997\t Average training accuracy 0.9152\n",
      "Epoch [191]\t Average validation loss 0.2385\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [192][200]\t Batch [0][550]\t Training Loss 0.2454\t Accuracy 0.9500\n",
      "Epoch [192][200]\t Batch [50][550]\t Training Loss 0.2723\t Accuracy 0.9245\n",
      "Epoch [192][200]\t Batch [100][550]\t Training Loss 0.2836\t Accuracy 0.9222\n",
      "Epoch [192][200]\t Batch [150][550]\t Training Loss 0.2971\t Accuracy 0.9175\n",
      "Epoch [192][200]\t Batch [200][550]\t Training Loss 0.2942\t Accuracy 0.9175\n",
      "Epoch [192][200]\t Batch [250][550]\t Training Loss 0.2948\t Accuracy 0.9172\n",
      "Epoch [192][200]\t Batch [300][550]\t Training Loss 0.2952\t Accuracy 0.9177\n",
      "Epoch [192][200]\t Batch [350][550]\t Training Loss 0.2967\t Accuracy 0.9166\n",
      "Epoch [192][200]\t Batch [400][550]\t Training Loss 0.2974\t Accuracy 0.9161\n",
      "Epoch [192][200]\t Batch [450][550]\t Training Loss 0.2978\t Accuracy 0.9158\n",
      "Epoch [192][200]\t Batch [500][550]\t Training Loss 0.2988\t Accuracy 0.9155\n",
      "\n",
      "Epoch [192]\t Average training loss 0.2993\t Average training accuracy 0.9153\n",
      "Epoch [192]\t Average validation loss 0.2382\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [193][200]\t Batch [0][550]\t Training Loss 0.2450\t Accuracy 0.9500\n",
      "Epoch [193][200]\t Batch [50][550]\t Training Loss 0.2719\t Accuracy 0.9245\n",
      "Epoch [193][200]\t Batch [100][550]\t Training Loss 0.2833\t Accuracy 0.9222\n",
      "Epoch [193][200]\t Batch [150][550]\t Training Loss 0.2968\t Accuracy 0.9175\n",
      "Epoch [193][200]\t Batch [200][550]\t Training Loss 0.2938\t Accuracy 0.9175\n",
      "Epoch [193][200]\t Batch [250][550]\t Training Loss 0.2944\t Accuracy 0.9173\n",
      "Epoch [193][200]\t Batch [300][550]\t Training Loss 0.2948\t Accuracy 0.9178\n",
      "Epoch [193][200]\t Batch [350][550]\t Training Loss 0.2964\t Accuracy 0.9167\n",
      "Epoch [193][200]\t Batch [400][550]\t Training Loss 0.2970\t Accuracy 0.9161\n",
      "Epoch [193][200]\t Batch [450][550]\t Training Loss 0.2974\t Accuracy 0.9159\n",
      "Epoch [193][200]\t Batch [500][550]\t Training Loss 0.2985\t Accuracy 0.9155\n",
      "\n",
      "Epoch [193]\t Average training loss 0.2990\t Average training accuracy 0.9153\n",
      "Epoch [193]\t Average validation loss 0.2379\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [194][200]\t Batch [0][550]\t Training Loss 0.2446\t Accuracy 0.9500\n",
      "Epoch [194][200]\t Batch [50][550]\t Training Loss 0.2715\t Accuracy 0.9249\n",
      "Epoch [194][200]\t Batch [100][550]\t Training Loss 0.2829\t Accuracy 0.9224\n",
      "Epoch [194][200]\t Batch [150][550]\t Training Loss 0.2964\t Accuracy 0.9177\n",
      "Epoch [194][200]\t Batch [200][550]\t Training Loss 0.2935\t Accuracy 0.9178\n",
      "Epoch [194][200]\t Batch [250][550]\t Training Loss 0.2941\t Accuracy 0.9175\n",
      "Epoch [194][200]\t Batch [300][550]\t Training Loss 0.2945\t Accuracy 0.9180\n",
      "Epoch [194][200]\t Batch [350][550]\t Training Loss 0.2960\t Accuracy 0.9169\n",
      "Epoch [194][200]\t Batch [400][550]\t Training Loss 0.2966\t Accuracy 0.9163\n",
      "Epoch [194][200]\t Batch [450][550]\t Training Loss 0.2971\t Accuracy 0.9160\n",
      "Epoch [194][200]\t Batch [500][550]\t Training Loss 0.2981\t Accuracy 0.9156\n",
      "\n",
      "Epoch [194]\t Average training loss 0.2986\t Average training accuracy 0.9155\n",
      "Epoch [194]\t Average validation loss 0.2377\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [195][200]\t Batch [0][550]\t Training Loss 0.2442\t Accuracy 0.9500\n",
      "Epoch [195][200]\t Batch [50][550]\t Training Loss 0.2711\t Accuracy 0.9249\n",
      "Epoch [195][200]\t Batch [100][550]\t Training Loss 0.2826\t Accuracy 0.9224\n",
      "Epoch [195][200]\t Batch [150][550]\t Training Loss 0.2961\t Accuracy 0.9178\n",
      "Epoch [195][200]\t Batch [200][550]\t Training Loss 0.2931\t Accuracy 0.9178\n",
      "Epoch [195][200]\t Batch [250][550]\t Training Loss 0.2937\t Accuracy 0.9175\n",
      "Epoch [195][200]\t Batch [300][550]\t Training Loss 0.2941\t Accuracy 0.9180\n",
      "Epoch [195][200]\t Batch [350][550]\t Training Loss 0.2957\t Accuracy 0.9170\n",
      "Epoch [195][200]\t Batch [400][550]\t Training Loss 0.2963\t Accuracy 0.9163\n",
      "Epoch [195][200]\t Batch [450][550]\t Training Loss 0.2967\t Accuracy 0.9161\n",
      "Epoch [195][200]\t Batch [500][550]\t Training Loss 0.2978\t Accuracy 0.9157\n",
      "\n",
      "Epoch [195]\t Average training loss 0.2983\t Average training accuracy 0.9155\n",
      "Epoch [195]\t Average validation loss 0.2374\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [196][200]\t Batch [0][550]\t Training Loss 0.2438\t Accuracy 0.9500\n",
      "Epoch [196][200]\t Batch [50][550]\t Training Loss 0.2708\t Accuracy 0.9251\n",
      "Epoch [196][200]\t Batch [100][550]\t Training Loss 0.2822\t Accuracy 0.9226\n",
      "Epoch [196][200]\t Batch [150][550]\t Training Loss 0.2957\t Accuracy 0.9180\n",
      "Epoch [196][200]\t Batch [200][550]\t Training Loss 0.2928\t Accuracy 0.9180\n",
      "Epoch [196][200]\t Batch [250][550]\t Training Loss 0.2934\t Accuracy 0.9177\n",
      "Epoch [196][200]\t Batch [300][550]\t Training Loss 0.2938\t Accuracy 0.9182\n",
      "Epoch [196][200]\t Batch [350][550]\t Training Loss 0.2953\t Accuracy 0.9171\n",
      "Epoch [196][200]\t Batch [400][550]\t Training Loss 0.2959\t Accuracy 0.9164\n",
      "Epoch [196][200]\t Batch [450][550]\t Training Loss 0.2964\t Accuracy 0.9162\n",
      "Epoch [196][200]\t Batch [500][550]\t Training Loss 0.2974\t Accuracy 0.9158\n",
      "\n",
      "Epoch [196]\t Average training loss 0.2979\t Average training accuracy 0.9157\n",
      "Epoch [196]\t Average validation loss 0.2371\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [197][200]\t Batch [0][550]\t Training Loss 0.2434\t Accuracy 0.9500\n",
      "Epoch [197][200]\t Batch [50][550]\t Training Loss 0.2704\t Accuracy 0.9251\n",
      "Epoch [197][200]\t Batch [100][550]\t Training Loss 0.2819\t Accuracy 0.9226\n",
      "Epoch [197][200]\t Batch [150][550]\t Training Loss 0.2953\t Accuracy 0.9179\n",
      "Epoch [197][200]\t Batch [200][550]\t Training Loss 0.2924\t Accuracy 0.9180\n",
      "Epoch [197][200]\t Batch [250][550]\t Training Loss 0.2930\t Accuracy 0.9177\n",
      "Epoch [197][200]\t Batch [300][550]\t Training Loss 0.2934\t Accuracy 0.9182\n",
      "Epoch [197][200]\t Batch [350][550]\t Training Loss 0.2950\t Accuracy 0.9172\n",
      "Epoch [197][200]\t Batch [400][550]\t Training Loss 0.2956\t Accuracy 0.9165\n",
      "Epoch [197][200]\t Batch [450][550]\t Training Loss 0.2960\t Accuracy 0.9163\n",
      "Epoch [197][200]\t Batch [500][550]\t Training Loss 0.2971\t Accuracy 0.9159\n",
      "\n",
      "Epoch [197]\t Average training loss 0.2976\t Average training accuracy 0.9157\n",
      "Epoch [197]\t Average validation loss 0.2369\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [198][200]\t Batch [0][550]\t Training Loss 0.2430\t Accuracy 0.9500\n",
      "Epoch [198][200]\t Batch [50][550]\t Training Loss 0.2701\t Accuracy 0.9251\n",
      "Epoch [198][200]\t Batch [100][550]\t Training Loss 0.2815\t Accuracy 0.9227\n",
      "Epoch [198][200]\t Batch [150][550]\t Training Loss 0.2950\t Accuracy 0.9180\n",
      "Epoch [198][200]\t Batch [200][550]\t Training Loss 0.2920\t Accuracy 0.9181\n",
      "Epoch [198][200]\t Batch [250][550]\t Training Loss 0.2927\t Accuracy 0.9178\n",
      "Epoch [198][200]\t Batch [300][550]\t Training Loss 0.2931\t Accuracy 0.9183\n",
      "Epoch [198][200]\t Batch [350][550]\t Training Loss 0.2946\t Accuracy 0.9173\n",
      "Epoch [198][200]\t Batch [400][550]\t Training Loss 0.2952\t Accuracy 0.9166\n",
      "Epoch [198][200]\t Batch [450][550]\t Training Loss 0.2957\t Accuracy 0.9164\n",
      "Epoch [198][200]\t Batch [500][550]\t Training Loss 0.2967\t Accuracy 0.9160\n",
      "\n",
      "Epoch [198]\t Average training loss 0.2973\t Average training accuracy 0.9158\n",
      "Epoch [198]\t Average validation loss 0.2366\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [199][200]\t Batch [0][550]\t Training Loss 0.2427\t Accuracy 0.9500\n",
      "Epoch [199][200]\t Batch [50][550]\t Training Loss 0.2697\t Accuracy 0.9253\n",
      "Epoch [199][200]\t Batch [100][550]\t Training Loss 0.2812\t Accuracy 0.9229\n",
      "Epoch [199][200]\t Batch [150][550]\t Training Loss 0.2946\t Accuracy 0.9181\n",
      "Epoch [199][200]\t Batch [200][550]\t Training Loss 0.2917\t Accuracy 0.9181\n",
      "Epoch [199][200]\t Batch [250][550]\t Training Loss 0.2923\t Accuracy 0.9178\n",
      "Epoch [199][200]\t Batch [300][550]\t Training Loss 0.2927\t Accuracy 0.9184\n",
      "Epoch [199][200]\t Batch [350][550]\t Training Loss 0.2943\t Accuracy 0.9174\n",
      "Epoch [199][200]\t Batch [400][550]\t Training Loss 0.2949\t Accuracy 0.9166\n",
      "Epoch [199][200]\t Batch [450][550]\t Training Loss 0.2953\t Accuracy 0.9164\n",
      "Epoch [199][200]\t Batch [500][550]\t Training Loss 0.2964\t Accuracy 0.9160\n",
      "\n",
      "Epoch [199]\t Average training loss 0.2969\t Average training accuracy 0.9159\n",
      "Epoch [199]\t Average validation loss 0.2364\t Average validation accuracy 0.9368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weight_decays = [1.0, 0.1, 0.001, 0.0001]\n",
    "losses, accs = [], []\n",
    "for weight_decay in weight_decays:\n",
    "    reluMLP, relu_loss, relu_acc = train(model=reluMLP, criterion=criterion, optimizer=SGD(0.001, weight_decay), dataset=data_train, max_epoch=200, batch_size=100, disp_freq=disp_freq)\n",
    "    losses.append(relu_loss)\n",
    "    accs.append(relu_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'weight_decay=1.0': [losses[0], accs[0]], \n",
    "                   'weight_decay=0.1': [losses[1], accs[1]],\n",
    "                   'weight_decay=0.001': [losses[2], accs[2]],\n",
    "                    'weight_decay=0.0001': [losses[3], accs[3]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最终模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 25\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.005\n",
    "weight_decay = 0.00001\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer, SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "from layers import FCLayer, ReLULayer, SigmoidLayer \n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Network()\n",
    "final_model.add(FCLayer(784, 128))\n",
    "final_model.add(ReLULayer())\n",
    "final_model.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/maffinnn/Desktop/DL Lab/Lab2/mlp/solver.py:13: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 17:16:55.594127: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-13 17:16:55.594442: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-03-13 17:16:55.864165: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][25]\t Batch [0][550]\t Training Loss 2.7192\t Accuracy 0.1000\n",
      "Epoch [0][25]\t Batch [50][550]\t Training Loss 2.3113\t Accuracy 0.1800\n",
      "Epoch [0][25]\t Batch [100][550]\t Training Loss 2.1233\t Accuracy 0.2690\n",
      "Epoch [0][25]\t Batch [150][550]\t Training Loss 1.9763\t Accuracy 0.3496\n",
      "Epoch [0][25]\t Batch [200][550]\t Training Loss 1.8552\t Accuracy 0.4126\n",
      "Epoch [0][25]\t Batch [250][550]\t Training Loss 1.7468\t Accuracy 0.4662\n",
      "Epoch [0][25]\t Batch [300][550]\t Training Loss 1.6532\t Accuracy 0.5089\n",
      "Epoch [0][25]\t Batch [350][550]\t Training Loss 1.5784\t Accuracy 0.5418\n",
      "Epoch [0][25]\t Batch [400][550]\t Training Loss 1.5075\t Accuracy 0.5704\n",
      "Epoch [0][25]\t Batch [450][550]\t Training Loss 1.4461\t Accuracy 0.5935\n",
      "Epoch [0][25]\t Batch [500][550]\t Training Loss 1.3898\t Accuracy 0.6135\n",
      "\n",
      "Epoch [0]\t Average training loss 1.3392\t Average training accuracy 0.6309\n",
      "Epoch [0]\t Average validation loss 0.7276\t Average validation accuracy 0.8492\n",
      "\n",
      "Epoch [1][25]\t Batch [0][550]\t Training Loss 0.8073\t Accuracy 0.8300\n",
      "Epoch [1][25]\t Batch [50][550]\t Training Loss 0.7603\t Accuracy 0.8276\n",
      "Epoch [1][25]\t Batch [100][550]\t Training Loss 0.7572\t Accuracy 0.8223\n",
      "Epoch [1][25]\t Batch [150][550]\t Training Loss 0.7536\t Accuracy 0.8191\n",
      "Epoch [1][25]\t Batch [200][550]\t Training Loss 0.7423\t Accuracy 0.8208\n",
      "Epoch [1][25]\t Batch [250][550]\t Training Loss 0.7265\t Accuracy 0.8240\n",
      "Epoch [1][25]\t Batch [300][550]\t Training Loss 0.7129\t Accuracy 0.8273\n",
      "Epoch [1][25]\t Batch [350][550]\t Training Loss 0.7063\t Accuracy 0.8281\n",
      "Epoch [1][25]\t Batch [400][550]\t Training Loss 0.6949\t Accuracy 0.8305\n",
      "Epoch [1][25]\t Batch [450][550]\t Training Loss 0.6854\t Accuracy 0.8324\n",
      "Epoch [1][25]\t Batch [500][550]\t Training Loss 0.6761\t Accuracy 0.8342\n",
      "\n",
      "Epoch [1]\t Average training loss 0.6661\t Average training accuracy 0.8360\n",
      "Epoch [1]\t Average validation loss 0.4777\t Average validation accuracy 0.8904\n",
      "\n",
      "Epoch [2][25]\t Batch [0][550]\t Training Loss 0.5676\t Accuracy 0.8800\n",
      "Epoch [2][25]\t Batch [50][550]\t Training Loss 0.5294\t Accuracy 0.8757\n",
      "Epoch [2][25]\t Batch [100][550]\t Training Loss 0.5383\t Accuracy 0.8658\n",
      "Epoch [2][25]\t Batch [150][550]\t Training Loss 0.5460\t Accuracy 0.8603\n",
      "Epoch [2][25]\t Batch [200][550]\t Training Loss 0.5430\t Accuracy 0.8608\n",
      "Epoch [2][25]\t Batch [250][550]\t Training Loss 0.5364\t Accuracy 0.8618\n",
      "Epoch [2][25]\t Batch [300][550]\t Training Loss 0.5315\t Accuracy 0.8630\n",
      "Epoch [2][25]\t Batch [350][550]\t Training Loss 0.5317\t Accuracy 0.8623\n",
      "Epoch [2][25]\t Batch [400][550]\t Training Loss 0.5272\t Accuracy 0.8632\n",
      "Epoch [2][25]\t Batch [450][550]\t Training Loss 0.5240\t Accuracy 0.8634\n",
      "Epoch [2][25]\t Batch [500][550]\t Training Loss 0.5209\t Accuracy 0.8640\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5166\t Average training accuracy 0.8651\n",
      "Epoch [2]\t Average validation loss 0.3889\t Average validation accuracy 0.9058\n",
      "\n",
      "Epoch [3][25]\t Batch [0][550]\t Training Loss 0.4750\t Accuracy 0.8800\n",
      "Epoch [3][25]\t Batch [50][550]\t Training Loss 0.4410\t Accuracy 0.8892\n",
      "Epoch [3][25]\t Batch [100][550]\t Training Loss 0.4524\t Accuracy 0.8822\n",
      "Epoch [3][25]\t Batch [150][550]\t Training Loss 0.4626\t Accuracy 0.8770\n",
      "Epoch [3][25]\t Batch [200][550]\t Training Loss 0.4611\t Accuracy 0.8776\n",
      "Epoch [3][25]\t Batch [250][550]\t Training Loss 0.4571\t Accuracy 0.8782\n",
      "Epoch [3][25]\t Batch [300][550]\t Training Loss 0.4546\t Accuracy 0.8790\n",
      "Epoch [3][25]\t Batch [350][550]\t Training Loss 0.4565\t Accuracy 0.8781\n",
      "Epoch [3][25]\t Batch [400][550]\t Training Loss 0.4539\t Accuracy 0.8789\n",
      "Epoch [3][25]\t Batch [450][550]\t Training Loss 0.4525\t Accuracy 0.8786\n",
      "Epoch [3][25]\t Batch [500][550]\t Training Loss 0.4513\t Accuracy 0.8788\n",
      "\n",
      "Epoch [3]\t Average training loss 0.4489\t Average training accuracy 0.8795\n",
      "Epoch [3]\t Average validation loss 0.3427\t Average validation accuracy 0.9132\n",
      "\n",
      "Epoch [4][25]\t Batch [0][550]\t Training Loss 0.4231\t Accuracy 0.9100\n",
      "Epoch [4][25]\t Batch [50][550]\t Training Loss 0.3926\t Accuracy 0.9006\n",
      "Epoch [4][25]\t Batch [100][550]\t Training Loss 0.4047\t Accuracy 0.8941\n",
      "Epoch [4][25]\t Batch [150][550]\t Training Loss 0.4158\t Accuracy 0.8891\n",
      "Epoch [4][25]\t Batch [200][550]\t Training Loss 0.4145\t Accuracy 0.8899\n",
      "Epoch [4][25]\t Batch [250][550]\t Training Loss 0.4117\t Accuracy 0.8898\n",
      "Epoch [4][25]\t Batch [300][550]\t Training Loss 0.4104\t Accuracy 0.8898\n",
      "Epoch [4][25]\t Batch [350][550]\t Training Loss 0.4127\t Accuracy 0.8887\n",
      "Epoch [4][25]\t Batch [400][550]\t Training Loss 0.4110\t Accuracy 0.8894\n",
      "Epoch [4][25]\t Batch [450][550]\t Training Loss 0.4103\t Accuracy 0.8889\n",
      "Epoch [4][25]\t Batch [500][550]\t Training Loss 0.4100\t Accuracy 0.8888\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4085\t Average training accuracy 0.8893\n",
      "Epoch [4]\t Average validation loss 0.3137\t Average validation accuracy 0.9176\n",
      "\n",
      "Epoch [5][25]\t Batch [0][550]\t Training Loss 0.3886\t Accuracy 0.9200\n",
      "Epoch [5][25]\t Batch [50][550]\t Training Loss 0.3611\t Accuracy 0.9065\n",
      "Epoch [5][25]\t Batch [100][550]\t Training Loss 0.3736\t Accuracy 0.9005\n",
      "Epoch [5][25]\t Batch [150][550]\t Training Loss 0.3850\t Accuracy 0.8966\n",
      "Epoch [5][25]\t Batch [200][550]\t Training Loss 0.3837\t Accuracy 0.8971\n",
      "Epoch [5][25]\t Batch [250][550]\t Training Loss 0.3815\t Accuracy 0.8968\n",
      "Epoch [5][25]\t Batch [300][550]\t Training Loss 0.3808\t Accuracy 0.8963\n",
      "Epoch [5][25]\t Batch [350][550]\t Training Loss 0.3833\t Accuracy 0.8952\n",
      "Epoch [5][25]\t Batch [400][550]\t Training Loss 0.3820\t Accuracy 0.8957\n",
      "Epoch [5][25]\t Batch [450][550]\t Training Loss 0.3816\t Accuracy 0.8951\n",
      "Epoch [5][25]\t Batch [500][550]\t Training Loss 0.3818\t Accuracy 0.8948\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3809\t Average training accuracy 0.8952\n",
      "Epoch [5]\t Average validation loss 0.2934\t Average validation accuracy 0.9210\n",
      "\n",
      "Epoch [6][25]\t Batch [0][550]\t Training Loss 0.3631\t Accuracy 0.9200\n",
      "Epoch [6][25]\t Batch [50][550]\t Training Loss 0.3385\t Accuracy 0.9116\n",
      "Epoch [6][25]\t Batch [100][550]\t Training Loss 0.3510\t Accuracy 0.9060\n",
      "Epoch [6][25]\t Batch [150][550]\t Training Loss 0.3625\t Accuracy 0.9020\n",
      "Epoch [6][25]\t Batch [200][550]\t Training Loss 0.3610\t Accuracy 0.9024\n",
      "Epoch [6][25]\t Batch [250][550]\t Training Loss 0.3594\t Accuracy 0.9021\n",
      "Epoch [6][25]\t Batch [300][550]\t Training Loss 0.3591\t Accuracy 0.9014\n",
      "Epoch [6][25]\t Batch [350][550]\t Training Loss 0.3615\t Accuracy 0.9007\n",
      "Epoch [6][25]\t Batch [400][550]\t Training Loss 0.3604\t Accuracy 0.9007\n",
      "Epoch [6][25]\t Batch [450][550]\t Training Loss 0.3603\t Accuracy 0.9004\n",
      "Epoch [6][25]\t Batch [500][550]\t Training Loss 0.3608\t Accuracy 0.8999\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3602\t Average training accuracy 0.9001\n",
      "Epoch [6]\t Average validation loss 0.2781\t Average validation accuracy 0.9242\n",
      "\n",
      "Epoch [7][25]\t Batch [0][550]\t Training Loss 0.3433\t Accuracy 0.9200\n",
      "Epoch [7][25]\t Batch [50][550]\t Training Loss 0.3210\t Accuracy 0.9155\n",
      "Epoch [7][25]\t Batch [100][550]\t Training Loss 0.3336\t Accuracy 0.9111\n",
      "Epoch [7][25]\t Batch [150][550]\t Training Loss 0.3450\t Accuracy 0.9062\n",
      "Epoch [7][25]\t Batch [200][550]\t Training Loss 0.3434\t Accuracy 0.9068\n",
      "Epoch [7][25]\t Batch [250][550]\t Training Loss 0.3420\t Accuracy 0.9067\n",
      "Epoch [7][25]\t Batch [300][550]\t Training Loss 0.3420\t Accuracy 0.9058\n",
      "Epoch [7][25]\t Batch [350][550]\t Training Loss 0.3444\t Accuracy 0.9048\n",
      "Epoch [7][25]\t Batch [400][550]\t Training Loss 0.3435\t Accuracy 0.9050\n",
      "Epoch [7][25]\t Batch [450][550]\t Training Loss 0.3435\t Accuracy 0.9046\n",
      "Epoch [7][25]\t Batch [500][550]\t Training Loss 0.3442\t Accuracy 0.9042\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3438\t Average training accuracy 0.9043\n",
      "Epoch [7]\t Average validation loss 0.2660\t Average validation accuracy 0.9278\n",
      "\n",
      "Epoch [8][25]\t Batch [0][550]\t Training Loss 0.3271\t Accuracy 0.9400\n",
      "Epoch [8][25]\t Batch [50][550]\t Training Loss 0.3070\t Accuracy 0.9208\n",
      "Epoch [8][25]\t Batch [100][550]\t Training Loss 0.3194\t Accuracy 0.9153\n",
      "Epoch [8][25]\t Batch [150][550]\t Training Loss 0.3308\t Accuracy 0.9107\n",
      "Epoch [8][25]\t Batch [200][550]\t Training Loss 0.3290\t Accuracy 0.9113\n",
      "Epoch [8][25]\t Batch [250][550]\t Training Loss 0.3279\t Accuracy 0.9111\n",
      "Epoch [8][25]\t Batch [300][550]\t Training Loss 0.3281\t Accuracy 0.9102\n",
      "Epoch [8][25]\t Batch [350][550]\t Training Loss 0.3303\t Accuracy 0.9089\n",
      "Epoch [8][25]\t Batch [400][550]\t Training Loss 0.3295\t Accuracy 0.9089\n",
      "Epoch [8][25]\t Batch [450][550]\t Training Loss 0.3297\t Accuracy 0.9084\n",
      "Epoch [8][25]\t Batch [500][550]\t Training Loss 0.3305\t Accuracy 0.9080\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3303\t Average training accuracy 0.9080\n",
      "Epoch [8]\t Average validation loss 0.2559\t Average validation accuracy 0.9318\n",
      "\n",
      "Epoch [9][25]\t Batch [0][550]\t Training Loss 0.3136\t Accuracy 0.9500\n",
      "Epoch [9][25]\t Batch [50][550]\t Training Loss 0.2952\t Accuracy 0.9229\n",
      "Epoch [9][25]\t Batch [100][550]\t Training Loss 0.3076\t Accuracy 0.9182\n",
      "Epoch [9][25]\t Batch [150][550]\t Training Loss 0.3188\t Accuracy 0.9137\n",
      "Epoch [9][25]\t Batch [200][550]\t Training Loss 0.3169\t Accuracy 0.9145\n",
      "Epoch [9][25]\t Batch [250][550]\t Training Loss 0.3160\t Accuracy 0.9141\n",
      "Epoch [9][25]\t Batch [300][550]\t Training Loss 0.3163\t Accuracy 0.9133\n",
      "Epoch [9][25]\t Batch [350][550]\t Training Loss 0.3185\t Accuracy 0.9123\n",
      "Epoch [9][25]\t Batch [400][550]\t Training Loss 0.3178\t Accuracy 0.9121\n",
      "Epoch [9][25]\t Batch [450][550]\t Training Loss 0.3180\t Accuracy 0.9117\n",
      "Epoch [9][25]\t Batch [500][550]\t Training Loss 0.3189\t Accuracy 0.9112\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3188\t Average training accuracy 0.9112\n",
      "Epoch [9]\t Average validation loss 0.2474\t Average validation accuracy 0.9340\n",
      "\n",
      "Epoch [10][25]\t Batch [0][550]\t Training Loss 0.3020\t Accuracy 0.9500\n",
      "Epoch [10][25]\t Batch [50][550]\t Training Loss 0.2852\t Accuracy 0.9261\n",
      "Epoch [10][25]\t Batch [100][550]\t Training Loss 0.2975\t Accuracy 0.9206\n",
      "Epoch [10][25]\t Batch [150][550]\t Training Loss 0.3085\t Accuracy 0.9160\n",
      "Epoch [10][25]\t Batch [200][550]\t Training Loss 0.3065\t Accuracy 0.9170\n",
      "Epoch [10][25]\t Batch [250][550]\t Training Loss 0.3057\t Accuracy 0.9165\n",
      "Epoch [10][25]\t Batch [300][550]\t Training Loss 0.3062\t Accuracy 0.9158\n",
      "Epoch [10][25]\t Batch [350][550]\t Training Loss 0.3082\t Accuracy 0.9150\n",
      "Epoch [10][25]\t Batch [400][550]\t Training Loss 0.3076\t Accuracy 0.9148\n",
      "Epoch [10][25]\t Batch [450][550]\t Training Loss 0.3079\t Accuracy 0.9143\n",
      "Epoch [10][25]\t Batch [500][550]\t Training Loss 0.3088\t Accuracy 0.9140\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3088\t Average training accuracy 0.9139\n",
      "Epoch [10]\t Average validation loss 0.2400\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [11][25]\t Batch [0][550]\t Training Loss 0.2919\t Accuracy 0.9500\n",
      "Epoch [11][25]\t Batch [50][550]\t Training Loss 0.2764\t Accuracy 0.9282\n",
      "Epoch [11][25]\t Batch [100][550]\t Training Loss 0.2886\t Accuracy 0.9231\n",
      "Epoch [11][25]\t Batch [150][550]\t Training Loss 0.2994\t Accuracy 0.9181\n",
      "Epoch [11][25]\t Batch [200][550]\t Training Loss 0.2973\t Accuracy 0.9189\n",
      "Epoch [11][25]\t Batch [250][550]\t Training Loss 0.2967\t Accuracy 0.9185\n",
      "Epoch [11][25]\t Batch [300][550]\t Training Loss 0.2972\t Accuracy 0.9182\n",
      "Epoch [11][25]\t Batch [350][550]\t Training Loss 0.2991\t Accuracy 0.9172\n",
      "Epoch [11][25]\t Batch [400][550]\t Training Loss 0.2986\t Accuracy 0.9173\n",
      "Epoch [11][25]\t Batch [450][550]\t Training Loss 0.2989\t Accuracy 0.9167\n",
      "Epoch [11][25]\t Batch [500][550]\t Training Loss 0.2999\t Accuracy 0.9163\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3000\t Average training accuracy 0.9163\n",
      "Epoch [11]\t Average validation loss 0.2334\t Average validation accuracy 0.9382\n",
      "\n",
      "Epoch [12][25]\t Batch [0][550]\t Training Loss 0.2829\t Accuracy 0.9500\n",
      "Epoch [12][25]\t Batch [50][550]\t Training Loss 0.2686\t Accuracy 0.9304\n",
      "Epoch [12][25]\t Batch [100][550]\t Training Loss 0.2807\t Accuracy 0.9254\n",
      "Epoch [12][25]\t Batch [150][550]\t Training Loss 0.2913\t Accuracy 0.9206\n",
      "Epoch [12][25]\t Batch [200][550]\t Training Loss 0.2891\t Accuracy 0.9212\n",
      "Epoch [12][25]\t Batch [250][550]\t Training Loss 0.2886\t Accuracy 0.9207\n",
      "Epoch [12][25]\t Batch [300][550]\t Training Loss 0.2892\t Accuracy 0.9204\n",
      "Epoch [12][25]\t Batch [350][550]\t Training Loss 0.2911\t Accuracy 0.9195\n",
      "Epoch [12][25]\t Batch [400][550]\t Training Loss 0.2905\t Accuracy 0.9194\n",
      "Epoch [12][25]\t Batch [450][550]\t Training Loss 0.2909\t Accuracy 0.9188\n",
      "Epoch [12][25]\t Batch [500][550]\t Training Loss 0.2919\t Accuracy 0.9184\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2921\t Average training accuracy 0.9184\n",
      "Epoch [12]\t Average validation loss 0.2276\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [13][25]\t Batch [0][550]\t Training Loss 0.2748\t Accuracy 0.9500\n",
      "Epoch [13][25]\t Batch [50][550]\t Training Loss 0.2617\t Accuracy 0.9324\n",
      "Epoch [13][25]\t Batch [100][550]\t Training Loss 0.2736\t Accuracy 0.9266\n",
      "Epoch [13][25]\t Batch [150][550]\t Training Loss 0.2839\t Accuracy 0.9218\n",
      "Epoch [13][25]\t Batch [200][550]\t Training Loss 0.2817\t Accuracy 0.9226\n",
      "Epoch [13][25]\t Batch [250][550]\t Training Loss 0.2813\t Accuracy 0.9226\n",
      "Epoch [13][25]\t Batch [300][550]\t Training Loss 0.2820\t Accuracy 0.9222\n",
      "Epoch [13][25]\t Batch [350][550]\t Training Loss 0.2837\t Accuracy 0.9213\n",
      "Epoch [13][25]\t Batch [400][550]\t Training Loss 0.2832\t Accuracy 0.9212\n",
      "Epoch [13][25]\t Batch [450][550]\t Training Loss 0.2837\t Accuracy 0.9208\n",
      "Epoch [13][25]\t Batch [500][550]\t Training Loss 0.2847\t Accuracy 0.9204\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2849\t Average training accuracy 0.9205\n",
      "Epoch [13]\t Average validation loss 0.2222\t Average validation accuracy 0.9412\n",
      "\n",
      "Epoch [14][25]\t Batch [0][550]\t Training Loss 0.2677\t Accuracy 0.9500\n",
      "Epoch [14][25]\t Batch [50][550]\t Training Loss 0.2553\t Accuracy 0.9324\n",
      "Epoch [14][25]\t Batch [100][550]\t Training Loss 0.2671\t Accuracy 0.9275\n",
      "Epoch [14][25]\t Batch [150][550]\t Training Loss 0.2772\t Accuracy 0.9232\n",
      "Epoch [14][25]\t Batch [200][550]\t Training Loss 0.2749\t Accuracy 0.9241\n",
      "Epoch [14][25]\t Batch [250][550]\t Training Loss 0.2746\t Accuracy 0.9241\n",
      "Epoch [14][25]\t Batch [300][550]\t Training Loss 0.2754\t Accuracy 0.9240\n",
      "Epoch [14][25]\t Batch [350][550]\t Training Loss 0.2770\t Accuracy 0.9232\n",
      "Epoch [14][25]\t Batch [400][550]\t Training Loss 0.2766\t Accuracy 0.9230\n",
      "Epoch [14][25]\t Batch [450][550]\t Training Loss 0.2771\t Accuracy 0.9226\n",
      "Epoch [14][25]\t Batch [500][550]\t Training Loss 0.2781\t Accuracy 0.9223\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2783\t Average training accuracy 0.9224\n",
      "Epoch [14]\t Average validation loss 0.2174\t Average validation accuracy 0.9434\n",
      "\n",
      "Epoch [15][25]\t Batch [0][550]\t Training Loss 0.2613\t Accuracy 0.9500\n",
      "Epoch [15][25]\t Batch [50][550]\t Training Loss 0.2496\t Accuracy 0.9339\n",
      "Epoch [15][25]\t Batch [100][550]\t Training Loss 0.2612\t Accuracy 0.9290\n",
      "Epoch [15][25]\t Batch [150][550]\t Training Loss 0.2711\t Accuracy 0.9248\n",
      "Epoch [15][25]\t Batch [200][550]\t Training Loss 0.2687\t Accuracy 0.9257\n",
      "Epoch [15][25]\t Batch [250][550]\t Training Loss 0.2685\t Accuracy 0.9257\n",
      "Epoch [15][25]\t Batch [300][550]\t Training Loss 0.2693\t Accuracy 0.9256\n",
      "Epoch [15][25]\t Batch [350][550]\t Training Loss 0.2709\t Accuracy 0.9249\n",
      "Epoch [15][25]\t Batch [400][550]\t Training Loss 0.2705\t Accuracy 0.9246\n",
      "Epoch [15][25]\t Batch [450][550]\t Training Loss 0.2710\t Accuracy 0.9242\n",
      "Epoch [15][25]\t Batch [500][550]\t Training Loss 0.2721\t Accuracy 0.9237\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2723\t Average training accuracy 0.9239\n",
      "Epoch [15]\t Average validation loss 0.2129\t Average validation accuracy 0.9448\n",
      "\n",
      "Epoch [16][25]\t Batch [0][550]\t Training Loss 0.2554\t Accuracy 0.9500\n",
      "Epoch [16][25]\t Batch [50][550]\t Training Loss 0.2442\t Accuracy 0.9349\n",
      "Epoch [16][25]\t Batch [100][550]\t Training Loss 0.2558\t Accuracy 0.9305\n",
      "Epoch [16][25]\t Batch [150][550]\t Training Loss 0.2654\t Accuracy 0.9264\n",
      "Epoch [16][25]\t Batch [200][550]\t Training Loss 0.2630\t Accuracy 0.9275\n",
      "Epoch [16][25]\t Batch [250][550]\t Training Loss 0.2629\t Accuracy 0.9275\n",
      "Epoch [16][25]\t Batch [300][550]\t Training Loss 0.2637\t Accuracy 0.9273\n",
      "Epoch [16][25]\t Batch [350][550]\t Training Loss 0.2652\t Accuracy 0.9266\n",
      "Epoch [16][25]\t Batch [400][550]\t Training Loss 0.2648\t Accuracy 0.9262\n",
      "Epoch [16][25]\t Batch [450][550]\t Training Loss 0.2653\t Accuracy 0.9259\n",
      "Epoch [16][25]\t Batch [500][550]\t Training Loss 0.2664\t Accuracy 0.9255\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2667\t Average training accuracy 0.9256\n",
      "Epoch [16]\t Average validation loss 0.2088\t Average validation accuracy 0.9456\n",
      "\n",
      "Epoch [17][25]\t Batch [0][550]\t Training Loss 0.2502\t Accuracy 0.9500\n",
      "Epoch [17][25]\t Batch [50][550]\t Training Loss 0.2393\t Accuracy 0.9361\n",
      "Epoch [17][25]\t Batch [100][550]\t Training Loss 0.2507\t Accuracy 0.9311\n",
      "Epoch [17][25]\t Batch [150][550]\t Training Loss 0.2601\t Accuracy 0.9272\n",
      "Epoch [17][25]\t Batch [200][550]\t Training Loss 0.2576\t Accuracy 0.9285\n",
      "Epoch [17][25]\t Batch [250][550]\t Training Loss 0.2576\t Accuracy 0.9286\n",
      "Epoch [17][25]\t Batch [300][550]\t Training Loss 0.2584\t Accuracy 0.9285\n",
      "Epoch [17][25]\t Batch [350][550]\t Training Loss 0.2599\t Accuracy 0.9277\n",
      "Epoch [17][25]\t Batch [400][550]\t Training Loss 0.2595\t Accuracy 0.9273\n",
      "Epoch [17][25]\t Batch [450][550]\t Training Loss 0.2601\t Accuracy 0.9270\n",
      "Epoch [17][25]\t Batch [500][550]\t Training Loss 0.2612\t Accuracy 0.9265\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2615\t Average training accuracy 0.9266\n",
      "Epoch [17]\t Average validation loss 0.2049\t Average validation accuracy 0.9472\n",
      "\n",
      "Epoch [18][25]\t Batch [0][550]\t Training Loss 0.2454\t Accuracy 0.9500\n",
      "Epoch [18][25]\t Batch [50][550]\t Training Loss 0.2347\t Accuracy 0.9375\n",
      "Epoch [18][25]\t Batch [100][550]\t Training Loss 0.2460\t Accuracy 0.9322\n",
      "Epoch [18][25]\t Batch [150][550]\t Training Loss 0.2552\t Accuracy 0.9283\n",
      "Epoch [18][25]\t Batch [200][550]\t Training Loss 0.2526\t Accuracy 0.9295\n",
      "Epoch [18][25]\t Batch [250][550]\t Training Loss 0.2527\t Accuracy 0.9298\n",
      "Epoch [18][25]\t Batch [300][550]\t Training Loss 0.2535\t Accuracy 0.9297\n",
      "Epoch [18][25]\t Batch [350][550]\t Training Loss 0.2549\t Accuracy 0.9290\n",
      "Epoch [18][25]\t Batch [400][550]\t Training Loss 0.2545\t Accuracy 0.9288\n",
      "Epoch [18][25]\t Batch [450][550]\t Training Loss 0.2551\t Accuracy 0.9283\n",
      "Epoch [18][25]\t Batch [500][550]\t Training Loss 0.2563\t Accuracy 0.9279\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2566\t Average training accuracy 0.9280\n",
      "Epoch [18]\t Average validation loss 0.2013\t Average validation accuracy 0.9474\n",
      "\n",
      "Epoch [19][25]\t Batch [0][550]\t Training Loss 0.2409\t Accuracy 0.9500\n",
      "Epoch [19][25]\t Batch [50][550]\t Training Loss 0.2303\t Accuracy 0.9388\n",
      "Epoch [19][25]\t Batch [100][550]\t Training Loss 0.2415\t Accuracy 0.9339\n",
      "Epoch [19][25]\t Batch [150][550]\t Training Loss 0.2505\t Accuracy 0.9305\n",
      "Epoch [19][25]\t Batch [200][550]\t Training Loss 0.2479\t Accuracy 0.9314\n",
      "Epoch [19][25]\t Batch [250][550]\t Training Loss 0.2480\t Accuracy 0.9316\n",
      "Epoch [19][25]\t Batch [300][550]\t Training Loss 0.2489\t Accuracy 0.9314\n",
      "Epoch [19][25]\t Batch [350][550]\t Training Loss 0.2502\t Accuracy 0.9307\n",
      "Epoch [19][25]\t Batch [400][550]\t Training Loss 0.2499\t Accuracy 0.9306\n",
      "Epoch [19][25]\t Batch [450][550]\t Training Loss 0.2505\t Accuracy 0.9302\n",
      "Epoch [19][25]\t Batch [500][550]\t Training Loss 0.2516\t Accuracy 0.9298\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2519\t Average training accuracy 0.9298\n",
      "Epoch [19]\t Average validation loss 0.1979\t Average validation accuracy 0.9476\n",
      "\n",
      "Epoch [20][25]\t Batch [0][550]\t Training Loss 0.2366\t Accuracy 0.9500\n",
      "Epoch [20][25]\t Batch [50][550]\t Training Loss 0.2262\t Accuracy 0.9392\n",
      "Epoch [20][25]\t Batch [100][550]\t Training Loss 0.2373\t Accuracy 0.9342\n",
      "Epoch [20][25]\t Batch [150][550]\t Training Loss 0.2461\t Accuracy 0.9310\n",
      "Epoch [20][25]\t Batch [200][550]\t Training Loss 0.2434\t Accuracy 0.9320\n",
      "Epoch [20][25]\t Batch [250][550]\t Training Loss 0.2436\t Accuracy 0.9323\n",
      "Epoch [20][25]\t Batch [300][550]\t Training Loss 0.2445\t Accuracy 0.9321\n",
      "Epoch [20][25]\t Batch [350][550]\t Training Loss 0.2458\t Accuracy 0.9314\n",
      "Epoch [20][25]\t Batch [400][550]\t Training Loss 0.2454\t Accuracy 0.9313\n",
      "Epoch [20][25]\t Batch [450][550]\t Training Loss 0.2461\t Accuracy 0.9309\n",
      "Epoch [20][25]\t Batch [500][550]\t Training Loss 0.2472\t Accuracy 0.9306\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2475\t Average training accuracy 0.9306\n",
      "Epoch [20]\t Average validation loss 0.1948\t Average validation accuracy 0.9484\n",
      "\n",
      "Epoch [21][25]\t Batch [0][550]\t Training Loss 0.2324\t Accuracy 0.9500\n",
      "Epoch [21][25]\t Batch [50][550]\t Training Loss 0.2223\t Accuracy 0.9408\n",
      "Epoch [21][25]\t Batch [100][550]\t Training Loss 0.2334\t Accuracy 0.9354\n",
      "Epoch [21][25]\t Batch [150][550]\t Training Loss 0.2419\t Accuracy 0.9323\n",
      "Epoch [21][25]\t Batch [200][550]\t Training Loss 0.2392\t Accuracy 0.9332\n",
      "Epoch [21][25]\t Batch [250][550]\t Training Loss 0.2394\t Accuracy 0.9334\n",
      "Epoch [21][25]\t Batch [300][550]\t Training Loss 0.2404\t Accuracy 0.9333\n",
      "Epoch [21][25]\t Batch [350][550]\t Training Loss 0.2416\t Accuracy 0.9326\n",
      "Epoch [21][25]\t Batch [400][550]\t Training Loss 0.2413\t Accuracy 0.9325\n",
      "Epoch [21][25]\t Batch [450][550]\t Training Loss 0.2419\t Accuracy 0.9321\n",
      "Epoch [21][25]\t Batch [500][550]\t Training Loss 0.2431\t Accuracy 0.9317\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2434\t Average training accuracy 0.9318\n",
      "Epoch [21]\t Average validation loss 0.1918\t Average validation accuracy 0.9492\n",
      "\n",
      "Epoch [22][25]\t Batch [0][550]\t Training Loss 0.2285\t Accuracy 0.9500\n",
      "Epoch [22][25]\t Batch [50][550]\t Training Loss 0.2186\t Accuracy 0.9418\n",
      "Epoch [22][25]\t Batch [100][550]\t Training Loss 0.2296\t Accuracy 0.9365\n",
      "Epoch [22][25]\t Batch [150][550]\t Training Loss 0.2379\t Accuracy 0.9334\n",
      "Epoch [22][25]\t Batch [200][550]\t Training Loss 0.2352\t Accuracy 0.9343\n",
      "Epoch [22][25]\t Batch [250][550]\t Training Loss 0.2355\t Accuracy 0.9346\n",
      "Epoch [22][25]\t Batch [300][550]\t Training Loss 0.2364\t Accuracy 0.9344\n",
      "Epoch [22][25]\t Batch [350][550]\t Training Loss 0.2376\t Accuracy 0.9338\n",
      "Epoch [22][25]\t Batch [400][550]\t Training Loss 0.2373\t Accuracy 0.9337\n",
      "Epoch [22][25]\t Batch [450][550]\t Training Loss 0.2379\t Accuracy 0.9332\n",
      "Epoch [22][25]\t Batch [500][550]\t Training Loss 0.2391\t Accuracy 0.9328\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2394\t Average training accuracy 0.9329\n",
      "Epoch [22]\t Average validation loss 0.1889\t Average validation accuracy 0.9492\n",
      "\n",
      "Epoch [23][25]\t Batch [0][550]\t Training Loss 0.2248\t Accuracy 0.9500\n",
      "Epoch [23][25]\t Batch [50][550]\t Training Loss 0.2150\t Accuracy 0.9427\n",
      "Epoch [23][25]\t Batch [100][550]\t Training Loss 0.2260\t Accuracy 0.9376\n",
      "Epoch [23][25]\t Batch [150][550]\t Training Loss 0.2341\t Accuracy 0.9347\n",
      "Epoch [23][25]\t Batch [200][550]\t Training Loss 0.2314\t Accuracy 0.9355\n",
      "Epoch [23][25]\t Batch [250][550]\t Training Loss 0.2317\t Accuracy 0.9357\n",
      "Epoch [23][25]\t Batch [300][550]\t Training Loss 0.2326\t Accuracy 0.9355\n",
      "Epoch [23][25]\t Batch [350][550]\t Training Loss 0.2337\t Accuracy 0.9349\n",
      "Epoch [23][25]\t Batch [400][550]\t Training Loss 0.2335\t Accuracy 0.9349\n",
      "Epoch [23][25]\t Batch [450][550]\t Training Loss 0.2341\t Accuracy 0.9344\n",
      "Epoch [23][25]\t Batch [500][550]\t Training Loss 0.2353\t Accuracy 0.9341\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2357\t Average training accuracy 0.9341\n",
      "Epoch [23]\t Average validation loss 0.1862\t Average validation accuracy 0.9502\n",
      "\n",
      "Epoch [24][25]\t Batch [0][550]\t Training Loss 0.2212\t Accuracy 0.9500\n",
      "Epoch [24][25]\t Batch [50][550]\t Training Loss 0.2116\t Accuracy 0.9439\n",
      "Epoch [24][25]\t Batch [100][550]\t Training Loss 0.2225\t Accuracy 0.9385\n",
      "Epoch [24][25]\t Batch [150][550]\t Training Loss 0.2304\t Accuracy 0.9357\n",
      "Epoch [24][25]\t Batch [200][550]\t Training Loss 0.2277\t Accuracy 0.9364\n",
      "Epoch [24][25]\t Batch [250][550]\t Training Loss 0.2280\t Accuracy 0.9366\n",
      "Epoch [24][25]\t Batch [300][550]\t Training Loss 0.2290\t Accuracy 0.9364\n",
      "Epoch [24][25]\t Batch [350][550]\t Training Loss 0.2301\t Accuracy 0.9359\n",
      "Epoch [24][25]\t Batch [400][550]\t Training Loss 0.2298\t Accuracy 0.9358\n",
      "Epoch [24][25]\t Batch [450][550]\t Training Loss 0.2305\t Accuracy 0.9354\n",
      "Epoch [24][25]\t Batch [500][550]\t Training Loss 0.2317\t Accuracy 0.9349\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2321\t Average training accuracy 0.9350\n",
      "Epoch [24]\t Average validation loss 0.1836\t Average validation accuracy 0.9506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model, final_model_loss, final_model_acc = train(final_model, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9349.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(final_model, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
